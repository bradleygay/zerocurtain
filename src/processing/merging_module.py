"""
Merging module for Arctic zero-curtain pipeline.
Auto-generated from Jupyter notebook.
"""

from src.common.imports import *
from src.common.utilities import *

# CALM and GTNP (ALT, ST) Preprocessing
import pandas as pd
import numpy as np
import re
import json
import os
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from tqdm import tqdm
import logging
import time
import matplotlib.pyplot as plt
import seaborn as sns
def setup_logging():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')
    return logging.getLogger(__name__)
def normalize_datetime(dt_series):
    return pd.to_datetime(dt_series, format='mixed', errors='coerce')
def standardize_name(name):
    if not name:
        return None
    return re.sub(r'_+', '_', re.sub(r'[^a-zA-Z0-9]', '_', str(name).lower())).strip('_')
def add_depth_zones(df):
    conditions = [(df['depth'] <= 0.25), (df['depth'] > 0.25) & (df['depth'] <= 0.5),
                  (df['depth'] > 0.5) & (df['depth'] <= 1.0), (df['depth'] > 1.0)]
    df['depth_zone'] = np.select(conditions, ['shallow', 'intermediate', 'deep', 'very_deep'], default='unknown')
    return df
class GTNPScraper:
    def __init__(self, headless=True):
        self.logger = setup_logging()
        chrome_options = Options()
        if headless:
            chrome_options.add_argument('--headless')
        for arg in ['--no-sandbox', '--disable-dev-shm-usage', '--window-size=1920,1080', 
                    '--disable-extensions', '--disable-gpu', '--remote-debugging-port=9222']:
            chrome_options.add_argument(arg)
        try:
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.wait = WebDriverWait(self.driver, timeout=30, poll_frequency=1)
            self.logger.info("ChromeDriver initialized")
        except Exception as e:
            self.logger.error(f"ChromeDriver init failed: {e}")
            raise
    def scrape_all_boreholes(self, output_file='gtnp_coordinates.csv', start_id=0, end_id=2000):
        data = []
        try:
            for borehole_id in tqdm(range(start_id, end_id + 1), desc="Scraping"):
                url = f"{self.base_url}/boreholes/view/{borehole_id}/"
                try:
                    result = self.process_borehole_page(url, borehole_id)
                    if result:
                        data.append(result)
                        if len(data) % 10 == 0:
                            pd.DataFrame(data).to_csv(f"{output_file}.temp", index=False)
                    time.sleep(2)
                except Exception as e:
                    self.logger.warning(f"Error processing {borehole_id}: {e}")
        except KeyboardInterrupt:
            self.logger.warning("Interrupted by user")
        finally:
            df = pd.DataFrame(data) if data else pd.DataFrame(columns=['name', 'latitude', 'longitude', 'url'])
            df.to_csv(output_file, index=False)
            if os.path.exists(f"{output_file}.temp"):
                os.remove(f"{output_file}.temp")
            self.logger.info(f"Saved {len(df)} records to {output_file}")
            return df
class MetadataProcessor:
    @staticmethod
    def extract_metadata(metadata_file):
        try:
            with open(metadata_file, 'r') as f:
                content = f.read()
            metadata = {}
            for pattern, key in [('Latitude:\s*([-+]?\d*\.?\d+)', 'latitude'),
                                 ('Longitude:\s*([-+]?\d*\.?\d+)', 'longitude')]:
                match = re.search(pattern, content)
                if match:
                    metadata[key] = float(match.group(1))
            for field in ['Station', 'Elevation', 'Start_date', 'End_date']:
                match = re.search(f'{field}:\s*([^\n]+)', content)
                if match:
                    metadata[field.lower()] = match.group(1).strip()
            return metadata
        except Exception as e:
            print(f"Error extracting metadata: {e}")
            return None
class ExternalALTProcessor:
    def __init__(self, base_path):
        self.base_path = Path(base_path)
        self.sources = {'calm': 'calm/calm_alt.csv', 'rocha': 'rocha/rocha_alt.csv',
            'fbx': 'fbx/fbx_alt.csv', 'gtnp': 'gtnp/gtnp_alt.csv',
            'neon': 'neon/neon_alt.csv', 'watts': 'watts/watts_alt.csv',
            'smalt': 'smalt/smalt_alt.csv'}
    def read_source(self, source_name, file_path):
        try:
            df = pd.read_csv(file_path).drop(columns=['Unnamed: 0'], errors='ignore')
            return df[['datetime', 'lat', 'lon', 'alt']]
        except Exception as e:
            print(f"Error processing {source_name}: {e}")
            return None
    def process_all_sources(self):
        all_data = []
        for source_name, relative_path in self.sources.items():
            file_path = self.base_path / relative_path
            if file_path.exists():
                df = self.read_source(source_name, file_path)
                if df is not None:
                    df['source'] = source_name
                    all_data.append(df)
        if all_data:
            combined = pd.concat(all_data, axis=0).replace(-9999, np.nan)
            combined['datetime'] = normalize_datetime(combined['datetime'])
            combined = combined[combined['datetime'] < pd.Timestamp('2023-01-01')]
            combined = combined.sort_values('datetime').reset_index(drop=True)
            combined = combined.rename(columns={'lat': 'latitude', 'lon': 'longitude', 'alt': 'thickness'})
            combined['year'] = combined['datetime'].dt.year
            return combined
        return None
class ActiveLayerProcessor:
    def __init__(self):
        self.required_columns = ['year', 'grid_point', 'thickness']
        self.metadata_processor = MetadataProcessor()
    def read_alt_file(self, file_path):
        try:
            with open(file_path, 'r') as f:
                header = f.readline().strip()
                lines = f.readlines()
            headers = [h.strip() for h in header.split(',')]
            data = []
            for line in lines:
                if not line.strip():
                    continue
                values = line.strip().split(',')
                if len(values) < 2:
                    continue
                try:
                    year = pd.to_numeric(values[0])
                    for i, val in enumerate(values[1:], 1):
                        if val.strip() and val.strip() != '-999':
                            try:
                                thickness = float(val)
                                if thickness > 0:
                                    data.append({'year': year, 'grid_point': i, 'thickness': thickness,
                                        'point_label': headers[i] if i < len(headers) else f'Point_{i}'})
                            except ValueError:
                                continue
                except ValueError:
                    continue
            if data:
                df = pd.DataFrame(data)
                measurement_info = self._parse_filename(file_path.name)
                for key, value in measurement_info.items():
                    df[key] = value
                return df
            return None
        except Exception as e:
            print(f"Error processing {file_path.name}: {e}")
            return None
    def _parse_filename(self, filename):
        parts = filename.replace('.timeserie.csv', '').split('-')
        return {'site_name': parts[1], 'dataset_id': parts[2].replace('Dataset_', ''),
            'frequency': parts[3] if len(parts) > 3 else None,
            'temporal_res': parts[4] if len(parts) > 4 else None,
            'measurement_method': parts[-1] if parts[-1] in ['Mechanical_Probing', 'Ground_Subsidence', 'Borehole'] else None}
class CALMDataProcessor:
    def read_calm_csv(self, file_path):
        try:
            df = pd.read_csv(file_path)
            year_columns = [col for col in df.columns if str(col).isdigit() and 1990 <= int(col) <= 2024]
            cols_to_keep = ['Continent', 'Region', 'Country', 'Site Code', 'Site Name', 
                           'Latitude', 'Longitude', 'Method'] + year_columns
            df = df[cols_to_keep]
            df.columns = ['continent', 'region', 'country', 'site_code', 'site_name', 
                         'latitude', 'longitude', 'method'] + [str(col) for col in year_columns]
            df_long = df.melt(id_vars=['continent', 'region', 'country', 'site_code', 'site_name', 
                        'latitude', 'longitude', 'method'], var_name='year', value_name='thickness')
            df_long['year'] = pd.to_numeric(df_long['year'], errors='coerce')
            df_long['thickness'] = pd.to_numeric(df_long['thickness'], errors='coerce')
            df_long = df_long.dropna(subset=['thickness'])
            df_long = df_long[df_long['thickness'] != '-'].reset_index(drop=True)
            return df_long
        except Exception as e:
            print(f"Error processing CALM CSV: {e}")
            return None
class UnifiedDataProcessor:
    def __init__(self, gtnp_path, siberia_path=None, coordinates_path=None):
        self.gtnp_path = Path(gtnp_path)
        self.siberia_path = Path(siberia_path) if siberia_path else None
        self.coordinates_path = Path(coordinates_path) if coordinates_path else None
        self.metadata_processor = MetadataProcessor()
        self.data = {}
        self.metadata = {}
        self.coordinates_df = self._load_coordinates()
    def _load_coordinates(self):
        if self.coordinates_path and self.coordinates_path.exists():
            try:
                coords_df = pd.read_csv(self.coordinates_path)
                required_cols = ['name', 'latitude', 'longitude']
                if all(col in coords_df.columns for col in required_cols):
                    coords_df = coords_df[required_cols].copy()
                    coords_df['name'] = coords_df['name'].astype(str).str.strip()
                    coords_df = coords_df.dropna()
                    coords_df = coords_df[coords_df['name'].str.len() > 0]
                    coords_df['latitude'] = pd.to_numeric(coords_df['latitude'], errors='coerce')
                    coords_df['longitude'] = pd.to_numeric(coords_df['longitude'], errors='coerce')
                    coords_df = coords_df.dropna()
                    print(f"Loaded {len(coords_df)} coordinate entries")
                    return coords_df
            except Exception as e:
                print(f"Error loading coordinates: {e}")
        return None
    def process_borehole_data(self):
        temp_files = list(self.gtnp_path.glob('*Temperature*.timeserie.csv'))
        for file_path in temp_files:
            try:
                metadata_file = file_path.with_suffix('.metadata.txt')
                metadata = self.metadata_processor.extract_metadata(metadata_file) if metadata_file.exists() else {}
                df = self._read_temperature_file(file_path)
                if df is not None:
                    if 'latitude' not in metadata or 'longitude' not in metadata:
                        lat, lon = self._match_site_coordinates(file_path.name)
                        if lat and lon:
                            metadata.update({'latitude': lat, 'longitude': lon})
                    if 'latitude' in metadata and 'longitude' in metadata:
                        df['latitude'] = metadata['latitude']
                        df['longitude'] = metadata['longitude']
                    self.data[file_path.stem] = df
                    self.metadata[file_path.stem] = metadata
            except Exception as e:
                print(f"Error processing {file_path}: {e}")
    def _match_site_coordinates(self, filename):
        if self.coordinates_df is None:
            return None, None
        try:
            site_name = filename.split('-Dataset_')[0].split('-', 2)[1] if '-Dataset_' in filename else None
            if not site_name:
                return None, None
            std_filename_site = standardize_name(site_name)
            self.coordinates_df['std_name'] = self.coordinates_df['name'].apply(standardize_name)
            exact_match = self.coordinates_df[self.coordinates_df['std_name'] == std_filename_site]
            if not exact_match.empty:
                coords = exact_match.iloc[0]
                return coords['latitude'], coords['longitude']
            for _, row in self.coordinates_df.iterrows():
                std_coord_name = row['std_name']
                if std_coord_name and len(std_coord_name) > 3:
                    if std_coord_name in std_filename_site or std_filename_site in std_coord_name:
                        return row['latitude'], row['longitude']
        except Exception as e:
            print(f"Error matching coordinates: {e}")
        return None, None
    def _read_temperature_file(self, file_path):
        try:
            with open(file_path, 'r') as f:
                header = f.readline()
            delimiter = ',' if ',' in header else '\t'
            df = pd.read_csv(file_path, delimiter=delimiter)
            date_col = next((col for col in ['Date', 'datetime', 'Date/Depth'] if col in df.columns), None)
            if date_col:
                df['datetime'] = pd.to_datetime(df[date_col], errors='coerce')
            if 'Surface temperature average weight' in df.columns:
                df['temperature'] = df['Surface temperature average weight']
                if df['temperature'].mean() > 200:
                    df['temperature'] -= 273.15
            elif 'Date/Depth' in df.columns:
                depth_cols = [col for col in df.columns if col not in ['Date/Depth', 'datetime']]
                df = pd.melt(df, id_vars=['datetime'], value_vars=depth_cols, 
                           var_name='depth', value_name='temperature')
                df['depth'] = pd.to_numeric(df['depth'], errors='coerce')
            return df
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
            return None
    def combine_datasets(self, alt_data=None):
        combined = []
        for key, df in self.data.items():
            df['source'] = key
            combined.append(df)
        if alt_data is not None:
            alt_data['source'] = 'external_alt'
            combined.append(alt_data)
        if combined:
            master_df = pd.concat(combined, ignore_index=True)
            for col in ['temperature', 'thickness', 'depth', 'datetime', 'year']:
                if col not in master_df.columns:
                    master_df[col] = None
            return master_df
        return None
    def save_processed_data(self, output_dir='processed_data', alt_data=None):
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        combined_df = self.combine_datasets(alt_data)
        if combined_df is not None:
            combined_df.to_csv(output_dir / 'master_dataset.csv', index=False)
            temp_data = combined_df[combined_df['temperature'].notna()]
            if not temp_data.empty:
                temp_data.to_csv(output_dir / 'temperature_data.csv', index=False)
            thick_data = combined_df[combined_df['thickness'].notna()]
            if not thick_data.empty:
                thick_data.to_csv(output_dir / 'active_layer_data.csv', index=False)
            with open(output_dir / 'metadata.json', 'w') as f:
                json.dump(self.metadata, f, indent=2)
            print(f"\nTotal records: {len(combined_df)}")
            print(f"Temperature records: {len(temp_data)}")
            print(f"Active layer records: {len(thick_data)}")
            return combined_df
        return None
def process_active_layer_data(gtnp_path):
    processor = ActiveLayerProcessor()
    alt_files = sorted(Path(gtnp_path).glob('*Active_Layer*.timeserie.csv'))
    all_data = []
    for file_path in alt_files:
        try:
            df = processor.read_alt_file(file_path)
            if df is not None and not df.empty:
                all_data.append(df)
        except Exception as e:
            print(f"Error: {file_path.name}: {e}")
    if all_data:
        combined_df = pd.concat(all_data, ignore_index=True)
        combined_df.to_csv('processed_data/gtnp_active_layer_data.csv', index=False)
        return combined_df
    return None
def process_combined_alt_data(gtnp_path, alt_base_path, calm_path):
    datasets = []
    gtnp_alt = process_active_layer_data(gtnp_path)
    if gtnp_alt is not None:
        gtnp_alt['source'] = 'gtnp'
        datasets.append(gtnp_alt)
    external_processor = ExternalALTProcessor(alt_base_path)
    external_alt = external_processor.process_all_sources()
    if external_alt is not None:
        datasets.append(external_alt)
    calm_processor = CALMDataProcessor()
    calm_data = calm_processor.read_calm_csv(calm_path)
    if calm_data is not None:
        calm_data['source'] = 'calm'
        datasets.append(calm_data)
    if datasets:
        return pd.concat(datasets, ignore_index=True)
    return None
def update_coordinates_from_metadata(df, base_path):
    metadata_processor = MetadataProcessor()
    metadata_files = list(Path(base_path).glob('Activelayer_*.metadata.txt'))
    metadata_map = {}
    for f in metadata_files:
        try:
            site_name = f.stem.split('-', 1)[1].split('.')[0]
            metadata = metadata_processor.extract_metadata(str(f))
            if metadata and 'latitude' in metadata and 'longitude' in metadata:
                metadata_map[site_name] = metadata
        except Exception as e:
            print(f"Error: {f}: {e}")
    updates = 0
    for site_name, metadata in metadata_map.items():
        mask = df['site_name'] == site_name
        if mask.any():
            df.loc[mask, ['latitude', 'longitude']] = [metadata['latitude'], metadata['longitude']]
            updates += mask.sum()
    print(f"Updated {updates} records")
    return df
def update_datetime_values(df):
    df['datetime'] = normalize_datetime(df['datetime'])
    mask = df['datetime'].isna() & df['point_label'].notna() & df['point_label'].astype(str).str.match(r'\d{4}-\d{2}-\d{2}')
    df.loc[mask, 'datetime'] = pd.to_datetime(df.loc[mask, 'point_label'], errors='coerce')
    mask = df['datetime'].isna() & df['year'].notna() & (df['year'] >= 1900) & (df['year'] <= 2024)
    df.loc[mask, 'datetime'] = pd.to_datetime(df.loc[mask, 'year'].astype(int).astype(str) + '-07-15')
    season_map = {'winter': '01', 'spring': '04', 'summer': '07', 'fall': '10'}
    mask = df['datetime'].isna() & df['year'].notna() & df['season'].notna()
    for season, month in season_map.items():
        season_mask = mask & (df['season'].str.lower() == season)
        df.loc[season_mask, 'datetime'] = pd.to_datetime(df.loc[season_mask, 'year'].astype(int).astype(str) + f'-{month}-15')
    return df
def main():
    gtnp_path = '/Users/bgay/Library/CloudStorage/OneDrive-[REDACTED_AFFILIATION]/zerocurtain/preprocessing_raw_data/gtnp'
    siberia_path = '/Users/bgay/Library/CloudStorage/OneDrive-[REDACTED_AFFILIATION]/zerocurtain/preprocessing_raw_data/siberia'
    alt_base_path = '/Users/bgay/Library/CloudStorage/OneDrive-[REDACTED_AFFILIATION]/zerocurtain/preprocessing_raw_data/alt_sources'
    calm_path = '/Users/bgay/Library/CloudStorage/OneDrive-[REDACTED_AFFILIATION]/zerocurtain/preprocessing_raw_data/calm.csv'
    coordinates_path = '/Users/bgay/Library/CloudStorage/OneDrive-[REDACTED_AFFILIATION]/zerocurtain/preprocessing_raw_data/gtnp_coordinates.csv'
    print("Processing Combined ALT Data...")
    alt_data = process_combined_alt_data(gtnp_path, alt_base_path, calm_path)
    print("\nProcessing Borehole Data...")
    processor = UnifiedDataProcessor(gtnp_path, siberia_path, coordinates_path)
    processor.process_borehole_data()
    print("\nCombining and Saving...")
    final_data = processor.save_processed_data(alt_data=alt_data)
    if final_data is not None:
        final_data = update_coordinates_from_metadata(final_data, gtnp_path)
        final_data = update_datetime_values(final_data)
        final_data = final_data[final_data['latitude'] >= 49]
        final_data = final_data[final_data['datetime'] < '2025-01-01']
        final_data = final_data.sort_values('datetime').reset_index(drop=True)
        final_data.to_csv('final_cleaned_main_df_insitu.csv', index=False)
        print(f"\nFinal temporal coverage: {final_data['datetime'].min()} - {final_data['datetime'].max()}")
    return processor, final_data
if __name__ == '__main__':
    processor, data = main()

# External ALT
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

base_path = '/Users/bgay/Library/CloudStorage/OneDrive-[REDACTED_AFFILIATION]/zerocurtain/preprocessing_raw_data/alt_sources/'
alt_sources = {
    'calm': 'CALM',
    'rocha': 'ABoVE_Rocha',
    'fbx': 'ABoVE_Douglas',
    'gtnp': 'GTNP',
    'neon': 'NEON',
    'watts': 'ABoVE_Watts',
    'smalt': 'ABoVE_Schaefer'
}

alt_dfs = []
for folder, source_name in alt_sources.items():
    df = pd.read_csv(f"{base_path}{folder}/{folder}_alt.csv").drop(columns=['Unnamed: 0', 'alt_diff'])
    df['source'] = source_name
    alt_dfs.append(df)

tundra_cols = ['date', 'yr_data', 'latitude', 'longitude', 'plot_id', 'dataset_id', 'dataset_name',
               'soil_temp_10cm', 'water_table', 'soil_moist', 'ALT_mean']
tundrafielddb = pd.read_csv('/Users/bgay/Library/CloudStorage/OneDrive-[REDACTED_AFFILIATION]/zerocurtain/preprocessing_raw_data/Tundra_field_database.csv')[tundra_cols]

tundrafielddb['date'] = tundrafielddb['date'].astype(str)
invalid_months = tundrafielddb[tundrafielddb['date'].str[4:6] == '00']
if not invalid_months.empty:
    print("Invalid month found in dataset:", invalid_months['date'].unique())

tundrafielddb['date'] = tundrafielddb['date'].apply(lambda x: x[:4] + '01' + x[6:] if x[4:6] == '00' else x)
tundrafielddb['date'] = tundrafielddb['date'].apply(lambda x: x[:6] + '01' if x[6:] == '00' else x)
tundrafielddb['date'] = pd.to_datetime(tundrafielddb['date'], format='%Y%m%d').dt.strftime('%Y-%m-%d %H:%M:%S')

date_corrections = {
    '1997-01-01 00:00:00': '07',
    '2017-01-01 00:00:00': '07',
    '1983-01-01 00:00:00': '08',
    '2006-01-01 00:00:00': '08'
}

tundrafielddb['date'] = pd.to_datetime(tundrafielddb['date'], format='%Y-%m-%d %H:%M:%S')
tundrafielddb['date'] = tundrafielddb['date'].astype(str).replace(date_corrections, regex=True)
tundrafielddb['date'] = pd.to_datetime(tundrafielddb['date']).dt.strftime('%Y-%m-%d %H:%M:%S')
tundrafielddb = tundrafielddb.sort_values('date').reset_index(drop=True)

tundrafielddb.to_csv('tundrafieldobservations_st_smc_alt_insitu.csv', index=False)

tundrafielddb.columns = ['datetime', 'yr_data', 'lat', 'lon', 'plot_id', 'source', 'dataset_name', 
                         'st', 'water_table', 'smc', 'alt']

tundrafielddb_alt = tundrafielddb[['datetime', 'lat', 'lon', 'alt', 'source']].replace(-999, np.nan).dropna().sort_values('datetime').reset_index(drop=True)
tundrafielddb_alt['alt'] = tundrafielddb_alt['alt'] / 100
tundrafielddb_alt.to_csv('tundrafieldobservations_alt_insitu.csv', index=False)

tundrafielddb_st = tundrafielddb[['datetime', 'lat', 'lon', 'st', 'source']].replace(-999, np.nan).dropna().sort_values('datetime').reset_index(drop=True)
tundrafielddb_st.to_csv('tundrafieldobservations_st_insitu.csv', index=False)

tundrafielddb_smc = tundrafielddb[['datetime', 'lat', 'lon', 'smc', 'source']].replace(-999, np.nan).dropna()
tundrafielddb_smc = tundrafielddb_smc[tundrafielddb_smc['smc'] < 100].sort_values('datetime').reset_index(drop=True)
tundrafielddb_smc.to_csv('tundrafieldobservations_smc_insitu.csv', index=False)

combined_df = pd.concat(alt_dfs + [tundrafielddb_alt], axis=0).sort_values('datetime').reset_index(drop=True)
combined_df['alt'] = combined_df['alt'] * -1
combined_df['datetime'] = pd.to_datetime(combined_df['datetime'], format='mixed', errors='coerce')
combined_df = combined_df.rename(columns={'lat': 'latitude', 'lon': 'longitude', 'alt': 'thickness'})
combined_df['year'] = combined_df['datetime'].dt.year
combined_df = combined_df[['datetime', 'year', 'latitude', 'longitude', 'thickness', 'source']].sort_values('datetime').reset_index(drop=True)
combined_df['thickness'] = combined_df['thickness'].abs()
combined_df.to_csv('calm_above_neon_tundra_alt_combined_insitu_df.csv', index=False)
combined_df['thickness'].plot()

calm_above_neon_merge = combined_df.copy()
calm_above_neon_merge.columns = ['datetime', 'year', 'latitude', 'longitude', 'thickness_m', 'source']
calm_above_neon_merge.to_csv('calm_above_neon_tundra_alt_combined_insitu_df.csv', index=False)

external_alt = newdf3[newdf3['thickness'].isna() != True][['datetime', 'latitude', 'longitude', 'source', 'year', 'thickness']].sort_values('datetime').reset_index(drop=True)
external_alt = external_alt[(external_alt['latitude'] >= 49) & (external_alt['thickness'] < 600000.0)].sort_values('datetime').reset_index(drop=True)
external_alt['thickness_m'] = external_alt['thickness'].copy()

cm_mask = (external_alt['thickness'] > 10) & (external_alt['thickness'] < 600000.0)
external_alt.loc[cm_mask, 'thickness_m'] = external_alt.loc[cm_mask, 'thickness'] / 100.0

neg_mask = external_alt['thickness'] < 0
external_alt.loc[neg_mask, 'thickness_m'] = abs(external_alt.loc[neg_mask, 'thickness_m'])

external_alt['needs_review'] = False
ambiguous_mask = (external_alt['thickness'] > 0) & (external_alt['thickness'] <= 10)
external_alt.loc[ambiguous_mask, 'needs_review'] = True
large_mask = external_alt['thickness'] >= 600000.0
external_alt.loc[large_mask, 'needs_review'] = True

external_alt['standardization_note'] = ''
external_alt.loc[cm_mask, 'standardization_note'] = 'Converted from cm to m'
external_alt.loc[neg_mask, 'standardization_note'] = 'Converted from negative depth to positive thickness'
external_alt.loc[ambiguous_mask, 'standardization_note'] = 'Ambiguous units - requires review'
external_alt.loc[large_mask, 'standardization_note'] = 'Extremely large value - likely error'

print(f"Standardization summary:\nTotal records: {len(external_alt)}\nRecords converted from cm to m: {cm_mask.sum()}\nRecords converted from negative depth: {neg_mask.sum()}\nRecords flagged for review: {external_alt['needs_review'].sum()}")
print("\nSample of standardized data:")
print(external_alt[['datetime', 'thickness', 'thickness_m', 'standardization_note']].head(10))

external_alt = external_alt[['datetime', 'latitude', 'longitude', 'source', 'thickness', 'thickness_m', 'standardization_note']].sort_values('datetime').reset_index(drop=True)
external_alt.to_csv('external_alt_final_insitu_df.csv', index=False)

calm_above_neon_merge = pd.read_csv('calm_above_neon_tundra_alt_combined_insitu_df.csv')

def standardize_active_layer_thickness(df, thickness_col='thickness_m', output_col='thickness_m_standardized'):
    result_df = df.copy()
    if 'standardization_note' not in result_df.columns:
        result_df['standardization_note'] = ""
    result_df[output_col] = np.nan
    
    negative_mask = result_df[thickness_col] < 0
    result_df.loc[negative_mask, output_col] = -1 * result_df.loc[negative_mask, thickness_col]
    result_df.loc[negative_mask, 'standardization_note'] = "Negative depth converted to positive thickness (meters)"
    
    mm_mask = (result_df[thickness_col] > 1000) & (~negative_mask)
    result_df.loc[mm_mask, output_col] = result_df.loc[mm_mask, thickness_col] / 1000
    result_df.loc[mm_mask, 'standardization_note'] = "Converted from mm to m"
    
    cm_mask = (result_df[thickness_col] >= 10) & (result_df[thickness_col] <= 1000) & (~negative_mask)
    result_df.loc[cm_mask, output_col] = result_df.loc[cm_mask, thickness_col] / 100
    result_df.loc[cm_mask, 'standardization_note'] = "Converted from cm to m"
    
    m_mask = (result_df[thickness_col] >= 0) & (result_df[thickness_col] < 10) & (~negative_mask)
    result_df.loc[m_mask, output_col] = result_df.loc[m_mask, thickness_col]
    result_df.loc[m_mask, 'standardization_note'] = "Already in meters"
    
    if 'thickness_m' in result_df.columns:
        result_df['discrepancy'] = np.abs(result_df[output_col] - result_df['thickness_m'])
        significant_diff = result_df['discrepancy'] > 0.01
        print(f"Found {significant_diff.sum()} rows with significant discrepancies")
        
        if significant_diff.sum() > 0:
            diagnostic_df = result_df.loc[significant_diff, [thickness_col, 'thickness_m', output_col, 'discrepancy', 
                                          'standardization_note', 'latitude', 'longitude', 'datetime', 'source']]
            diagnostic_df.sort_values('discrepancy', ascending=False).to_csv('thickness_standardization_issues.csv', index=False)
    
    return result_df

standardized_df = standardize_active_layer_thickness(calm_above_neon_merge)

plt.figure(figsize=(12, 6))
plt.hist(standardized_df['thickness_m_standardized'], bins=100, range=(0, 3))
plt.title('Distribution of Standardized Active Layer Thickness Values')
plt.xlabel('Thickness (m)')
plt.ylabel('Frequency')
plt.axvline(x=0.5, color='r', linestyle='--', label='Typical continuous permafrost (0.5m)')
plt.axvline(x=2.0, color='g', linestyle='--', label='Typical discontinuous permafrost (2.0m)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig('standardized_thickness_distribution.png')
plt.show()

print("\nSummary Statistics for Standardized Thickness (meters):")
print(standardized_df['thickness_m_standardized'].describe())

if 'thickness_m' in standardized_df.columns:
    print(f"\nComparison with existing thickness_m column:\nCorrelation: {standardized_df['thickness_m_standardized'].corr(standardized_df['thickness_m']):.4f}")
    print(f"Mean absolute difference: {standardized_df['discrepancy'].mean():.4f} m")
    print(f"Median absolute difference: {standardized_df['discrepancy'].median():.4f} m")
    print(f"Max absolute difference: {standardized_df['discrepancy'].max():.4f} m")
    
    if standardized_df['discrepancy'].max() > 0.01:
        source_groups = standardized_df.groupby('source')['discrepancy'].agg(['mean', 'max', 'count'])
        print("\nDiscrepancies by source:")
        print(source_groups.sort_values('mean', ascending=False))

standardized_df['thickness_m_standardized'].plot()

def enhanced_standardize_thickness(df, thickness_col='thickness_m', output_col='thickness_m_standardized'):
    result_df = standardize_active_layer_thickness(df, thickness_col, output_col)
    
    conditions = [
        (result_df[output_col] > 4),
        (result_df['discrepancy'] > 0.1) if 'discrepancy' in result_df.columns else False,
        (result_df[output_col] > 2) & (result_df[output_col] <= 4),
        (result_df[output_col] <= 2)
    ]
    confidence_levels = ['low', 'low', 'medium', 'high']
    result_df['standardization_confidence'] = np.select(conditions, confidence_levels, default='high')
    
    result_df['is_physical_outlier'] = result_df[output_col] > 4
    
    if 'latitude' in result_df.columns and 'longitude' in result_df.columns:
        result_df['lat_bin'] = np.floor(result_df['latitude'])
        result_df['lon_bin'] = np.floor(result_df['longitude'])
        spatial_groups = result_df.groupby(['lat_bin', 'lon_bin'])
        result_df['thickness_zscore'] = np.nan
        
        for (lat, lon), group in spatial_groups:
            if len(group) >= 5:
                bin_mean = group[output_col].mean()
                bin_std = group[output_col].std()
                if bin_std > 0:
                    idx = group.index
                    result_df.loc[idx, 'thickness_zscore'] = (result_df.loc[idx, output_col] - bin_mean) / bin_std
        
        result_df['is_spatial_outlier'] = abs(result_df['thickness_zscore']) > 3
    
    extreme_mask = (result_df[output_col] > 5) & (result_df['standardization_confidence'] == 'low')
    result_df.loc[extreme_mask, output_col] = 5.0
    result_df.loc[extreme_mask, 'standardization_note'] += " (Capped at 5m due to implausible value)"
    
    return result_df

standardized_df = enhanced_standardize_thickness(calm_above_neon_merge)

print("\nSummary Statistics for Standardized Thickness (meters):")
print(standardized_df['thickness_m_standardized'].describe())

if 'thickness_m' in standardized_df.columns:
    print(f"\nComparison with existing thickness_m column:\nCorrelation: {standardized_df['thickness_m_standardized'].corr(standardized_df['thickness_m']):.4f}")
    print(f"Mean absolute difference: {standardized_df['discrepancy'].mean():.4f} m")
    print(f"Median absolute difference: {standardized_df['discrepancy'].median():.4f} m")
    print(f"Max absolute difference: {standardized_df['discrepancy'].max():.4f} m")
    
    if standardized_df['discrepancy'].max() > 0.01:
        source_groups = standardized_df.groupby('source')['discrepancy'].agg(['mean', 'max', 'count'])
        print("\nDiscrepancies by source:")
        print(source_groups.sort_values('mean', ascending=False))

standardized_df['thickness_m_standardized'].plot()
standardized_df.to_csv('calm_above_neon_tundra_final_insitu_df_standardized.csv', index=False)

# GTNP, ST
import pandas as pd
import numpy as np
import os
import glob
import re
import csv
from datetime import datetime

newdf3[newdf3.temperature.isna()!=True]
newdf3[newdf3.temperature.isna()!=True][['datetime','year','season','frequency','temporal_res','season','latitude','longitude','source','site_name','grid_point','dataset_id','site_code','continent','country','region','temperature','thickness','measurement_method','method']].isna().sum()
temp = newdf3[newdf3.temperature.isna()!=True][['datetime','latitude','longitude','source','temperature']].sort_values('datetime').reset_index(drop=True)
temp = temp[temp.latitude>=49].sort_values('datetime').reset_index(drop=True)
print(f'Minimum Temperature (ºC): {temp.temperature.min()} \nMaximum Temperature (ºC): {temp.temperature.max()}')

def process_gtnp_borehole_files(input_dir, output_file=None):
    if not os.path.exists(input_dir):
        raise FileNotFoundError(f"Directory not found: {input_dir}")
    csv_files = glob.glob(os.path.join(input_dir, "Borehole*.csv"))
    if not csv_files:
        raise FileNotFoundError(f"No Borehole CSV files found in {input_dir}")
    print(f"Found {len(csv_files)} Borehole files to process")
    processed_data = []
    for file_path in csv_files:
        try:
            filename = os.path.basename(file_path)
            metadata = extract_metadata_from_borehole_filename(filename)
            with open(file_path, 'r', encoding='utf-8') as f:
                header_line = f.readline().strip()
            process_borehole_temperature_file(file_path, metadata, processed_data)
            print(f"Processed {filename}")
        except Exception as e:
            print(f"Error processing {file_path}: {str(e)}")
    if not processed_data:
        raise ValueError("No data was successfully processed")
    combined_df = pd.concat(processed_data, ignore_index=True)
    combined_df['processing_date'] = pd.Timestamp.now().strftime('%Y-%m-%d')
    combined_df['processing_version'] = '1.0'
    combined_df['data_source'] = 'GTNP_Borehole'
    if output_file:
        combined_df.to_csv(output_file, index=False)
        print(f"Saved processed data to {output_file}")
    return combined_df

def process_borehole_temperature_file(file_path, metadata, output_list):
    with open(file_path, 'r', encoding='utf-8') as f:
        header = f.readline().strip().split(',')
    if 'Date/Depth' in header:
        process_date_depth_format(file_path, metadata, output_list)
    else:
        process_standard_borehole_format(file_path, metadata, output_list)

def process_date_depth_format(file_path, metadata, output_list):
    df = pd.read_csv(file_path)
    if 'Date/Depth' not in df.columns:
        print(f"Warning: Expected column 'Date/Depth' not found in {os.path.basename(file_path)}")
        return
    depth_columns = []
    for col in df.columns:
        if col != 'Date/Depth':
            try:
                depth_columns.append(col)
            except:
                pass
    if not depth_columns:
        print(f"Warning: No depth columns found in {os.path.basename(file_path)}")
        return
    long_df = pd.melt(df, id_vars=['Date/Depth'], value_vars=depth_columns, var_name='depth_label', value_name='temperature')
    long_df['datetime'] = pd.to_datetime(long_df['Date/Depth'], errors='coerce')
    long_df['depth_m'] = pd.to_numeric(long_df['depth_label'], errors='coerce')
    mask = long_df['depth_m'].isna()
    if mask.any():
        for idx in long_df[mask].index:
            label = long_df.loc[idx, 'depth_label']
            if label == '0':
                long_df.loc[idx, 'depth_m'] = 0.0
            elif re.match(r'^\d+$', label):
                try:
                    depth = float(label) / 100.0
                    long_df.loc[idx, 'depth_m'] = depth
                except:
                    pass
    long_df = long_df.dropna(subset=['datetime', 'depth_m'])
    for key, value in metadata.items():
        long_df[key] = value
    long_df['year'] = long_df['datetime'].dt.year
    long_df = long_df[long_df['temperature'].notna()]
    long_df = long_df[long_df['temperature'] != -999]
    long_df = long_df.drop(columns=['Date/Depth', 'depth_label'])
    output_list.append(long_df)

def process_standard_borehole_format(file_path, metadata, output_list):
    try:
        try:
            df = pd.read_csv(file_path)
            required_cols = ['datetime', 'depth', 'temperature']
            if not all(col in df.columns for col in required_cols):
                raise ValueError(f"Missing required columns. Found: {df.columns.tolist()}")
        except:
            df = pd.read_csv(file_path)
            datetime_col = None
            depth_col = None
            temp_col = None
            for col in df.columns:
                if datetime_col is None and any(keyword in col.lower() for keyword in ['date', 'time']):
                    datetime_col = col
                elif depth_col is None and any(keyword in col.lower() for keyword in ['depth', 'meter']):
                    depth_col = col
                elif temp_col is None and any(keyword in col.lower() for keyword in ['temp', 'celsius']):
                    temp_col = col
            if datetime_col is None or depth_col is None or temp_col is None:
                if len(df.columns) >= 3:
                    datetime_col = df.columns[0]
                    depth_col = df.columns[1]
                    temp_col = df.columns[2]
                else:
                    raise ValueError("Could not identify required columns and file has fewer than 3 columns")
            df = df.rename(columns={datetime_col: 'datetime', depth_col: 'depth', temp_col: 'temperature'})
        df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')
        df['depth_m'] = pd.to_numeric(df['depth'], errors='coerce')
        df['temperature'] = pd.to_numeric(df['temperature'], errors='coerce')
        df = df.dropna(subset=['datetime', 'depth_m', 'temperature'])
        df = df[df['temperature'] != -999]
        for key, value in metadata.items():
            df[key] = value
        df['year'] = df['datetime'].dt.year
        relevant_cols = ['datetime', 'depth_m', 'temperature', 'year'] + list(metadata.keys())
        df = df[relevant_cols]
        output_list.append(df)
    except Exception as e:
        print(f"Error processing standard format: {str(e)}")

def extract_metadata_from_borehole_filename(filename):
    metadata = {'site_name': 'Unknown', 'dataset_id': None, 'frequency': 'Unknown', 'temporal_res': 'Unknown', 'measurement_method': 'Borehole'}
    site_match = re.search(r'Borehole_(\d+)-([^-]+)', filename)
    if site_match:
        metadata['site_id'] = site_match.group(1)
        metadata['site_name'] = site_match.group(2).replace('_', ' ')
    dataset_match = re.search(r'Dataset_(\d+)', filename)
    if dataset_match:
        metadata['dataset_id'] = int(dataset_match.group(1))
    freq_match = re.search(r'(Daily|Weekly|Monthly|Annually|Sporadic|Average)', filename)
    if freq_match:
        metadata['frequency'] = freq_match.group(1)
    country_match = re.search(r'-(Austria|Russia|USA|Canada|Norway|Iceland|Switzerland)-', filename)
    if country_match:
        metadata['country'] = country_match.group(1)
    return metadata

def match_borehole_with_metadata(processed_df, metadata_dir):
    metadata_files = glob.glob(os.path.join(metadata_dir, "*metadata.txt"))
    if not metadata_files:
        print(f"No metadata files found in {metadata_dir}")
        return processed_df
    print(f"Found {len(metadata_files)} metadata files")
    if 'latitude' not in processed_df.columns:
        processed_df['latitude'] = None
    if 'longitude' not in processed_df.columns:
        processed_df['longitude'] = None
    if 'elevation' not in processed_df.columns:
        processed_df['elevation'] = None
    if 'country' not in processed_df.columns:
        processed_df['country'] = None
    dataset_ids = processed_df['dataset_id'].unique()
    matched_datasets = 0
    for metadata_file in metadata_files:
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata_content = f.read()
            lat_match = re.search(r'Latitude:\s*([\d.-]+)', metadata_content)
            lon_match = re.search(r'Longitude:\s*([\d.-]+)', metadata_content)
            name_match = re.search(r'Name:\s*([^\n]+)', metadata_content)
            country_match = re.search(r'Country:\s*([^\n]+)', metadata_content)
            elevation_match = re.search(r'Elevation:\s*([\d.-]+)', metadata_content)
            if not (lat_match and lon_match and name_match):
                continue
            latitude = float(lat_match.group(1))
            longitude = float(lon_match.group(1))
            site_name = name_match.group(1).strip()
            country = country_match.group(1).strip() if country_match else None
            elevation = float(elevation_match.group(1)) if elevation_match else None
            dataset_matches = re.findall(r'Dataset_(\d+)', metadata_content)
            if not dataset_matches:
                continue
            metadata_dataset_ids = [int(id) for id in dataset_matches]
            for dataset_id in metadata_dataset_ids:
                if dataset_id in dataset_ids:
                    mask = processed_df['dataset_id'] == dataset_id
                    if mask.any():
                        processed_df.loc[mask, 'latitude'] = latitude
                        processed_df.loc[mask, 'longitude'] = longitude
                        if country:
                            processed_df.loc[mask, 'country'] = country
                        if elevation:
                            processed_df.loc[mask, 'elevation'] = elevation
                        matched_datasets += 1
                        print(f"Matched dataset {dataset_id} with site {site_name} at ({latitude}, {longitude})")
            site_mask = processed_df['site_name'] == site_name
            if site_mask.any():
                processed_df.loc[site_mask, 'latitude'] = latitude
                processed_df.loc[site_mask, 'longitude'] = longitude
                if country:
                    processed_df.loc[site_mask, 'country'] = country
                if elevation:
                    processed_df.loc[site_mask, 'elevation'] = elevation
                print(f"Matched site name {site_name} at ({latitude}, {longitude})")
        except Exception as e:
            print(f"Error processing {metadata_file}: {str(e)}")
    print(f"Matched {matched_datasets} datasets with coordinates")
    missing_mask = processed_df['latitude'].isna()
    missing_count = missing_mask.sum()
    if missing_count > 0:
        missing_datasets = processed_df.loc[missing_mask, 'dataset_id'].unique()
        print(f"Still missing coordinates for {len(missing_datasets)} datasets: {missing_datasets}")
    return processed_df

borehole_df = process_gtnp_borehole_files('/Users/bgay/Downloads/zerocurtain/gtnp', 'processed_borehole_temp.csv')
borehole_df
borehole_df = match_borehole_with_metadata(borehole_df, '/Users/bgay/Downloads/zerocurtain/gtnp')
borehole_df.isna().sum()
borehole_df
borehole_df.to_csv('matched_processed_borehole_gtnp_soiltemp_insitu_df.csv', index=False)
borehole_df = borehole_df[borehole_df.temperature.isna()!=True].sort_values('datetime').reset_index(drop=True)
borehole_df = borehole_df[borehole_df.latitude>=49]
borehole_df
borehole_df = pd.read_csv('matched_processed_borehole_gtnp_soiltemp_insitu_df.csv')

def clean_temperature_data(borehole_df):
    cleaned_df = borehole_df.copy()
    cleaned_df['temp_quality'] = 'valid'
    impossible_mask = (cleaned_df['temperature'] < -70) | (cleaned_df['temperature'] > 50)
    cleaned_df.loc[impossible_mask, 'temp_quality'] = 'physically_impossible'
    suspicious_mask = ((cleaned_df['temperature'] < -60) | (cleaned_df['temperature'] > 40)) & ~impossible_mask
    cleaned_df.loc[suspicious_mask, 'temp_quality'] = 'suspicious'
    if 'depth_m' in cleaned_df.columns:
        neg_depth_mask = cleaned_df['depth_m'] < 0
        cleaned_df.loc[neg_depth_mask, 'temp_quality'] = 'negative_depth'
    grouped = cleaned_df.groupby(['site_name', 'datetime'])
    inversion_indices = []
    for _, group in grouped:
        if len(group) >= 3:
            sorted_group = group.sort_values('depth_m')
            depths = sorted_group['depth_m'].values
            temps = sorted_group['temperature'].values
            for i in range(len(depths)-2):
                if depths[i+1] - depths[i] > 0:
                    gradient1 = (temps[i+1] - temps[i]) / (depths[i+1] - depths[i])
                    if depths[i+2] - depths[i+1] > 0:
                        gradient2 = (temps[i+2] - temps[i+1]) / (depths[i+2] - depths[i+1])
                        if (gradient1 * gradient2 < 0) and (abs(gradient1) > 10 or abs(gradient2) > 10):
                            inversion_indices.extend(sorted_group.iloc[i:i+3].index.tolist())
    cleaned_df.loc[inversion_indices, 'temp_quality'] = 'temperature_inversion'
    total_records = len(cleaned_df)
    valid_records = (cleaned_df['temp_quality'] == 'valid').sum()
    print(f"Total records: {total_records}")
    print(f"Valid records: {valid_records} ({valid_records/total_records*100:.1f}%)")
    print(f"Invalid records: {total_records - valid_records} ({(total_records - valid_records)/total_records*100:.1f}%)")
    flag_counts = cleaned_df['temp_quality'].value_counts()
    for flag, count in flag_counts.items():
        print(f"  {flag}: {count} ({count/total_records*100:.1f}%)")
    valid_temps = cleaned_df.loc[cleaned_df['temp_quality'] == 'valid', 'temperature']
    print(f"\nValid temperature range: {valid_temps.min():.1f}°C to {valid_temps.max():.1f}°C")
    print(f"Valid temperature mean: {valid_temps.mean():.1f}°C")
    return cleaned_df

cleaned_borehole_df = clean_temperature_data(borehole_df)
valid_borehole_df = cleaned_borehole_df[cleaned_borehole_df['temp_quality'] == 'valid']
valid_borehole_df
valid_borehole_df = valid_borehole_df.sort_values('datetime').reset_index(drop=True)
valid_borehole_df.isna().sum()
valid_borehole_df = valid_borehole_df[['datetime', 'depth_m', 'temperature', 'year', 'site_name', 'dataset_id', 'frequency', 'temporal_res', 'measurement_method', 'site_id', 'processing_date', 'processing_version', 'data_source', 'latitude', 'longitude', 'country', 'temp_quality']]
valid_borehole_df.isna().sum()
valid_borehole_df = valid_borehole_df.sort_values('datetime').reset_index(drop=True)
valid_borehole_df.to_csv('cleaned_validated_processed_borehole_gtnp_soiltemp_insitu_df.csv', index=False)
valid_borehole_df
valid_borehole_df = valid_borehole_df[['datetime', 'year', 'frequency', 'temporal_res', 'site_name', 'dataset_id', 'latitude', 'longitude', 'site_id', 'data_source', 'country', 'depth_m', 'temperature', 'temp_quality', 'measurement_method']].sort_values('datetime').reset_index(drop=True)
valid_borehole_df.to_csv('updated_cleaned_validated_processed_borehole_gtnp_soiltemp_insitu_df.csv', index=False)
valid_borehole_df = pd.read_csv('updated_cleaned_validated_processed_borehole_gtnp_soiltemp_insitu_df.csv')
valid_borehole_df

def add_depth_zones(df):
    conditions = [(df['depth_m'] <= 0.25), (df['depth_m'] > 0.25) & (df['depth_m'] <= 0.5), (df['depth_m'] > 0.5) & (df['depth_m'] <= 1.0), (df['depth_m'] > 1.0)]
    choices = ['shallow', 'intermediate', 'deep', 'very_deep']
    df['depth_zone'] = np.select(conditions, choices, default='unknown')
    return df

gtnp_st = add_depth_zones(valid_borehole_df)
gtnp_st.isna().sum()
gtnp_st = gtnp_st[['datetime', 'year', 'frequency', 'temporal_res', 'site_name', 'dataset_id', 'latitude', 'longitude', 'site_id', 'data_source', 'country', 'temperature', 'depth_m', 'depth_zone', 'temp_quality', 'measurement_method']]
gtnp_st.to_csv('cleaned_df_final_gtnp_st_w_depth_insitu_df.csv',index=False)

required_columns = ['datetime', 'year', 'site_name', 'latitude', 'longitude', 'site_id', 'data_source', 'temperature', 'depth_m', 'depth_zone']
for col in required_columns:
    if col not in gtnp_st.columns:
        if col == 'site_id':
            gtnp_st['site_id'] = gtnp_st['site_name']
        elif col == 'season':
            gtnp_st['season'] = pd.to_datetime(gtnp_st['datetime']).dt.quarter.map({1: 'Winter', 2: 'Spring', 3: 'Summer', 4: 'Fall'})
gtnp_st = gtnp_st.sort_values('datetime').reset_index(drop=True)
gtnp_st
gtnp_st.isna().sum()
gtnp_st.to_csv('ZC_df_data.csv', index=False)

gtnp_st.datetime = pd.to_datetime(gtnp_st.datetime,format='mixed')
month_list = [pd.to_datetime(gtnp_st.datetime[i]).month for i in range(len(gtnp_st))]

def assign_season(month):
    if month in [12, 1, 2]:
        return "Winter"
    elif month in [3, 4, 5]:
        return "Spring"
    elif month in [6, 7, 8]:
        return "Summer"
    else:
        return "Fall"

seasons = [assign_season(month) for month in month_list]
gtnp_st['season']=seasons
gtnp_st

def standardize_soil_temperature(gtnp_st):
    std_df = gtnp_st.copy()
    if 'temp_quality' not in std_df.columns:
        std_df['temp_quality'] = 'valid'
    impossible_mask = (std_df['temperature'] < -70) | (std_df['temperature'] > 50)
    std_df.loc[impossible_mask, 'temp_quality'] = 'physically_impossible'
    suspicious_mask = ((std_df['temperature'] < -60) | (std_df['temperature'] > 40)) & ~impossible_mask
    std_df.loc[suspicious_mask, 'temp_quality'] = 'suspicious'
    grouped = std_df.groupby(['site_id', 'datetime'])
    inconsistent_indices = []
    for _, group in grouped:
        if len(group) >= 2:
            sorted_group = group.sort_values('depth_m')
            is_winter = sorted_group['season'].iloc[0] in ['Winter', 'Fall']
            depths = sorted_group['depth_m'].values
            temps = sorted_group['temperature'].values
            for i in range(len(depths)-1):
                if depths[i+1] - depths[i] > 0:
                    temp_gradient = (temps[i+1] - temps[i]) / (depths[i+1] - depths[i])
                    if abs(temp_gradient) > 15:
                        inconsistent_indices.extend(sorted_group.iloc[i:i+2].index.tolist())
    std_df.loc[inconsistent_indices, 'temp_quality'] = 'inconsistent_profile'
    total_records = len(std_df)
    valid_records = (std_df['temp_quality'] == 'valid').sum()
    print(f"Total records: {total_records}")
    print(f"Valid records: {valid_records} ({valid_records/total_records*100:.1f}%)")
    print(f"Invalid records: {total_records - valid_records} ({(total_records - valid_records)/total_records*100:.1f}%)")
    flag_counts = std_df['temp_quality'].value_counts()
    for flag, count in flag_counts.items():
        print(f"  {flag}: {count} ({count/total_records*100:.1f}%)")
    valid_temps = std_df.loc[std_df['temp_quality'] == 'valid', 'temperature']
    print(f"\nValid temperature range: {valid_temps.min():.1f}°C to {valid_temps.max():.1f}°C")
    print(f"Valid temperature mean: {valid_temps.mean():.1f}°C")
    std_df['temperature_standardized'] = std_df['temperature']
    return std_df

standardized_gtnp_st = standardize_soil_temperature(gtnp_st)
standardized_gtnp_st
valid_gtnp_st = standardized_gtnp_st[standardized_gtnp_st['temp_quality'] == 'valid']
valid_gtnp_st = valid_gtnp_st.sort_values('datetime').reset_index(drop=True)
gtnp_st = valid_gtnp_st
del valid_gtnp_st
gtnp_st.to_csv('ZC_data_standardized_gtnp_st_df.csv', index=False)

# GTNP, ALT
import pandas as pd
import numpy as np
import os
import glob
import re
import csv
from datetime import datetime

def process_gtnp_active_layer_files(input_dir, output_file=None):
    if not os.path.exists(input_dir):
        raise FileNotFoundError(f"Directory not found: {input_dir}")
    csv_files = glob.glob(os.path.join(input_dir, "Activelayer*.csv"))
    if not csv_files:
        raise FileNotFoundError(f"No ActiveLayer CSV files found in {input_dir}")
    print(f"Found {len(csv_files)} ActiveLayer files to process")
    standard_format_data = []
    borehole_format_data = []
    for file_path in csv_files:
        try:
            filename = os.path.basename(file_path)
            metadata = extract_metadata_from_filename(filename)
            with open(file_path, 'r', encoding='utf-8') as f:
                header_line = f.readline().strip()
            if header_line.startswith('offset_x'):
                process_standard_format_robust(file_path, metadata, standard_format_data)
            elif header_line.startswith('Date/Depth'):
                process_borehole_format(file_path, metadata, borehole_format_data)
            else:
                print(f"Warning: Unknown file format for {filename}, skipping file")
                continue
            print(f"Processed {filename}")
        except Exception as e:
            print(f"Error processing {file_path}: {str(e)}")
    all_data = []
    if standard_format_data:
        standard_df = pd.concat(standard_format_data, ignore_index=True)
        all_data.append(standard_df)
        print(f"Processed {len(standard_format_data)} standard format files")
    if borehole_format_data:
        borehole_df = pd.concat(borehole_format_data, ignore_index=True)
        all_data.append(borehole_df)
        print(f"Processed {len(borehole_format_data)} borehole format files")
    if not all_data:
        raise ValueError("No data was successfully processed")
    combined_df = pd.concat(all_data, ignore_index=True)
    combined_df['processing_date'] = pd.Timestamp.now().strftime('%Y-%m-%d')
    combined_df['processing_version'] = '1.0'
    combined_df['data_source'] = 'GTNP_ActiveLayer'
    if output_file:
        combined_df.to_csv(output_file, index=False)
        print(f"Saved processed data to {output_file}")
    return combined_df

def process_standard_format_robust(file_path, metadata, output_list):
    with open(file_path, 'r', encoding='utf-8') as f:
        header = f.readline().strip().split(',')
        expected_cols = len(header)
    rows = []
    coord_columns = ['offset_x', 'offset_y']
    date_columns = [col for col in header if re.match(r'\d{4}-\d{2}-\d{2}', col)]
    if not date_columns:
        print(f"Warning: No date columns found in {os.path.basename(file_path)}")
        return
    with open(file_path, 'r', encoding='utf-8') as f:
        csv_reader = csv.reader(f)
        header_row = next(csv_reader)
        for i, row in enumerate(csv_reader, start=2):
            try:
                if len(row) >= expected_cols:
                    row_data = row[:expected_cols]
                    rows.append(row_data)
                else:
                    print(f"Warning: Skipping row {i} in {os.path.basename(file_path)} - insufficient columns")
            except Exception as e:
                print(f"Error processing row {i} in {os.path.basename(file_path)}: {str(e)}")
    df = pd.DataFrame(rows, columns=header)
    for col in coord_columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    for col in date_columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    long_df = pd.melt(df, id_vars=coord_columns, value_vars=date_columns, var_name='datetime', value_name='thickness_cm')
    long_df['datetime'] = pd.to_datetime(long_df['datetime'])
    long_df['year'] = long_df['datetime'].dt.year
    for key, value in metadata.items():
        long_df[key] = value
    long_df = long_df[long_df['thickness_cm'].notna()]
    long_df = long_df[long_df['thickness_cm'] != -999]
    long_df['thickness_m'] = long_df['thickness_cm'] / 100.0
    long_df['measurement_flag'] = 'valid'
    suspicious_mask = (long_df['thickness_cm'] < 0) | (long_df['thickness_cm'] > 150)
    long_df.loc[suspicious_mask, 'measurement_flag'] = 'suspicious'
    long_df['utm_x'] = long_df['offset_x']
    long_df['utm_y'] = long_df['offset_y']
    output_list.append(long_df)

def process_borehole_format(file_path, metadata, output_list):
    df = pd.read_csv(file_path)
    df.columns = ['datetime', 'depth_0m', 'depth_20_16m']
    df['datetime'] = pd.to_datetime(df['datetime'])
    long_df = pd.melt(df, id_vars=['datetime'], value_vars=['depth_0m', 'depth_20_16m'], var_name='depth_location', value_name='temperature')
    long_df['depth_m'] = long_df['depth_location'].map({'depth_0m': 0.0, 'depth_20_16m': 20.16})
    long_df = long_df[long_df['temperature'].notna()]
    long_df = long_df[long_df['temperature'] != -999]
    for key, value in metadata.items():
        long_df[key] = value
    output_list.append(long_df)

def extract_metadata_from_filename(filename):
    metadata = {
        'site_name': 'Unknown',
        'dataset_id': None,
        'frequency': 'Unknown',
        'temporal_res': 'Unknown',
        'measurement_method': 'Unknown',
        'file_type': 'Unknown'
    }
    dataset_match = re.search(r'Dataset_(\d+)', filename)
    if dataset_match:
        metadata['dataset_id'] = int(dataset_match.group(1))
    site_match = re.search(r'-([A-Za-z0-9_]+)-Dataset_', filename)
    if site_match:
        metadata['site_name'] = site_match.group(1).replace('_', ' ')
    freq_match = re.search(r'-(Sporadic|Annual|Average|Unknown)-', filename)
    if freq_match:
        metadata['frequency'] = freq_match.group(1)
    temp_match = re.search(r'-(As_Needed|Weekly|Monthly|Daily|Annually|Unknown)-', filename)
    if temp_match:
        metadata['temporal_res'] = temp_match.group(1)
    method_match = re.search(r'-(Mechanical_Probing|Frost_Tube|Borehole|Ground_Subsidence)', filename)
    if method_match:
        metadata['measurement_method'] = method_match.group(1)
    if 'borehole' in filename.lower():
        metadata['file_type'] = 'borehole'
    else:
        metadata['file_type'] = 'grid_transect'
    return metadata

def identify_and_fix_problematic_files(input_dir, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    problematic_files = [
        "Activelayer_229-Toolik_MAT-Dataset_1509",
        "Activelayer_120-Bolvansky-Dataset_239",
        "Activelayer_6-Betty_Pingo-Dataset_1506",
        "Activelayer_233-Betty_Pingo_WET-Dataset_1508",
        "Activelayer_234-Betty_Pingo_MNT-Dataset_1507",
        "Activelayer_23-Sagwon_Hills_MNT-Dataset_1519",
        "Activelayer_2-Barrow-Dataset_14"
    ]
    fixed_files = []
    csv_files = glob.glob(os.path.join(input_dir, "Activelayer*.csv"))
    for file_path in csv_files:
        filename = os.path.basename(file_path)
        is_problematic = any(problem_name in filename for problem_name in problematic_files)
        if is_problematic:
            print(f"Fixing problematic file: {filename}")
            output_path = os.path.join(output_dir, filename)
            try:
                fix_problematic_csv(file_path, output_path)
                fixed_files.append(output_path)
                print(f"Fixed file saved to: {output_path}")
            except Exception as e:
                print(f"Error fixing {filename}: {str(e)}")
    return fixed_files

def fix_problematic_csv(input_file, output_file):
    with open(input_file, 'r', encoding='utf-8') as f:
        header = f.readline().strip().split(',')
    expected_cols = len(header)
    with open(output_file, 'w', encoding='utf-8', newline='') as out_f:
        writer = csv.writer(out_f)
        writer.writerow(header)
        with open(input_file, 'r', encoding='utf-8') as in_f:
            csv_reader = csv.reader(in_f)
            next(csv_reader)
            for row in csv_reader:
                if len(row) >= expected_cols:
                    writer.writerow(row[:expected_cols])
                else:
                    padded_row = row + [''] * (expected_cols - len(row))
                    writer.writerow(padded_row)

def extract_site_coordinates(metadata_dir):
    metadata_files = glob.glob(os.path.join(metadata_dir, "*metadata.txt"))
    if not metadata_files:
        print(f"No metadata files found in {metadata_dir}")
        return {}
    print(f"Found {len(metadata_files)} metadata files")
    site_coordinates = {}
    for metadata_file in metadata_files:
        try:
            filename = os.path.basename(metadata_file)
            match = re.search(r'Activelayer_(\d+)-([^.]+)', filename)
            if not match:
                match = re.search(r'Activelayer_([^.]+)\.metadata', filename)
                if match:
                    site_id_name = match.group(1)
                else:
                    print(f"Could not extract site ID/name from {filename}")
                    continue
            else:
                site_id = match.group(1)
                site_name = match.group(2)
                site_id_name = f"{site_id}-{site_name}"
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata_content = f.read()
            lat_match = re.search(r'Latitude:\s*([\d.-]+)', metadata_content)
            lon_match = re.search(r'Longitude:\s*([\d.-]+)', metadata_content)
            if lat_match and lon_match:
                latitude = float(lat_match.group(1))
                longitude = float(lon_match.group(1))
                name_match = re.search(r'Name:\s*([^\n]+)', metadata_content)
                if name_match:
                    site_name = name_match.group(1).strip()
                site_coordinates[site_id_name] = (latitude, longitude)
                site_coordinates[site_name] = (latitude, longitude)
                print(f"Extracted coordinates for {site_name}: ({latitude}, {longitude})")
            else:
                print(f"Could not find coordinates in {metadata_file}")
        except Exception as e:
            print(f"Error processing {metadata_file}: {str(e)}")
    return site_coordinates

def enrich_dataset_with_coordinates(processed_df, metadata_dir):
    site_coordinates = extract_site_coordinates(metadata_dir)
    if not site_coordinates:
        print("No coordinates found in metadata files")
        return processed_df
    processed_df['latitude'] = None
    processed_df['longitude'] = None
    matched_sites = set()
    for site_name, (lat, lon) in site_coordinates.items():
        mask = processed_df['site_name'] == site_name
        if mask.any():
            processed_df.loc[mask, 'latitude'] = lat
            processed_df.loc[mask, 'longitude'] = lon
            matched_sites.add(site_name)
        site_name_underscores = site_name.replace(' ', '_')
        mask = processed_df['site_name'] == site_name_underscores
        if mask.any():
            processed_df.loc[mask, 'latitude'] = lat
            processed_df.loc[mask, 'longitude'] = lon
            matched_sites.add(site_name_underscores)
        site_name_spaces = site_name.replace('_', ' ')
        mask = processed_df['site_name'] == site_name_spaces
        if mask.any():
            processed_df.loc[mask, 'latitude'] = lat
            processed_df.loc[mask, 'longitude'] = lon
            matched_sites.add(site_name_spaces)
    unique_sites = processed_df['site_name'].unique()
    print(f"Found coordinates for {len(matched_sites)} out of {len(unique_sites)} unique sites")
    missing_sites = [site for site in unique_sites if site not in matched_sites]
    if missing_sites:
        print(f"Sites without coordinates: {missing_sites}")
    return processed_df

def match_dataset_with_metadata(processed_df, metadata_dir):
    metadata_files = glob.glob(os.path.join(metadata_dir, "*metadata.txt"))
    if not metadata_files:
        print(f"No metadata files found in {metadata_dir}")
        return processed_df
    print(f"Found {len(metadata_files)} metadata files")
    processed_df['latitude'] = None
    processed_df['longitude'] = None
    dataset_ids = processed_df['dataset_id'].unique()
    matched_datasets = 0
    for metadata_file in metadata_files:
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata_content = f.read()
            name_match = re.search(r'Name:\s*([^\n]+)', metadata_content)
            site_name = name_match.group(1).strip() if name_match else None
            lat_match = re.search(r'Latitude:\s*([\d.-]+)', metadata_content)
            lon_match = re.search(r'Longitude:\s*([\d.-]+)', metadata_content)
            if not (lat_match and lon_match and site_name):
                continue
            latitude = float(lat_match.group(1))
            longitude = float(lon_match.group(1))
            dataset_matches = re.findall(r'Dataset_(\d+)', metadata_content)
            if not dataset_matches:
                continue
            metadata_dataset_ids = [int(id) for id in dataset_matches]
            for dataset_id in metadata_dataset_ids:
                if dataset_id in dataset_ids:
                    mask = processed_df['dataset_id'] == dataset_id
                    if mask.any():
                        processed_df.loc[mask, 'latitude'] = latitude
                        processed_df.loc[mask, 'longitude'] = longitude
                        matched_datasets += 1
                        print(f"Matched dataset {dataset_id} with site {site_name} at ({latitude}, {longitude})")
        except Exception as e:
            print(f"Error processing {metadata_file}: {str(e)}")
    print(f"Matched {matched_datasets} datasets with coordinates")
    missing_mask = processed_df['latitude'].isna()
    missing_count = missing_mask.sum()
    if missing_count > 0:
        missing_datasets = processed_df.loc[missing_mask, 'dataset_id'].unique()
        print(f"Still missing coordinates for {len(missing_datasets)} datasets: {missing_datasets}")
    return processed_df

def standardize_active_layer_thickness(df, thickness_col='thickness_m', output_col='thickness_m_standardized'):
    result_df = df.copy()
    if 'standardization_note' not in result_df.columns:
        result_df['standardization_note'] = ""
    result_df[output_col] = np.nan
    negative_mask = result_df[thickness_col] < 0
    result_df.loc[negative_mask, output_col] = -1 * result_df.loc[negative_mask, thickness_col]
    result_df.loc[negative_mask, 'standardization_note'] = "Negative depth converted to positive thickness (meters)"
    mm_mask = (result_df[thickness_col] > 1000) & (~negative_mask)
    result_df.loc[mm_mask, output_col] = result_df.loc[mm_mask, thickness_col] / 1000
    result_df.loc[mm_mask, 'standardization_note'] = "Converted from mm to m"
    cm_mask = (result_df[thickness_col] >= 10) & (result_df[thickness_col] <= 1000) & (~negative_mask)
    result_df.loc[cm_mask, output_col] = result_df.loc[cm_mask, thickness_col] / 100
    result_df.loc[cm_mask, 'standardization_note'] = "Converted from cm to m"
    m_mask = (result_df[thickness_col] >= 0) & (result_df[thickness_col] < 10) & (~negative_mask)
    result_df.loc[m_mask, output_col] = result_df.loc[m_mask, thickness_col]
    result_df.loc[m_mask, 'standardization_note'] = "Already in meters"
    if 'thickness_m' in result_df.columns:
        result_df['discrepancy'] = np.abs(result_df[output_col] - result_df['thickness_m'])
        significant_diff = result_df['discrepancy'] > 0.01
        print(f"Found {significant_diff.sum()} rows with significant discrepancies")
        if significant_diff.sum() > 0:
            diagnostic_df = result_df.loc[significant_diff, [thickness_col, 'thickness_m', output_col, 'discrepancy', 'standardization_note', 'latitude', 'longitude', 'datetime', 'source']]
            diagnostic_df.sort_values('discrepancy', ascending=False).to_csv('thickness_standardization_issues.csv', index=False)
    return result_df

def enhanced_standardize_thickness(df, thickness_col='thickness_m', output_col='thickness_m_standardized'):
    result_df = standardize_active_layer_thickness(df, thickness_col, output_col)
    conditions = [
        (result_df[output_col] > 4),
        (result_df['discrepancy'] > 0.1) if 'discrepancy' in result_df.columns else False,
        (result_df[output_col] > 2) & (result_df[output_col] <= 4),
        (result_df[output_col] <= 2)
    ]
    confidence_levels = ['low', 'low', 'medium', 'high']
    result_df['standardization_confidence'] = np.select(conditions, confidence_levels, default='high')
    result_df['is_physical_outlier'] = False
    result_df.loc[result_df[output_col] > 4, 'is_physical_outlier'] = True
    if 'latitude' in result_df.columns and 'longitude' in result_df.columns:
        result_df['lat_bin'] = np.floor(result_df['latitude'])
        result_df['lon_bin'] = np.floor(result_df['longitude'])
        spatial_groups = result_df.groupby(['lat_bin', 'lon_bin'])
        result_df['thickness_zscore'] = np.nan
        for (lat, lon), group in spatial_groups:
            if len(group) >= 5:
                bin_mean = group[output_col].mean()
                bin_std = group[output_col].std()
                if bin_std > 0:
                    idx = group.index
                    result_df.loc[idx, 'thickness_zscore'] = (result_df.loc[idx, output_col] - bin_mean) / bin_std
        result_df['is_spatial_outlier'] = False
        result_df.loc[abs(result_df['thickness_zscore']) > 3, 'is_spatial_outlier'] = True
    extreme_mask = (result_df[output_col] > 5) & (result_df['standardization_confidence'] == 'low')
    result_df.loc[extreme_mask, output_col] = 5.0
    result_df.loc[extreme_mask, 'standardization_note'] += " (Capped at 5m due to implausible value)"
    return result_df

import shutil
fixed_files = identify_and_fix_problematic_files('/Users/bgay/Downloads/zerocurtain/gtnp', '/Users/bgay/Downloads/zerocurtain/gtnp_fixed_files')
if fixed_files:
    print(f"Fixed {len(fixed_files)} files")
else:
    print("No files were fixed")
processed_df = process_gtnp_active_layer_files('/Users/bgay/Downloads/zerocurtain/gtnp_fixed_files', 'processed_active_layer_data.csv')
processed_df.site_name.unique()
problematic_filenames = [os.path.basename(f) for f in fixed_files]
orig_files = glob.glob('/Users/bgay/Downloads/zerocurtain/gtnp/Activelayer*.csv')
copied_count = 0
for file in orig_files:
    filename = os.path.basename(file)
    if filename not in problematic_filenames:
        dest_path = os.path.join('/Users/bgay/Downloads/zerocurtain/gtnp_fixed_files', filename)
        shutil.copy2(file, dest_path)
        copied_count += 1
print(f"Copied {copied_count} non-problematic files to the fixed directory")
processed_df = process_gtnp_active_layer_files('/Users/bgay/Downloads/zerocurtain/gtnp_fixed_files', 'processed_active_layer_data.csv')
processed_df[(processed_df.measurement_flag=='suspicious') & (processed_df.thickness_m<-4)]
processed_df = processed_df.iloc[~processed_df.index.isin(processed_df[(processed_df.measurement_flag == 'suspicious') & (processed_df.thickness_m < -4)].index)]
processed_df = processed_df[['datetime', 'year', 'frequency', 'temporal_res', 'site_name', 'dataset_id', 'utm_x', 'utm_y', 'thickness_cm', 'thickness_m', 'depth_location', 'temperature', 'depth_m', 'measurement_flag', 'measurement_method', 'data_source']]
processed_df = match_dataset_with_metadata(processed_df, '/Users/bgay/Downloads/zerocurtain/gtnp')
processed_df = processed_df[processed_df.latitude>=49].sort_values('datetime').reset_index(drop=True)
processed_df = processed_df[['datetime', 'year', 'frequency', 'temporal_res', 'site_name', 'dataset_id', 'data_source', 'utm_x', 'utm_y', 'latitude', 'longitude', 'thickness_cm', 'thickness_m', 'measurement_flag', 'measurement_method']]
processed_df = processed_df.sort_values('datetime').reset_index(drop=True)
processed_df.to_csv('enriched_gtnp_active_layer_data.csv', index=False)
negative_rows = processed_df[processed_df.thickness_m < 0]
print(f"Removing {len(negative_rows)} rows with negative thickness values:")
print(negative_rows)
cleaned_df = processed_df[processed_df.thickness_m >= 0].copy()
print(f"Original dataframe: {len(processed_df)} rows")
print(f"Cleaned dataframe: {len(cleaned_df)} rows")
cleaned_df = cleaned_df[cleaned_df.latitude>=49]
cleaned_df.datetime = pd.to_datetime(cleaned_df.datetime,format='mixed')
cleaned_df = cleaned_df.sort_values('datetime').reset_index(drop=True)
cleaned_df.to_csv('cleaned_df_final_gtnp_alt_insitu_df.csv',index=False)
gtnp_alt = cleaned_df
del cleaned_df
standardized_df = enhanced_standardize_thickness(gtnp_alt)
standardized_df.to_csv('gtnpalt_matched_processed_cleaned_validated_borehole_gtnp_insitu_df.csv',index=False)
gtnp_alt = standardized_df
del standardized_df

# Combine Data (Pre-ISMN, CALM RU)
import pandas as pd
import numpy as np
import os
from datetime import datetime

def standardize_datetime_sort(df, datetime_col='datetime'):
    df[datetime_col] = pd.to_datetime(df[datetime_col], format='mixed')
    return df.sort_values(datetime_col).reset_index(drop=True)

def assign_season(month):
    if month in [12, 1, 2]:
        return "Winter"
    elif month in [3, 4, 5]:
        return "Spring"
    elif month in [6, 7, 8]:
        return "Summer"
    else:
        return "Fall"

def add_season_column(df):
    month_list = [pd.to_datetime(df.datetime[i]).month for i in range(len(df))]
    df['season'] = [assign_season(month) for month in month_list]
    return df

def add_depth_zones(df, depth_col='soil_temp_depth'):
    conditions = [
        (df[depth_col] <= 0.25),
        (df[depth_col] > 0.25) & (df[depth_col] <= 0.5),
        (df[depth_col] > 0.5) & (df[depth_col] <= 1.0),
        (df[depth_col] > 1.0)
    ]
    choices = ['shallow', 'intermediate', 'deep', 'very_deep']
    df[f'{depth_col}_zone'] = np.select(conditions, choices, default='unknown')
    return df

def combine_source_columns(df, col1, col2, new_col='source'):
    site_name = df[col1].fillna('')
    source = df[col2].fillna('')
    df['data_source'] = site_name + '_' + source
    df['data_source'] = df['data_source'].str.strip('_')
    df.loc[(site_name == '') & (source == ''), 'data_source'] = None
    df = df.drop(columns=[col1, col2])
    df.columns = [new_col if col == 'data_source' else col for col in df.columns]
    return df

def standardize_soil_temperature(df, temp_col='soil_temp', depth_col='soil_temp_depth', 
                                source_col='source', season_col='season'):
    std_df = df.copy()
    if 'temp_quality' not in std_df.columns:
        std_df['temp_quality'] = 'valid'
    
    impossible_mask = (std_df[temp_col] < -70) | (std_df[temp_col] > 50)
    std_df.loc[impossible_mask, 'temp_quality'] = 'physically_impossible'
    
    suspicious_mask = ((std_df[temp_col] < -60) | (std_df[temp_col] > 40)) & ~impossible_mask
    std_df.loc[suspicious_mask, 'temp_quality'] = 'suspicious'
    
    grouped = std_df.groupby([source_col, 'datetime'])
    inconsistent_indices = []
    
    for _, group in grouped:
        if len(group) >= 2:
            sorted_group = group.sort_values(depth_col)
            is_winter = sorted_group[season_col].iloc[0] in ['Winter', 'Fall']
            depths = sorted_group[depth_col].values
            temps = sorted_group[temp_col].values
            
            for i in range(len(depths)-1):
                if depths[i+1] - depths[i] > 0:
                    temp_gradient = (temps[i+1] - temps[i]) / (depths[i+1] - depths[i])
                    if abs(temp_gradient) > 15:
                        inconsistent_indices.extend(sorted_group.iloc[i:i+2].index.tolist())
    
    std_df.loc[inconsistent_indices, 'temp_quality'] = 'inconsistent_profile'
    
    total_records = len(std_df)
    valid_records = (std_df['temp_quality'] == 'valid').sum()
    print(f"Total records: {total_records}")
    print(f"Valid records: {valid_records} ({valid_records/total_records*100:.1f}%)")
    print(f"Invalid records: {total_records - valid_records} ({(total_records - valid_records)/total_records*100:.1f}%)")
    
    flag_counts = std_df['temp_quality'].value_counts()
    for flag, count in flag_counts.items():
        print(f"  {flag}: {count} ({count/total_records*100:.1f}%)")
    
    valid_temps = std_df.loc[std_df['temp_quality'] == 'valid', temp_col]
    print(f"\nValid temperature range: {valid_temps.min():.1f}°C to {valid_temps.max():.1f}°C")
    print(f"Valid temperature mean: {valid_temps.mean():.1f}°C")
    
    std_df[f'{temp_col}_standardized'] = std_df[temp_col]
    return std_df

calm_above_neon_tundra_alt = pd.read_csv('calm_above_neon_tundra_alt_final_insitu_df_standardized.csv')
calm_above_neon_tundra_alt = standardize_datetime_sort(calm_above_neon_tundra_alt)
calm_above_neon_tundra_alt = calm_above_neon_tundra_alt[~calm_above_neon_tundra_alt.thickness_m.isna()]
calm_above_neon_tundra_alt = calm_above_neon_tundra_alt[calm_above_neon_tundra_alt.standardization_confidence=='high'][['datetime','year','latitude','longitude','thickness_m','thickness_m_standardized','source']]
calm_above_neon_tundra_alt = standardize_datetime_sort(calm_above_neon_tundra_alt)

derived_alt = pd.read_csv('derivedaltfromgtnpst_matched_processed_cleaned_validated_borehole_gtnp_insitu_df.csv')
derived_alt = standardize_datetime_sort(derived_alt)
derived_alt = derived_alt[derived_alt.standardization_confidence=='high'][['datetime','year','latitude','longitude','alt_m','thickness_m_standardized','site_name','data_source']]
derived_alt.columns = ['datetime','year','latitude','longitude','thickness_m','thickness_m_standardized','site_name','source']
derived_alt = standardize_datetime_sort(derived_alt)
derived_alt = combine_source_columns(derived_alt, 'site_name', 'source')
derived_alt.columns = ['datetime','year','latitude','longitude','thickness_m','thickness_m_standardized','source']
derived_alt = standardize_datetime_sort(derived_alt)

gtnp_alt = pd.read_csv('gtnpalt_matched_processed_cleaned_validated_borehole_gtnp_insitu_df.csv')
gtnp_alt = standardize_datetime_sort(gtnp_alt)
gtnp_alt = gtnp_alt[gtnp_alt.standardization_confidence=='high'][['datetime','year','latitude','longitude','thickness_m','thickness_m_standardized','site_name','data_source']]
gtnp_alt.columns = ['datetime','year','latitude','longitude','thickness_m','thickness_m_standardized','site_name','source']
gtnp_alt = standardize_datetime_sort(gtnp_alt)
gtnp_alt = combine_source_columns(gtnp_alt, 'site_name', 'source')
gtnp_alt.columns = ['datetime','year','latitude','longitude','thickness_m','thickness_m_standardized','source']
gtnp_alt = standardize_datetime_sort(gtnp_alt)

alt = pd.concat([calm_above_neon_tundra_alt, derived_alt, gtnp_alt])
alt = standardize_datetime_sort(alt)
alt = add_season_column(alt)
alt = alt[['datetime','year','season','latitude','longitude','thickness_m','thickness_m_standardized','source']]
alt = standardize_datetime_sort(alt)
alt.to_csv('calm_above_neon_tundra_derivedalt_gtnpalt_insitu_df.csv', index=False)

ru_dst['source'] = ru_dst['station_name'].fillna('') + '_' + ru_dst['site_id'].astype(str).fillna('')
ru_dst['source'] = ru_dst['source'].str.strip('_')
mask = (ru_dst['station_name'].fillna('') == '') & (ru_dst['site_id'].astype(str).fillna('') == '')
ru_dst.loc[mask, 'source'] = None
ru_dst = ru_dst.drop(columns=['site_id','data_source','station_id','station_name'])
ru_dst = ru_dst[['datetime','year','season','latitude','longitude','soil_temp','soil_temp_standardized','soil_temp_depth','soil_temp_depth_zone','source']]

gtnp_st = pd.read_csv('ZC_data_standardized_gtnp_st_df.csv')
gtnp_st = standardize_datetime_sort(gtnp_st)
gtnp_st.loc[(gtnp_st.site_name.isna()) & (gtnp_st.dataset_id == 1246), 'site_name'] = 'Ny-Ålesund'
gtnp_st.loc[(gtnp_st.site_name.isna()) & (gtnp_st.dataset_id == 1041), 'site_name'] = 'Ny-Ålesund'
gtnp_st.sort_values('datetime').reset_index(drop=True).to_csv('ZC_data_standardized_gtnp_st_df.csv', index=False)
gtnp_st = pd.read_csv('ZC_data_standardized_gtnp_st_df.csv')
gtnp_st = standardize_datetime_sort(gtnp_st)
gtnp_st = gtnp_st[['datetime','year','season','latitude','longitude','temperature','temperature_standardized','depth_m','depth_zone','site_id','data_source']]
gtnp_st['source'] = gtnp_st['data_source'].fillna('') + '_' + gtnp_st['site_id'].astype(str).fillna('')
gtnp_st['source'] = gtnp_st['source'].str.strip('_')
mask = (gtnp_st['data_source'].fillna('') == '') & (gtnp_st['site_id'].astype(str).fillna('') == '')
gtnp_st.loc[mask, 'source'] = None
gtnp_st = gtnp_st.drop(columns=['site_id','data_source'])
gtnp_st.columns = ['datetime','year','season','latitude','longitude','soil_temp','soil_temp_standardized','soil_temp_depth','soil_temp_depth_zone','source']
gtnp_st = standardize_datetime_sort(gtnp_st)

tundrafielddb_st = pd.read_csv('tundrafieldobservations_st_insitu.csv')
tundrafielddb_st = standardize_datetime_sort(tundrafielddb_st)
tundrafielddb_st['year'] = pd.to_datetime(tundrafielddb_st.datetime).dt.year
tundrafielddb_st = add_season_column(tundrafielddb_st)
tundrafielddb_st = tundrafielddb_st[['datetime','year','season','lat','lon','st','source']]
tundrafielddb_st.columns = ['datetime','year','season','latitude','longitude','soil_temp','source']
tundrafielddb_st['soil_temp_depth'] = 0.1
tundrafielddb_st = add_depth_zones(tundrafielddb_st)
standardized_tundrafielddb_st = standardize_soil_temperature(tundrafielddb_st)
valid_tundrafielddb_st = standardized_tundrafielddb_st[standardized_tundrafielddb_st['temp_quality'] == 'valid']
valid_tundrafielddb_st = standardize_datetime_sort(valid_tundrafielddb_st)
tundrafielddb_st = valid_tundrafielddb_st
del valid_tundrafielddb_st
tundrafielddb_st.to_csv('ZC_data_standardized_tundrafielddb_st_df.csv', index=False)
tundrafielddb_st = tundrafielddb_st[['datetime','year','season','latitude','longitude','soil_temp','soil_temp_standardized','soil_temp_depth','soil_temp_depth_zone','source']]
tundrafielddb_st = standardize_datetime_sort(tundrafielddb_st)

ru_dst = standardize_datetime_sort(ru_dst)
gtnp_st = standardize_datetime_sort(gtnp_st)
tundrafielddb_st = standardize_datetime_sort(tundrafielddb_st)
temp = pd.concat([ru_dst, gtnp_st, tundrafielddb_st])
temp = standardize_datetime_sort(temp)
temp.to_csv('ru_dst_gtnp_st_tundrafielddb_st_insitu_df.csv', index=False)

tundrafielddb_smc = pd.read_csv('tundrafieldobservations_smc_insitu.csv')
tundrafielddb_smc = standardize_datetime_sort(tundrafielddb_smc)
tundrafielddb_smc['year'] = pd.to_datetime(tundrafielddb_smc.datetime).dt.year
tundrafielddb_smc = add_season_column(tundrafielddb_smc)
tundrafielddb_smc = tundrafielddb_smc[['datetime','year','season','lat','lon','smc','source']]
tundrafielddb_smc.columns = ['datetime','year','season','latitude','longitude','soil_moist','source']
tundrafielddb_smc = standardize_datetime_sort(tundrafielddb_smc)

neon = pd.read_csv('neon_vswc.csv')
neon['datetime'] = pd.to_datetime(neon['datetime'], format='mixed', errors='coerce')
neon['year'] = pd.to_datetime(neon.datetime).dt.year
neon = add_season_column(neon)
neon = neon[['datetime','year','season','latitude','longitude','vwc','site_id']]
neon.columns = ['datetime','year','season','latitude','longitude','soil_moist','source']
neon = standardize_datetime_sort(neon)

smc = pd.concat([tundrafielddb_smc, neon])
smc = standardize_datetime_sort(smc)
smc.to_csv('tundrafielddb_smc_neon_vwc_insitu_df.csv', index=False)

alt['soil_temp'] = None
alt['soil_temp_standardized'] = None
alt['soil_temp_depth'] = None
alt['soil_temp_depth_zone'] = None
temp['thickness_m'] = None
temp['thickness_m_standardized'] = None
smc['thickness_m'] = None
smc['thickness_m_standardized'] = None
smc['soil_temp'] = None
smc['soil_temp_standardized'] = None
smc['soil_temp_depth'] = None
smc['soil_temp_depth_zone'] = None
alt['soil_moist'] = None
temp['soil_moist'] = None
alt['data_type'] = 'active_layer'
temp['data_type'] = 'soil_temperature'
smc['data_type'] = 'soil_moisture'

columns = ['datetime', 'year', 'season', 'latitude', 'longitude', 'thickness_m', 'thickness_m_standardized',
           'soil_temp', 'soil_temp_standardized', 'soil_temp_depth', 'soil_temp_depth_zone',
           'soil_moist', 'source', 'data_type']

alt = alt[columns]
temp = temp[columns]
smc = smc[columns]

combined_df = pd.concat([alt, temp, smc], ignore_index=True)
combined_df = standardize_datetime_sort(combined_df)
print(f"Combined dataframe shape: {combined_df.shape}")
print(f"Records by data type:")
print(combined_df['data_type'].value_counts())
combined_df.to_csv('alt_temp_smc_withoutismncalmrfiles_combined_df.csv', index=False)

def extract_siberian_data(file_paths):
    all_data = []
    for file_path in file_paths:
        site_name = os.path.basename(file_path).split('_')[1]
        print(f"Processing {site_name} file...")
        try:
            xls = pd.ExcelFile(file_path)
            sheet_names = xls.sheet_names
            print(f"Available sheets: {sheet_names}")
            jan_sheet = next((s for s in sheet_names if 'jan' in s.lower()), None)
            jul_sheet = next((s for s in sheet_names if 'jul' in s.lower()), None)
            if not jan_sheet or not jul_sheet:
                print(f"Could not find January and July sheets in {file_path}")
                continue
            print(f"Using sheets: {jan_sheet} and {jul_sheet}")
            jan_metadata = pd.read_excel(file_path, sheet_name=jan_sheet, header=None, nrows=7, skiprows=2)
            site_coords = {
                'Yakutsk': (62.0, 129.7, "24959"),
                'Verkhoyansk': (67.5, 133.4, "24266"),
                'Pokrovsk': (61.5, 129.1, "24065"),
                'Isit': (61.0, 125.3, "24679"),
                'Churapcha': (62.0, 132.4, "24859")
            }
            default_wmo, default_lat, default_lon = "", 0.0, 0.0
            if site_name in site_coords:
                default_lat, default_lon, default_wmo = site_coords[site_name]
            try:
                wmo = str(jan_metadata.iloc[2, 1])
                if wmo == 'nan':
                    wmo = default_wmo
            except:
                wmo = default_wmo
            try:
                name_val = jan_metadata.iloc[3, 1]
                name = name_val.strip() if isinstance(name_val, str) else site_name
            except:
                name = site_name
            try:
                latitude = float(jan_metadata.iloc[4, 1])
                if np.isnan(latitude):
                    latitude = default_lat
            except:
                latitude = default_lat
            try:
                longitude = float(jan_metadata.iloc[5, 1])
                if np.isnan(longitude):
                    longitude = default_lon
            except:
                longitude = default_lon
            print(f"Metadata: WMO={wmo}, Name={name}, Lat={latitude}, Lon={longitude}")
            try:
                header_df = pd.read_excel(file_path, sheet_name=jan_sheet, skiprows=10, nrows=1)
                header_names = header_df.columns.tolist()
                print(f"Header names at skiprows=10: {header_names}")
                data_df = pd.read_excel(file_path, sheet_name=jan_sheet, skiprows=11)
                data_columns = data_df.columns.tolist()
                print(f"Data columns at skiprows=11: {data_columns}")
                column_mapping = {data_columns[i]: header_names[i] for i in range(min(len(header_names), len(data_columns)))}
                print(f"Column mapping: {column_mapping}")
                jan_data = data_df.rename(columns=column_mapping)
                jul_data = pd.read_excel(file_path, sheet_name=jul_sheet, skiprows=11).rename(columns=column_mapping)
                print(f"January columns after mapping: {jan_data.columns.tolist()}")
                print(f"First row after mapping: {jan_data.iloc[0].tolist()}")
                if 'YEAR' in jan_data.columns:
                    print(f"Found YEAR column with values: {jan_data['YEAR'].dropna().head(5).tolist()}")
                    jan_processed = process_monthly_data(jan_data, 1, site_name, wmo, name, latitude, longitude)
                    jul_processed = process_monthly_data(jul_data, 7, site_name, wmo, name, latitude, longitude)
                    site_data = pd.concat([jan_processed, jul_processed], ignore_index=True)
                    all_data.append(site_data)
                    print(f"Successfully processed {len(jan_processed)} January records and {len(jul_processed)} July records")
                    continue
                else:
                    print("YEAR column not found after mapping, trying alternative approach")
            except Exception as e:
                print(f"Error with two-pass approach: {str(e)}")
                for skiprows in range(9, 15):
                    try:
                        jan_data = pd.read_excel(file_path, sheet_name=jan_sheet, skiprows=skiprows)
                        jul_data = pd.read_excel(file_path, sheet_name=jul_sheet, skiprows=skiprows)
                        print(f"With skiprows={skiprows}:")
                        print(f"January columns: {jan_data.columns.tolist()}")
                        print(f"First row: {jan_data.iloc[0].tolist()}")
                        year_col = None
                        for col in jan_data.columns:
                            values = jan_data[col].dropna().head(5).tolist()
                            if values and all(isinstance(v, (int, float)) for v in values):
                                if all(1900 <= v <= 2025 for v in values if not np.isnan(v)):
                                    year_col = col
                                    print(f"Found year column: {col} with values: {values}")
                                    break
                        if year_col is not None:
                            jan_processed = process_monthly_data(jan_data, 1, site_name, wmo, name, latitude, longitude, year_column=year_col)
                            jul_processed = process_monthly_data(jul_data, 7, site_name, wmo, name, latitude, longitude, year_column=year_col)
                            site_data = pd.concat([jan_processed, jul_processed], ignore_index=True)
                            all_data.append(site_data)
                            print(f"Successfully processed {len(jan_processed)} January records and {len(jul_processed)} July records")
                            break
                    except Exception as e:
                        print(f"Error with skiprows={skiprows}: {str(e)}")
                        continue
        except Exception as e:
            import traceback
            print(f"Error processing {file_path}: {str(e)}")
            print(traceback.format_exc())
    if all_data:
        combined_data = pd.concat(all_data, ignore_index=True)
        print(f"Total combined records: {len(combined_data)}")
        return combined_data
    else:
        print("No data was processed successfully")
        return pd.DataFrame()

def process_monthly_data(df, month, site_name, wmo, name, latitude, longitude, year_column=None):
    data = df.copy()
    if year_column is not None and year_column != 'YEAR':
        data = data.rename(columns={year_column: 'YEAR'})
    if 'YEAR' not in data.columns:
        print("YEAR column not found. Data cannot be processed.")
        return pd.DataFrame()
    data = data.dropna(subset=['YEAR'])
    data['YEAR'] = data['YEAR'].astype(int)
    processed_records = []
    standard_depths = {
        '0.20 m': 0.2, '0.40 m': 0.4, '0.60 m ': 0.6, '0.60 m': 0.6,
        '0.80 m': 0.8, '1.20 m': 1.2, '1.60 m': 1.6, '2.00 m': 2.0,
        '2.40 m': 2.4, '3.20 m': 3.2
    }
    depth_columns = []
    depth_values = {}
    for col in data.columns:
        col_str = str(col)
        if col_str in standard_depths:
            depth_columns.append(col_str)
            depth_values[col_str] = standard_depths[col_str]
    surface_columns = [col for col in data.columns if 'surface' in str(col).lower() or 'air' in str(col).lower()]
    print(f"Found depth columns: {depth_columns}")
    print(f"Found surface columns: {surface_columns}")
    if not depth_columns and not surface_columns:
        print(f"No valid depth or surface columns found for {site_name}, month {month}")
        return pd.DataFrame()
    for _, row in data.iterrows():
        try:
            year = int(row['YEAR'])
            date_str = f"{year}-{month:02d}-01"
            for col in depth_columns:
                try:
                    value = row[col]
                    if value == -999 or pd.isna(value):
                        continue
                    record = {
                        'datetime': date_str, 'year': year, 'month': month, 'temperature': float(value),
                        'depth': depth_values[col], 'latitude': latitude, 'longitude': longitude,
                        'site_id': str(wmo), 'site_name': name,
                        'source': f"siberia_{site_name}_{'jan' if month == 1 else 'jul'}",
                        'depth_zone': assign_depth_zone(depth_values[col]),
                        'season': 'Winter' if month == 1 else 'Summer'
                    }
                    processed_records.append(record)
                except Exception as e:
                    print(f"Error processing depth column {col} for year {year}: {str(e)}")
                    continue
            for col in surface_columns:
                try:
                    value = row[col]
                    if value == -999 or pd.isna(value):
                        continue
                    record = {
                        'datetime': date_str, 'year': year, 'month': month, 'temperature': float(value),
                        'depth': 0.0, 'latitude': latitude, 'longitude': longitude,
                        'site_id': str(wmo), 'site_name': name,
                        'source': f"siberia_{site_name}_{'jan' if month == 1 else 'jul'}",
                        'depth_zone': 'shallow', 'season': 'Winter' if month == 1 else 'Summer'
                    }
                    processed_records.append(record)
                except Exception as e:
                    print(f"Error processing surface column {col} for year {year}: {str(e)}")
                    continue
        except Exception as e:
            print(f"Error processing row with YEAR {row.get('YEAR', 'unknown')}: {str(e)}")
            continue
    result_df = pd.DataFrame(processed_records)
    if result_df.empty:
        print(f"WARNING: No valid records found for {site_name}, month {month}")
    return result_df

def assign_depth_zone(depth):
    if depth <= 0.25:
        return 'shallow'
    elif depth <= 0.5:
        return 'intermediate'
    elif depth <= 1.0:
        return 'deep'
    else:
        return 'very_deep'

def get_file_paths(directory='/Users/bgay/Library/CloudStorage/OneDrive-[REDACTED_AFFILIATION]/zerocurtain/zerocurtain/siberia', pattern='HPIP_'):
    if not os.path.exists(directory):
        print(f"Directory does not exist: {directory}")
        return []
    try:
        all_files = os.listdir(directory)
        matching_files = [f for f in all_files if pattern in f]
        return [os.path.join(directory, name) for name in matching_files]
    except Exception as e:
        print(f"Error accessing directory {directory}: {str(e)}")
        return []

def flag_extreme_temperatures(df, low_threshold=-40, high_threshold=40):
    if df.empty:
        print("No data to flag for extreme temperatures")
        return df
    flagged_df = df.copy()
    flagged_df['extreme_low'] = flagged_df['temperature'] < low_threshold
    flagged_df['extreme_high'] = flagged_df['temperature'] > high_threshold
    flagged_df['is_extreme'] = flagged_df['extreme_low'] | flagged_df['extreme_high']
    extreme_count = flagged_df['is_extreme'].sum()
    if extreme_count > 0:
        print(f"Found {extreme_count} extreme temperature values:")
        print(f"  Low (<{low_threshold}°C): {flagged_df['extreme_low'].sum()}")
        print(f"  High (>{high_threshold}°C): {flagged_df['extreme_high'].sum()}")
        extreme_by_site = flagged_df[flagged_df['is_extreme']].groupby(['site_name', 'season'])['is_extreme'].count()
        print("\nExtreme values by site and season:")
        print(extreme_by_site)
    return flagged_df

def save_processed_data(df, output_file="siberian_permafrost_data.csv"):
    if df.empty:
        print("No data to save")
        return None
    df.to_csv(output_file, index=False)
    print(f"Data saved to {output_file}")
    return output_file

def process_siberian_permafrost_data():
    file_paths = get_file_paths()
    print(f"Found {len(file_paths)} files to process")
    if not file_paths:
        print("No files found. Please check directory path.")
        return pd.DataFrame()
    data = extract_siberian_data(file_paths)
    if not data.empty:
        flagged_data = flag_extreme_temperatures(data)
        save_processed_data(flagged_data)
        return flagged_data
    else:
        return pd.DataFrame()

def main():
    siberian_data = process_siberian_permafrost_data()
    if siberian_data.empty:
        print("No data processed. Exiting.")
        return
    print("\nSummary statistics:")
    print(siberian_data['temperature'].describe())
    try:
        verkhoyansk_extremes = siberian_data[(siberian_data['site_name'] == 'Verkhoyansk') & (siberian_data['is_extreme'])]
        print(f"\nVerkhoyansk extreme values: {len(verkhoyansk_extremes)}")
        if len(verkhoyansk_extremes) > 0:
            print(verkhoyansk_extremes[['temperature', 'depth', 'year', 'season']].head(10))
    except Exception as e:
        print(f"Error analyzing Verkhoyansk data: {str(e)}")

if __name__ == "__main__":
    main()

siberian = pd.read_csv('siberian_permafrost_data.csv')
siberian = standardize_datetime_sort(siberian)

source_to_name = {
    'siberia_Yakutsk_jan': 'Якутск', 'siberia_Yakutsk_jul': 'Якутск',
    'siberia_Pokrovsk_jan': 'Покровск', 'siberia_Pokrovsk_jul': 'Покровск',
    'siberia_Verkhoyansk_jan': 'Верхоянск', 'siberia_Verkhoyansk_jul': 'Верхоянск',
    'siberia_Isit_jan': 'Изит', 'siberia_Isit_jul': 'Изит',
    'siberia_Churapcha_jan': 'Чурапча', 'siberia_Churapcha_jul': 'Чурапча'
}

nan_count_before = siberian['source'].isna().sum()
print(f"NaN values in station_name before fixing: {nan_count_before}")
for source, name in source_to_name.items():
    mask = (siberian['site_name'] == source) & (siberian['source'].isna())
    siberian.loc[mask, 'source'] = name
nan_count_after = siberian['source'].isna().sum()
print(f"NaN values in station_name after fixing: {nan_count_after}")
print(f"Fixed {nan_count_before - nan_count_after} NaN values")

def combine_name_and_source(source):
    russian_name = source_to_name.get(source, '')
    return f"{russian_name}_{source}"

siberian['name_source_combined'] = siberian['source'].apply(combine_name_and_source)
siberian = add_season_column(siberian)

def standardize_siberian_temperature(siberian):
    std_df = siberian.copy()
    if 'temp_quality' not in std_df.columns:
        std_df['temp_quality'] = 'valid'
    impossible_mask = (std_df['temperature'] < -70) | (std_df['temperature'] > 50)
    std_df.loc[impossible_mask, 'temp_quality'] = 'physically_impossible'
    suspicious_mask = ((std_df['temperature'] < -60) | (std_df['temperature'] > 40)) & ~impossible_mask
    std_df.loc[suspicious_mask, 'temp_quality'] = 'suspicious'
    grouped = std_df.groupby(['source', 'datetime'])
    inconsistent_indices = []
    for _, group in grouped:
        if len(group) >= 2:
            sorted_group = group.sort_values('depth')
            is_winter = sorted_group['season'].iloc[0] in ['Winter', 'Fall']
            depths = sorted_group['depth'].values
            temps = sorted_group['temperature'].values
            for i in range(len(depths)-1):
                if depths[i+1] - depths[i] > 0:
                    temp_gradient = (temps[i+1] - temps[i]) / (depths[i+1] - depths[i])
                    if is_winter:
                        threshold = 25.0 if depths[i] < 1.0 else 15.0
                    else:
                        threshold = 20.0 if depths[i] < 1.0 else 10.0
                    if abs(temp_gradient) > threshold:
                        inconsistent_indices.extend(sorted_group.iloc[i:i+2].index.tolist())
    std_df.loc[inconsistent_indices, 'temp_quality'] = 'inconsistent_profile'
    total_records = len(std_df)
    valid_records = (std_df['temp_quality'] == 'valid').sum()
    print(f"Total records: {total_records}")
    print(f"Valid records: {valid_records} ({valid_records/total_records*100:.1f}%)")
    print(f"Invalid records: {total_records - valid_records} ({(total_records - valid_records)/total_records*100:.1f}%)")
    flag_counts = std_df['temp_quality'].value_counts()
    for flag, count in flag_counts.items():
        print(f"  {flag}: {count} ({count/total_records*100:.1f}%)")
    valid_temps = std_df.loc[std_df['temp_quality'] == 'valid', 'temperature']
    print(f"\nValid temperature range: {valid_temps.min():.1f}°C to {valid_temps.max():.1f}°C")
    print(f"Valid temperature mean: {valid_temps.mean():.1f}°C")
    std_df['temperature_standardized'] = std_df['temperature']
    return std_df

standardized_siberian = standardize_siberian_temperature(siberian)
valid_siberian = standardized_siberian[standardized_siberian['temp_quality'] == 'valid']
valid_siberian = standardize_datetime_sort(valid_siberian)
siberian = valid_siberian
del valid_siberian
siberian.to_csv('siberian_standardized_st_df.csv', index=False)
siberian = siberian[['datetime','year','season','latitude','longitude','temperature','temperature_standardized','depth','depth_zone','name_source_combined']]
siberian.columns = ['datetime','year','season','latitude','longitude','soil_temp','soil_temp_standardized','soil_temp_depth','soil_temp_depth_zone','source']
siberian['thickness_m'] = None
siberian['thickness_m_standardized'] = None
siberian['soil_moist'] = None
siberian['data_type'] = 'soil_temperature'
siberian = siberian[columns]
combined_df = pd.concat([combined_df, siberian], ignore_index=True)
combined_df = standardize_datetime_sort(combined_df)
print(f"Combined dataframe shape: {combined_df.shape}")
print(f"Records by data type:")
print(combined_df['data_type'].value_counts())
combined_df.to_csv('alt_temp_smc_siberian_withoutismncalmrfiles_combined_df.csv', index=False)

# ISMN
import pandas as pd
import numpy as np
import geopandas as gpd
from shapely.geometry import Point, box
from shapely.validation import make_valid
from shapely.strtree import STRtree
from datetime import datetime
import glob
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

def assign_season(month):
    if month in [12, 1, 2]:
        return "Winter"
    elif month in [3, 4, 5]:
        return "Spring"
    elif month in [6, 7, 8]:
        return "Summer"
    else:
        return "Fall"

def assign_depth_zone(depth):
    if pd.isna(depth):
        return None
    elif depth <= 0.2:
        return 'shallow'
    elif 0.2 < depth <= 0.5:
        return 'intermediate'
    elif 0.5 < depth <= 1.0:
        return 'deep'
    else:
        return 'very_deep'

def prepare_dataframe_for_parquet(df):
    df = df.copy()
    df['datetime'] = pd.to_datetime(df['datetime'])
    numeric_columns = ['latitude', 'longitude', 'alt', 'soil_temp_depth', 'soil_temp', 
                      'soil_moist_depth', 'soil_moist', 'soil_moist_std']
    for col in numeric_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    categorical_columns = ['season', 'site_id', 'soil_temp_depth_zone']
    for col in categorical_columns:
        if col in df.columns:
            df[col] = df[col].astype(str).replace('nan', None)
    return df

def process_ismn_data(filepath, latitude_threshold=49):
    ismn = pd.read_parquet(filepath)
    ismn.columns = ['datetime', 'latitude', 'longitude', 'source', 'depth_bottom_vwc', 'vwc']
    ismn.datetime = pd.to_datetime(ismn.datetime, format='mixed')
    ismn = ismn[ismn.latitude >= latitude_threshold]
    ismn = ismn[ismn['latitude'].notna()].sort_values('datetime').reset_index(drop=True)
    ismn = ismn[ismn.longitude.notna()].sort_values('datetime').reset_index(drop=True)
    
    month_list = [pd.to_datetime(ismn.datetime[i]).month for i in range(len(ismn))]
    ismn['season'] = [assign_season(month) for month in month_list]
    
    ismn_original = ismn.copy()
    ismn_filtered = ismn[(ismn.vwc >= 0) & (ismn.vwc <= 1.0)]
    filtered_count = len(ismn) - len(ismn_filtered)
    print(f"Filtered out {filtered_count} records ({filtered_count/len(ismn)*100:.2f}% of data)")
    
    percentage_values = ismn[(ismn.vwc > 1.0) & (ismn.vwc <= 100.0)]
    if len(percentage_values) > 0:
        print(f"Found {len(percentage_values)} values that appear to be percentages")
        ismn_corrected = ismn.copy()
        ismn_corrected.loc[(ismn_corrected.vwc > 1.0) & (ismn_corrected.vwc <= 100.0), 'vwc'] = \
            ismn_corrected.loc[(ismn_corrected.vwc > 1.0) & (ismn_corrected.vwc <= 100.0), 'vwc'] / 100.0
        ismn_filtered = ismn_corrected[(ismn_corrected.vwc >= 0) & (ismn_corrected.vwc <= 1.0)]
        recovered = len(ismn_filtered) - len(ismn[(ismn.vwc >= 0) & (ismn.vwc <= 1.0)])
        print(f"Recovered {recovered} values by converting percentages to fractions")
    
    print("\nOutlier analysis by source:")
    outliers = ismn[(ismn.vwc < 0) | (ismn.vwc > 100)]
    if len(outliers) > 0:
        source_counts = outliers.groupby('source').size().sort_values(ascending=False)
        print(source_counts.head(10))
    
    ismn_standardized = ismn.copy()
    ismn_standardized.loc[(ismn_standardized.vwc > 1.0) & (ismn_standardized.vwc <= 100.0), 'vwc'] = \
        ismn_standardized.loc[(ismn_standardized.vwc > 1.0) & (ismn_standardized.vwc <= 100.0), 'vwc'] / 100.0
    ismn_clean = ismn_standardized[(ismn_standardized.vwc >= 0) & (ismn_standardized.vwc <= 1.0)]
    
    problem_sources = outliers.source.unique().tolist()
    source_total_counts = ismn.groupby('source').size()
    source_outlier_counts = outliers.groupby('source').size()
    source_outlier_rates = source_outlier_counts / source_total_counts
    problematic_sources = source_outlier_rates[source_outlier_rates > 0.05].index.tolist()
    ismn_clean['quality_flag'] = 'good'
    ismn_clean.loc[ismn_clean['source'].isin(problematic_sources), 'quality_flag'] = 'questionable_source'
    ismn_clean = ismn_clean[ismn_clean.quality_flag == 'good']
    
    ismn_clean['year'] = ismn_clean.datetime.dt.year
    ismn_clean = ismn_clean[['datetime', 'year', 'season', 'latitude', 'longitude', 'vwc', 'depth_bottom_vwc', 'source']]
    ismn_clean.columns = ['datetime', 'year', 'season', 'latitude', 'longitude', 'soil_moist', 'soil_moist_depth', 'source']
    
    return ismn_clean.sort_values('datetime').reset_index(drop=True)

def merge_neon_above_datasets(neon_path, above_path):
    neon = pd.read_csv(neon_path)
    above = pd.read_csv(above_path)
    neon['datetime'] = pd.to_datetime(neon['datetime'], errors='coerce')
    above['datetime'] = pd.to_datetime(above['datetime'], errors='coerce')
    
    merge_keys = ['datetime', 'latitude', 'longitude', 'site_id']
    for col in merge_keys:
        if col not in neon.columns:
            neon[col] = None
        if col not in above.columns:
            above[col] = None
    
    merged_df = pd.merge(neon, above, on=merge_keys, how='outer', suffixes=('_neon', '_above'))
    numeric_columns = merged_df.select_dtypes(include=['number']).columns.tolist()
    for col in numeric_columns:
        if col.endswith('_neon') or col.endswith('_above'):
            base_col = col.rsplit('_', 1)[0]
            merged_df[base_col] = merged_df[[f"{base_col}_neon", f"{base_col}_above"]].mean(axis=1, skipna=True)
    
    drop_cols = [col for col in merged_df.columns if col.endswith('_neon') or col.endswith('_above')]
    merged_df.drop(columns=drop_cols, inplace=True)
    
    merged_df = merged_df[merged_df['latitude'].notna()].sort_values('datetime').reset_index(drop=True)
    merged_df = merged_df[merged_df.longitude.notna()].sort_values('datetime').reset_index(drop=True)
    
    month_list = [pd.to_datetime(merged_df.datetime[i]).month for i in range(len(merged_df))]
    merged_df['season'] = [assign_season(month) for month in month_list]
    merged_df['depth_zone'] = merged_df['depth_bottom_st'].apply(assign_depth_zone)
    merged_df.datetime = pd.to_datetime(merged_df.datetime, format='mixed')
    
    return merged_df

def final_merge(old_df, new_df):
    for df in [old_df, new_df]:
        df['datetime'] = pd.to_datetime(df['datetime'], format='mixed')
        df['latitude'] = pd.to_numeric(df['latitude']).round(6)
        df['longitude'] = pd.to_numeric(df['longitude']).round(6)
    
    base_columns = ['datetime', 'latitude', 'longitude', 'site_id', 'depth_zone', 'season', 'source', 'point_label']
    
    result = pd.concat([
        pd.DataFrame({
            **{col: old_df[col] for col in base_columns if col in old_df.columns},
            'depth': old_df['depth'],
            'temperature': old_df['temperature'],
            'thickness': old_df['thickness'] if 'thickness' in old_df.columns else np.nan,
            'vwc': np.nan,
            'vwc_std': np.nan,
            'depth_bottom_vwc': np.nan,
            'alt_instrument': np.nan,
            'water_table': np.nan
        }),
        pd.DataFrame({
            **{col: new_df[col] for col in base_columns if col in new_df.columns},
            'depth': new_df['depth_bottom_st'],
            'temperature': new_df['soil_temp'],
            'thickness': new_df['alt'] if 'alt' in new_df.columns else np.nan,
            'vwc': new_df['vwc'],
            'vwc_std': new_df['vwc_std'] if 'vwc_std' in new_df.columns else np.nan,
            'depth_bottom_vwc': new_df['depth_bottom_vwc'] if 'depth_bottom_vwc' in new_df.columns else np.nan,
            'alt_instrument': new_df['alt_instrument'] if 'alt_instrument' in new_df.columns else np.nan,
            'water_table': new_df['water_table'] if 'water_table' in new_df.columns else np.nan
        })
    ], ignore_index=True)
    
    result = result.sort_values(['datetime', 'latitude', 'longitude', 'depth'])
    
    temp_count = (old_df['temperature'].notna().sum() + new_df['soil_temp'].notna().sum())
    assert (result['temperature'].notna().sum() == temp_count), "Lost temperature measurements!"
    assert (result['vwc'].notna().sum() == new_df['vwc'].notna().sum()), "Lost VWC measurements!"
    
    return result

def verify_merge(merged_df, old_df, new_df):
    verification = {
        'Original Measurements': {
            'Temperature + Soil Temp': (old_df['temperature'].notna().sum() + new_df['soil_temp'].notna().sum()),
            'VWC': new_df['vwc'].notna().sum(),
            'Thickness/ALT': (old_df['thickness'].notna().sum() if 'thickness' in old_df.columns else 0) + 
                            (new_df['alt'].notna().sum() if 'alt' in new_df.columns else 0)
        },
        'Merged Measurements': {
            'Temperature': merged_df['temperature'].notna().sum(),
            'VWC': merged_df['vwc'].notna().sum(),
            'Thickness': merged_df['thickness'].notna().sum()
        },
        'Spatial Coverage': {
            'Unique Locations': merged_df[['latitude', 'longitude']].drop_duplicates().shape[0],
            'Unique Sites': merged_df['source'].nunique()
        },
        'Date Range': f"{merged_df['datetime'].min()} to {merged_df['datetime'].max()}"
    }
    return verification

def process_network(network_dir):
    print(f"Processing network: {os.path.basename(network_dir)}")
    stm_files = [f for f in glob.glob(os.path.join(network_dir, '**/*.stm'), recursive=True) if '_sm_' in f.lower()]
    if not stm_files:
        return None
    network_data = []
    for file in stm_files:
        try:
            with open(file, 'r') as f:
                header = f.readline().strip().split()
                if len(header) < 8:
                    continue
                lat = np.float32(header[3])
                lon = np.float32(header[4])
                depth_to = np.float32(header[7])
                site_id = os.path.basename(os.path.dirname(file))
                data = []
                for line in f:
                    if not line.strip():
                        continue
                    try:
                        parts = line.strip().split()
                        dt = datetime.strptime(parts[0], '%Y/%m/%d').date()
                        smc = np.float32(parts[2])
                        data.append([dt, lat, lon, site_id, depth_to, smc])
                    except:
                        continue
                if data:
                    df = pd.DataFrame(data, columns=['date', 'latitude', 'longitude', 'site_id', 'depth_to', 'smc'])
                    df['date'] = pd.to_datetime(df['date'])
                    network_data.append(df)
                    print(f"Processed {os.path.basename(file)}")
        except Exception as e:
            print(f"Error with file {file}: {str(e)}")
            continue
    if network_data:
        network_df = pd.concat(network_data, ignore_index=True)
        network_df = network_df.drop_duplicates()
        network_df = network_df.astype({
            'latitude': 'float32',
            'longitude': 'float32',
            'site_id': 'category',
            'depth_to': 'float32',
            'smc': 'float32'
        })
        network_name = os.path.basename(network_dir)
        output_file = f'ismn_{network_name.lower()}.parquet'
        network_df.to_parquet(output_file, compression='snappy')
        print(f"Saved {output_file}: {len(network_df):,} records")
        return output_file
    return None

def combine_network_files():
    print("Combining network files...")
    network_files = glob.glob('ismn_*.parquet')
    print(f"Found {len(network_files)} network files")
    all_networks = []
    for file in network_files:
        print(f"Reading {file}")
        df = pd.read_parquet(file)
        all_networks.append(df)
    print("Concatenating networks...")
    combined_df = pd.concat(all_networks, ignore_index=True)
    combined_df = combined_df.drop_duplicates()
    print("Saving combined ISMN data...")
    combined_df.to_parquet('ismn_all_networks.parquet', compression='snappy')
    print(f"Saved combined data: {len(combined_df):,} records")
    return combined_df

def filter_by_permafrost(shapefile_path, data_df):
    print("Reading permafrost shapefile...")
    permafrost_gdf = gpd.read_file(shapefile_path).to_crs(epsg=4326)
    permafrost_gdf["geometry"] = permafrost_gdf["geometry"].apply(lambda geom: make_valid(geom) if not geom.is_valid else geom)
    permafrost_gdf["geometry"] = permafrost_gdf["geometry"].buffer(0)
    
    print("Loading soil dataset...")
    soil_gdf = gpd.GeoDataFrame(data_df, geometry=gpd.points_from_xy(data_df.longitude, data_df.latitude), crs="EPSG:4326")
    
    conus_bounds = box(-125, 24, -66, 50)
    soil_gdf["within_CONUS"] = soil_gdf.geometry.within(conus_bounds)
    
    permafrost_polygons = permafrost_gdf.geometry.tolist()
    str_tree = STRtree(permafrost_polygons)
    
    def fast_permafrost_check(point):
        possible_matches = str_tree.query(point)
        return any(point.intersects(p) for p in possible_matches)
    
    soil_gdf["within_permafrost"] = [fast_permafrost_check(pt) for pt in soil_gdf.geometry]
    soil_gdf.loc[(soil_gdf["within_CONUS"]) & (~soil_gdf["within_permafrost"]), "soil_moist"] = float("nan")
    
    filtered_output = soil_gdf.drop(columns=["geometry", "within_CONUS", "within_permafrost"])
    return filtered_output

def main_processing_pipeline():
    combined_df = pd.read_csv('alt_temp_smc_siberian_withoutismncalmrfiles_combined_df.csv')
    ismn = process_ismn_data('ismn_all_networks.parquet')
    ismn.to_parquet('ismn_combined_data_final.parquet', compression='snappy')
    
    ismn['thickness_m'] = None
    ismn['thickness_m_standardized'] = None
    ismn['soil_temp'] = None
    ismn['soil_temp_standardized'] = None
    ismn['soil_temp_depth'] = None
    ismn['soil_temp_depth_zone'] = None
    combined_df['soil_moist_depth'] = None
    ismn['data_type'] = 'soil_moisture'
    
    columns = ['datetime', 'year', 'season', 'latitude', 'longitude', 'thickness_m', 'thickness_m_standardized',
               'soil_temp', 'soil_temp_standardized', 'soil_temp_depth', 'soil_temp_depth_zone',
               'soil_moist', 'soil_moist_depth', 'source', 'data_type']
    
    ismn = ismn[columns]
    combined_df = combined_df[columns]
    
    new_combined_df = pd.concat([combined_df, ismn], ignore_index=True)
    new_combined_df.datetime = pd.to_datetime(new_combined_df.datetime, format='mixed')
    new_combined_df = new_combined_df.sort_values('datetime').reset_index(drop=True)
    
    print(f"Combined dataframe shape: {new_combined_df.shape}")
    print(f"Records by data type:")
    print(new_combined_df['data_type'].value_counts())
    
    new_combined_df.to_csv('alt_temp_smc_siberian_ismn_withoutcalmrfiles_combined_df.csv', index=False)
    
    merged_df = merge_neon_above_datasets('neon_vswc.csv', 'above_smalt_simplifid.csv')
    merged_df.to_csv('merged_st_vwc_alt_data.csv', index=False)
    
    merged_old = pd.read_csv('merged_insitu_dataset.csv')
    merged_new = pd.read_csv('merged_st_vwc_alt_data.csv')
    merged_old.datetime = pd.to_datetime(merged_old.datetime, format='mixed')
    merged_new.datetime = pd.to_datetime(merged_new.datetime, format='mixed')
    merged_old.source = merged_old.site_id
    merged_new.source = merged_new.site_id
    
    final_merged = final_merge(merged_old, merged_new)
    verification = verify_merge(final_merged, merged_old, merged_new)
    
    print("\nVerification Results:")
    for category, stats in verification.items():
        print(f"\n{category}:")
        if isinstance(stats, dict):
            for key, value in stats.items():
                print(f"  {key}: {value:,}")
        else:
            print(f"  {stats}")
    
    final_merged.to_csv('merged_df_all_but_ismn.csv', index=False)
    
    processed_df = prepare_dataframe_for_parquet(new_combined_df)
    processed_df.to_parquet('combined_df.parquet', compression='snappy', engine='pyarrow', index=False)
    
    filtered_data = filter_by_permafrost('/Users/bgay/nsidc/permaice.shp', processed_df)
    filtered_data.datetime = pd.to_datetime(filtered_data.datetime, format='mixed')
    filtered_data = filtered_data[pd.to_datetime(filtered_data.datetime) <= '2024-12-31 00:00:00']
    filtered_data = filtered_data.sort_values(['datetime', 'latitude', 'longitude'])
    filtered_data.to_parquet('permafrost_measurements_filtered.parquet', compression='snappy')
    
    print("Processing complete!")

if __name__ == "__main__":
    main_processing_pipeline()

# Continue
import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt

def inspect_file_structure(file_path):
    xl = pd.ExcelFile(file_path)
    for sheet in xl.sheet_names:
        df = pd.read_excel(xl, sheet_name=sheet)
        print(f"\nSheet: {sheet}, Shape: {df.shape}")
        print("First 5 rows:")
        print(df.head())
        print("Column names:", df.columns.tolist())

def standardize_active_layer_thickness(df, thickness_col='alt', output_col='thickness_m_standardized'):
    result_df = df.copy()
    if 'standardization_note' not in result_df.columns:
        result_df['standardization_note'] = ""
    result_df[output_col] = np.nan
    negative_mask = result_df[thickness_col] < 0
    result_df.loc[negative_mask, output_col] = -1 * result_df.loc[negative_mask, thickness_col]
    result_df.loc[negative_mask, 'standardization_note'] = "Negative depth converted to positive thickness (meters)"
    mm_mask = (result_df[thickness_col] > 1000) & (~negative_mask)
    result_df.loc[mm_mask, output_col] = result_df.loc[mm_mask, thickness_col] / 1000
    result_df.loc[mm_mask, 'standardization_note'] = "Converted from mm to m"
    cm_mask = (result_df[thickness_col] >= 10) & (result_df[thickness_col] <= 1000) & (~negative_mask)
    result_df.loc[cm_mask, output_col] = result_df.loc[cm_mask, thickness_col] / 100
    result_df.loc[cm_mask, 'standardization_note'] = "Converted from cm to m"
    m_mask = (result_df[thickness_col] >= 0) & (result_df[thickness_col] < 10) & (~negative_mask)
    result_df.loc[m_mask, output_col] = result_df.loc[m_mask, thickness_col]
    result_df.loc[m_mask, 'standardization_note'] = "Already in meters"
    if 'thickness_m' in result_df.columns:
        result_df['discrepancy'] = np.abs(result_df[output_col] - result_df['thickness_m'])
        significant_diff = result_df['discrepancy'] > 0.01
        print(f"Found {significant_diff.sum()} rows with significant discrepancies")
        if significant_diff.sum() > 0:
            diagnostic_df = result_df.loc[significant_diff, [thickness_col, 'thickness_m', output_col, 'discrepancy', 'standardization_note', 'latitude', 'longitude', 'datetime', 'source']]
            diagnostic_df.sort_values('discrepancy', ascending=False).to_csv('thickness_standardization_issues.csv', index=False)
    return result_df

def enhanced_standardize_thickness(df, thickness_col='alt', output_col='thickness_m_standardized'):
    result_df = standardize_active_layer_thickness(df, thickness_col, output_col)
    conditions = [
        (result_df[output_col] > 4),
        (result_df['discrepancy'] > 0.1) if 'discrepancy' in result_df.columns else False,
        (result_df[output_col] > 2) & (result_df[output_col] <= 4),
        (result_df[output_col] <= 2)
    ]
    confidence_levels = ['low', 'low', 'medium', 'high']
    result_df['standardization_confidence'] = np.select(conditions, confidence_levels, default='high')
    result_df['is_physical_outlier'] = False
    result_df.loc[result_df[output_col] > 4, 'is_physical_outlier'] = True
    if 'latitude' in result_df.columns and 'longitude' in result_df.columns:
        result_df['lat_bin'] = np.floor(result_df['latitude'])
        result_df['lon_bin'] = np.floor(result_df['longitude'])
        spatial_groups = result_df.groupby(['lat_bin', 'lon_bin'])
        result_df['thickness_zscore'] = np.nan
        for (lat, lon), group in spatial_groups:
            if len(group) >= 5:
                bin_mean = group[output_col].mean()
                bin_std = group[output_col].std()
                if bin_std > 0:
                    idx = group.index
                    result_df.loc[idx, 'thickness_zscore'] = (result_df.loc[idx, output_col] - bin_mean) / bin_std
        result_df['is_spatial_outlier'] = False
        result_df.loc[abs(result_df['thickness_zscore']) > 3, 'is_spatial_outlier'] = True
    extreme_mask = (result_df[output_col] > 5) & (result_df['standardization_confidence'] == 'low')
    result_df.loc[extreme_mask, output_col] = 5.0
    result_df.loc[extreme_mask, 'standardization_note'] += " (Capped at 5m due to implausible value)"
    return result_df

def assign_season(month):
    if month in [12, 1, 2]:
        return "Winter"
    elif month in [3, 4, 5]:
        return "Spring"
    elif month in [6, 7, 8]:
        return "Summer"
    else:
        return "Fall"

if __name__ == "__main__":
    print("Initializing parser...")
    parser = ALTDataParser('/Users/bgay/Downloads/calm/alt')
    print("Processing files...")
    alt_data = parser.process_r_series(start_r=1, end_r=61)
    parser.print_validation_summary()
    print("\nData Quality Checks")
    print("=" * 50)
    print(f"Total rows in final dataset: {len(alt_data)}")
    print(f"Unique sites: {sorted(alt_data['site_id'].unique())}")
    print("\nDate range:")
    print(f"Earliest: {alt_data['datetime'].min()}")
    print(f"Latest: {alt_data['datetime'].max()}")
    output_path = Path('/Users/bgay/Downloads/calm/processed/CALM_ALT_master.csv')
    alt_data.to_csv(output_path, index=False)
    print(f"\nProcessed data saved to: {output_path}")

df = pd.read_csv('/Users/bgay/Downloads/calm/processed/CALM_ALT_master.csv')
df.alt = df.alt/100

df1 = pd.read_excel('/Users/bgay/Downloads/calm/alt/R18A_ALT_2003-2022.xls', sheet_name='data')
df1 = df1.iloc[4:-6,1:]
df1.columns = df1.iloc[0]
df1 = df1[1:].reset_index(drop=True).melt()
df1.columns=['datetime','alt']
df1.datetime=[pd.to_datetime(df1.datetime[i].split('-')[1],format='mixed') for i in range(len(df1.datetime))]
df1['site_id']='R18A'
df1.alt=df1.alt/100
df1.datetime = pd.to_datetime(df1.datetime)
df1 = df1.sort_values('datetime').reset_index(drop=True)
df1.to_csv('R18A_alt_data.csv', index=False)

df2 = pd.read_excel('/Users/bgay/Downloads/calm/alt/R38b_(Burn)_ALT_2003_2017.xls', sheet_name='data')
df2 = df2.iloc[4:-6,1:]
df2.columns = df2.iloc[0]
df2 = df2[1:].reset_index(drop=True).melt()
df2.columns=['datetime','alt']
df2['site_id']='R38B'
df2.alt=df2.alt/100
df2.datetime = pd.to_datetime(df2.datetime)
df2 = df2.sort_values('datetime').reset_index(drop=True)
df2.to_csv('R38B_alt_data.csv', index=False)

df3 = pd.read_excel('/Users/bgay/Downloads/calm/alt/R40_Igarka_alt_2008_2023.xlsx', sheet_name='data')
df3 = df3.iloc[:-6,6:].melt()
df3.columns=['datetime','alt']
df3['site_id']='R40'
df3.alt=df3.alt/100
df3.datetime = pd.to_datetime(df3.datetime)
df3 = df3.sort_values('datetime').reset_index(drop=True)
df3.to_csv('R40_alt_data.csv', index=False)

df4 = pd.read_excel('/Users/bgay/Downloads/calm/alt/R3_Marre-Sale_ALT_1995_2023.xls', sheet_name='ALT Data')
df4 = df4.iloc[:-6,7:-1].melt()
df4.columns=['datetime','alt']
df4['site_id']='R3'
df4 = df4[df4.alt.isna()!=True]
df4.alt = df4.alt.astype(float)/100
df4.datetime = pd.to_datetime(df4.datetime)
df4 = df4.sort_values('datetime').reset_index(drop=True)
df4.to_csv('R3_alt_data.csv', index=False)

df5_sheets = ['Skip VD-1(R5a)', 'Skip VD-2(R5b)', 'Skip VD-3(R5c)', 'PeatVD(R5d)']
df5_list = []
for sheet in df5_sheets:
    temp_df = pd.read_excel('/Users/bgay/Downloads/calm/alt/R5_alt_2007-2022.xlsx', sheet_name=sheet)
    temp_df = temp_df.iloc[:-6,1:]
    temp_df.columns = temp_df.iloc[0]
    temp_df = temp_df[1:].reset_index(drop=True).melt()
    temp_df.columns=['datetime','alt']
    temp_df['site_id']='R5'
    temp_df.alt = temp_df[temp_df.alt.isna()!=True].alt.astype(float)/100
    temp_df = temp_df[temp_df.alt.isna()!=True]
    temp_df.datetime = pd.to_datetime(temp_df.datetime,format='mixed')
    temp_df = temp_df.sort_values('datetime').reset_index(drop=True)
    df5_list.append(temp_df)
df5 = pd.concat(df5_list, ignore_index=True).sort_values('datetime').reset_index(drop=True)
df5.to_csv('R5_alt_data.csv', index=False)

df6 = pd.read_excel('/Users/bgay/Downloads/calm/alt/R6_labaz_lake.xls', sheet_name='lab_al94')
df6 = df6.iloc[:-11,:]
df6.columns = df6.iloc[0]
df6 = df6[1:].reset_index(drop=True).melt()
df6.columns=['datetime','alt']
df6['site_id']='R6'
df6.alt = df6[df6.alt.isna()!=True].alt.astype(float)/100
df6 = df6[df6.alt.isna()!=True]
df6.datetime = pd.to_datetime(df6.datetime,format='mixed')
df6 = df6.sort_values('datetime').reset_index(drop=True)
df6.to_csv('R6_alt_data.csv', index=False)

df = pd.concat([df,df1,df2,df3,df4,df5,df6],ignore_index=True)
df.datetime = pd.to_datetime(df.datetime, format='mixed')

standardized_df = enhanced_standardize_thickness(df)
print("\nSummary Statistics for Standardized Thickness (meters):")
print(standardized_df['thickness_m_standardized'].describe())

if 'thickness_m' in standardized_df.columns:
    print("\nComparison with existing thickness_m column:")
    print(f"Correlation: {standardized_df['thickness_m_standardized'].corr(standardized_df['thickness_m']):.4f}")
    print(f"Mean absolute difference: {standardized_df['discrepancy'].mean():.4f} m")
    print(f"Median absolute difference: {standardized_df['discrepancy'].median():.4f} m")
    print(f"Max absolute difference: {standardized_df['discrepancy'].max():.4f} m")
    if standardized_df['discrepancy'].max() > 0.01:
        source_groups = standardized_df.groupby('source')['discrepancy'].agg(['mean', 'max', 'count'])
        print("\nDiscrepancies by source:")
        print(source_groups.sort_values('mean', ascending=False))

standardized_df = standardized_df[standardized_df.standardization_confidence=='high']
standardized_df['year'] = standardized_df.datetime.dt.year
standardized_df.datetime = pd.to_datetime(standardized_df.datetime,format='mixed')
standardized_df = standardized_df.sort_values('datetime').reset_index(drop=True)
month_list = [pd.to_datetime(standardized_df.datetime[i]).month for i in range(len(standardized_df))]
seasons = [assign_season(month) for month in month_list]
standardized_df['season']=seasons

latlon = pd.read_excel('/Users/bgay/Downloads/calm/calm_siteid_sitename_lat_lon.xlsx', engine='openpyxl', index_col=0)
latlon = latlon.reset_index()
latlon.columns=['site_id','site_name','latitude','longitude']
latlon['clean_site_id'] = latlon['site_id'].str.replace(' ', '')

merged_df = standardized_df.merge(latlon[['clean_site_id', 'latitude', 'longitude']], left_on='site_id', right_on='clean_site_id', how='left')

missing_coords = pd.DataFrame([
    {'site_id': 'R29', 'latitude': 71.7855, 'longitude': 129.4192},
    {'site_id': 'R50A', 'latitude': 66.313250, 'longitude': 76.903478},
    {'site_id': 'R50B', 'latitude': 67.47791, 'longitude': 76.69529},
    {'site_id': 'R37', 'latitude': 69.98333, 'longitude': 153.5833},
    {'site_id': 'R54', 'latitude': 70.8893, 'longitude': 78.4171}
])

for _, row in missing_coords.iterrows():
    mask = (merged_df['site_id'] == row['site_id']) & (merged_df['latitude'].isna())
    merged_df.loc[mask, 'latitude'] = row['latitude']
    merged_df.loc[mask, 'longitude'] = row['longitude']

merged_df = merged_df[['datetime','year','season','latitude','longitude','alt','thickness_m_standardized','clean_site_id']]
merged_df.columns = ['datetime','year','season','latitude','longitude','thickness_m','thickness_m_standardized','source']
merged_df.datetime = pd.to_datetime(merged_df.datetime,format='mixed')
merged_df = merged_df.sort_values('datetime').reset_index(drop=True)
merged_df.to_csv('calm_rfiles_standardized_latlon_insitu_df.csv',index=False)

merged_df['soil_temp'] = None
merged_df['soil_temp_standardized'] = None
merged_df['soil_temp_depth'] = None
merged_df['soil_temp_depth_zone'] = None
merged_df['soil_moist'] = None
merged_df['soil_moist_depth'] = None
merged_df['data_type'] = 'active_layer_thickness'

columns = ['datetime', 'year', 'season', 'latitude', 'longitude', 'thickness_m', 'thickness_m_standardized', 'soil_temp', 'soil_temp_standardized', 'soil_temp_depth', 'soil_temp_depth_zone', 'soil_moist', 'source', 'data_type']
merged_df = merged_df[columns]

combined_df = pd.concat([combined_df, merged_df], ignore_index=True)
combined_df = combined_df.sort_values('datetime').reset_index(drop=True)
print(f"Combined dataframe shape: {combined_df.shape}")
print(f"Records by data type:")
print(combined_df['data_type'].value_counts())
combined_df.to_csv('alt_temp_smc_siberian_ismn_calmrfiles_combined_df.csv',index=False)

# Archived
import pandas as pd
import numpy as np
import re
from pathlib import Path
from datetime import datetime
import calendar
import warnings
warnings.filterwarnings('ignore')

column_standards = {
    'datetime': 'measurement_date',
    'site_id': 'site_identifier',
    'site_name': 'site_name',
    'latitude': 'lat',
    'longitude': 'lon',
    'alt': 'active_layer_thickness',
    'soil_temp_depth': 'temperature_measurement_depth',
    'soil_temp': 'soil_temperature',
    'soil_moist_depth': 'moisture_measurement_depth',
    'soil_moist': 'soil_moisture',
    'soil_temp_depth_zone': 'depth_classification',
    'season': 'measurement_season',
    'soil_moist_std': 'moisture_standard_deviation'
}

calm_rename = {
    'datetime': column_standards['datetime'],
    'site_id': column_standards['site_id'],
    'site_name': column_standards['site_name'],
    'latitude': column_standards['latitude'],
    'longitude': column_standards['longitude'],
    'alt': column_standards['alt'],
    'soil_temp_depth': column_standards['soil_temp_depth'],
    'soil_temp': column_standards['soil_temp'],
    'soil_moist_depth': column_standards['soil_moist_depth'],
    'soil_moist': column_standards['soil_moist']
}

alt_rename = {
    'datetime': column_standards['datetime'],
    'site_id': column_standards['site_id'],
    'latitude': column_standards['latitude'],
    'longitude': column_standards['longitude'],
    'alt': column_standards['alt']
}

combined_rename = {
    'datetime': column_standards['datetime'],
    'site_id': column_standards['site_id'],
    'latitude': column_standards['latitude'],
    'longitude': column_standards['longitude'],
    'alt': column_standards['alt'],
    'soil_temp_depth': column_standards['soil_temp_depth'],
    'soil_temp': column_standards['soil_temp'],
    'soil_moist_depth': column_standards['soil_moist_depth'],
    'soil_moist': column_standards['soil_moist'],
    'soil_temp_depth_zone': column_standards['soil_temp_depth_zone'],
    'season': column_standards['season'],
    'soil_moist_std': column_standards['soil_moist_std']
}

def standardize_columns(df, rename_dict):
    df_std = df.copy()
    existing_columns = {k: v for k, v in rename_dict.items() if k in df.columns}
    return df_std.rename(columns=existing_columns)

def merge_permafrost_datasets(calm, alt, combined):
    calm_std = standardize_columns(calm, calm_rename)
    alt_std = standardize_columns(alt, alt_rename)
    combined_std = standardize_columns(combined, combined_rename)
    
    date_col = column_standards['datetime']
    for df in [calm_std, alt_std, combined_std]:
        df[date_col] = pd.to_datetime(df[date_col])
    
    calm_std['source'] = 'CALM'
    alt_std['source'] = 'ALT'
    combined_std['source'] = 'Combined'
    
    merge_keys = [
        column_standards['datetime'],
        column_standards['site_id'],
        column_standards['latitude'],
        column_standards['longitude']
    ]
    
    print("Merging CALM and ALT datasets...")
    interim_merge = pd.merge(calm_std, alt_std, on=merge_keys, how='outer', suffixes=('_CALM', '_ALT'))
    
    print("Merging with combined dataset...")
    final_merge = pd.merge(interim_merge, combined_std, on=merge_keys, how='outer', suffixes=('', '_Combined'))
    
    measurement_columns = {
        'active_layer_thickness': column_standards['alt'],
        'soil_temperature': column_standards['soil_temp'],
        'temperature_depth': column_standards['soil_temp_depth'],
        'soil_moisture': column_standards['soil_moist'],
        'moisture_depth': column_standards['soil_moist_depth']
    }
    
    rename_dict = {}
    for col in final_merge.columns:
        if col in merge_keys:
            continue
        if any(col.endswith(suffix) for suffix in ['_CALM', '_ALT', '_Combined']):
            continue
        if col in final_merge.columns:
            base_name = col.split('_')[0]
            if col + '_CALM' in final_merge.columns:
                rename_dict[col] = col + '_Combined'
    
    final_merge = final_merge.rename(columns=rename_dict)
    final_merge = final_merge.sort_values([column_standards['datetime'], column_standards['site_id']])
    
    print(f"\nMerge complete! Final shape: {final_merge.shape}")
    
    source_counts = {}
    for col in final_merge.columns:
        if col not in merge_keys:
            non_null = final_merge[col].notna().sum()
            source = col.split('_')[-1] if '_' in col else 'Unknown'
            source_counts[source] = source_counts.get(source, 0) + non_null
    
    print("\nMeasurements by source:")
    for source, count in source_counts.items():
        print(f"{source}: {count:,} measurements")
    
    return final_merge

def optimize_permafrost_merge(calm_df, alt_df, combined_df):
    for df in [calm_df, alt_df, combined_df]:
        df['datetime'] = pd.to_datetime(df['datetime'])
    
    calm_cols = {
        'site_name': 'site_name',
        'alt': 'alt_calm',
        'soil_temp_depth': 'soil_temp_depth_calm',
        'soil_temp': 'soil_temp_calm',
        'soil_moist_depth': 'soil_moist_depth_calm',
        'soil_moist': 'soil_moist_calm'
    }
    
    alt_cols = {'alt': 'alt_gtnp'}
    
    combined_cols = {
        'soil_temp_depth': 'soil_temp_depth_combined',
        'soil_temp': 'soil_temp_combined',
        'soil_moist_depth': 'soil_moist_depth_combined',
        'soil_moist': 'soil_moist_combined',
        'soil_temp_depth_zone': 'soil_temp_depth_zone',
        'season': 'season'
    }
    
    calm_df = calm_df.rename(columns=calm_cols)
    alt_df = alt_df.rename(columns=alt_cols)
    combined_df = combined_df.rename(columns=combined_cols)
    
    merge_keys = ['datetime', 'site_id', 'latitude', 'longitude']
    interim_df = pd.merge(calm_df, alt_df, on=merge_keys, how='outer', indicator='merge_calm_alt')
    final_df = pd.merge(interim_df, combined_df, on=merge_keys, how='outer', indicator='merge_combined')
    
    conditions = [
        (final_df['merge_calm_alt'] == 'left_only') & (final_df['merge_combined'] == 'left_only'),
        (final_df['merge_calm_alt'] == 'right_only') & (final_df['merge_combined'] == 'left_only'),
        (final_df['merge_combined'] == 'right_only')
    ]
    choices = ['CALM', 'GTNP', 'Combined']
    final_df['data_source'] = np.select(conditions, choices, default='Multiple')
    
    final_df['alt'] = final_df['alt_calm'].fillna(final_df['alt_gtnp'])
    final_df['soil_temp'] = final_df['soil_temp_calm'].fillna(final_df['soil_temp_combined'])
    final_df['soil_temp_depth'] = final_df['soil_temp_depth_calm'].fillna(final_df['soil_temp_depth_combined'])
    final_df['soil_moist'] = final_df['soil_moist_calm'].fillna(final_df['soil_moist_combined'])
    final_df['soil_moist_depth'] = final_df['soil_moist_depth_calm'].fillna(final_df['soil_moist_depth_combined'])
    
    keep_cols = [
        'datetime', 'site_id', 'site_name', 'latitude', 'longitude',
        'alt', 'soil_temp', 'soil_temp_depth', 'soil_moist', 'soil_moist_depth',
        'soil_temp_depth_zone', 'season', 'data_source'
    ]
    
    final_df = final_df[keep_cols]
    final_df = final_df.sort_values(['datetime', 'site_id'])
    final_df = final_df.drop(['merge_calm_alt', 'merge_combined'], axis=1, errors='ignore')
    
    return final_df

def convert_date_format(date_str):
    match = re.match(r"(\d{2})\.(\d{2})\.(\d{2,4})", date_str)
    if match:
        month, day, year = match.groups()
        if len(year) == 2:
            year = f"20{year}"
        return f"{int(month)}/{int(day)}/{year}"
    return date_str

def reshape_soil_moisture_data(df):
    datetimes = []
    soil_moist_values = []
    
    for col in df.columns:
        date_str = col[2:]
        year = int(date_str[:2])
        month = int(date_str[2:4])
        day = int(date_str[4:])
        year = 1900 + year if year > 90 else 2000 + year
        date = datetime(year, month, day)
        dates = [date] * len(df[col])
        datetimes.extend(dates)
        soil_moist_values.extend(df[col].values)
    
    result_df = pd.DataFrame({
        'datetime': datetimes,
        'soil_moist': soil_moist_values
    })
    result_df = result_df.sort_values('datetime')
    return result_df

def add_latlon_to_alt_data(alt_data, latlon):
    result_df = alt_data.copy()
    result_df['latitude'] = None
    result_df['longitude'] = None
    
    latlon_dict = {}
    for _, row in latlon.iterrows():
        site_base = row['site_id'][:3]
        latlon_dict[site_base] = (row['latitude'], row['longitude'])
    
    result_df['site_base'] = result_df['site_id'].str[:3]
    
    def get_lat(site_base):
        return latlon_dict.get(site_base, (None, None))[0]
    
    def get_lon(site_base):
        return latlon_dict.get(site_base, (None, None))[1]
    
    result_df['latitude'] = result_df['site_base'].map(get_lat)
    result_df['longitude'] = result_df['site_base'].map(get_lon)
    result_df.drop('site_base', axis=1, inplace=True)
    
    unmatched_sites = result_df[result_df['latitude'].isna()]['site_id'].unique()
    if len(unmatched_sites) > 0:
        print(f"Warning: The following site_ids were not found in the latlon data: {unmatched_sites}")
    
    return result_df

class ALTDataParser:
    def __init__(self, base_dir: str):
        self.base_dir = Path(base_dir)
        self.min_year = 1950
        self.max_year = 2024
        self.debug_info = {}
        self.month_map = {month.lower(): index for index, month in enumerate(calendar.month_abbr[1:], 1)}
        self.month_map.update({month.lower(): index for index, month in enumerate(calendar.month_name[1:], 1)})
        self.special_handlers = {
            'R54': self.handle_r54_format,
            'R23': self.handle_r23_format,
            'R32': self.handle_r32_format,
            'R38': self.handle_r38_format,
            'R18A': self.handle_r38_format,
            'R40': self.handle_r40_format,
            'R56': self.handle_r56_r57_r58_format,
            'R57': self.handle_r56_r57_r58_format,
            'R58': self.handle_r56_r57_r58_format,
            'R24': self.handle_r24_format
        }

    def extract_year_from_filename(self, filename: str) -> tuple:
        range_patterns = [
            r'(?:19|20)?(\d{2})[-_](?:19|20)?(\d{2})',
            r'(\d{4})[-_](\d{4})',
            r'(\d{4})[-_](\d{2})',
        ]
        
        for pattern in range_patterns:
            if match := re.search(pattern, filename):
                start_year = int(match.group(1))
                end_year = int(match.group(2))
                if start_year < 100:
                    start_year = 2000 + start_year if start_year < 50 else 1900 + start_year
                if end_year < 100:
                    end_year = 2000 + end_year if end_year < 50 else 1900 + end_year
                return start_year, end_year
        
        year_matches = re.findall(r'(?:19|20)?(\d{2}|\d{4})', filename)
        if year_matches:
            years = []
            for year in year_matches:
                year = int(year)
                if year < 100:
                    year = 2000 + year if year < 50 else 1900 + year
                if self.min_year <= year <= self.max_year:
                    years.append(year)
            if years:
                return min(years), max(years)
        return None, None

    def identify_alt_columns(self, df: pd.DataFrame, filename: str) -> dict:
        alt_patterns = [
            r'al-', r'al-\*', r'alt_', r'alt\(cm\)', r'thaw depth',
            r'TD\d{6}', r'alt', r'ALT', r'thaw', r'al\(cm\)'
        ]
        exclusion_patterns = ['temp', 'temperature', 'moisture', 'moist', 'precip', 'rain', 'snow', 'depth_to']
        file_start_year, file_end_year = self.extract_year_from_filename(filename)
        alt_cols_with_dates = {}
        
        for col in df.columns:
            col_str = str(col).lower()
            if any(term in col_str for term in exclusion_patterns):
                continue
            if any(re.search(pattern, col_str, re.IGNORECASE) for pattern in alt_patterns):
                year_match = re.search(r'(?:19|20)?(\d{2})', col_str)
                if year_match:
                    year = self.convert_two_digit_year(int(year_match.group(1)))
                    if self.min_year <= year <= self.max_year:
                        alt_cols_with_dates[col] = f"{year}-07-15"
                elif file_start_year and file_end_year:
                    alt_cols_with_dates[col] = f"{file_start_year}-07-15"
        return alt_cols_with_dates

    def convert_two_digit_year(self, year: int) -> int:
        if year < 100:
            return 2000 + year if year < 50 else 1900 + year
        return year

    def parse_complex_date(self, date_str: str) -> str:
        try:
            date_str = str(date_str).lower().strip()
            if re.match(r'\d{1,2}/\d{1,2}/\d{2}', date_str):
                month, day, year = map(int, date_str.split('/'))
                year = 2000 + year if year < 50 else 1900 + year
                return f"{year:04d}-{month:02d}-{day:02d}"
            match = re.match(r'(\d{1,2})-([a-zA-Z]{3})-(\d{2})', date_str)
            if match:
                day, month_str, year = match.groups()
                month = self.month_map.get(month_str.lower())
                if month:
                    year = 2000 + int(year) if int(year) < 50 else 1900 + int(year)
                    return f"{year:04d}-{month:02d}-{int(day):02d}"
            match = re.match(r'(\d{4}),\s*([a-zA-Z]{3,})\.?\s*(\d{1,2})', date_str)
            if match:
                year, month_str, day = match.groups()
                month = self.month_map.get(month_str.lower())
                if month:
                    return f"{year}-{month:02d}-{int(day):02d}"
            if re.match(r'\d{8}', date_str):
                year = int(date_str[:4])
                month = int(date_str[4:6])
                day = int(date_str[6:8])
                return f"{year}-{month:02d}-{day:02d}"
            if re.match(r'\d{4}', date_str):
                year = int(date_str)
                return f"{year}-07-15"
        except (ValueError, AttributeError, KeyError) as e:
            print(f"Error parsing date {date_str}: {str(e)}")
            return None
        return None

    def validate_date(self, year: int, month: int, day: int) -> bool:
        try:
            if self.min_year <= year <= self.max_year and 1 <= month <= 12 and 1 <= day <= 31:
                datetime(year, month, day)
                return True
        except ValueError:
            return False
        return False

    def handle_r54_format(self, df: pd.DataFrame, sheet_name: str) -> pd.DataFrame:
        plot = sheet_name[-1] if sheet_name.endswith(('A', 'B', 'C', 'D')) else ''
        site_id = f"R54{plot}" if plot else "R54"
        date_cols = [col for col in df.columns if isinstance(col, str) and '/' in col]
        dfs = []
        for date_col in date_cols:
            date_str = self.parse_complex_date(date_col)
            if date_str:
                temp_df = pd.DataFrame({
                    'datetime': date_str,
                    'site_id': site_id,
                    'alt': pd.to_numeric(df[date_col], errors='coerce').dropna()
                })
                dfs.append(temp_df)
        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

    def handle_r23_format(self, df: pd.DataFrame, **kwargs) -> pd.DataFrame:
        dfs = []
        td_cols = [col for col in df.columns if isinstance(col, str) and col.startswith('TD') and len(col) == 8 and col[2:].isdigit()]
        for col in td_cols:
            date_str = col[2:]
            try:
                yy = int(date_str[:2])
                mm = int(date_str[2:4])
                dd = int(date_str[4:])
                year = 2000 + yy if yy < 50 else 1900 + yy
                date_str = f"{year:04d}-{mm:02d}-{dd:02d}"
                alt_values = pd.to_numeric(df[col], errors='coerce')
                valid_values = alt_values.dropna()
                if not valid_values.empty:
                    temp_df = pd.DataFrame({'datetime': date_str, 'site_id': 'R23', 'alt': valid_values})
                    dfs.append(temp_df)
            except (ValueError, IndexError) as e:
                print(f"Error processing column {col}: {str(e)}")
                continue
        if dfs:
            return pd.concat(dfs, ignore_index=True)
        return pd.DataFrame()

    def handle_r32_format(self, df: pd.DataFrame, **kwargs) -> pd.DataFrame:
        dfs = []
        date_cols = [col for col in df.columns if isinstance(col, (str, int)) and str(col).isdigit() and len(str(col)) == 8]
        for col in date_cols:
            try:
                date_str = str(col)
                year = int(date_str[:4])
                month = int(date_str[4:6])
                day = int(date_str[6:])
                if self.validate_date(year, month, day):
                    date_str = f"{year:04d}-{month:02d}-{day:02d}"
                    alt_values = pd.to_numeric(df[col], errors='coerce')
                    valid_values = alt_values.dropna()
                    if not valid_values.empty:
                        temp_df = pd.DataFrame({'datetime': date_str, 'site_id': 'R32', 'alt': valid_values})
                        dfs.append(temp_df)
            except (ValueError, IndexError) as e:
                print(f"Error processing column {col}: {str(e)}")
                continue
        if dfs:
            return pd.concat(dfs, ignore_index=True)
        return pd.DataFrame()

    def handle_r38_format(self, df: pd.DataFrame, **kwargs) -> pd.DataFrame:
        try:
            header_row = None
            for i in range(min(10, len(df))):
                if df.iloc[i].astype(str).str.contains('al\(cm\)|alt\(cm\)').any():
                    header_row = i
                    break
            if header_row is None:
                return pd.DataFrame()
            headers = df.iloc[header_row]
            data = df.iloc[header_row + 1:].reset_index(drop=True)
            df_cleaned = pd.DataFrame(data.values, columns=headers)
            date_patterns = [
                (r'alt\(cm\)-(\d{1,2})/(\d{1,2})/(\d{4})', 'MDY'),
                (r'al\(cm\)-(\d{1,2})/(\d{1,2})/(\d{2})', 'MDY'),
                (r'al\(cm\)-(\d{1,2})/(\d{1,2})/(\d{4})', 'MDY')
            ]
            dfs = []
            for col in df_cleaned.columns:
                col_str = str(col).strip()
                if pd.isna(col_str) or col_str == '' or col_str == '-':
                    continue
                date_str = None
                for pattern, date_format in date_patterns:
                    if match := re.search(pattern, col_str, re.IGNORECASE):
                        try:
                            if date_format == 'MDY':
                                month, day, year = map(int, match.groups())
                                if year < 100:
                                    year = 2000 + year if year < 50 else 1900 + year
                                if self.validate_date(year, month, day):
                                    date_str = f"{year:04d}-{month:02d}-{day:02d}"
                                    break
                        except (ValueError, TypeError, AttributeError) as e:
                            print(f"Error parsing date from column {col}: {str(e)}")
                            continue
                if date_str:
                    alt_values = df_cleaned[col].replace(['no data', '-'], np.nan)
                    alt_values = alt_values.apply(lambda x: str(x).strip('-') if isinstance(x, str) else x)
                    alt_values = pd.to_numeric(alt_values, errors='coerce')
                    valid_values = alt_values.dropna()
                    if not valid_values.empty:
                        temp_df = pd.DataFrame({'datetime': date_str, 'site_id': kwargs.get('site_id', 'R38'), 'alt': valid_values})
                        dfs.append(temp_df)
            if dfs:
                result_df = pd.concat(dfs, ignore_index=True)
                print(f"Successfully extracted {len(result_df)} measurements for {kwargs.get('site_id', 'R38')}")
                return result_df
        except Exception as e:
            print(f"Error processing R38 format: {str(e)}")
        return pd.DataFrame()

    def handle_r40_format(self, df: pd.DataFrame, **kwargs) -> pd.DataFrame:
        if 'datetime' not in df.columns:
            return pd.DataFrame()
        df['datetime'] = df['datetime'].apply(lambda x: f"{str(x)[:4]}-{str(x)[4:6]}-{str(x)[6:8]}" if len(str(x)) == 8 else None)
        alt_cols = [col for col in df.columns if isinstance(col, str) and 'alt' in col.lower()]
        if not alt_cols:
            return pd.DataFrame()
        df['alt'] = pd.to_numeric(df[alt_cols[0]], errors='coerce')
        df['site_id'] = 'R40'
        return df[['datetime', 'site_id', 'alt']].dropna()

    def handle_r56_r57_r58_format(self, df: pd.DataFrame, **kwargs) -> pd.DataFrame:
        try:
            header_row = None
            for i in range(min(10, len(df))):
                if df.iloc[i].astype(str).str.contains(r'\d{1,2}/\d{1,2}/\d{2}').any():
                    header_row = i
                    break
            if header_row is None:
                return pd.DataFrame()
            headers = df.iloc[header_row]
            data = df.iloc[header_row + 1:].reset_index(drop=True)
            df_cleaned = pd.DataFrame(data.values, columns=headers)
            dfs = []
            for col in df_cleaned.columns:
                col_str = str(col).strip()
                if pd.isna(col_str) or col_str == '':
                    continue
                if match := re.match(r'(\d{1,2})/(\d{1,2})/(\d{2})', col_str):
                    try:
                        month, day, year = map(int, match.groups())
                        year = 2000 + year if year < 50 else 1900 + year
                        if self.validate_date(year, month, day):
                            date_str = f"{year:04d}-{month:02d}-{day:02d}"
                            alt_values = df_cleaned[col].replace(['', '-'], np.nan)
                            alt_values = pd.to_numeric(alt_values, errors='coerce')
                            valid_values = alt_values.dropna()
                            if not valid_values.empty:
                                temp_df = pd.DataFrame({'datetime': date_str, 'site_id': kwargs.get('site_id', 'R56'), 'alt': valid_values})
                                dfs.append(temp_df)
                    except (ValueError, TypeError) as e:
                        print(f"Error parsing date from column {col}: {str(e)}")
                        continue
            if dfs:
                result_df = pd.concat(dfs, ignore_index=True)
                print(f"Successfully extracted {len(result_df)} measurements for {kwargs.get('site_id', 'R56')}")
                return result_df
        except Exception as e:
            print(f"Error processing simple date format: {str(e)}")
        return pd.DataFrame()

    def handle_r24_format(self, df: pd.DataFrame, **kwargs) -> pd.DataFrame:
        dfs = []
        for col in df.columns:
            if not isinstance(col, str):
                continue
            date_str = None
            if re.match(r'\d{1,2}/\d{1,2}/\d{2}', col):
                date_str = self.parse_complex_date(col)
            elif re.match(r'\d{4},\s*[a-zA-Z]{3,}\.?\s*\d{1,2}', col):
                date_str = self.parse_complex_date(col)
            if date_str:
                temp_df = pd.DataFrame({'datetime': date_str, 'site_id': 'R24', 'alt': pd.to_numeric(df[col], errors='coerce').dropna()})
                dfs.append(temp_df)
        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

    def process_file(self, file_path: Path) -> pd.DataFrame:
        try:
            site_id_match = re.match(r'(R\d+[A-Za-z]?)', file_path.stem, re.IGNORECASE)
            if not site_id_match:
                return pd.DataFrame()
            site_id = site_id_match.group(1).upper()
            base_site = re.match(r'(R\d+)', site_id).group(1)
            self.debug_info[file_path.name] = {'site_id': site_id, 'sheets_processed': [], 'data_points': 0, 'processing_method': 'default'}
            special_handler = self.special_handlers.get(base_site)
            xl = pd.ExcelFile(file_path)
            dfs = []
            for sheet in xl.sheet_names:
                try:
                    df = pd.read_excel(xl, sheet_name=sheet)
                    if df.empty:
                        continue
                    self.debug_info[file_path.name]['sheets_processed'].append(sheet)
                    result_df = pd.DataFrame()
                    if special_handler:
                        try:
                            self.debug_info[file_path.name]['processing_method'] = f'special_{base_site}'
                            result_df = special_handler(df, sheet_name=sheet)
                        except Exception as e:
                            print(f"Special handler failed for {file_path.name}, sheet {sheet}: {str(e)}")
                            result_df = pd.DataFrame()
                    if result_df.empty:
                        self.debug_info[file_path.name]['processing_method'] = 'default'
                        result_df = self.default_process(df, file_path.name, site_id)
                    if not result_df.empty:
                        dfs.append(result_df)
                except Exception as e:
                    print(f"Error processing sheet {sheet} in {file_path.name}: {str(e)}")
            if dfs:
                final_df = pd.concat(dfs, ignore_index=True)
                self.debug_info[file_path.name]['data_points'] = len(final_df)
                return final_df
        except Exception as e:
            print(f"Error processing {file_path}: {str(e)}")
        return pd.DataFrame()

    def default_process(self, df: pd.DataFrame, filename: str, site_id: str) -> pd.DataFrame:
        dfs = []
        df.columns = [str(col).strip() for col in df.columns]
        alt_cols_with_dates = self.identify_alt_columns(df, filename)
        for col, date_str in alt_cols_with_dates.items():
            if date_str:
                alt_values = pd.to_numeric(df[col], errors='coerce')
                valid_values = alt_values.dropna()
                if not valid_values.empty:
                    temp_df = pd.DataFrame({'datetime': date_str, 'site_id': site_id, 'alt': valid_values})
                    dfs.append(temp_df)
        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

    def process_r_series(self, start_r: int = 11, end_r: int = 61) -> pd.DataFrame:
        combined_data = []
        processed_files = set()
        for file_path in self.base_dir.glob('**/*'):
            if ('alt' in file_path.name.lower() or 'ALT' in file_path.name) and file_path.suffix.lower() in ['.xls', '.xlsx']:
                site_id_match = re.search(r'R(\d+)', file_path.stem, re.IGNORECASE)
                if site_id_match:
                    site_num = int(site_id_match.group(1))
                    if start_r <= site_num <= end_r:
                        file_key = (site_num, file_path.stem)
                        if file_key not in processed_files:
                            print(f"Processing {file_path.name}...")
                            df = self.process_file(file_path)
                            if not df.empty:
                                combined_data.append(df)
                            processed_files.add(file_key)
        if combined_data:
            final_df = pd.concat(combined_data, ignore_index=True)
            final_df['datetime'] = pd.to_datetime(final_df['datetime'])
            final_df = final_df[(final_df['datetime'].dt.year >= self.min_year) & (final_df['datetime'].dt.year <= self.max_year)]
            final_df = final_df.sort_values(['site_id', 'datetime'])
            final_df = final_df.drop_duplicates(['site_id', 'datetime', 'alt'])
            return final_df
        return pd.DataFrame()

    def print_debug_summary(self):
        print("\nProcessing Summary:")
        print("-" * 50)
        total_data_points = 0
        sites_with_data = []
        processing_methods = {}
        for filename, info in self.debug_info.items():
            print(f"\nFile: {filename}")
            print(f"Site ID: {info['site_id']}")
            print(f"Sheets processed: {len(info['sheets_processed'])}")
            print(f"Processing method: {info['processing_method']}")
            print(f"Data points extracted: {info['data_points']}")
            total_data_points += info['data_points']
            if info['data_points'] > 0:
                sites_with_data.append(info['site_id'])
                processing_methods[info['site_id']] = info['processing_method']
        print("\nOverall Summary:")
        print("-" * 50)
        print(f"Total data points extracted: {total_data_points}")
        print(f"Total sites with data: {len(set(sites_with_data))}")
        print("Sites with data and their processing methods:")
        for site in sorted(set(sites_with_data)):
            print(f"  {site}: {processing_methods[site]}")

if __name__ == "__main__":
    final_dataset = merge_permafrost_datasets(calm, alt, combined)
    final_dataset.to_parquet("insitu_combined_data_final_df.parquet", engine="pyarrow")

# # data_loader.py
# import os
# import pandas as pd
# import numpy as np
# import pyarrow as pa
# import pyarrow.feather as pf
# import pyarrow.compute as pc
# import pyarrow.dataset as ds
# import gc
# from tqdm.auto import tqdm

# def get_time_range(feather_path):
#     """Get min and max datetime from feather file efficiently"""
#     print("Determining dataset time range")
#     try:
#         # Use PyArrow for efficient metadata access
#         table = pf.read_table(feather_path, columns=['datetime'])
#         datetime_col = table['datetime']
#         min_date = pd.Timestamp(pc.min(datetime_col).as_py())
#         max_date = pd.Timestamp(pc.max(datetime_col).as_py())
#         del table, datetime_col
#         gc.collect()
#     except Exception as e:
#         print(f"Error with PyArrow min/max: {str(e)}")
#         print("Falling back to reading sample rows...")
        
#         # Read just first and last rows after sorting
#         # This is more efficient than reading all data
#         try:
#             dataset = ds.dataset(feather_path, format='feather')
            
#             # Get min date from sorted data (first row)
#             table_min = dataset.to_table(
#                 columns=['datetime'],
#                 filter=None,
#                 use_threads=True,
#                 limit=1
#             )
#             min_date = pd.Timestamp(table_min['datetime'][0].as_py())
            
#             # Get max date from reverse sorted data (first row)
#             # Note: PyArrow Dataset API doesn't support reverse sort directly
#             # So we read all datetime values and find max
#             table_all = dataset.to_table(columns=['datetime'])
#             max_date = pd.Timestamp(pc.max(table_all['datetime']).as_py())
            
#             del table_min, table_all, dataset
#             gc.collect()
#         except Exception as inner_e:
#             print(f"Error with optimized approach: {str(inner_e)}")
#             print("Using full scan method (slow)...")
            
#             # Last resort: read all timestamps in chunks
#             min_date = pd.Timestamp.max
#             max_date = pd.Timestamp.min
            
#             # Open file directly to avoid loading everything
#             table = pf.read_table(feather_path, columns=['datetime'])
#             datetime_values = table['datetime'].to_pandas()
            
#             min_date = datetime_values.min()
#             max_date = datetime_values.max()
            
#             del datetime_values, table
#             gc.collect()
    
#     print(f"Data timespan: {min_date} to {max_date}")
#     return min_date, max_date

# def get_unique_site_depths(feather_path):
#     """Get unique site-depth combinations efficiently"""
#     print("Finding unique site-depth combinations")
#     print(f"Memory before processing: {memory_usage():.1f} MB")
    
#     try:
#         # Use PyArrow for efficient operation
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Read only the columns we need
#         table = dataset.to_table(columns=['source', 'soil_temp_depth'])
        
#         # Convert to pandas
#         df = table.to_pandas()
        
#         # Get valid combinations (where temp_depth is not NaN)
#         valid_df = df.dropna(subset=['soil_temp_depth'])
        
#         # Get unique combinations
#         site_depths = valid_df.drop_duplicates(['source', 'soil_temp_depth'])
        
#         # Clean up
#         del table, df, valid_df, dataset
#         gc.collect()
        
#     except Exception as e:
#         print(f"Error with PyArrow approach: {str(e)}")
#         print("Falling back to chunked pandas approach...")
        
#         # Fallback: Process in chunks
#         chunk_size = 1000000
#         unique_combos = set()
        
#         # Read feather in chunks (this is slower but more robust)
#         with pd.read_feather(feather_path, columns=['source', 'soil_temp_depth']) as reader:
#             for chunk in reader:
#                 # Get valid rows and unique combinations
#                 valid_rows = chunk.dropna(subset=['soil_temp_depth'])
#                 chunk_combinations = set(zip(valid_rows['source'], valid_rows['soil_temp_depth']))
#                 unique_combos.update(chunk_combinations)
                
#                 # Clean up
#                 del chunk, valid_rows, chunk_combinations
#                 gc.collect()
        
#         # Convert to DataFrame
#         site_depths = pd.DataFrame(list(unique_combos), columns=['source', 'soil_temp_depth'])
    
#     print(f"Found {len(site_depths)} unique site-depth combinations")
#     print(f"Memory after processing: {memory_usage():.1f} MB")
    
#     return site_depths[['source', 'soil_temp_depth']]

# def load_site_depth_data(feather_path, site, temp_depth):
#     """Load ONLY data for a specific site and depth using PyArrow filtering"""
#     print(f"Loading data for site: {site}, depth: {temp_depth}")
#     print(f"Memory before loading: {memory_usage():.1f} MB")
    
#     try:
#         # Use PyArrow Dataset API for efficient filtering
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Create filter expressions
#         site_filter = ds.field('source') == site
#         depth_filter = ds.field('soil_temp_depth') == float(temp_depth)
#         combined_filter = site_filter & depth_filter
        
#         # Apply filter and read only filtered data
#         table = dataset.to_table(filter=combined_filter)
#         filtered_df = table.to_pandas()
        
#         # Clean up
#         del table, dataset
#         gc.collect()
        
#     except Exception as e:
#         print(f"Error with PyArrow filtering: {str(e)}")
#         print("Falling back to pandas filtering...")
        
#         # Define chunk size based on available memory
#         available_mem = psutil.virtual_memory().available / (1024**2)  # MB
#         chunk_size = min(500000, max(100000, int(available_mem / 10)))
        
#         filtered_chunks = []
        
#         # Read and filter in chunks
#         for chunk in pd.read_feather(feather_path, columns=None, chunksize=chunk_size):
#             # Filter by site and depth
#             chunk_filtered = chunk[(chunk['source'] == site) & 
#                                    (chunk['soil_temp_depth'] == temp_depth)]
            
#             if len(chunk_filtered) > 0:
#                 filtered_chunks.append(chunk_filtered)
            
#             # Clean up
#             del chunk, chunk_filtered
#             gc.collect()
        
#         # Combine filtered chunks
#         if filtered_chunks:
#             filtered_df = pd.concat(filtered_chunks, ignore_index=True)
#             del filtered_chunks
#         else:
#             filtered_df = pd.DataFrame()
        
#         gc.collect()
    
#     # Ensure datetime is in datetime format
#     if 'datetime' in filtered_df.columns and not pd.api.types.is_datetime64_dtype(filtered_df['dat...
#         filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])
    
#     print(f"Loaded {len(filtered_df)} rows for site-depth")
#     print(f"Memory after loading: {memory_usage():.1f} MB")
    
#     return filtered_df

# # zero_curtain_processing.py
# import numpy as np
# import pandas as pd
# import gc
# from scipy.interpolate import interp1d

# def process_site_for_zero_curtain(site_df, site, temp_depth, max_gap_hours=6, interpolation_method...
#     """Process a single site-depth for zero curtain events"""
#     # Initialize list to store events
#     site_events = []
    
#     # Skip if too few points
#     if len(site_df) < 10:
#         return []
    
#     # Sort by time
#     site_df = site_df.sort_values('datetime')
    
#     # Calculate time differences
#     site_df['time_diff'] = site_df['datetime'].diff().dt.total_seconds() / 3600
    
#     # Identify gaps for interpolation
#     interpolation_needed = (site_df['time_diff'] > 1.0) & (site_df['time_diff'] <= max_gap_hours)
    
#     # Perform interpolation if needed
#     if interpolation_needed.any():
#         interp_rows = []
#         gap_indices = site_df.index[interpolation_needed].tolist()
        
#         for idx in gap_indices:
#             try:
#                 # Get before and after rows
#                 before_row = site_df.loc[site_df.index[site_df.index.get_loc(idx) - 1]]
#                 after_row = site_df.loc[idx]
                
#                 # Calculate gap and intervals
#                 time_gap = after_row['time_diff']
#                 n_intervals = int(time_gap)
                
#                 if n_intervals > 0:
#                     # Create timestamps
#                     timestamps = pd.date_range(
#                         start=before_row['datetime'],
#                         end=after_row['datetime'],
#                         periods=n_intervals + 2  # Include endpoints
#                     )[1:-1]  # Exclude endpoints
                    
#                     # Create temperature values
#                     if interpolation_method == 'linear' or len(site_df) < 5:
#                         # Linear interpolation
#                         temp_start = before_row['soil_temp_standardized']
#                         temp_end = after_row['soil_temp_standardized']
#                         temp_values = np.linspace(temp_start, temp_end, n_intervals + 2)[1:-1]
#                     else:
#                         # Try cubic interpolation
#                         try:
#                             # Get neighboring points
#                             idx_loc = site_df.index.get_loc(idx)
#                             start_idx = max(0, idx_loc - 3)
#                             end_idx = min(len(site_df), idx_loc + 2)
                            
#                             temp_points = site_df.iloc[start_idx:end_idx]['soil_temp_standardized'...
#                             time_points = [(t - before_row['datetime']).total_seconds() / 3600 
#                                          for t in site_df.iloc[start_idx:end_idx]['datetime']]
                            
#                             interp_times = [(t - before_row['datetime']).total_seconds() / 3600 
#                                           for t in timestamps]
                            
#                             # Perform cubic interpolation if enough points
#                             if len(time_points) >= 4:
#                                 interp_func = interp1d(time_points, temp_points, 
#                                                      kind='cubic', bounds_error=False)
#                                 temp_values = interp_func(interp_times)
#                             else:
#                                 # Fallback to linear
#                                 temp_start = before_row['soil_temp_standardized']
#                                 temp_end = after_row['soil_temp_standardized']
#                                 temp_values = np.linspace(temp_start, temp_end, n_intervals + 2)[1...
#                         except:
#                             # Fallback to linear if cubic fails
#                             temp_start = before_row['soil_temp_standardized']
#                             temp_end = after_row['soil_temp_standardized']
#                             temp_values = np.linspace(temp_start, temp_end, n_intervals + 2)[1:-1]
                    
#                     # Check for moisture data
#                     moist_values = None
#                     has_moisture = ('soil_moist_standardized' in site_df.columns and 
#                                    not pd.isna(before_row['soil_moist_standardized']) and 
#                                    not pd.isna(after_row['soil_moist_standardized']))
                    
#                     if has_moisture:
#                         moist_start = before_row['soil_moist_standardized']
#                         moist_end = after_row['soil_moist_standardized']
#                         moist_values = np.linspace(moist_start, moist_end, n_intervals + 2)[1:-1]
                    
#                     # Create new rows
#                     for j, timestamp in enumerate(timestamps):
#                         new_row = before_row.copy()
#                         new_row['datetime'] = timestamp
#                         new_row['soil_temp_standardized'] = temp_values[j]
#                         new_row['interpolated'] = True
                        
#                         if has_moisture and moist_values is not None:
#                             new_row['soil_moist_standardized'] = moist_values[j]
                        
#                         interp_rows.append(new_row)
#             except Exception as e:
#                 print(f"  Error during interpolation at idx {idx}: {str(e)}")
#                 continue
        
#         # Add interpolated rows
#         if interp_rows:
#             interp_df = pd.DataFrame(interp_rows)
#             site_df = pd.concat([site_df, interp_df], ignore_index=True)
#             site_df = site_df.sort_values('datetime')
            
#             # Clean up
#             del interp_df, interp_rows
#             gc.collect()
    
#     # Calculate temperature gradient
#     site_df['temp_gradient'] = site_df['soil_temp_standardized'].diff() / \
#                              (site_df['datetime'].diff().dt.total_seconds() / 3600)
    
#     # Check for phase change periods
#     mask_temp = (site_df['soil_temp_standardized'].abs() <= 0.5)
#     mask_gradient = (site_df['temp_gradient'].abs() <= 0.02)
    
#     # Check for moisture data
#     use_moisture = False
#     if 'soil_moist_standardized' in site_df.columns and not site_df['soil_moist_standardized'].isn...
#         use_moisture = True
        
#         # Calculate moisture gradient
#         site_df['moist_gradient'] = site_df['soil_moist_standardized'].diff() / \
#                                   (site_df['datetime'].diff().dt.total_seconds() / 3600)
        
#         # Add moisture depth info
#         if 'soil_moist_depth' in site_df.columns and not site_df['soil_moist_depth'].isna().all():
#             site_df['closest_moist_depth'] = site_df['soil_moist_depth']
#         else:
#             site_df['closest_moist_depth'] = np.nan
        
#         # Moisture mask
#         mask_moisture = (site_df['moist_gradient'].abs() >= 0.0005)
        
#         # Combined mask
#         combined_mask = mask_temp & (mask_gradient | mask_moisture)
#     else:
#         # Use only temperature
#         combined_mask = mask_temp & mask_gradient
    
#     # Find continuous events
#     site_df['zero_curtain_flag'] = combined_mask
#     site_df['event_start'] = combined_mask & ~combined_mask.shift(1, fill_value=False)
#     site_df['event_end'] = combined_mask & ~combined_mask.shift(-1, fill_value=False)
    
#     # Get event starts and ends
#     event_starts = site_df[site_df['event_start']]['datetime'].tolist()
#     event_ends = site_df[site_df['event_end']]['datetime'].tolist()
    
#     if len(event_starts) == 0 or len(event_ends) == 0:
#         return []
    
#     # Handle mismatched starts/ends
#     if len(event_starts) > len(event_ends):
#         event_starts = event_starts[:len(event_ends)]
#     elif len(event_ends) > len(event_starts):
#         event_ends = event_ends[:len(event_starts)]
    
#     # Process each event
#     for start, end in zip(event_starts, event_ends):
#         event_duration = (end - start).total_seconds() / 3600
        
#         # Skip short events
#         if event_duration < 12:
#             continue
        
#         # Get event data
#         event_data = site_df[(site_df['datetime'] >= start) & (site_df['datetime'] <= end)]
        
#         if len(event_data) < 3:
#             continue
        
#         # Extract event info
#         try:
#             event_info = extract_event_info(event_data, site, temp_depth, start, end, 
#                                            event_duration, use_moisture)
#             site_events.append(event_info)
#         except Exception as e:
#             print(f"  Error extracting event info: {str(e)}")
    
#     # Clean up to free memory
#     del site_df
#     gc.collect()
    
#     return site_events

# def extract_event_info(event_data, site, temp_depth, start, end, event_duration, use_moisture):
#     """Extract comprehensive information about a zero curtain event"""
#     # Basic event info
#     event_info = {
#         'source': site,
#         'soil_temp_depth': temp_depth,
#         'soil_temp_depth_zone': event_data['soil_temp_depth_zone'].iloc[0] if 'soil_temp_depth_zon...
#         'datetime_min': start,
#         'datetime_max': end,
#         'duration_hours': event_duration,
#         'observation_count': len(event_data),
#         'observations_per_day': len(event_data) / (event_duration / 24) if event_duration > 0 else...
#         'soil_temp_mean': event_data['soil_temp_standardized'].mean(),
#         'soil_temp_min': event_data['soil_temp_standardized'].min(),
#         'soil_temp_max': event_data['soil_temp_standardized'].max(),
#         'soil_temp_std': event_data['soil_temp_standardized'].std(),
#         'season': event_data['season'].iloc[0] if 'season' in event_data.columns else None,
#         'latitude': event_data['latitude'].iloc[0] if 'latitude' in event_data.columns else None,
#         'longitude': event_data['longitude'].iloc[0] if 'longitude' in event_data.columns else Non...
#         'year': event_data['year'].iloc[0] if 'year' in event_data.columns else None,
#         'month': start.month
#     }
    
#     # Moisture info
#     if use_moisture and not event_data['soil_moist_standardized'].isna().all():
#         event_info['soil_moist_mean'] = event_data['soil_moist_standardized'].mean()
#         event_info['soil_moist_std'] = event_data['soil_moist_standardized'].std()
#         event_info['soil_moist_min'] = event_data['soil_moist_standardized'].min()
#         event_info['soil_moist_max'] = event_data['soil_moist_standardized'].max()
#         event_info['soil_moist_change'] = event_info['soil_moist_max'] - event_info['soil_moist_mi...
        
#         # Moisture depth
#         if 'closest_moist_depth' in event_data.columns and not event_data['closest_moist_depth'].i...
#             event_info['soil_moist_depth'] = event_data['closest_moist_depth'].iloc[0]
#         else:
#             event_info['soil_moist_depth'] = np.nan
#     else:
#         # Empty moisture values
#         event_info['soil_moist_mean'] = np.nan
#         event_info['soil_moist_std'] = np.nan
#         event_info['soil_moist_min'] = np.nan
#         event_info['soil_moist_max'] = np.nan
#         event_info['soil_moist_change'] = np.nan
#         event_info['soil_moist_depth'] = np.nan
    
#     # Temperature gradient info
#     if 'temp_gradient' in event_data.columns and not event_data['temp_gradient'].isna().all():
#         event_info['temp_gradient_mean'] = event_data['temp_gradient'].mean()
#         event_info['temp_stability'] = event_data['temp_gradient'].abs().mean()
    
#     # Add year-month
#     event_info['year_month'] = f"{event_info['year']}-{event_info['month']:02d}"
    
#     # Add region classification
#     if 'latitude' in event_info and event_info['latitude'] is not None:
#         lat = event_info['latitude']
#         if lat >= 66.5:
#             event_info['region'] = 'Arctic'
#         elif lat >= 60:
#             event_info['region'] = 'Subarctic'
#         elif lat >= 49:
#             event_info['region'] = 'Boreal'
#         else:
#             event_info['region'] = 'Other'
            
#         # Add latitude band
#         if lat < 49:
#             event_info['lat_band'] = '<49°N'
#         elif lat < 55:
#             event_info['lat_band'] = '<49-55°N (Boreal)'
#         elif lat < 60:
#             event_info['lat_band'] = '55-60°N (Boreal)'
#         elif lat < 66.5:
#             event_info['lat_band'] = '60-66.5°N (Sub_Arctic)'
#         elif lat < 70:
#             event_info['lat_band'] = '66.5-70°N (Arctic)'
#         elif lat < 75:
#             event_info['lat_band'] = '70-75°N (Arctic)'
#         elif lat < 80:
#             event_info['lat_band'] = '75-80°N (Arctic)'
#         else:
#             event_info['lat_band'] = '>80°N (Arctic)'
#     else:
#         event_info['region'] = None
#         event_info['lat_band'] = None
    
#     return event_info

# Create a detailed component diagram showing the different layers within each component
def create_component_diagram():
    fig, ax = plt.subplots(figsize=(15, 10))
    
    # Define components with their internal layers
    components = {
        "Input Processing": [
            "Input (168×3)",
            "BatchNormalization",
            "Reshape (5D)"
        ],
        "Temporal Module": [
            "ConvLSTM2D (32 filters)",
            "BatchNorm5D",
            "Reshape (3D)"
        ],
        "Attention Module": [
            "BatchNormalization",
            "MultiHeadAttention",
            "Add (Residual)",
            "LayerNormalization"
        ],
        "FFN Module": [
            "Dense (64)",
            "BatchNormalization",
            "Dropout (0.1)",
            "Dense (32)",
            "Add (Residual)",
            "LayerNormalization"
        ],
        "Parallel Conv": [
            "Conv1D (kernel=3)",
            "BatchNormalization",
            "Dropout",
            "Conv1D (kernel=5)",
            "BatchNormalization",
            "Dropout"
        ],
        "Pooling Operations": [
            "GlobalMaxPooling1D",
            "GlobalAveragePooling1D",
            "GlobalMaxPooling1D (Conv)",
            "GlobalMaxPooling1D (Conv)"
        ],
        "Variational Component": [
            "Dense (z_mean)",
            "Dense (z_log_var)",
            "Lambda (sampling)",
            "KL Divergence Loss"
        ],
        "Classification Head": [
            "Concatenate Features",
            "BatchNormalization",
            "Dense (64)",
            "BatchNormalization",
            "Dropout (0.2)",
            "Dense (32)",
            "BatchNormalization",
            "Dense (1)"
        ]
    }
    
    # Set positions for components
    positions = {
        "Input Processing": (0.1, 0.85),
        "Temporal Module": (0.3, 0.85),
        "Attention Module": (0.5, 0.85),
        "FFN Module": (0.7, 0.85),
        "Parallel Conv": (0.2, 0.5),
        "Pooling Operations": (0.5, 0.5),
        "Variational Component": (0.8, 0.5),
        "Classification Head": (0.5, 0.2)
    }
    
    # Define colors for components
    colors = {
        "Input Processing": "lightblue",
        "Temporal Module": "lightgreen",
        "Attention Module": "lightsalmon",
        "FFN Module": "khaki",
        "Parallel Conv": "thistle",
        "Pooling Operations": "lightcyan",
        "Variational Component": "paleturquoise",
        "Classification Head": "lightcoral"
    }
    
    # Draw components and their layers
    for component, layers in components.items():
        x, y = positions[component]
        num_layers = len(layers)
        
        # Component title
        plt.text(x, y + 0.05, component, ha='center', va='center', fontsize=12, fontweight='bold')
        
        # Component box
        box_width = 0.18
        box_height = 0.04 * num_layers + 0.02
        rect = plt.Rectangle((x - box_width/2, y - box_height/2), 
                             box_width, box_height, 
                             fill=True, color=colors[component], alpha=0.5)
        ax.add_patch(rect)
        
        # Add layers within the component
        for i, layer in enumerate(layers):
            layer_y = y + (num_layers/2 - i - 0.5) * 0.04
            plt.text(x, layer_y, layer, ha='center', va='center', fontsize=8)
    
    # Draw connections between components
    connections = [
        ("Input Processing", "Temporal Module"),
        ("Input Processing", "Parallel Conv"),
        ("Temporal Module", "Attention Module"),
        ("Attention Module", "FFN Module"),
        ("FFN Module", "Pooling Operations"),
        ("Parallel Conv", "Pooling Operations"),
        ("Pooling Operations", "Variational Component"),
        ("Pooling Operations", "Classification Head"),
        ("Variational Component", "Classification Head")
    ]
    
    for src, dst in connections:
        src_x, src_y = positions[src]
        dst_x, dst_y = positions[dst]
        
        # Adjust starting and ending points to be on the edge of the boxes
        src_box_height = 0.04 * len(components[src]) + 0.02
        dst_box_height = 0.04 * len(components[dst]) + 0.02
        
        if abs(dst_x - src_x) > abs(dst_y - src_y):
            # Horizontal connection
            if dst_x > src_x:
                plt.arrow(src_x + 0.09, src_y, dst_x - src_x - 0.18, 0, 
                         head_width=0.01, head_length=0.01, fc='black', ec='black')
            else:
                plt.arrow(src_x - 0.09, src_y, dst_x - src_x + 0.18, 0, 
                         head_width=0.01, head_length=0.01, fc='black', ec='black')
        else:
            # Vertical connection
            if dst_y > src_y:
                plt.arrow(src_x, src_y + src_box_height/2, 0, dst_y - src_y - (src_box_height + dst_box_height)/2, 
                         head_width=0.01, head_length=0.01, fc='black', ec='black')
            else:
                plt.arrow(src_x, src_y - src_box_height/2, 0, dst_y - src_y + (src_box_height + dst_box_height)/2, 
                         head_width=0.01, head_length=0.01, fc='black', ec='black')
    
    # Add model info
    plt.text(0.5, 0.98, "Zero Curtain Model - Detailed Component Architecture", 
             ha='center', fontsize=14, fontweight='bold')
    
    # Parameter count by component
    param_counts = {
        "Temporal Module (ConvLSTM)": "13,696",
        "Attention Module": "16,992",
        "FFN Module": "4,576",
        "Parallel Conv": "480",
        "Variational Component": "2,080",
        "Classification Head": "9,681",
        "Total": "48,525"
    }
    
    param_text = "Parameter Distribution:\n"
    for comp, count in param_counts.items():
        param_text += f"{comp}: {count}\n"
    
    plt.text(0.05, 0.05, param_text, fontsize=9, va='bottom')
    
    # Remove axes
    plt.axis('off')
    plt.xlim(0, 1)
    plt.ylim(0, 1)
    
    plt.tight_layout()
    plt.savefig('zero_curtain_detailed_diagram.png', dpi=300, bbox_inches='tight')
    plt.close()

create_component_diagram()

# Now create a flowchart-style visualization for a clearer understanding of the data flow
def create_flowchart_visualization():
    import matplotlib.patches as patches
    
    fig, ax = plt.subplots(figsize=(12, 10))
    
    # Define block styles
    block_styles = {
        'input': {'facecolor': 'lightgreen', 'edgecolor': 'green', 'alpha': 0.7},
        'process': {'facecolor': 'lightblue', 'edgecolor': 'blue', 'alpha': 0.7},
        'attention': {'facecolor': 'lightsalmon', 'edgecolor': 'red', 'alpha': 0.7},
        'pooling': {'facecolor': 'wheat', 'edgecolor': 'orange', 'alpha': 0.7},
        'output': {'facecolor': 'lightcoral', 'edgecolor': 'darkred', 'alpha': 0.7},
        'vae': {'facecolor': 'thistle', 'edgecolor': 'purple', 'alpha': 0.7}
    }
    
    # Define blocks for the flowchart (name, position, size, style)
    blocks = [
        ('Input\n(168×3)', (0.5, 0.95), (0.25, 0.06), 'input'),
        
        # Main branch
        ('BatchNorm + Reshape', (0.3, 0.85), (0.2, 0.06), 'process'),
        ('ConvLSTM2D', (0.3, 0.75), (0.2, 0.06), 'process'),
        ('BatchNorm5D + Reshape', (0.3, 0.65), (0.2, 0.06), 'process'),
        ('Multi-Head\nAttention', (0.3, 0.55), (0.2, 0.06), 'attention'),
        ('Feed-Forward Network\nwith Residual Connection', (0.3, 0.45), (0.2, 0.08), 'process'),
        ('Layer Normalization', (0.3, 0.35), (0.2, 0.06), 'process'),
        
        # Parallel branches
        ('Conv1D\n(kernel=3)', (0.7, 0.8), (0.15, 0.06), 'process'),
        ('Conv1D\n(kernel=5)', (0.7, 0.7), (0.15, 0.06), 'process'),
        ('BatchNorm + Dropout', (0.7, 0.6), (0.15, 0.06), 'process'),
        
        # Pooling operations
        ('GlobalMaxPool', (0.3, 0.25), (0.15, 0.06), 'pooling'),
        ('GlobalAvgPool', (0.45, 0.25), (0.15, 0.06), 'pooling'),
        ('GlobalMaxPool\n(Conv Paths)', (0.7, 0.5), (0.15, 0.06), 'pooling'),
        
        # VAE component
        ('z_mean', (0.85, 0.4), (0.1, 0.05), 'vae'),
        ('z_log_var', (0.85, 0.3), (0.1, 0.05), 'vae'),
        ('Sampling +\nKL Divergence', (0.85, 0.2), (0.15, 0.06), 'vae'),
        
        # Feature aggregation and final layers
        ('Feature\nConcatenation', (0.5, 0.15), (0.2, 0.06), 'process'),
        ('Classification\nHead', (0.5, 0.05), (0.2, 0.06), 'process'),
        ('Zero Curtain\nPrediction', (0.5, -0.05), (0.2, 0.06), 'output')
    ]
    
    # Draw blocks
    for name, pos, size, style in blocks:
        x, y = pos
        width, height = size
        rect = patches.Rectangle((x - width/2, y - height/2), width, height, 
                               **block_styles[style], linewidth=1)
        ax.add_patch(rect)
        plt.text(x, y, name, ha='center', va='center', fontsize=9, fontweight='bold')
    
    # Draw arrows
    arrows = [
        # Main path
        ((0.5, 0.92), (0.3, 0.88)),  # Input to BatchNorm+Reshape
        ((0.3, 0.82), (0.3, 0.78)),  # BatchNorm+Reshape to ConvLSTM2D
        ((0.3, 0.72), (0.3, 0.68)),  # ConvLSTM2D to BatchNorm5D+Reshape
        ((0.3, 0.62), (0.3, 0.58)),  # BatchNorm5D+Reshape to Multi-Head Attention
        ((0.3, 0.52), (0.3, 0.49)),  # Multi-Head Attention to FFN
        ((0.3, 0.41), (0.3, 0.38)),  # FFN to Layer Normalization
        ((0.3, 0.32), (0.3, 0.28)),  # Layer Normalization to GlobalMaxPool
        
        # Input to parallel convs
        ((0.5, 0.92), (0.7, 0.83)),  # Input to Conv1D(3)
        ((0.5, 0.92), (0.7, 0.73)),  # Input to Conv1D(5)
        
        # Conv paths
        ((0.7, 0.77), (0.7, 0.73)),  # Conv1D(3) to Conv1D(5)
        ((0.7, 0.67), (0.7, 0.63)),  # Conv1D(5) to BatchNorm+Dropout
        ((0.7, 0.57), (0.7, 0.53)),  # BatchNorm+Dropout to GlobalMaxPool(Conv)
        
        # Layer Norm to AvgPool
        ((0.3, 0.32), (0.45, 0.28)),  # Layer Normalization to GlobalAvgPool
        
        # Feature aggregation
        ((0.3, 0.22), (0.5, 0.18)),   # GlobalMaxPool to Feature Concatenation
        ((0.45, 0.22), (0.5, 0.18)),  # GlobalAvgPool to Feature Concatenation
        ((0.7, 0.47), (0.5, 0.18)),   # GlobalMaxPool(Conv) to Feature Concatenation
        
        # VAE path
        ((0.45, 0.22), (0.85, 0.4)),  # GlobalAvgPool to z_mean
        ((0.45, 0.22), (0.85, 0.3)),  # GlobalAvgPool to z_log_var
        ((0.85, 0.38), (0.85, 0.23)),  # z_mean to Sampling
        ((0.85, 0.28), (0.85, 0.23)),  # z_log_var to Sampling
        ((0.85, 0.17), (0.5, 0.15)),   # Sampling to Feature Concatenation
        
        # Final classification
        ((0.5, 0.12), (0.5, 0.08)),  # Feature Concatenation to Classification Head
        ((0.5, 0.02), (0.5, -0.02))  # Classification Head to Output
    ]
    
    # Draw the arrows
    for start, end in arrows:
        plt.arrow(start[0], start[1], end[0] - start[0], end[1] - start[1], 
                 head_width=0.01, head_length=0.01, fc='black', ec='black', 
                 length_includes_head=True)
    
    # Add diagram title
    plt.text(0.5, 1.05, "Zero Curtain Model - Flowchart Visualization", 
             ha='center', fontsize=14, fontweight='bold')
    
    # Add model information
    info_text = (
        "Model Information:\n"
        "- Input: Time series data (168 timesteps × 3 features)\n"
        "- Total Parameters: 48,525 (47,527 trainable)\n"
        "- Architecture: Hybrid with ConvLSTM, Self-Attention, and Variational components\n"
        "- Output: Binary classification for zero curtain events"
    )
    plt.text(0.02, -0.15, info_text, fontsize=9, va='top')
    
    # Remove axes
    plt.axis('off')
    plt.xlim(0, 1)
    plt.ylim(-0.2, 1.1)
    
    plt.tight_layout()
    plt.savefig('zero_curtain_flowchart.png', dpi=300, bbox_inches='tight')
    plt.close()

"""
Zero Curtain Model Visualization
================================
This script provides multiple visualization methods for the Zero Curtain model architecture.
Each function generates a different type of visualization and saves it to disk.

[REDACTED_NAME]
Arctic Research & Data Scientist, [REDACTED_AFFILIATION]
"""

import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import os

#------------------------------------------------------------------------------
# SECTION 1: PUBLICATION-READY MAIN ARCHITECTURE DIAGRAM 
#------------------------------------------------------------------------------

def create_publication_model_diagram(save_path='publication_zero_curtain_model.pdf'):
    """
    Creates a publication-ready visualization of the Zero Curtain model architecture
    using matplotlib. The diagram is clean, professional, and suitable for academic papers.
    
    Args:
        save_path: Path to save the visualization (PDF format recommended for publications)
    """
    # Create figure and axis
    fig, ax = plt.subplots(figsize=(10, 12), dpi=300)
    
    # Define colors for different components
    colors = {
        'input': '#e4f1e4',
        'temporal': '#e4ecf1',
        'attention': '#f1e4e4',
        'ffn': '#f1f1e4',
        'conv': '#ecf1e4',
        'pooling': '#e4f1ec',
        'vae': '#ece4f1',
        'concat': '#f1ece4',
        'classifier': '#f1e4ec',
        'output': '#f1e4f1'
    }
    
    # Define edge colors
    edge_colors = {
        'input': '#628962',
        'temporal': '#627889',
        'attention': '#896262',
        'ffn': '#898962',
        'conv': '#788962',
        'pooling': '#628978',
        'vae': '#786289',
        'concat': '#897862',
        'classifier': '#896278',
        'output': '#896289'
    }
    
    # Define component blocks with positions, sizes, and styles
    components = [
        # Input
        {'name': 'Input\n(168 × 3)', 'pos': (0.5, 0.95), 'width': 0.5, 'height': 0.05, 
         'color': colors['input'], 'edge': edge_colors['input']},
        
        # Temporal branch
        {'name': 'Temporal Processing Branch', 'pos': (0.3, 0.85), 'width': 0.4, 'height': 0.05, 
         'color': colors['temporal'], 'edge': edge_colors['temporal']},
        {'name': 'BatchNorm → Reshape → ConvLSTM2D', 'pos': (0.3, 0.79), 'width': 0.4, 'height': 0.05, 
         'color': colors['temporal'], 'edge': edge_colors['temporal']},
        
        # Attention module
        {'name': 'Self-Attention Module', 'pos': (0.3, 0.7), 'width': 0.4, 'height': 0.05, 
         'color': colors['attention'], 'edge': edge_colors['attention']},
        {'name': 'MultiHeadAttention + Residual', 'pos': (0.3, 0.64), 'width': 0.4, 'height': 0.05, 
         'color': colors['attention'], 'edge': edge_colors['attention']},
        
        # FFN
        {'name': 'Feed-Forward Network', 'pos': (0.3, 0.55), 'width': 0.4, 'height': 0.05, 
         'color': colors['ffn'], 'edge': edge_colors['ffn']},
        {'name': 'Dense(64) → Dense(32) + Residual', 'pos': (0.3, 0.49), 'width': 0.4, 'height': 0.05, 
         'color': colors['ffn'], 'edge': edge_colors['ffn']},
        
        # Parallel Conv1D
        {'name': 'Parallel Conv1D Paths', 'pos': (0.7, 0.85), 'width': 0.4, 'height': 0.05, 
         'color': colors['conv'], 'edge': edge_colors['conv']},
        {'name': 'Conv1D(k=3) and Conv1D(k=5)', 'pos': (0.7, 0.79), 'width': 0.4, 'height': 0.05, 
         'color': colors['conv'], 'edge': edge_colors['conv']},
        
        # Pooling operations
        {'name': 'Global Pooling Operations', 'pos': (0.3, 0.4), 'width': 0.4, 'height': 0.05, 
         'color': colors['pooling'], 'edge': edge_colors['pooling']},
        {'name': 'MaxPool and AvgPool', 'pos': (0.3, 0.34), 'width': 0.4, 'height': 0.05, 
         'color': colors['pooling'], 'edge': edge_colors['pooling']},
        
        # Pooling from Conv1D
        {'name': 'Global Pooling (Conv1D)', 'pos': (0.7, 0.7), 'width': 0.4, 'height': 0.05, 
         'color': colors['pooling'], 'edge': edge_colors['pooling']},
        {'name': 'MaxPool from Conv Paths', 'pos': (0.7, 0.64), 'width': 0.4, 'height': 0.05, 
         'color': colors['pooling'], 'edge': edge_colors['pooling']},
        
        # VAE
        {'name': 'Variational Component', 'pos': (0.7, 0.55), 'width': 0.4, 'height': 0.05, 
         'color': colors['vae'], 'edge': edge_colors['vae']},
        {'name': 'z_mean, z_log_var → Sampling', 'pos': (0.7, 0.49), 'width': 0.4, 'height': 0.05, 
         'color': colors['vae'], 'edge': edge_colors['vae']},
        {'name': 'KL Divergence Loss', 'pos': (0.7, 0.4), 'width': 0.4, 'height': 0.05, 
         'color': colors['vae'], 'edge': edge_colors['vae'], 'style': 'dashed'},
        
        # Feature concatenation
        {'name': 'Feature Concatenation', 'pos': (0.5, 0.25), 'width': 0.6, 'height': 0.05, 
         'color': colors['concat'], 'edge': edge_colors['concat']},
        
        # Classification
        {'name': 'Classification Layers', 'pos': (0.5, 0.16), 'width': 0.6, 'height': 0.05, 
         'color': colors['classifier'], 'edge': edge_colors['classifier']},
        {'name': 'Dense(64) → Dense(32) → Dense(1)', 'pos': (0.5, 0.1), 'width': 0.6, 'height': 0.05, 
         'color': colors['classifier'], 'edge': edge_colors['classifier']},
        
        # Output
        {'name': 'Zero Curtain Prediction', 'pos': (0.5, 0.02), 'width': 0.5, 'height': 0.05, 
         'color': colors['output'], 'edge': edge_colors['output']}
    ]
    
    # Draw component boxes
    for comp in components:
        x, y = comp['pos']
        width, height = comp['width'], comp['height']
        color = comp['color']
        edge_color = comp['edge']
        
        # Define linestyle based on component style
        linestyle = comp.get('style', 'solid')
        
        # Create rectangle
        rect = patches.FancyBboxPatch(
            (x - width/2, y - height/2), width, height,
            boxstyle=patches.BoxStyle("Round", pad=0.02),
            facecolor=color, edgecolor=edge_color, linewidth=1.5,
            linestyle=linestyle, alpha=0.9
        )
        ax.add_patch(rect)
        
        # Add text
        plt.text(x, y, comp['name'], ha='center', va='center', fontsize=9, fontweight='bold')
    
    # Define connections between components
    connections = [
        # Input to main branches
        (components[0], components[1]),  # Input to Temporal
        (components[0], components[7]),  # Input to Parallel Conv1D
        
        # Temporal branch flow
        (components[1], components[2]),  # Temporal title to details
        (components[2], components[3]),  # Temporal to Attention
        (components[3], components[4]),  # Attention title to details
        (components[4], components[5]),  # Attention to FFN
        (components[5], components[6]),  # FFN title to details
        (components[6], components[9]),  # FFN to Pooling
        
        # Parallel Conv1D flow
        (components[7], components[8]),  # Conv1D title to details
        (components[8], components[11]),  # Conv1D to Pooling (Conv1D)
        (components[11], components[12]),  # Pooling title to details
        
        # VAE flow
        (components[9], components[13]),  # Pooling to VAE
        (components[13], components[14]),  # VAE title to details
        (components[14], components[15]),  # VAE to KL Loss
        
        # Feature concatenation
        (components[9], components[16]),  # Pooling to Feature Concatenation
        (components[10], components[16]),  # Pooling details to Feature Concatenation
        (components[12], components[16]),  # Pooling (Conv1D) to Feature Concatenation
        (components[14], components[16]),  # VAE to Feature Concatenation
        
        # Classification and output
        (components[16], components[17]),  # Feature Concatenation to Classification
        (components[17], components[18]),  # Classification title to details
        (components[18], components[19])   # Classification to Output
    ]
    
    # Draw arrows for connections
    for src, dst in connections:
        src_x, src_y = src['pos']
        dst_x, dst_y = dst['pos']
        src_height = src['height']
        dst_height = dst['height']
        
        # Calculate arrow start and end points
        if abs(dst_y - src_y) > abs(dst_x - src_x):
            # Vertical connection
            start_y = src_y - src_height/2
            end_y = dst_y + dst_height/2
            
            # Draw arrow
            arrow = patches.FancyArrowPatch(
                (src_x, start_y), (dst_x, end_y),
                connectionstyle=f'arc3,rad=0.0',
                arrowstyle='-|>', color='#555555', linewidth=1.2,
                shrinkA=2, shrinkB=2
            )
        else:
            # Horizontal/diagonal connection
            start_x = src_x + src['width']/2 * np.sign(dst_x - src_x)
            start_y = src_y
            end_x = dst_x - dst['width']/2 * np.sign(dst_x - src_x)
            end_y = dst_y
            
            # Draw arrow
            arrow = patches.FancyArrowPatch(
                (start_x, start_y), (end_x, end_y),
                connectionstyle=f'arc3,rad=0.2',
                arrowstyle='-|>', color='#555555', linewidth=1.2,
                shrinkA=2, shrinkB=2
            )
        
        ax.add_patch(arrow)
    
    # Add special arrow for KL loss
    kl_comp = components[15]
    kl_x, kl_y = kl_comp['pos']
    
    # Create dashed arrow for loss contribution
    loss_arrow = patches.FancyArrowPatch(
        (kl_x, kl_y - kl_comp['height']/2), (kl_x, kl_y - 0.1),
        connectionstyle='arc3,rad=0.0',
        arrowstyle='-|>', color='#AA5555', linewidth=1.2,
        linestyle='dashed', shrinkA=2, shrinkB=2
    )
    ax.add_patch(loss_arrow)
    
    # Add title and parameter info
    plt.text(0.5, 1.02, "Zero Curtain Model Architecture", 
             fontsize=14, fontweight='bold', ha='center', transform=ax.transAxes)
    
    # Add parameter information
    param_info = (
        "Total Parameters: 48,525 (189.55 KB)\n"
        "Trainable: 47,527 (185.65 KB)  •  Non-trainable: 998 (3.90 KB)"
    )
    plt.text(0.5, -0.02, param_info, fontsize=9, ha='center', transform=ax.transAxes)
    
    # Add parameter distribution
    components_info = (
        "Parameter Distribution:\n"
        "ConvLSTM: 13,568 (28%)  •  Attention: 16,800 (35%)  •  FFN: 4,576 (9%)\n"
        "Conv1D: 416 (1%)  •  VAE: 2,080 (4%)  •  Classifier: 9,345 (19%)  •  Normalization: 1,740 (4%)"
    )
    plt.text(0.5, -0.06, components_info, fontsize=8, ha='center', transform=ax.transAxes)
    
    # Remove axes
    plt.axis('off')
    plt.tight_layout()
    
    # Save the figure (PDF for vector graphics)
    plt.savefig(save_path, bbox_inches='tight', dpi=300)
    
    # Also save as PNG for easy viewing
    plt.savefig(save_path.replace('.pdf', '.png'), bbox_inches='tight', dpi=300)
    
    plt.close()
    print(f"Created publication-ready model diagram: {save_path}")
    
    return fig, ax

#------------------------------------------------------------------------------
# SECTION 2: SIMPLIFIED ARCHITECTURE DIAGRAM
#------------------------------------------------------------------------------

def create_simplified_architecture_diagram(save_path='simplified_zero_curtain_model.pdf'):
    """
    Creates a highly simplified diagram of the Zero Curtain model architecture,
    focusing only on the key functional components for conceptual understanding.
    
    Args:
        save_path: Path to save the visualization
    """
    # Create figure and axis
    fig, ax = plt.subplots(figsize=(8, 6), dpi=300)
    
    # Define the main components
    components = [
        # Input
        {'name': 'Time Series Input\n(168 × 3)', 'pos': (0.5, 0.9), 'width': 0.3, 'height': 0.08, 
         'color': '#e4f1e4', 'edge': '#628962'},
        
        # Main processing branches
        {'name': 'Temporal Processing\n(ConvLSTM2D)', 'pos': (0.3, 0.75), 'width': 0.25, 'height': 0.1, 
         'color': '#e4ecf1', 'edge': '#627889'},
        {'name': 'Self-Attention\nMechanism', 'pos': (0.3, 0.6), 'width': 0.25, 'height': 0.1, 
         'color': '#f1e4e4', 'edge': '#896262'},
        {'name': 'Feed-Forward\nNetwork', 'pos': (0.3, 0.45), 'width': 0.25, 'height': 0.1, 
         'color': '#f1f1e4', 'edge': '#898962'},
        
        # Parallel branch
        {'name': 'Parallel Conv1D\nFeature Extraction', 'pos': (0.7, 0.7), 'width': 0.25, 'height': 0.1, 
         'color': '#ecf1e4', 'edge': '#788962'},
        {'name': 'Variational\nComponent', 'pos': (0.7, 0.55), 'width': 0.25, 'height': 0.1, 
         'color': '#ece4f1', 'edge': '#786289'},
        {'name': 'KL Divergence\nRegularization', 'pos': (0.85, 0.45), 'width': 0.2, 'height': 0.08, 
         'color': '#f1e4ec', 'edge': '#896278', 'style': 'dashed'},
        
        # Integration
        {'name': 'Feature Aggregation', 'pos': (0.5, 0.3), 'width': 0.3, 'height': 0.1, 
         'color': '#f1ece4', 'edge': '#897862'},
        {'name': 'Classification Layers', 'pos': (0.5, 0.15), 'width': 0.3, 'height': 0.1, 
         'color': '#f1e4ec', 'edge': '#896278'},
        
        # Output
        {'name': 'Zero Curtain\nPrediction', 'pos': (0.5, 0.05), 'width': 0.3, 'height': 0.06, 
         'color': '#f1e4f1', 'edge': '#896289'}
    ]
    
    # Draw component boxes with rounded corners
    for comp in components:
        x, y = comp['pos']
        width, height = comp['width'], comp['height']
        color = comp['color']
        edge_color = comp['edge']
        
        # Define linestyle based on component style
        linestyle = comp.get('style', 'solid')
        
        # Create rectangle
        rect = patches.FancyBboxPatch(
            (x - width/2, y - height/2), width, height,
            boxstyle=patches.BoxStyle("Round", pad=0.03),
            facecolor=color, edgecolor=edge_color, linewidth=2,
            linestyle=linestyle, alpha=0.9
        )
        ax.add_patch(rect)
        
        # Add text
        plt.text(x, y, comp['name'], ha='center', va='center', fontsize=10, fontweight='bold')
    
    # Define connections between components
    connections = [
        # Input to branches
        (components[0], components[1]),  # Input to Temporal Processing
        (components[0], components[4]),  # Input to Parallel Conv1D
        
        # Main flow
        (components[1], components[2]),  # Temporal to Attention
        (components[2], components[3]),  # Attention to FFN
        
        # Parallel flow
        (components[4], components[5]),  # Conv1D to Variational
        (components[5], components[6]),  # Variational to KL Loss (dashed)
        
        # Integration
        (components[3], components[7]),  # FFN to Feature Aggregation
        (components[5], components[7]),  # Variational to Feature Aggregation
        
        # Output flow
        (components[7], components[8]),  # Feature Aggregation to Classification
        (components[8], components[9])   # Classification to Output
    ]
    
    # Draw arrows for connections
    for src, dst in connections:
        src_x, src_y = src['pos']
        dst_x, dst_y = dst['pos']
        
        # Adjust linestyle for special connections
        linestyle = 'dashed' if src.get('style') == 'dashed' or dst.get('style') == 'dashed' else 'solid'
        
        # Determine if it's a vertical or horizontal/diagonal connection
        if abs(dst_y - src_y) > abs(dst_x - src_x):
            # Vertical connection
            arrow = patches.FancyArrowPatch(
                (src_x, src_y - src['height']/2), (dst_x, dst_y + dst['height']/2),
                connectionstyle='arc3,rad=0.0',
                arrowstyle='-|>', color='#555555', linewidth=1.5,
                linestyle=linestyle, shrinkA=4, shrinkB=4
            )
        else:
            # Horizontal/diagonal connection
            curvature = 0.3 if abs(dst_x - src_x) > 0.3 else 0.1
            arrow = patches.FancyArrowPatch(
                (src_x, src_y), (dst_x, dst_y),
                connectionstyle=f'arc3,rad={curvature}',
                arrowstyle='-|>', color='#555555', linewidth=1.5,
                linestyle=linestyle, shrinkA=10, shrinkB=10
            )
        
        ax.add_patch(arrow)
    
    # Add title and description
    plt.text(0.5, 0.98, "Zero Curtain Model - Simplified Architecture", 
             fontsize=14, fontweight='bold', ha='center', transform=ax.transAxes)
    
    # Add model information
    info_text = (
        "Model Information:\n"
        "• Input: Time series data with 168 timesteps and 3 features\n"
        "• Hybrid architecture combining temporal, attention, and variational components\n"
        "• Regularized with KL divergence and dropout\n"
        "• 48,525 total parameters (47,527 trainable)"
    )
    plt.text(0.02, 0.02, info_text, fontsize=8, va='bottom', transform=ax.transAxes)
    
    # Remove axes
    plt.axis('off')
    plt.tight_layout()
    
    # Save as PDF (vector) and PNG
    plt.savefig(save_path, bbox_inches='tight', dpi=300)
    plt.savefig(save_path.replace('.pdf', '.png'), bbox_inches='tight', dpi=300)
    
    plt.close()
    print(f"Created simplified architecture diagram: {save_path}")
    
    return fig, ax

#------------------------------------------------------------------------------
# SECTION 3: PARAMETER DISTRIBUTION CHART
#------------------------------------------------------------------------------

def create_parameter_distribution_chart(save_path='parameter_distribution_chart.pdf'):
    """
    Creates a horizontal bar chart showing the parameter distribution across model components.
    This chart is designed for direct inclusion in publications.
    
    Args:
        save_path: Path to save the visualization
    """
    # Component parameter counts
    components = [
        'ConvLSTM', 'Attention', 'Feed-Forward', 'Conv1D', 
        'Variational', 'Classification', 'Normalization'
    ]
    
    parameters = [13568, 16800, 4576, 416, 2080, 9345, 1740]
    percentages = [p / sum(parameters) * 100 for p in parameters]
    
    # Create figure
    fig, ax = plt.subplots(figsize=(8, 4), dpi=300)
    
    # Create gradient colormap
    component_colors = [
        '#627889', '#896262', '#898962', '#789962', 
        '#786289', '#896278', '#555555'
    ]
    
    # Create horizontal bars
    bars = ax.barh(components, parameters, color=component_colors, alpha=0.8, height=0.6)
    
    # Add percentage and absolute values
    for i, (bar, percentage) in enumerate(zip(bars, percentages)):
        width = bar.get_width()
        
        # Display parameter count
        ax.text(
            width + 300, bar.get_y() + bar.get_height()/2,
            f"{parameters[i]:,} ({percentage:.1f}%)",
            va='center', fontsize=9
        )
    
    # Customize plot
    ax.set_title('Zero Curtain Model - Parameter Distribution', fontsize=14, fontweight='bold')
    ax.set_xlabel('Number of Parameters', fontsize=10)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.tick_params(axis='y', labelsize=10)
    
    # Add total parameter count
    total_params = sum(parameters)
    plt.text(
        0.98, 0.02, f"Total Parameters: {total_params:,}",
        transform=ax.transAxes, ha='right', fontsize=10, fontweight='bold'
    )
    
    plt.tight_layout()
    
    # Save as PDF (vector) and PNG
    plt.savefig(save_path, bbox_inches='tight', dpi=300)
    plt.savefig(save_path.replace('.pdf', '.png'), bbox_inches='tight', dpi=300)
    
    plt.close()
    print(f"Created parameter distribution chart: {save_path}")
    
    return fig, ax

#------------------------------------------------------------------------------
# SECTION 4: MODEL FLOWCHART DIAGRAM
#------------------------------------------------------------------------------

def create_model_flowchart(save_path='zero_curtain_flowchart.pdf'):
    """
    Creates a flowchart-style visualization of the Zero Curtain model,
    emphasizing the data flow between components.
    
    Args:
        save_path: Path to save the visualization
    """
    # Create figure and axis
    fig, ax = plt.subplots(figsize=(10, 12), dpi=300)
    
    # Define flowchart shapes and styles
    shapes = {
        'input': {'shape': 'parallelogram', 'color': '#e4f1e4', 'edge': '#628962'},
        'process': {'shape': 'rectangle', 'color': '#e4ecf1', 'edge': '#627889'},
        'decision': {'shape': 'diamond', 'color': '#f1e4e4', 'edge': '#896262'},
        'output': {'shape': 'parallelogram', 'color': '#f1e4f1', 'edge': '#896289'}
    }
    
    # Define flowchart nodes
    nodes = [
        # Input
        {'id': 'input', 'name': 'Time Series Input\n(168 × 3)', 'pos': (0.5, 0.95), 
         'type': 'input', 'width': 0.4, 'height': 0.06},
        
        # Data preprocessing
        {'id': 'preprocess', 'name': 'BatchNormalize & Reshape', 'pos': (0.5, 0.88), 
         'type': 'process', 'width': 0.4, 'height': 0.05},
        
        # Parallel paths
        {'id': 'split', 'name': 'Split Data Flow', 'pos': (0.5, 0.82), 
         'type': 'decision', 'width': 0.25, 'height': 0.06},
        
        # Left path (Temporal)
        {'id': 'convlstm', 'name': 'Temporal Processing\n(ConvLSTM2D)', 'pos': (0.3, 0.74), 
         'type': 'process', 'width': 0.3, 'height': 0.06},
        {'id': 'attention', 'name': 'Self-Attention', 'pos': (0.3, 0.67), 
         'type': 'process', 'width': 0.3, 'height': 0.05},
        {'id': 'ffn', 'name': 'Feed-Forward Network', 'pos': (0.3, 0.6), 
         'type': 'process', 'width': 0.3, 'height': 0.05},
        {'id': 'pooling1', 'name': 'Global Pooling', 'pos': (0.3, 0.53), 
         'type': 'process', 'width': 0.3, 'height': 0.05},
        
        # Right path (Spatial)
        {'id': 'conv1d', 'name': 'Parallel Conv1D', 'pos': (0.7, 0.74), 
         'type': 'process', 'width': 0.3, 'height': 0.06},
        {'id': 'pooling2', 'name': 'Global Pooling', 'pos': (0.7, 0.67), 
         'type': 'process', 'width': 0.3, 'height': 0.05},
        
        # Variational component
        {'id': 'vae', 'name': 'Variational Encoding', 'pos': (0.5, 0.45), 
         'type': 'process', 'width': 0.35, 'height': 0.06},
        {'id': 'kl', 'name': 'KL Divergence Loss', 'pos': (0.8, 0.45), 
         'type': 'process', 'width': 0.3, 'height': 0.05},
        
        # Feature integration
        {'id': 'concat', 'name': 'Feature Concatenation', 'pos': (0.5, 0.36), 
         'type': 'process', 'width': 0.4, 'height': 0.05},
        
        # Classification
        {'id': 'dense', 'name': 'Dense Classification Layers', 'pos': (0.5, 0.28), 
         'type': 'process', 'width': 0.4, 'height': 0.06},
        {'id': 'dropout', 'name': 'Dropout Regularization', 'pos': (0.5, 0.21), 
         'type': 'process', 'width': 0.4, 'height': 0.05},
        
        # Output
        {'id': 'output', 'name': 'Zero Curtain Prediction', 'pos': (0.5, 0.13), 
         'type': 'output', 'width': 0.4, 'height': 0.06}
    ]
    
    # Draw nodes
    for node in nodes:
        x, y = node['pos']
        width, height = node['width'], node['height']
        node_type = node['type']
        shape_info = shapes[node_type]
        
        # Create shape based on type
        if shape_info['shape'] == 'rectangle':
            shape = patches.FancyBboxPatch(
                (x - width/2, y - height/2), width, height,
                boxstyle=patches.BoxStyle("Round", pad=0.02),
                facecolor=shape_info['color'], edgecolor=shape_info['edge'],
                linewidth=1.5, alpha=0.9
            )
        elif shape_info['shape'] == 'parallelogram':
            # Create a parallelogram shape
            xy = np.array([
                [x - width/2 + height/4, y - height/2],
                [x + width/2 + height/4, y - height/2],
                [x + width/2 - height/4, y + height/2],
                [x - width/2 - height/4, y + height/2]
            ])
            shape = patches.Polygon(
                xy, closed=True, facecolor=shape_info['color'],
                edgecolor=shape_info['edge'], linewidth=1.5, alpha=0.9
            )
        elif shape_info['shape'] == 'diamond':
            # Create a diamond shape
            xy = np.array([
                [x, y - height/2],
                [x + width/2, y],
                [x, y + height/2],
                [x - width/2, y]
            ])
            shape = patches.Polygon(
                xy, closed=True, facecolor=shape_info['color'],
                edgecolor=shape_info['edge'], linewidth=1.5, alpha=0.9
            )
        
        ax.add_patch(shape)
        
        # Add text
        plt.text(x, y, node['name'], ha='center', va='center', fontsize=9, fontweight='bold')
    
    # Define connections between nodes
    connections = [
        ('input', 'preprocess'),
        ('preprocess', 'split'),
        ('split', 'convlstm'),
        ('split', 'conv1d'),
        ('convlstm', 'attention'),
        ('attention', 'ffn'),
        ('ffn', 'pooling1'),
        ('conv1d', 'pooling2'),
        ('pooling1', 'vae'),
        ('pooling2', 'vae'),
        ('vae', 'kl', 'dashed'),
        ('pooling1', 'concat'),
        ('pooling2', 'concat'),
        ('vae', 'concat'),
        ('concat', 'dense'),
        ('dense', 'dropout'),
        ('dropout', 'output')
    ]
    
    # Helper function to find node by id
    def get_node(node_id):
        for node in nodes:
            if node['id'] == node_id:
                return node
        return None
    
    # Draw connections
    for conn in connections:
        if len(conn) == 2:
            src_id, dst_id = conn
            linestyle = 'solid'
        else:
            src_id, dst_id, linestyle = conn
        
        src = get_node(src_id)
        dst = get_node(dst_id)
        
        src_x, src_y = src['pos']
        dst_x, dst_y = dst['pos']
        
        # Calculate arrow start and end points based on node shapes
        if src['type'] == 'decision':
            # For diamond shape, adjust start point
            if dst_x < src_x:  # Left
                start_x, start_y = src_x - src['width']/2, src_y
            elif dst_x > src_x:  # Right
                start_x, start_y = src_x + src['width']/2, src_y
            elif dst_y < src_y:  # Down
                start_x, start_y = src_x, src_y - src['height']/2
            else:  # Up
                start_x, start_y = src_x, src_y + src['height']/2
        else:
            # For other shapes, start from the bottom center
            start_x, start_y = src_x, src_y - src['height']/2
        
        # Set end point to the top of destination
        end_x, end_y = dst_x, dst_y + dst['height']/2
        
        # Create arrow
        if linestyle == 'dashed':
            arrow = patches.FancyArrowPatch(
                (start_x, start_y), (end_x, end_y),
                connectionstyle='arc3,rad=0.1',
                arrowstyle='-|>', color='#AA5555', linewidth=1.2,
                linestyle='dashed', shrinkA=2, shrinkB=2
            )
        else:
            arrow = patches.FancyArrowPatch(
                (start_x, start_y), (end_x, end_y),
                connectionstyle='arc3,rad=0.1',
                arrowstyle='-|>', color='#555555', linewidth=1.2,
                shrinkA=2, shrinkB=2
            )
        
        ax.add_patch(arrow)
    
    # Add title and annotation
    plt.text(0.5, 1.02, "Zero Curtain Model - Data Flow Chart", 
             fontsize=14, fontweight='bold', ha='center', transform=ax.transAxes)
    
    # Add annotation about model purpose
    annotation = (
        "The Zero Curtain model processes time series data through parallel temporal and spatial paths.\n"
        "It combines ConvLSTM2D for capturing temporal patterns, self-attention for long-range dependencies,\n"
        "and variational components with KL divergence regularization for more robust feature learning."
    )
    plt.text(0.5, 0.03, annotation, fontsize=8, ha='center', transform=ax.transAxes)
    
    # Remove axes
    plt.axis('off')
    plt.tight_layout()
    
    # Save as PDF (vector) and PNG
    plt.savefig(save_path, bbox_inches='tight', dpi=300)
    plt.savefig(save_path.replace('.pdf', '.png'), bbox_inches='tight', dpi=300)
    
    plt.close()
    print(f"Created model flowchart: {save_path}")
    
    return fig, ax

#------------------------------------------------------------------------------
# SECTION 5: LAYERWISE DIMENSION VISUALIZATION
#------------------------------------------------------------------------------

def create_dimension_visualization(save_path='zero_curtain_dimensions.pdf'):
    """
    Creates a visualization showing how data dimensions change through different layers 
    of the Zero Curtain model.
    
    Args:
        save_path: Path to save the visualization
    """
    # Define layers and their output shapes
    layers = [
        {"name": "Input", "shape": [168, 3], "type": "input"},
        {"name": "BatchNormalization", "shape": [168, 3], "type": "norm"},
        {"name": "Reshape (5D)", "shape": [168, 1, 1, 3], "type": "reshape"},
        {"name": "ConvLSTM2D", "shape": [168, 1, 1, 32], "type": "temporal"},
        {"name": "BatchNorm5D", "shape": [168, 1, 1, 32], "type": "norm"},
        {"name": "Reshape (3D)", "shape": [168, 32], "type": "reshape"},
        {"name": "BatchNormalization", "shape": [168, 32], "type": "norm"},
        {"name": "MultiHeadAttention", "shape": [168, 32], "type": "attention"},
        {"name": "Add (Residual)", "shape": [168, 32], "type": "add"},
        {"name": "LayerNormalization", "shape": [168, 32], "type": "norm"},
        {"name": "Dense (64)", "shape": [168, 64], "type": "dense"},
        {"name": "Dense (32)", "shape": [168, 32], "type": "dense"},
        {"name": "Add (Residual)", "shape": [168, 32], "type": "add"},
        {"name": "LayerNormalization", "shape": [168, 32], "type": "norm"},
        {"name": "GlobalMaxPooling1D", "shape": [32], "type": "pooling"},
        {"name": "GlobalAveragePooling1D", "shape": [32], "type": "pooling"},
        {"name": "Conv1D (k=3)", "shape": [168, 16], "type": "conv"},
        {"name": "Conv1D (k=5)", "shape": [168, 16], "type": "conv"},
        {"name": "BatchNormalization", "shape": [168, 16], "type": "norm"},
        {"name": "GlobalMaxPooling1D", "shape": [16], "type": "pooling"},
        {"name": "Dense (z_mean)", "shape": [16], "type": "vae"},
        {"name": "Dense (z_log_var)", "shape": [16], "type": "vae"},
        {"name": "Sampling", "shape": [16], "type": "vae"},
        {"name": "Concatenate", "shape": [112], "type": "concat"},
        {"name": "Dense (64)", "shape": [64], "type": "dense"},
        {"name": "Dense (32)", "shape": [32], "type": "dense"},
        {"name": "Dense (1)", "shape": [1], "type": "output"}
    ]
    
    # Create figure
    fig, ax = plt.subplots(figsize=(12, 6), dpi=300)
    
    # Define colors for different layer types
    colors = {
        'input': '#a8e6cf',
        'norm': '#dcedc1',
        'reshape': '#ffd3b6',
        'temporal': '#ffaaa5',
        'attention': '#ff8b94',
        'add': '#eecda3',
        'dense': '#98ddca',
        'pooling': '#d5d5d5',
        'conv': '#ffcccc',
        'vae': '#c7ceea',
        'concat': '#e1ccec',
        'output': '#ffbcd9'
    }
    
    # Calculate total elements for each layer
    for layer in layers:
        layer['elements'] = np.prod(layer['shape'])
    
    # Create bar positions
    x_positions = np.arange(len(layers))
    bar_width = 0.8
    
    # Create bars with varying heights based on element count
    for i, layer in enumerate(layers):
        bar_color = colors[layer['type']]
        ax.bar(x_positions[i], layer['elements'], width=bar_width, color=bar_color, 
              edgecolor='black', linewidth=0.5, alpha=0.8)
        
        # Add text with dimension information
        shape_text = 'x'.join(str(dim) for dim in layer['shape'])
        if layer['elements'] > 1000:
            # Handle different text positions/colors based on bar height
            ax.text(x_positions[i], layer['elements'] + 200, shape_text, 
                  ha='center', va='bottom', fontsize=7, rotation=90)
        else:
            ax.text(x_positions[i], layer['elements'] + 100, shape_text, 
                  ha='center', va='bottom', fontsize=7)
    
    # Add layer names
    plt.xticks(x_positions, [layer['name'] for layer in layers], rotation=90, fontsize=8)
    
    # Customize plot
    ax.set_yscale('log')  # Use log scale for better visibility
    ax.set_ylabel('Number of Elements (log scale)', fontsize=10)
    ax.set_title('Zero Curtain Model - Layer Dimensions', fontsize=14, fontweight='bold')
    
    # Add gridlines for better readability
    ax.grid(axis='y', linestyle='--', alpha=0.7)
    
    # Create legend for layer types
    unique_types = list(set(layer['type'] for layer in layers))
    legend_handles = [plt.Rectangle((0, 0), 1, 1, color=colors[t], alpha=0.8) for t in unique_types]
    ax.legend(legend_handles, [t.capitalize() for t in unique_types], 
             loc='upper right', fontsize=8, ncol=3)
    
    # Add annotation about model architecture
    plt.figtext(0.5, 0.01, 
               "The Zero Curtain model transforms input time series (168×3) through multiple layers,\n"
               "showing dimension changes across different processing stages.", 
               ha="center", fontsize=8)
    
    plt.tight_layout()
    plt.subplots_adjust(bottom=0.2, top=0.9)
    
    # Save as PDF and PNG
    plt.savefig(save_path, bbox_inches='tight', dpi=300)
    plt.savefig(save_path.replace('.pdf', '.png'), bbox_inches='tight', dpi=300)
    
    plt.close()
    print(f"Created dimension visualization: {save_path}")
    
    return fig, ax

#------------------------------------------------------------------------------
# SECTION 6: MAIN EXECUTION FUNCTION AND HELPER UTILITIES
#------------------------------------------------------------------------------

def create_all_visualizations(output_dir=None):
    """
    Create all visualizations for the Zero Curtain model architecture
    and save them to the specified directory.
    
    Args:
        output_dir: Directory to save visualizations (created if doesn't exist)
    """
    # Create output directory if specified and doesn't exist
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        
        # Update paths to include output directory
        publication_path = os.path.join(output_dir, 'publication_zero_curtain_model.pdf')
        simplified_path = os.path.join(output_dir, 'simplified_zero_curtain_model.pdf')
        parameter_path = os.path.join(output_dir, 'parameter_distribution_chart.pdf')
        flowchart_path = os.path.join(output_dir, 'zero_curtain_flowchart.pdf')
        dimensions_path = os.path.join(output_dir, 'zero_curtain_dimensions.pdf')
    else:
        # Use default paths in current directory
        publication_path = 'publication_zero_curtain_model.pdf'
        simplified_path = 'simplified_zero_curtain_model.pdf'
        parameter_path = 'parameter_distribution_chart.pdf'
        flowchart_path = 'zero_curtain_flowchart.pdf'
        dimensions_path = 'zero_curtain_dimensions.pdf'
    
    # Generate all visualizations
    print("Generating Zero Curtain model visualizations...")
    create_publication_model_diagram(publication_path)
    create_simplified_architecture_diagram(simplified_path)
    create_parameter_distribution_chart(parameter_path)
    create_model_flowchart(flowchart_path)
    create_dimension_visualization(dimensions_path)
    
    print("\nAll visualizations created successfully!")
    print("Generated files:")
    print(f"  1. Publication-quality diagram: {publication_path}")
    print(f"  2. Simplified architecture diagram: {simplified_path}")
    print(f"  3. Parameter distribution chart: {parameter_path}")
    print(f"  4. Model flowchart: {flowchart_path}")
    print(f"  5. Layer dimensions visualization: {dimensions_path}")
    
    return {
        'publication': publication_path,
        'simplified': simplified_path,
        'parameters': parameter_path,
        'flowchart': flowchart_path,
        'dimensions': dimensions_path
    }

# Execute all visualizations if this script is run directly
if __name__ == "__main__":
    create_all_visualizations()

"""
[REDACTED_NAME]
Arctic Research & Data Scientist, [REDACTED_AFFILIATION]
"""

import os
import graphviz as gv

#------------------------------------------------------------------------------
# SECTION 1: DETAILED MODEL DIAGRAM
#------------------------------------------------------------------------------

def create_model_diagram(output_filename='zero_curtain_graphviz_model.pdf'):
    """
    Creates a publication-quality diagram of the Zero Curtain model using Graphviz.
    This generates a clean, hierarchical visualization suitable for academic papers.
    
    Args:
        output_filename: Output file name (PDF recommended for vector graphics)
    """
    # Create a new directed graph
    g = gv.Digraph('zero_curtain_model', format='pdf')
    g.attr(rankdir='TB', splines='ortho', nodesep='0.8', ranksep='0.5')
    g.attr('node', shape='box', style='filled,rounded', fontname='Helvetica', fontsize='10')
    g.attr('edge', fontname='Helvetica', fontsize='9')
    
    # Define node styles
    input_style = {'fillcolor': '#E8F8E8', 'color': '#5D8B5D', 'penwidth': '2'}
    process_style = {'fillcolor': '#E8F0F8', 'color': '#5D7B8B', 'penwidth': '1.5'}
    attention_style = {'fillcolor': '#F8E8E8', 'color': '#8B5D5D', 'penwidth': '1.5'}
    ffn_style = {'fillcolor': '#F8F8E8', 'color': '#8B8B5D', 'penwidth': '1.5'}
    pooling_style = {'fillcolor': '#E8F8F0', 'color': '#5D8B7B', 'penwidth': '1.5'}
    vae_style = {'fillcolor': '#F0E8F8', 'color': '#7B5D8B', 'penwidth': '1.5'}
    output_style = {'fillcolor': '#F8E8F0', 'color': '#8B5D7B', 'penwidth': '2'}
    
    # Create subgraphs for different components
    with g.subgraph(name='cluster_input') as c:
        c.attr(label='Input Processing', style='rounded,filled', fillcolor='#F5FFF5', color='#5D8B5D')
        c.node('input', 'Input\n(168×3)', **input_style)
        c.node('bn1', 'BatchNormalization', **process_style)
        c.node('reshape1', 'Reshape\n(168, 1, 1, 3)', **process_style)
    
    with g.subgraph(name='cluster_temporal') as c:
        c.attr(label='Temporal Processing', style='rounded,filled', fillcolor='#F5FAFF', color='#5D7B8B')
        c.node('convlstm', 'ConvLSTM2D\n32 filters', **process_style)
        c.node('bn_5d', 'BatchNorm5D', **process_style)
        c.node('reshape2', 'Reshape\n(168, 32)', **process_style)
        c.node('bn2', 'BatchNormalization', **process_style)
    
    with g.subgraph(name='cluster_attention') as c:
        c.attr(label='Self-Attention Module', style='rounded,filled', fillcolor='#FFF5F5', color='#8B5D5D')
        c.node('mha', 'MultiHeadAttention', **attention_style)
        c.node('add1', 'Add\n(Residual Connection)', **attention_style)
        c.node('ln1', 'LayerNormalization', **attention_style)
        c.node('bn3', 'BatchNormalization', **attention_style)
        c.node('dropout1', 'Dropout (0.1)', **attention_style)
    
    with g.subgraph(name='cluster_ffn') as c:
        c.attr(label='Feed-Forward Network', style='rounded,filled', fillcolor='#FFFFF5', color='#8B8B5D')
        c.node('dense1', 'Dense (64)', **ffn_style)
        c.node('bn4', 'BatchNormalization', **ffn_style)
        c.node('dropout2', 'Dropout (0.1)', **ffn_style)
        c.node('dense2', 'Dense (32)', **ffn_style)
        c.node('bn5', 'BatchNormalization', **ffn_style)
        c.node('add2', 'Add\n(Residual Connection)', **ffn_style)
        c.node('ln2', 'LayerNormalization', **ffn_style)
    
    with g.subgraph(name='cluster_conv') as c:
        c.attr(label='Parallel Conv1D Paths', style='rounded,filled', fillcolor='#F5FFF5', color='#5D8B5D')
        c.node('conv1', 'Conv1D\nkernel=3, filters=16', **process_style)
        c.node('bn6', 'BatchNormalization', **process_style)
        c.node('dropout3', 'Dropout (0.1)', **process_style)
        c.node('conv2', 'Conv1D\nkernel=5, filters=16', **process_style)
        c.node('bn7', 'BatchNormalization', **process_style)
        c.node('dropout4', 'Dropout (0.1)', **process_style)
    
    with g.subgraph(name='cluster_pooling') as c:
        c.attr(label='Pooling Operations', style='rounded,filled', fillcolor='#F5FFF9', color='#5D8B7B')
        c.node('maxpool', 'GlobalMaxPooling1D', **pooling_style)
        c.node('avgpool', 'GlobalAveragePooling1D', **pooling_style)
        c.node('maxpool1', 'GlobalMaxPooling1D', **pooling_style)
        c.node('maxpool2', 'GlobalMaxPooling1D', **pooling_style)
    
    with g.subgraph(name='cluster_vae') as c:
        c.attr(label='Variational Component', style='rounded,filled', fillcolor='#F9F5FF', color='#7B5D8B')
        c.node('concat1', 'Concatenate', **vae_style)
        c.node('bn8', 'BatchNormalization', **vae_style)
        c.node('z_mean', 'Dense (16)\nz_mean', **vae_style)
        c.node('z_log_var', 'Dense (16)\nz_log_var', **vae_style)
        c.node('sampling', 'Lambda\n(Sampling)', **vae_style)
        c.node('kl_loss', 'KL Divergence Loss', **vae_style)
    
    with g.subgraph(name='cluster_classifier') as c:
        c.attr(label='Classification Head', style='rounded,filled', fillcolor='#FFF5F9', color='#8B5D7B')
        c.node('concat2', 'Concatenate\nAll Features', **output_style)
        c.node('bn9', 'BatchNormalization', **output_style)
        c.node('dense3', 'Dense (64)', **output_style)
        c.node('bn10', 'BatchNormalization', **output_style)
        c.node('dropout5', 'Dropout (0.2)', **output_style)
        c.node('dense4', 'Dense (32)', **output_style)
        c.node('bn11', 'BatchNormalization', **output_style)
        c.node('dropout6', 'Dropout (0.2)', **output_style)
        c.node('output', 'Dense (1)\nSigmoid', **output_style)
    
    # Add invisible nodes for better layout
    g.node('invisible1', style='invis')
    g.node('invisible2', style='invis')
    
    # Connect nodes
    # Input processing
    g.edge('input', 'bn1')
    g.edge('bn1', 'reshape1')
    g.edge('reshape1', 'convlstm')
    
    # Temporal processing
    g.edge('convlstm', 'bn_5d')
    g.edge('bn_5d', 'reshape2')
    g.edge('reshape2', 'bn2')
    g.edge('bn2', 'mha')
    
    # Self-attention module
    g.edge('mha', 'add1')
    g.edge('bn2', 'add1', constraint='false')
    g.edge('add1', 'ln1')
    g.edge('ln1', 'bn3')
    g.edge('bn3', 'dropout1')
    g.edge('dropout1', 'dense1')
    
    # Feed-forward network
    g.edge('dense1', 'bn4')
    g.edge('bn4', 'dropout2')
    g.edge('dropout2', 'dense2')
    g.edge('dense2', 'bn5')
    g.edge('bn5', 'add2')
    g.edge('dropout1', 'add2', constraint='false')
    g.edge('add2', 'ln2')
    
    # Pooling from transformer output
    g.edge('ln2', 'maxpool')
    g.edge('ln2', 'avgpool')
    
    # Parallel conv paths
    g.edge('input', 'conv1', constraint='false')
    g.edge('input', 'conv2', constraint='false')
    g.edge('conv1', 'bn6')
    g.edge('bn6', 'dropout3')
    g.edge('dropout3', 'maxpool1')
    g.edge('conv2', 'bn7')
    g.edge('bn7', 'dropout4')
    g.edge('dropout4', 'maxpool2')
    
    # Connect to concatenation
    g.edge('maxpool', 'concat1')
    g.edge('avgpool', 'concat1')
    g.edge('concat1', 'bn8')
    g.edge('bn8', 'z_mean')
    g.edge('bn8', 'z_log_var')
    g.edge('z_mean', 'sampling')
    g.edge('z_log_var', 'sampling')
    g.edge('z_mean', 'kl_loss')
    g.edge('z_log_var', 'kl_loss')
    
    # Feature concatenation
    g.edge('maxpool', 'concat2', constraint='false')
    g.edge('avgpool', 'concat2', constraint='false')
    g.edge('maxpool1', 'concat2')
    g.edge('maxpool2', 'concat2')
    g.edge('sampling', 'concat2')
    
    # Classification head
    g.edge('concat2', 'bn9')
    g.edge('bn9', 'dense3')
    g.edge('dense3', 'bn10')
    g.edge('bn10', 'dropout5')
    g.edge('dropout5', 'dense4')
    g.edge('dense4', 'bn11')
    g.edge('bn11', 'dropout6')
    g.edge('dropout6', 'output')
    
    # Add title
    g.attr(label='Zero Curtain Model Architecture', fontsize='20', fontname='Helvetica')
    
    # Add parameter info as a title note
    g.attr(labelloc='t')
    g.attr(labeljust='c')
    
    # Render the graph
    output_path = os.path.splitext(output_filename)[0]  # Remove extension
    g.render(output_path, format='pdf', cleanup=True)
    
    # Also generate a PNG version for quick viewing
    g.format = 'png'
    g.render(output_path, cleanup=True)
    
    print(f"Generated Graphviz model diagram: {output_path}.pdf and {output_path}.png")
    
    return g

#------------------------------------------------------------------------------
# SECTION 2: SIMPLIFIED ARCHITECTURE DIAGRAM
#------------------------------------------------------------------------------

def create_simplified_diagram(output_filename='zero_curtain_simplified_diagram.pdf'):
    """
    Creates a simplified diagram showing only the main functional blocks,
    suitable for high-level explanation in publications.
    
    Args:
        output_filename: Output file name (PDF recommended for vector graphics)
    """
    g = gv.Digraph('zero_curtain_simplified', format='pdf')
    g.attr(rankdir='TB', splines='ortho')
    g.attr('node', shape='box', style='filled,rounded', fontname='Helvetica', fontsize='12', 
          height='0.6', width='2.0')
    
    # Define node styles
    block_style = {'style': 'filled,rounded', 'fontname': 'Helvetica', 'fontsize': '12'}
    
    # Create nodes for main functional blocks
    g.node('input', 'Input\n(168 × 3)', fillcolor='#E8F8E8', **block_style)
    
    # Main branches
    g.node('temporal', 'Temporal Processing\n(ConvLSTM2D)', fillcolor='#E8F0F8', **block_style)
    g.node('attention', 'Self-Attention\nMechanism', fillcolor='#F8E8E8', **block_style)
    g.node('ffn', 'Feed-Forward Network\nw/ Residual Connections', fillcolor='#F8F8E8', **block_style)
    
    # Parallel branch
    g.node('conv', 'Parallel Conv1D\nFeature Extraction', fillcolor='#E8F8F0', **block_style)
    
    # Feature processing
    g.node('pooling', 'Global Pooling\nOperations', fillcolor='#F0E8F8', **block_style)
    g.node('vae', 'Variational\nComponent', fillcolor='#F8E8F0', **block_style)
    g.node('concat', 'Feature\nConcatenation', fillcolor='#F0F8F0', **block_style)
    
    # Classification
    g.node('classifier', 'Classification\nHead', fillcolor='#F8F0E8', **block_style)
    g.node('output', 'Zero Curtain\nPrediction', fillcolor='#F8E8F0', **block_style)
    
    # Add loss component
    g.node('kl_loss', 'KL Divergence\nLoss', fillcolor='#F0E8E8', **block_style)
    
    # Connect nodes with edges
    g.edge('input', 'temporal')
    g.edge('input', 'conv')
    g.edge('temporal', 'attention')
    g.edge('attention', 'ffn')
    g.edge('ffn', 'pooling')
    g.edge('conv', 'pooling')
    g.edge('pooling', 'vae')
    g.edge('vae', 'kl_loss', style='dashed')
    g.edge('pooling', 'concat')
    g.edge('vae', 'concat')
    g.edge('concat', 'classifier')
    g.edge('classifier', 'output')
    
    # Add title and parameter information
    g.attr(label='Zero Curtain Model - Simplified Architecture\n\nTotal: 48,525 parameters (47,527 trainable)', 
          fontsize='16', fontname='Helvetica')
    
    # Render the graph
    output_path = os.path.splitext(output_filename)[0]  # Remove extension
    g.render(output_path, format='pdf', cleanup=True)
    
    # Also generate a PNG version for quick viewing
    g.format = 'png'
    g.render(output_path, cleanup=True)
    
    print(f"Generated simplified diagram: {output_path}.pdf and {output_path}.png")
    
    return g

#------------------------------------------------------------------------------
# SECTION 3: PARAMETER DISTRIBUTION DIAGRAM
#------------------------------------------------------------------------------

def create_parameter_diagram(output_filename='zero_curtain_parameter_diagram.pdf'):
    """
    Creates a diagram highlighting the parameter distribution across the model components.
    """
    g = gv.Digraph('zero_curtain_parameters', format='pdf')
    g.attr(rankdir='LR', splines='ortho')
    g.attr('node', shape='box', style='filled,rounded', fontname='Helvetica', fontsize='12')
    
    # Define parameter counts for each component
    params = {
        'ConvLSTM': 13696,
        'Attention': 16992,
        'FFN': 4576,
        'Conv1D': 480,
        'VAE': 2080,
        'Classifier': 9681,
        'Normalization': 1000  # Approximate
    }
    
    total_params = sum(params.values())
    
    # Create nodes with parameter percentages
    for component, count in params.items():
        percentage = (count / total_params) * 100
        label = f"{component}\n{count:,} parameters\n({percentage:.1f}%)"
        
        # Calculate node width based on parameter percentage
        width = 0.5 + (percentage / 20)
        
        # Assign colors based on component type
        if component == 'ConvLSTM':
            fillcolor = '#E8F0F8'
        elif component == 'Attention':
            fillcolor = '#F8E8E8'
        elif component == 'FFN':
            fillcolor = '#F8F8E8'
        elif component == 'Conv1D':
            fillcolor = '#E8F8F0'
        elif component == 'VAE':
            fillcolor = '#F0E8F8'
        elif component == 'Classifier':
            fillcolor = '#F8F0E8'
        else:
            fillcolor = '#F0F0F0'
        
        g.node(component, label, fillcolor=fillcolor, width=str(width))
    
    # Connect components to show the flow of data
    g.edge('ConvLSTM', 'Attention')
    g.edge('Attention', 'FFN')
    g.edge('Conv1D', 'VAE')
    g.edge('FFN', 'Classifier')
    g.edge('VAE', 'Classifier')
    
    # Add title
    g.attr(label='Zero Curtain Model - Parameter Distribution\nTotal: 48,525 parameters', 
           fontsize='16', fontname='Helvetica')
    
    # Render the graph
    output_path = os.path.splitext(output_filename)[0]  # Remove extension
    g.render(output_path, format='pdf', cleanup=True)
    
    # Also generate a PNG version for quick viewing
    g.format = 'png'
    g.render(output_path, cleanup=True)
    
    print(f"Generated parameter distribution diagram: {output_path}.pdf and {output_path}.png")
    
    return g

#------------------------------------------------------------------------------
# SECTION 4: DATA FLOW DIAGRAM
#------------------------------------------------------------------------------

def create_dataflow_diagram(output_filename='zero_curtain_dataflow_diagram.pdf'):
    """
    Creates a diagram showing the logical data flow through the model.
    """
    g = gv.Digraph('zero_curtain_dataflow', format='pdf')
    g.attr(rankdir='TB', splines='ortho')
    g.attr('node', shape='box', style='filled,rounded', fontname='Helvetica', fontsize='10')
    
    # Input data
    g.node('input_data', 'Time Series Input\n(168 × 3)', 
           shape='parallelogram', fillcolor='#E8F8E8')
    
    # Feature extraction paths
    with g.subgraph(name='cluster_feature_extraction') as c:
        c.attr(label='Feature Extraction', style='rounded,filled', fillcolor='#F5FAFF')
        
        # Temporal path
        c.node('temporal_path', 'Temporal Processing Path\nConvLSTM2D → Attention → FFN', 
               fillcolor='#E8F0F8')
        
        # Spatial path
        c.node('spatial_path', 'Spatial Processing Path\nParallel Conv1D Layers', 
               fillcolor='#F0F8E8')
    
    # Feature aggregation
    with g.subgraph(name='cluster_aggregation') as c:
        c.attr(label='Feature Aggregation', style='rounded,filled', fillcolor='#FFF5FA')
        
        c.node('pooling', 'Global Pooling Operations', fillcolor='#F0E8F8')
        c.node('vae_component', 'Variational Component\nz_mean & z_log_var → Sampling', 
               fillcolor='#F8E8F0')
        c.node('feature_concat', 'Feature Concatenation', fillcolor='#F8F0F0')
    
    # Classification
    g.node('classification', 'Classification Layers\nDense → BatchNorm → Dropout', 
           fillcolor='#F8F0E8')
    
    # Output
    g.node('output', 'Zero Curtain Prediction\n(Binary Classification)', 
           shape='parallelogram', fillcolor='#F8E8F0')
    
    # KL Loss
    g.node('kl_loss', 'KL Divergence Loss\nRegularization Term', 
           shape='ellipse', fillcolor='#F0E8E8')
    
    # Connect components
    g.edge('input_data', 'temporal_path')
    g.edge('input_data', 'spatial_path')
    g.edge('temporal_path', 'pooling')
    g.edge('spatial_path', 'pooling')
    g.edge('pooling', 'vae_component')
    g.edge('pooling', 'feature_concat')
    g.edge('vae_component', 'feature_concat')
    g.edge('vae_component', 'kl_loss', style='dashed')
    g.edge('feature_concat', 'classification')
    g.edge('classification', 'output')
    
    # Add data dimensions as edge labels
    g.edge_attr.update(fontsize='8', fontname='Helvetica')
    
    # Add title and description
    g.attr(label='Zero Curtain Model - Data Flow Diagram', 
           fontsize='16', fontname='Helvetica')
    
    # Render the graph
    output_path = os.path.splitext(output_filename)[0]  # Remove extension
    g.render(output_path, format='pdf', cleanup=True)
    
    # Also generate a PNG version for quick viewing
    g.format = 'png'
    g.render(output_path, cleanup=True)
    
    print(f"Generated data flow diagram: {output_path}.pdf and {output_path}.png")
    
    return g

#------------------------------------------------------------------------------
# SECTION 5: MERMAID DIAGRAM GENERATION
#------------------------------------------------------------------------------

def generate_mermaid_diagram(output_filename='zero_curtain_mermaid_diagram.md'):
    """
    Creates a Mermaid.js diagram string representation of the model.
    This can be pasted into any Mermaid-compatible viewer like GitHub, GitLab,
    or dedicated Mermaid rendering tools.
    
    Args:
        output_filename: Output markdown file to save the diagram
    """
    mermaid_diagram = """
flowchart TB
    %% Define style classes
    classDef input fill:#d4f4d4,stroke:#5a8c5a,color:#000000
    classDef process fill:#d4e6f4,stroke:#5a7c8c,color:#000000
    classDef attention fill:#f4d4d4,stroke:#8c5a5a,color:#000000
    classDef ffn fill:#f4f4d4,stroke:#8c8c5a,color:#000000
    classDef conv fill:#e6f4d4,stroke:#7c8c5a,color:#000000
    classDef pooling fill:#d4f4e6,stroke:#5a8c7c,color:#000000
    classDef vae fill:#e6d4f4,stroke:#7c5a8c,color:#000000
    classDef classify fill:#f4e6d4,stroke:#8c7c5a,color:#000000
    classDef output fill:#f4d4e6,stroke:#8c5a7c,color:#000000
    
    %% Input block
    inp[Input (168×3)]:::input
    
    %% Main temporal branch
    subgraph Temporal["Temporal Processing Branch"]
        bn1[BatchNormalization]:::process
        reshape1[Reshape to 5D]:::process
        convlstm["ConvLSTM2D
        (32 filters)"]:::process
        bn5d[BatchNorm5D]:::process
        reshape2[Reshape to 3D]:::process
        bn2[BatchNormalization]:::process
    end
    
    %% Self-attention module
    subgraph Attention["Self-Attention Module"]
        mha["MultiHeadAttention
        (4 heads)"]:::attention
        add1[Add (Residual)]:::attention
        ln1[LayerNormalization]:::attention
        bn3[BatchNormalization]:::attention
        dropout1[Dropout (0.1)]:::attention
    end
    
    %% Feed-forward network
    subgraph FFN["Feed-Forward Network"]
        dense1[Dense (64)]:::ffn
        bn4[BatchNormalization]:::ffn
        dropout2[Dropout (0.1)]:::ffn
        dense2[Dense (32)]:::ffn
        bn5[BatchNormalization]:::ffn
        add2[Add (Residual)]:::ffn
        ln2[LayerNormalization]:::ffn
    end
    
    %% Pooling operations
    subgraph Pooling["Pooling Operations"]
        maxpool[GlobalMaxPooling1D]:::pooling
        avgpool[GlobalAveragePooling1D]:::pooling
    end
    
    %% Parallel Conv1D paths
    subgraph Parallel["Parallel Conv1D Paths"]
        conv1["Conv1D
        (kernel=3, filters=16)"]:::conv
        bn6[BatchNormalization]:::conv
        dropout3[Dropout (0.1)]:::conv
        conv2["Conv1D
        (kernel=5, filters=16)"]:::conv
        bn7[BatchNormalization]:::conv
        dropout4[Dropout (0.1)]:::conv
        maxpool1[GlobalMaxPooling1D]:::pooling
        maxpool2[GlobalMaxPooling1D]:::pooling
    end
    
    %% VAE component
    subgraph VAE["Variational Component"]
        concat1[Concatenate]:::vae
        bn8[BatchNormalization]:::vae
        zmean["Dense (16)
        z_mean"]:::vae
        zlogvar["Dense (16)
        z_log_var"]:::vae
        sampling[Lambda Sampling]:::vae
        klloss["KL Divergence
        Regularization"]:::vae
    end
    
    %% Classification head
    subgraph Classifier["Classification Head"]
        concat2["Concatenate
        All Features"]:::classify
        bn9[BatchNormalization]:::classify
        dense3[Dense (64)]:::classify
        bn10[BatchNormalization]:::classify
        dropout5[Dropout (0.2)]:::classify
        dense4[Dense (32)]:::classify
        bn11[BatchNormalization]:::classify
        dropout6[Dropout (0.2)]:::classify
        output["Dense (1)
        Sigmoid"]:::output
    end
    
    %% Define the connections
    inp --> bn1
    bn1 --> reshape1
    reshape1 --> convlstm
    convlstm --> bn5d
    bn5d --> reshape2
    reshape2 --> bn2
    
    bn2 --> mha
    mha --> add1
    bn2 --> add1
    add1 --> ln1
    ln1 --> bn3
    bn3 --> dropout1
    
    dropout1 --> dense1
    dense1 --> bn4
    bn4 --> dropout2
    dropout2 --> dense2
    dense2 --> bn5
    bn5 --> add2
    dropout1 --> add2
    add2 --> ln2
    
    ln2 --> maxpool
    ln2 --> avgpool
    
    inp --> conv1
    inp --> conv2
    conv1 --> bn6
    bn6 --> dropout3
    dropout3 --> maxpool1
    conv2 --> bn7
    bn7 --> dropout4
    dropout4 --> maxpool2
    
    maxpool --> concat1
    avgpool --> concat1
    concat1 --> bn8
    bn8 --> zmean
    bn8 --> zlogvar
    zmean --> sampling
    zlogvar --> sampling
    zmean --> klloss
    zlogvar --> klloss
    
    maxpool --> concat2
    avgpool --> concat2
    maxpool1 --> concat2
    maxpool2 --> concat2
    sampling --> concat2
    
    concat2 --> bn9
    bn9 --> dense3
    dense3 --> bn10
    bn10 --> dropout5
    dropout5 --> dense4
    dense4 --> bn11
    bn11 --> dropout6
    dropout6 --> output
    
    %% Title
    title["Zero Curtain Model Architecture
    Total: 48,525 parameters (47,527 trainable)"]
    
    %% Parameter info
    class title subtitle
"""
    
    # Write the Mermaid diagram to a file
    with open(output_filename, 'w') as f:
        f.write(mermaid_diagram)
    
    print(f"Generated Mermaid.js diagram in: {output_filename}")
    print("This can be pasted into any Mermaid-compatible viewer.")
    
    return mermaid_diagram

#------------------------------------------------------------------------------
# SECTION 6: MAIN EXECUTION AND UTILITIES
#------------------------------------------------------------------------------

def create_all_graphviz_visualizations(output_dir=None):
    """
    Create all Graphviz visualizations for the Zero Curtain model architecture
    and save them to the specified directory.
    
    Args:
        output_dir: Directory to save visualizations (created if doesn't exist)
    """
    # Create output directory if specified and doesn't exist
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        
        # Update paths to include output directory
        model_path = os.path.join(output_dir, 'zero_curtain_graphviz_model.pdf')
        simplified_path = os.path.join(output_dir, 'zero_curtain_simplified_diagram.pdf')
        parameter_path = os.path.join(output_dir, 'zero_curtain_parameter_diagram.pdf')
        dataflow_path = os.path.join(output_dir, 'zero_curtain_dataflow_diagram.pdf')
        mermaid_path = os.path.join(output_dir, 'zero_curtain_mermaid_diagram.md')
    else:
        # Use default paths in current directory
        model_path = 'zero_curtain_graphviz_model.pdf'
        simplified_path = 'zero_curtain_simplified_diagram.pdf'
        parameter_path = 'zero_curtain_parameter_diagram.pdf'
        dataflow_path = 'zero_curtain_dataflow_diagram.pdf'
        mermaid_path = 'zero_curtain_mermaid_diagram.md'
    
    # Generate all visualizations
    print("Generating Zero Curtain model Graphviz visualizations...")
    create_model_diagram(model_path)
    create_simplified_diagram(simplified_path)
    create_parameter_diagram(parameter_path)
    create_dataflow_diagram(dataflow_path)
    generate_mermaid_diagram(mermaid_path)
    
    print("\nAll Graphviz visualizations created successfully!")
    print("Generated files:")
    print(f"  1. Detailed model diagram: {model_path}")
    print(f"  2. Simplified architecture diagram: {simplified_path}")
    print(f"  3. Parameter distribution diagram: {parameter_path}")
    print(f"  4. Data flow diagram: {dataflow_path}")
    print(f"  5. Mermaid.js diagram: {mermaid_path}")
    
    return {
        'model': model_path,
        'simplified': simplified_path,
        'parameters': parameter_path,
        'dataflow': dataflow_path,
        'mermaid': mermaid_path
    }

# Execute all visualizations if this script is run directly
if __name__ == "__main__":
    create_all_graphviz_visualizations()

# import os
# import numpy as np
# import pandas as pd
# import gc
# import pickle
# import psutil
# from datetime import datetime, timedelta
# import time
# from scipy.interpolate import interp1d
# import matplotlib.pyplot as plt
# import seaborn as sns

# def memory_usage():
#     """Monitor memory usage in MB"""
#     process = psutil.Process(os.getpid())
#     mem_info = process.memory_info()
#     return mem_info.rss / 1024**2  # Memory in MB

# def save_checkpoint(data, checkpoint_dir, name):
#     """Save checkpoint data to pickle file"""
#     os.makedirs(checkpoint_dir, exist_ok=True)
#     checkpoint_path = os.path.join(checkpoint_dir, f'{name}.pkl')
#     with open(checkpoint_path, 'wb') as f:
#         pickle.dump(data, f)
#     print(f"Saved checkpoint to {checkpoint_path}")

# def load_checkpoint(checkpoint_dir, name):
#     """Load checkpoint data from pickle file"""
#     checkpoint_path = os.path.join(checkpoint_dir, f'{name}.pkl')
#     try:
#         with open(checkpoint_path, 'rb') as f:
#             data = pickle.load(f)
#         print(f"Loaded checkpoint from {checkpoint_path}")
#         return data
#     except:
#         print(f"No checkpoint found at {checkpoint_path}")
#         return None

# # def aggressive_cleanup():
# #     """Aggressively clean up memory"""
# #     # Call garbage collection multiple times to ensure objects are freed
# #     for _ in range(3):
# #         gc.collect()
    
# #     # Force a memory compaction if running on Linux
# #     try:
# #         import ctypes
# #         libc = ctypes.CDLL('libc.so.6')
# #         # MADV_DONTNEED = 4
# #         # MADV_FREE = 8 (only on newer kernels)
# #         libc.malloc_trim(0)
# #     except:
# #         pass

# # data_loader.py
# import pyarrow as pa
# import pyarrow.feather as pf
# import pyarrow.compute as pc
# import pyarrow.dataset as ds
# import gc
# from tqdm.auto import tqdm

# def get_time_range(feather_path):
#     """Get min and max datetime from feather file efficiently"""
#     print("Determining dataset time range")
#     try:
#         # Use PyArrow for efficient metadata access
#         table = pf.read_table(feather_path, columns=['datetime'])
#         datetime_col = table['datetime']
#         min_date = pd.Timestamp(pc.min(datetime_col).as_py())
#         max_date = pd.Timestamp(pc.max(datetime_col).as_py())
#         del table, datetime_col
#         gc.collect()
#     except Exception as e:
#         print(f"Error with PyArrow min/max: {str(e)}")
#         print("Falling back to reading sample rows...")
        
#         # Read just first and last rows after sorting
#         # This is more efficient than reading all data
#         try:
#             dataset = ds.dataset(feather_path, format='feather')
            
#             # Get min date from sorted data (first row)
#             table_min = dataset.to_table(
#                 columns=['datetime'],
#                 filter=None,
#                 use_threads=True,
#                 limit=1
#             )
#             min_date = pd.Timestamp(table_min['datetime'][0].as_py())
            
#             # Get max date from reverse sorted data (first row)
#             # Note: PyArrow Dataset API doesn't support reverse sort directly
#             # So we read all datetime values and find max
#             table_all = dataset.to_table(columns=['datetime'])
#             max_date = pd.Timestamp(pc.max(table_all['datetime']).as_py())
            
#             del table_min, table_all, dataset
#             gc.collect()
#         except Exception as inner_e:
#             print(f"Error with optimized approach: {str(inner_e)}")
#             print("Using full scan method (slow)...")
            
#             # Last resort: read all timestamps in chunks
#             min_date = pd.Timestamp.max
#             max_date = pd.Timestamp.min
            
#             # Open file directly to avoid loading everything
#             table = pf.read_table(feather_path, columns=['datetime'])
#             datetime_values = table['datetime'].to_pandas()
            
#             min_date = datetime_values.min()
#             max_date = datetime_values.max()
            
#             del datetime_values, table
#             gc.collect()
    
#     print(f"Data timespan: {min_date} to {max_date}")
#     return min_date, max_date

# def get_site_metadata(feather_path):
#     """Get comprehensive site metadata including moisture capabilities"""
#     print("Extracting site metadata including moisture measurement capabilities")
#     try:
#         # Using PyArrow for efficient column scanning
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Read site metadata columns
#         table = dataset.to_table(columns=[
#             'source', 'latitude', 'longitude', 'elevation', 
#             'soil_temp_depth', 'soil_moist_depth'
#         ])
        
#         # Convert to pandas
#         site_meta_df = table.to_pandas()
        
#         # Get unique site information
#         sites = site_meta_df.drop_duplicates('source')
        
#         # Calculate moisture availability by site
#         site_moisture_avail = site_meta_df.groupby('source')['soil_moist_depth'].apply(
#             lambda x: not x.isna().all()
#         ).reset_index()
#         site_moisture_avail.columns = ['source', 'has_moisture_data']
        
#         # Merge site information with moisture availability
#         sites = sites.merge(site_moisture_avail, on='source', how='left')
        
#         # Clean up
#         del table, site_meta_df, site_moisture_avail, dataset
#         gc.collect()
        
#         print(f"Found {len(sites)} unique sites, {sites['has_moisture_data'].sum()} with moisture ...
        
#         return sites
        
#     except Exception as e:
#         print(f"Error extracting site metadata: {str(e)}")
#         return None

# def get_unique_site_depths(feather_path, include_moisture=True):
#     """Get unique site-depth combinations with both temperature and moisture capabilities"""
#     print("Finding unique site-depth combinations")
#     print(f"Memory before processing: {memory_usage():.1f} MB")
    
#     df = None  # Initialize df to avoid UnboundLocalError in except block
    
#     try:
#         # Use PyArrow for efficient operation
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Read only the columns we need
#         cols = ['source', 'soil_temp_depth']
#         if include_moisture:
#             cols.extend(['soil_moist_depth'])
            
#         table = dataset.to_table(columns=cols)
        
#         # Convert to pandas
#         df = table.to_pandas()
        
#         # Get valid combinations (where temp_depth is not NaN)
#         valid_df = df.dropna(subset=['soil_temp_depth'])
        
#         # Get unique combinations
#         site_temps = valid_df.drop_duplicates(['source', 'soil_temp_depth'])
        
#         # If moisture is included, mark sites with moisture capabilities
#         if include_moisture and 'soil_moist_depth' in df.columns:
#             # Group by source and check for moisture data availability
#             moisture_avail = df.groupby('source')['soil_moist_depth'].apply(
#                 lambda x: not x.isna().all()
#             ).reset_index()
#             moisture_avail.columns = ['source', 'has_moisture_data']
            
#             # Merge with site_temps
#             site_temps = site_temps.merge(moisture_avail, on='source', how='left')
            
#             # Fill NaN values
#             site_temps['has_moisture_data'] = site_temps['has_moisture_data'].fillna(False)
            
#             # Clean up
#             del moisture_avail
#         else:
#             site_temps['has_moisture_data'] = False
        
#         # Add latitude and longitude if available
#         if 'latitude' in df.columns and 'longitude' in df.columns:
#             # Get site coordinates
#             site_coords = df.drop_duplicates('source')[['source', 'latitude', 'longitude']]
#             # Merge with site_temps
#             site_temps = site_temps.merge(site_coords, on='source', how='left')
        
#         # Clean up
#         del table, df, valid_df, dataset
#         gc.collect()
        
#     except Exception as e:
#         print(f"Error with PyArrow approach: {str(e)}")
#         print("Falling back to chunked pandas approach...")
        
#         # Fallback: Process in chunks
#         chunk_size = 1000000
#         unique_combos = set()
#         has_moisture_by_site = {}
#         site_coords = {}
        
#         # Read feather in chunks (this is slower but more robust)
#         cols = ['source', 'soil_temp_depth']
#         if include_moisture:
#             cols.extend(['soil_moist_depth'])
            
#         # Add coordinate columns if we have them
#         # Important: Don't reference df here since it might not exist if the exception occurred ea...
#         try:
#             # Try to peek at the feather file to see what columns exist
#             peek_df = pd.read_feather(feather_path, nrows=1)
#             if 'latitude' in peek_df.columns and 'longitude' in peek_df.columns:
#                 cols.extend(['latitude', 'longitude'])
#             del peek_df
#         except:
#             pass  # If peek fails, just continue without coordinates
        
#         # Process in chunks - NOT using context manager
#         # Read the feather file in chunks instead of using 'with' statement
#         try:
#             # Calculate number of rows to estimate chunks
#             total_rows = 0
#             try:
#                 # Try to get row count using pyarrow if possible
#                 import pyarrow.parquet as pq
#                 dataset = ds.dataset(feather_path, format='feather')
#                 total_rows = dataset.count_rows()
#                 del dataset
#             except:
#                 try:
#                     # Fallback: Try to get row estimate from file size
#                     import os
#                     file_size = os.path.getsize(feather_path)
#                     # Rough estimate: 1 million rows per GB
#                     total_rows = int(file_size / (1024**3) * 1000000)
#                     if total_rows < 1000000:
#                         total_rows = 1000000  # Minimum estimate
#                 except:
#                     total_rows = 10000000  # Just guess if we can't determine
            
#             # Process in chunks
#             for chunk_start in range(0, total_rows, chunk_size):
#                 try:
#                     chunk = pd.read_feather(
#                         feather_path, 
#                         columns=cols,
#                         nthreads=1
#                     )
                    
#                     # If we got all data at once, just process it directly
#                     valid_rows = chunk.dropna(subset=['soil_temp_depth'])
#                     chunk_combinations = set(zip(valid_rows['source'], valid_rows['soil_temp_depth...
#                     unique_combos.update(chunk_combinations)
                    
#                     # Track moisture data availability by site
#                     if include_moisture and 'soil_moist_depth' in chunk.columns:
#                         for site in chunk['source'].unique():
#                             site_chunk = chunk[chunk['source'] == site]
#                             has_moisture = not site_chunk['soil_moist_depth'].isna().all()
#                             has_moisture_by_site[site] = has_moisture
                    
#                     # Track coordinates
#                     if 'latitude' in chunk.columns and 'longitude' in chunk.columns:
#                         for site in chunk['source'].unique():
#                             if site not in site_coords:
#                                 site_chunk = chunk[chunk['source'] == site].iloc[0]
#                                 site_coords[site] = {
#                                     'latitude': site_chunk['latitude'],
#                                     'longitude': site_chunk['longitude']
#                                 }
                    
#                     # If we read all data at once, break the loop
#                     break
                    
#                 except Exception as chunk_error:
#                     print(f"Error reading chunk: {str(chunk_error)}")
#                     # Try reading smaller chunks if full read fails
#                     try:
#                         smaller_chunk_size = min(chunk_size // 2, 100000)
#                         chunk = pd.read_feather(
#                             feather_path, 
#                             columns=cols,
#                             nthreads=1,
#                             nrows=smaller_chunk_size,
#                             skip=chunk_start
#                         )
                        
#                         # Process smaller chunk
#                         valid_rows = chunk.dropna(subset=['soil_temp_depth'])
#                         chunk_combinations = set(zip(valid_rows['source'], valid_rows['soil_temp_d...
#                         unique_combos.update(chunk_combinations)
                        
#                         # Track moisture data
#                         if include_moisture and 'soil_moist_depth' in chunk.columns:
#                             for site in chunk['source'].unique():
#                                 site_chunk = chunk[chunk['source'] == site]
#                                 has_moisture = not site_chunk['soil_moist_depth'].isna().all()
#                                 has_moisture_by_site[site] = has_moisture
                        
#                         # Track coordinates
#                         if 'latitude' in chunk.columns and 'longitude' in chunk.columns:
#                             for site in chunk['source'].unique():
#                                 if site not in site_coords:
#                                     site_chunk = chunk[chunk['source'] == site].iloc[0]
#                                     site_coords[site] = {
#                                         'latitude': site_chunk['latitude'],
#                                         'longitude': site_chunk['longitude']
#                                     }
#                     except Exception as small_chunk_error:
#                         print(f"Error reading smaller chunk: {str(small_chunk_error)}")
#                         # Skip this chunk if both attempts fail
                
#                 # Clean up regardless of success
#                 if 'chunk' in locals():
#                     del chunk
#                 if 'valid_rows' in locals():
#                     del valid_rows
#                 if 'chunk_combinations' in locals():
#                     del chunk_combinations
#                 gc.collect()
        
#         except Exception as e:
#             print(f"Error during chunked reading: {str(e)}")
#             # As a last resort, try to read just the first part of the file
#             try:
#                 chunk = pd.read_feather(feather_path, columns=cols, nrows=10000)
                
#                 # Process what we've got
#                 valid_rows = chunk.dropna(subset=['soil_temp_depth'])
#                 chunk_combinations = set(zip(valid_rows['source'], valid_rows['soil_temp_depth']))
#                 unique_combos.update(chunk_combinations)
                
#                 # Track moisture data
#                 if include_moisture and 'soil_moist_depth' in chunk.columns:
#                     for site in chunk['source'].unique():
#                         site_chunk = chunk[chunk['source'] == site]
#                         has_moisture = not site_chunk['soil_moist_depth'].isna().all()
#                         has_moisture_by_site[site] = has_moisture
                
#                 # Clean up
#                 del chunk, valid_rows, chunk_combinations
#                 gc.collect()
#             except Exception as final_error:
#                 print(f"All attempts to read feather file failed: {str(final_error)}")
#                 # Return empty DataFrame if everything fails
#                 return pd.DataFrame(columns=['source', 'soil_temp_depth', 'has_moisture_data'])
        
#         # Convert to DataFrame
#         site_temps = pd.DataFrame(list(unique_combos), columns=['source', 'soil_temp_depth'])
        
#         # Add moisture data availability
#         if include_moisture:
#             site_temps['has_moisture_data'] = site_temps['source'].map(
#                 lambda x: has_moisture_by_site.get(x, False)
#             )
#         else:
#             site_temps['has_moisture_data'] = False
            
#         # Add coordinates
#         if site_coords:
#             site_temps['latitude'] = site_temps['source'].map(
#                 lambda x: site_coords.get(x, {}).get('latitude', np.nan)
#             )
#             site_temps['longitude'] = site_temps['source'].map(
#                 lambda x: site_coords.get(x, {}).get('longitude', np.nan)
#             )
    
#     print(f"Found {len(site_temps)} unique site-depth combinations")
#     if 'has_moisture_data' in site_temps.columns:
#         print(f"{site_temps['has_moisture_data'].sum()} sites have moisture data")
#     print(f"Memory after processing: {memory_usage():.1f} MB")
    
#     return site_temps

# def get_optimal_moisture_depth(df, temp_depth):
#     """
#     Find the closest moisture depth measurement to a given temperature depth
#     This ensures we compare temperature and moisture at comparable depths
#     """
#     if 'soil_moist_depth' not in df.columns or df['soil_moist_depth'].isna().all():
#         return np.nan
    
#     # Get unique moisture depths
#     moist_depths = df['soil_moist_depth'].dropna().unique()
    
#     if len(moist_depths) == 0:
#         return np.nan
    
#     # Find closest depth
#     closest_depth = moist_depths[np.abs(moist_depths - temp_depth).argmin()]
    
#     return closest_depth

# def load_site_depth_data(feather_path, site, temp_depth, include_moisture=True, verbose=False):
#     """Load ONLY data for a specific site and depth with moisture data when available"""
#     if verbose:
#         print(f"Loading data for site: {site}, depth: {temp_depth}")
#         print(f"Memory before loading: {memory_usage():.1f} MB")
    
#     try:
#         # Use PyArrow Dataset API for efficient filtering
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Create filter expressions
#         site_filter = ds.field('source') == site
#         depth_filter = ds.field('soil_temp_depth') == float(temp_depth)
#         combined_filter = site_filter & depth_filter
        
#         # Apply filter and read only filtered data
#         table = dataset.to_table(filter=combined_filter)
#         filtered_df = table.to_pandas()
        
#         # Get soil moisture data if available and requested
#         if include_moisture and len(filtered_df) > 0:
#             # First check if we already have moisture in the filtered data
#             has_moisture = (
#                 'soil_moist_standardized' in filtered_df.columns and 
#                 not filtered_df['soil_moist_standardized'].isna().all()
#             )
            
#             if not has_moisture:
#                 # Need to get moisture data separately - find optimal depth first
#                 # Create a small sample to check for available moisture depths
#                 sample_moisture = dataset.to_table(
#                     filter=site_filter,
#                     columns=['soil_moist_depth', 'datetime'],
#                     limit=1000
#                 ).to_pandas()
                
#                 if 'soil_moist_depth' in sample_moisture.columns and not sample_moisture['soil_moi...
#                     # Find closest moisture depth to current temperature depth
#                     optimal_moist_depth = get_optimal_moisture_depth(sample_moisture, temp_depth)
                    
#                     if not np.isnan(optimal_moist_depth):
#                         if verbose:
#                             print(f"  Found optimal moisture depth: {optimal_moist_depth} (temp de...
                        
#                         # Get moisture data at optimal depth
#                         moist_filter = site_filter & (ds.field('soil_moist_depth') == float(optima...
#                         moist_table = dataset.to_table(
#                             filter=moist_filter,
#                             columns=['datetime', 'soil_moist_standardized', 'soil_moist_depth']
#                         )
#                         moist_df = moist_table.to_pandas()
                        
#                         if len(moist_df) > 0:
#                             # Merge moisture data with temperature data
#                             filtered_df['datetime_key'] = filtered_df['datetime']
#                             moist_df['datetime_key'] = moist_df['datetime']
                            
#                             # Create a merged dataset
#                             merged_df = pd.merge_asof(
#                                 filtered_df.sort_values('datetime_key'),
#                                 moist_df.sort_values('datetime_key')[['datetime_key', 'soil_moist_...
#                                 on='datetime_key',
#                                 direction='nearest',
#                                 tolerance=pd.Timedelta('6h')
#                             )
                            
#                             # Clean up the merge
#                             merged_df.drop('datetime_key', axis=1, inplace=True)
                            
#                             # Use the merged dataset
#                             filtered_df = merged_df
#                             filtered_df['closest_moist_depth'] = optimal_moist_depth
                            
#                             if verbose:
#                                 print(f"  Merged {len(moist_df)} moisture records with temperature...
                            
#                             # Clean up
#                             del moist_table, moist_df, merged_df
#                             gc.collect()
                
#                 # Clean up
#                 del sample_moisture
#                 gc.collect()
        
#         # Clean up
#         del table, dataset
#         gc.collect()
        
#     except Exception as e:
#         if verbose:
#             print(f"Error with PyArrow filtering: {str(e)}")
#             print("Falling back to pandas filtering...")
        
#         # Define chunk size based on available memory
#         available_mem = psutil.virtual_memory().available / (1024**2)  # MB
#         chunk_size = min(500000, max(100000, int(available_mem / 10)))
        
#         filtered_chunks = []
        
#         # Columns to load - always include basic columns
#         cols = ['source', 'soil_temp_depth', 'soil_temp_standardized', 'datetime']
        
#         # Add moisture columns if needed
#         if include_moisture:
#             cols.extend(['soil_moist_depth', 'soil_moist_standardized'])
        
#         # Add other useful columns if they exist
#         extra_cols = ['latitude', 'longitude', 'elevation', 'season', 'year']
#         for col in extra_cols:
#             cols.append(col)
        
#         # Read and filter in chunks
#         try:
#             chunk = pd.read_feather(feather_path, columns=cols)
#             chunk_filtered = chunk[(chunk['source'] == site) & 
#                                  (chunk['soil_temp_depth'] == temp_depth)]
            
#             if len(chunk_filtered) > 0:
#                 filtered_df = chunk_filtered.copy()
#             else:
#                 filtered_df = pd.DataFrame()
                
#             # Clean up
#             del chunk, chunk_filtered
#             gc.collect()
            
#         except Exception as inner_e:
#             if verbose:
#                 print(f"Error with direct read: {str(inner_e)}")
#                 print("Trying with chunks...")
            
#             # Try with chunks if full read fails
#             for i in range(0, 100000000, chunk_size):  # Arbitrary large number
#                 try:
#                     chunk = pd.read_feather(
#                         feather_path, 
#                         columns=cols,
#                         nrows=chunk_size,
#                         skip=i
#                     )
                    
#                     if len(chunk) == 0:
#                         # No more data to read
#                         break
                        
#                     # Filter by site and depth
#                     chunk_filtered = chunk[(chunk['source'] == site) & 
#                                          (chunk['soil_temp_depth'] == temp_depth)]
                    
#                     if len(chunk_filtered) > 0:
#                         filtered_chunks.append(chunk_filtered)
                    
#                     # Clean up
#                     del chunk, chunk_filtered
#                     gc.collect()
#                 except Exception as chunk_e:
#                     if verbose:
#                         print(f"Error reading chunk at position {i}: {str(chunk_e)}")
#                     break
            
#             # Combine filtered chunks
#             if filtered_chunks:
#                 filtered_df = pd.concat(filtered_chunks, ignore_index=True)
#                 del filtered_chunks
#             else:
#                 filtered_df = pd.DataFrame()
            
#             gc.collect()
    
#     # Ensure datetime is in datetime format
#     if 'datetime' in filtered_df.columns and not pd.api.types.is_datetime64_dtype(filtered_df['dat...
#         filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])
    
#     # Add depth zone categorization
#     if 'soil_temp_depth' in filtered_df.columns:
#         filtered_df['soil_temp_depth_zone'] = pd.cut(
#             filtered_df['soil_temp_depth'],
#             bins=[-float('inf'), 5, 20, 50, 100, float('inf')],
#             labels=['surface', 'shallow', 'mid', 'deep', 'very_deep']
#         )
    
#     # Add year column if missing
#     if 'year' not in filtered_df.columns and 'datetime' in filtered_df.columns:
#         filtered_df['year'] = filtered_df['datetime'].dt.year
    
#     # Add season if missing
#     if 'season' not in filtered_df.columns and 'datetime' in filtered_df.columns:
#         # Define seasons - assume northern hemisphere
#         def get_season(month):
#             if month in [12, 1, 2]:
#                 return 'winter'
#             elif month in [3, 4, 5]:
#                 return 'spring'
#             elif month in [6, 7, 8]:
#                 return 'summer'
#             else:
#                 return 'fall'
        
#         filtered_df['season'] = filtered_df['datetime'].dt.month.apply(get_season)
    
#     if verbose:
#         print(f"Loaded {len(filtered_df)} rows for site-depth")
#         print(f"Memory after loading: {memory_usage():.1f} MB")
    
#     # Report moisture data only if verbose
#     if verbose and include_moisture and 'soil_moist_standardized' in filtered_df.columns:
#         valid_moisture = filtered_df['soil_moist_standardized'].notna().sum()
#         pct = valid_moisture / len(filtered_df) * 100 if len(filtered_df) > 0 else 0
#         print(f"  Moisture data coverage: {valid_moisture}/{len(filtered_df)} ({pct:.1f}%)")
    
#     return filtered_df

# # zero_curtain_processing.py
# # Critical fixes to ensure soil moisture is preserved throughout the workflow
# # Fix 1: Update the process_site_for_zero_curtain function to properly use moisture data
# def process_site_for_zero_curtain(site_df, site, temp_depth, max_gap_hours=8, interpolation_method...
#     """Process a single site-depth for zero curtain events with moisture integration"""
#     # Initialize list to store events
#     site_events = []
    
#     # Skip if too few points
#     if len(site_df) < 24:  # Require at least 24 measurements for meaningful analysis
#         return []
    
#     # Sort by time
#     site_df = site_df.sort_values('datetime')
    
#     # Calculate time differences
#     site_df['time_diff'] = site_df['datetime'].diff().dt.total_seconds() / 3600
    
#     # Check for moisture data availability - CRITICAL FIX
#     has_moisture = ('soil_moist_standardized' in site_df.columns and 
#                    not site_df['soil_moist_standardized'].isna().all())
    
#     if verbose:
#         print(f"  Processing site with moisture data: {has_moisture}")
    
#     # Identify gaps for interpolation - literature suggests 8 hours max
#     interpolation_needed = (site_df['time_diff'] > 1.0) & (site_df['time_diff'] <= max_gap_hours)
    
#     # Perform interpolation if needed
#     if interpolation_needed.any():
#         interp_rows = []
#         gap_indices = site_df.index[interpolation_needed].tolist()
        
#         for idx in gap_indices:
#             try:
#                 # Get before and after rows
#                 before_row = site_df.loc[site_df.index[site_df.index.get_loc(idx) - 1]]
#                 after_row = site_df.loc[idx]
                
#                 # Calculate gap and intervals
#                 time_gap = after_row['time_diff']
#                 n_intervals = int(time_gap)
                
#                 if n_intervals > 0:
#                     # Create timestamps
#                     timestamps = pd.date_range(
#                         start=before_row['datetime'],
#                         end=after_row['datetime'],
#                         periods=n_intervals + 2  # Include endpoints
#                     )[1:-1]  # Exclude endpoints
                    
#                     # Create temperature values
#                     if interpolation_method == 'linear' or len(site_df) < 5:
#                         # Linear interpolation
#                         temp_start = before_row['soil_temp_standardized']
#                         temp_end = after_row['soil_temp_standardized']
#                         temp_values = np.linspace(temp_start, temp_end, n_intervals + 2)[1:-1]
#                     else:
#                         # Try cubic interpolation
#                         try:
#                             # Get neighboring points
#                             idx_loc = site_df.index.get_loc(idx)
#                             start_idx = max(0, idx_loc - 3)
#                             end_idx = min(len(site_df), idx_loc + 2)
                            
#                             temp_points = site_df.iloc[start_idx:end_idx]['soil_temp_standardized'...
#                             time_points = [(t - before_row['datetime']).total_seconds() / 3600 
#                                          for t in site_df.iloc[start_idx:end_idx]['datetime']]
                            
#                             interp_times = [(t - before_row['datetime']).total_seconds() / 3600 
#                                           for t in timestamps]
                            
#                             # Perform cubic interpolation if enough points
#                             if len(time_points) >= 4:
#                                 interp_func = interp1d(time_points, temp_points, 
#                                                      kind='cubic', bounds_error=False)
#                                 temp_values = interp_func(interp_times)
#                             else:
#                                 # Fallback to linear
#                                 temp_start = before_row['soil_temp_standardized']
#                                 temp_end = after_row['soil_temp_standardized']
#                                 temp_values = np.linspace(temp_start, temp_end, n_intervals + 2)[1...
#                         except:
#                             # Fallback to linear if cubic fails
#                             temp_start = before_row['soil_temp_standardized']
#                             temp_end = after_row['soil_temp_standardized']
#                             temp_values = np.linspace(temp_start, temp_end, n_intervals + 2)[1:-1]
                    
#                     # CRITICAL FIX: ALWAYS interpolate moisture data when available (not condition...
#                     moist_values = None
#                     if has_moisture:
#                         moist_start = before_row.get('soil_moist_standardized', np.nan)
#                         moist_end = after_row.get('soil_moist_standardized', np.nan)
                        
#                         if not pd.isna(moist_start) and not pd.isna(moist_end):
#                             moist_values = np.linspace(moist_start, moist_end, n_intervals + 2)[1:...
#                         elif not pd.isna(moist_start):
#                             moist_values = np.full(len(timestamps), moist_start)
#                         elif not pd.isna(moist_end):
#                             moist_values = np.full(len(timestamps), moist_end)
                    
#                     # Create new rows
#                     for j, timestamp in enumerate(timestamps):
#                         new_row = before_row.copy()
#                         new_row['datetime'] = timestamp
#                         new_row['soil_temp_standardized'] = temp_values[j]
#                         new_row['interpolated'] = True
                        
#                         if has_moisture and moist_values is not None:
#                             new_row['soil_moist_standardized'] = moist_values[j]
                        
#                         interp_rows.append(new_row)
#             except Exception as e:
#                 if verbose:
#                     print(f"  Error during interpolation at idx {idx}: {str(e)}")
#                 continue
        
#         # Add interpolated rows
#         if interp_rows:
#             interp_df = pd.DataFrame(interp_rows)
#             site_df = pd.concat([site_df, interp_df], ignore_index=True)
#             site_df = site_df.sort_values('datetime')
            
#             # Clean up
#             del interp_df, interp_rows
#             gc.collect()
    
#     # Calculate temperature gradient
#     site_df['temp_gradient'] = site_df['soil_temp_standardized'].diff() / \
#                              (site_df['datetime'].diff().dt.total_seconds() / 3600)
    
#     # CRITICAL FIX: Always calculate moisture gradient when moisture data exists
#     if has_moisture:
#         site_df['moist_gradient'] = site_df['soil_moist_standardized'].diff() / \
#                                    (site_df['datetime'].diff().dt.total_seconds() / 3600)
    
#     # Literature-based detection criteria
#     mask_temp = (site_df['soil_temp_standardized'].abs() <= 0.5)  # Temperature near freezing
#     mask_gradient = (site_df['temp_gradient'].abs() <= 0.02)  # Stable temperature
    
#     # CRITICAL FIX: Properly integrate moisture in zero curtain detection
#     if has_moisture:
#         # Use moisture gradient for phase change detection
#         mask_moisture = (site_df['moist_gradient'].abs() >= 0.0005)  # Moisture changing during ph...
        
#         # Combined detection criteria
#         combined_mask = mask_temp & (mask_gradient | mask_moisture)
#     else:
#         # Use only temperature if no moisture data
#         combined_mask = mask_temp & mask_gradient
    
#     # Find continuous events
#     site_df['zero_curtain_flag'] = combined_mask
#     site_df['event_start'] = combined_mask & ~combined_mask.shift(1, fill_value=False)
#     site_df['event_end'] = combined_mask & ~combined_mask.shift(-1, fill_value=False)
    
#     # Get event starts and ends
#     event_starts = site_df[site_df['event_start']]['datetime'].tolist()
#     event_ends = site_df[site_df['event_end']]['datetime'].tolist()
    
#     if len(event_starts) == 0 or len(event_ends) == 0:
#         return []
    
#     # Handle mismatched starts/ends
#     if len(event_starts) > len(event_ends):
#         event_starts = event_starts[:len(event_ends)]
#     elif len(event_ends) > len(event_starts):
#         event_ends = event_ends[:len(event_starts)]
    
#     # Process each event
#     for start, end in zip(event_starts, event_ends):
#         event_duration = (end - start).total_seconds() / 3600
        
#         # Literature-based minimum duration (24 hours)
#         if event_duration < 24:
#             continue
        
#         # Get event data
#         event_data = site_df[(site_df['datetime'] >= start) & (site_df['datetime'] <= end)]
        
#         if len(event_data) < 5:
#             continue
        
#         # Extract event info
#         try:
#             event_info = extract_event_info(event_data, site, temp_depth, start, end, 
#                                            event_duration, has_moisture)
#             site_events.append(event_info)
#         except Exception as e:
#             print(f"  Error extracting event info: {str(e)}")
    
#     # Clean up to free memory
#     del site_df
#     gc.collect()
    
#     return site_events
    

# # Fix 2: Properly extract moisture metrics in event_info extraction
# def extract_event_info(event_data, site, temp_depth, start, end, event_duration, has_moisture):
#     """Extract comprehensive event info with proper moisture integration"""
#     # Basic event info
#     event_info = {
#         'source': site,
#         'soil_temp_depth': temp_depth,
#         'soil_temp_depth_zone': event_data['soil_temp_depth_zone'].iloc[0] if 'soil_temp_depth_zon...
#         'datetime_min': start,
#         'datetime_max': end,
#         'duration_hours': event_duration,
#         'observation_count': len(event_data),
#         'observations_per_day': len(event_data) / (event_duration / 24) if event_duration > 0 else...
#         'soil_temp_mean': event_data['soil_temp_standardized'].mean(),
#         'soil_temp_min': event_data['soil_temp_standardized'].min(),
#         'soil_temp_max': event_data['soil_temp_standardized'].max(),
#         'soil_temp_std': event_data['soil_temp_standardized'].std(),
#         'season': event_data['season'].iloc[0] if 'season' in event_data.columns else None,
#         'latitude': event_data['latitude'].iloc[0] if 'latitude' in event_data.columns else None,
#         'longitude': event_data['longitude'].iloc[0] if 'longitude' in event_data.columns else Non...
#         'year': event_data['year'].iloc[0] if 'year' in event_data.columns else None,
#         'month': start.month
#     }
    
#     # CRITICAL FIX: Always include moisture fields to avoid suppression
#     # This ensures consistent data structure even when moisture data isn't available
    
#     # Add moisture metrics when available
#     if has_moisture and not event_data['soil_moist_standardized'].isna().all():
#         event_info['soil_moist_mean'] = event_data['soil_moist_standardized'].mean()
#         event_info['soil_moist_std'] = event_data['soil_moist_standardized'].std()
#         event_info['soil_moist_min'] = event_data['soil_moist_standardized'].min()
#         event_info['soil_moist_max'] = event_data['soil_moist_standardized'].max()
#         event_info['soil_moist_change'] = event_data['soil_moist_standardized'].max() - event_data...
        
#         # Moisture depth information
#         if 'closest_moist_depth' in event_data.columns and not event_data['closest_moist_depth'].i...
#             event_info['soil_moist_depth'] = event_data['closest_moist_depth'].iloc[0]
#         elif 'soil_moist_depth' in event_data.columns and not event_data['soil_moist_depth'].isna(...
#             event_info['soil_moist_depth'] = event_data['soil_moist_depth'].iloc[0]
#         else:
#             event_info['soil_moist_depth'] = np.nan
            
#         # Add moisture gradient metrics
#         if 'moist_gradient' in event_data.columns:
#             event_info['soil_moist_gradient_mean'] = event_data['moist_gradient'].mean()
#             event_info['soil_moist_gradient_max'] = event_data['moist_gradient'].abs().max()
#     else:
#         # Set empty moisture values BUT PRESERVE THE COLUMNS
#         # This is critical - we don't want to suppress the moisture data structure
#         event_info['soil_moist_mean'] = np.nan
#         event_info['soil_moist_std'] = np.nan
#         event_info['soil_moist_min'] = np.nan
#         event_info['soil_moist_max'] = np.nan
#         event_info['soil_moist_change'] = np.nan
#         event_info['soil_moist_depth'] = np.nan
#         event_info['soil_moist_gradient_mean'] = np.nan
#         event_info['soil_moist_gradient_max'] = np.nan
    
#     # Temperature gradient info
#     if 'temp_gradient' in event_data.columns and not event_data['temp_gradient'].isna().all():
#         event_info['temp_gradient_mean'] = event_data['temp_gradient'].mean()
#         event_info['temp_stability'] = event_data['temp_gradient'].abs().mean()
    
#     # Add year-month
#     event_info['year_month'] = f"{event_info['year']}-{event_info['month']:02d}"
    
#     # Add region classification
#     if 'latitude' in event_info and event_info['latitude'] is not None:
#         lat = event_info['latitude']
#         if lat >= 66.5:
#             event_info['region'] = 'Arctic'
#         elif lat >= 60:
#             event_info['region'] = 'Subarctic'
#         elif lat >= 49:
#             event_info['region'] = 'Boreal'
#         else:
#             event_info['region'] = 'Other'
            
#         # Add latitude band
#         if lat < 49:
#             event_info['lat_band'] = '<49°N'
#         elif lat < 55:
#             event_info['lat_band'] = '<49-55°N (Boreal)'
#         elif lat < 60:
#             event_info['lat_band'] = '55-60°N (Boreal)'
#         elif lat < 66.5:
#             event_info['lat_band'] = '60-66.5°N (Sub_Arctic)'
#         elif lat < 70:
#             event_info['lat_band'] = '66.5-70°N (Arctic)'
#         elif lat < 75:
#             event_info['lat_band'] = '70-75°N (Arctic)'
#         elif lat < 80:
#             event_info['lat_band'] = '75-80°N (Arctic)'
#         else:
#             event_info['lat_band'] = '>80°N (Arctic)'
#     else:
#         event_info['region'] = None
#         event_info['lat_band'] = None
    
#     return event_info

# # Fix 3: Function to prepare ML features with moisture preservation
# def prepare_ml_features(events_df, output_dir=None):
#     """
#     Prepare features and labels for ML ensuring soil moisture features are preserved
#     """
#     print("Preparing ML features with moisture data integration")
    
#     # Ensure datetime columns are properly formatted
#     if isinstance(events_df, str):
#         events_df = pd.read_csv(events_df)
#         # Convert datetime columns to datetime
#         if 'datetime_min' in events_df.columns:
#             events_df['datetime_min'] = pd.to_datetime(events_df['datetime_min'])
#         if 'datetime_max' in events_df.columns:
#             events_df['datetime_max'] = pd.to_datetime(events_df['datetime_max'])
    
#     # Get all moisture columns - this is critical for not suppressing data
#     moisture_cols = [col for col in events_df.columns if 'moist' in col]
#     print(f"Found {len(moisture_cols)} moisture-related columns: {moisture_cols}")
    
#     # Create feature columns - KEEPING ALL MOISTURE COLUMNS
#     feature_cols = []
    
#     # Add all standard columns except non-numeric ones
#     exclude_cols = ['datetime_min', 'datetime_max', 'source', 'year_month']
#     for col in events_df.columns:
#         if col not in exclude_cols and col != 'duration_hours':  # duration is our target
#             feature_cols.append(col)
    
#     # Create a training dataset
#     X = events_df[feature_cols].copy()
#     y = events_df['duration_hours'].copy()
    
#     # Create spatial stratification
#     X['spatial_group'] = 'other'
#     if 'region' in X.columns and 'soil_temp_depth_zone' in X.columns:
#         X['spatial_group'] = X['region'] + '_' + X['soil_temp_depth_zone'].astype(str)
#     elif 'source' in events_df.columns:
#         X['spatial_group'] = events_df['source']
    
#     # Create train/val/test split that respects spatial dependencies
#     from sklearn.model_selection import train_test_split
    
#     # Split off test set first
#     train_val_idx, test_idx = train_test_split(
#         range(len(X)), test_size=0.2, 
#         stratify=X['spatial_group'], random_state=42
#     )
    
#     # Then split train/val
#     train_idx, val_idx = train_test_split(
#         train_val_idx, test_size=0.25,  # 25% of 80% = 20% of total
#         stratify=X.iloc[train_val_idx]['spatial_group'], random_state=42
#     )
    
#     # Create split datasets
#     X_train = X.iloc[train_idx].drop('spatial_group', axis=1)
#     X_val = X.iloc[val_idx].drop('spatial_group', axis=1)
#     X_test = X.iloc[test_idx].drop('spatial_group', axis=1)
    
#     y_train = y.iloc[train_idx]
#     y_val = y.iloc[val_idx]
#     y_test = y.iloc[test_idx]
    
#     # Report on moisture features
#     print("\nMoisture feature statistics:")
#     for col in moisture_cols:
#         if col in X_train.columns:
#             non_null = X_train[col].notna().sum()
#             pct = non_null / len(X_train) * 100
#             print(f"  {col}: {non_null}/{len(X_train)} non-null values ({pct:.1f}%)")
    
#     # Save datasets if output directory provided
#     if output_dir:
#         os.makedirs(output_dir, exist_ok=True)
        
#         # Save the features
#         X_train.to_csv(os.path.join(output_dir, 'X_train.csv'), index=False)
#         X_val.to_csv(os.path.join(output_dir, 'X_val.csv'), index=False)
#         X_test.to_csv(os.path.join(output_dir, 'X_test.csv'), index=False)
        
#         # Save the targets
#         y_train.to_csv(os.path.join(output_dir, 'y_train.csv'), index=False)
#         y_val.to_csv(os.path.join(output_dir, 'y_val.csv'), index=False)
#         y_test.to_csv(os.path.join(output_dir, 'y_test.csv'), index=False)
        
#         # Save complete dataset
#         full_dataset = events_df.copy()
#         full_dataset['split'] = 'train'
#         full_dataset.iloc[val_idx, full_dataset.columns.get_loc('split')] = 'val'
#         full_dataset.iloc[test_idx, full_dataset.columns.get_loc('split')] = 'test'
#         full_dataset.to_csv(os.path.join(output_dir, 'full_dataset.csv'), index=False)
        
#         print(f"Saved ML datasets to {output_dir}")
    
#     return {
#         'X_train': X_train,
#         'X_val': X_val,
#         'X_test': X_test,
#         'y_train': y_train,
#         'y_val': y_val,
#         'y_test': y_test,
#         'feature_cols': feature_cols,
#         'moisture_cols': moisture_cols
#     }

# # Fix for run_memory_efficient_pipeline to correctly pass include_moisture
# def run_memory_efficient_pipeline(feather_path, output_dir=None, 
#                                   site_batch_size=10, checkpoint_interval=5, 
#                                   max_gap_hours=8, interpolation_method='cubic', 
#                                   force_restart=False, include_moisture=True,
#                                   verbose=False):
#     """
#     Run the complete memory-efficient zero curtain detection pipeline
#     with soil moisture integration and improved memory management
    
#     Parameters:
#     -----------
#     feather_path : str
#         Path to the feather file
#     output_dir : str
#         Directory to save results and checkpoints
#     site_batch_size : int
#         Number of sites to process in each batch (smaller = less memory)
#     checkpoint_interval : int
#         Number of sites between saving checkpoints
#     max_gap_hours : float
#         Maximum gap hours for interpolation (literature suggests 8 hours max)
#     interpolation_method : str
#         Interpolation method ('linear', 'cubic')
#     force_restart : bool
#         Whether to force restart from scratch
#     include_moisture : bool
#         Whether to include soil moisture data in the analysis
#     verbose : bool
#         Whether to print detailed progress for each site
        
#     Returns:
#     --------
#     pandas.DataFrame
#         DataFrame with detected zero curtain events
#     """
#     print("=" * 80)
#     print("MEMORY-EFFICIENT ZERO CURTAIN DETECTION WITH MOISTURE INTEGRATION")
#     print("=" * 80)
#     print(f"Initial memory usage: {memory_usage():.1f} MB")

#     # Define a more aggressive cleanup function that stays in this function's scope
#     def aggressive_cleanup():
#         """Aggressively clean up memory"""
#         # Call garbage collection multiple times
#         for _ in range(3):
#             gc.collect()
    
#     # Set up directories
#     if output_dir is not None:
#         os.makedirs(output_dir, exist_ok=True)
#         checkpoint_dir = os.path.join(output_dir, 'checkpoints')
#         os.makedirs(checkpoint_dir, exist_ok=True)
#     else:
#         checkpoint_dir = None
    
#     # Start timing
#     start_time = time.time()

#     def save_checkpoint(data, name):
#         if checkpoint_dir:
#             backup_path = os.path.join(checkpoint_dir, f'{name}_backup.pkl')
#             target_path = os.path.join(checkpoint_dir, f'{name}.pkl')
            
#             # First save to backup file
#             with open(backup_path, 'wb') as f:
#                 pickle.dump(data, f)
            
#             # Then rename to target (atomic operation)
#             import shutil
#             shutil.move(backup_path, target_path)
            
#             print(f"Saved checkpoint to {target_path}")
            
#             # Force memory cleanup after saving checkpoints
#             aggressive_cleanup()
    
#     # Function to load checkpoint
#     def load_checkpoint(name):
#         if checkpoint_dir:
#             try:
#                 with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'rb') as f:
#                     data = pickle.load(f)
#                 print(f"Loaded checkpoint from {name}.pkl")
#                 return data
#             except:
#                 print(f"No checkpoint found for {name}.pkl")
#         return None
    
#     # Step 1: Get site-depth combinations
#     site_depths = None
#     if not force_restart:
#         site_depths = load_checkpoint('site_depths')
    
#     if site_depths is None:
#         print("\nStep 1: Finding unique site-depth combinations...")
#         # Make sure to pass include_moisture parameter here
#         site_depths = get_unique_site_depths(feather_path, include_moisture=include_moisture)
#         save_checkpoint(site_depths, 'site_depths')
    
#     # Force cleanup after loading site data
#     aggressive_cleanup()
    
#     total_combinations = len(site_depths)
#     print(f"Found {total_combinations} unique site-depth combinations")
    
#     # Report on moisture capabilities if column exists
#     if 'has_moisture_data' in site_depths.columns:
#         moisture_sites = site_depths['has_moisture_data'].sum()
#         print(f"  {moisture_sites} sites ({moisture_sites/total_combinations*100:.1f}%) have moist...
    
#     # Step 2: Initialize results
#     all_events = []
#     processed_indices = set()
    
#     if not force_restart:
#         saved_events = load_checkpoint('all_events')
#         if saved_events is not None:
#             if isinstance(saved_events, list):
#                 all_events = saved_events
#             else:
#                 # If it's a DataFrame, convert to list of dicts
#                 all_events = saved_events.to_dict('records') if len(saved_events) > 0 else []
        
#         saved_indices = load_checkpoint('processed_indices')
#         if saved_indices is not None:
#             processed_indices = set(saved_indices)
    
#     print(f"Starting from {len(processed_indices)}/{total_combinations} processed sites")
#     print(f"Current event count: {len(all_events)}")

#     # Step 3: Process in batches
#     print("\nProcessing site-depth combinations in batches")
    
#     # For tracking new events in this run
#     new_events_count = 0
    
#     # Create batches for processing
#     total_batches = (total_combinations + site_batch_size - 1) // site_batch_size
    
#     for batch_idx in range(total_batches):
#         batch_start = batch_idx * site_batch_size
#         batch_end = min(batch_start + site_batch_size, total_combinations)
        
#         # Skip if already processed
#         batch_indices = set(range(batch_start, batch_end))
#         if batch_indices.issubset(processed_indices):
#             print(f"Batch {batch_idx+1}/{total_batches} (sites {batch_start+1}-{batch_end}/{total_...
#             continue
        
#         print(f"\nProcessing batch {batch_idx+1}/{total_batches} (sites {batch_start+1}-{batch_end...
#         print(f"Memory before batch: {memory_usage():.1f} MB")
        
#         # Force garbage collection before each batch
#         aggressive_cleanup()
        
#         # Process each site in batch
#         for site_idx in range(batch_start, batch_end):
#             # Skip if already processed
#             if site_idx in processed_indices:
#                 continue
            
#             site = site_depths.iloc[site_idx]['source']
#             temp_depth = site_depths.iloc[site_idx]['soil_temp_depth']
            
#             # Only print detailed progress every 10 sites or if verbose mode is on
#             if verbose or (site_idx % 10 == 0):
#                 print(f"\nProcessing site {site_idx+1}/{total_combinations}: {site}, depth: {temp_...
            
#             try:
#                 # Check if site has moisture capabilities
#                 has_moisture = False
#                 if 'has_moisture_data' in site_depths.columns:
#                     has_moisture = site_depths.iloc[site_idx]['has_moisture_data']
                
#                 # Load site data efficiently - with soil moisture data if available
#                 site_df = load_site_depth_data(
#                     feather_path, site, temp_depth, 
#                     include_moisture=(include_moisture and has_moisture),
#                     verbose=verbose
#                 )
                
#                 # Skip if insufficient data
#                 if len(site_df) < 24:  # Require at least 24 observations (minimum 1 day)
#                     if verbose:
#                         print(f"  Insufficient data ({len(site_df)} rows), skipping")
#                     processed_indices.add(site_idx)
#                     continue
                
#                 # Process for zero curtain
#                 site_events = process_site_for_zero_curtain(
#                     site_df, site, temp_depth, 
#                     max_gap_hours, interpolation_method#,
#                     #verbose=verbose
#                 )
                
#                 # Add to all events
#                 new_events = len(site_events)
#                 all_events.extend(site_events)
#                 new_events_count += new_events
                
#                 if new_events > 0 or verbose:
#                     print(f"  Site {site_idx+1}: Found {new_events} events, total: {len(all_events...
                
#                 # Mark as processed
#                 processed_indices.add(site_idx)
                
#                 # Save checkpoint periodically
#                 if site_idx % checkpoint_interval == 0:
#                     save_checkpoint(all_events, 'all_events')
#                     save_checkpoint(list(processed_indices), 'processed_indices')
#                     print(f"Memory after checkpoint: {memory_usage():.1f} MB")
                
#                 # Clean up after each site to prevent memory buildup
#                 del site_df, site_events
#                 aggressive_cleanup()

#             except Exception as e:
#                 print(f"  Error processing site {site_idx+1} ({site}, depth {temp_depth}): {str(e)...
#                 # Continue with next site even after error
#                 continue
        
#         # Save checkpoint after each batch
#         print(f"Saving checkpoint after batch {batch_idx+1}/{total_batches}")
#         save_checkpoint(all_events, 'all_events')
#         save_checkpoint(list(processed_indices), 'processed_indices')
        
#         # Also save intermediate results as CSV
#         if len(all_events) > 0:
#             interim_df = pd.DataFrame(all_events)
#             if output_dir is not None:
#                 interim_path = os.path.join(output_dir, 'events_checkpoint.csv')
#                 interim_df.to_csv(interim_path, index=False)
#                 print(f"Saved interim results to {interim_path}")
            
#             # Clean up the interim DataFrame immediately
#             del interim_df
#             aggressive_cleanup()
        
#         print(f"Memory after batch: {memory_usage():.1f} MB")
    
#     # Step 4: Create final dataframe
#     print("\nCreating final events dataframe")
    
#     # Force cleanup before creating final dataframe
#     aggressive_cleanup()
    
#     if len(all_events) > 0:
#         events_df = pd.DataFrame(all_events)
#         print(f"Created events dataframe with {len(events_df)} total events ({new_events_count} ne...
        
#         # Report on moisture data
#         if 'soil_moist_mean' in events_df.columns:
#             moisture_present = events_df['soil_moist_mean'].notna().sum()
#             print(f"  {moisture_present} events ({moisture_present/len(events_df)*100:.1f}%) have ...
#     else:
#         # Create empty dataframe with correct columns
#         events_df = pd.DataFrame(columns=[
#             'source', 'soil_temp_depth', 'soil_temp_depth_zone', 
#             'datetime_min', 'datetime_max', 'duration_hours',
#             'observation_count', 'observations_per_day',
#             'soil_temp_mean', 'soil_temp_min', 'soil_temp_max', 'soil_temp_std',
#             'season', 'latitude', 'longitude', 'year', 'month',
#             'soil_moist_mean', 'soil_moist_std', 'soil_moist_min', 'soil_moist_max', 
#             'soil_moist_change', 'soil_moist_depth', 
#             'soil_moist_gradient_mean', 'soil_moist_gradient_max',
#             'temp_gradient_mean', 'temp_stability',
#             'year_month', 'region', 'lat_band'
#         ])
#         print("No events found")
    
#     # Save final results
#     if output_dir is not None:
#         final_path = os.path.join(output_dir, 'zero_curtain_events.csv')

#         temp_path = os.path.join(output_dir, 'zero_curtain_events_temp.csv')
#         events_df.to_csv(temp_path, index=False)

#         import shutil
#         shutil.move(temp_path, final_path)
        
#         print(f"Saved final results to {final_path}")
    
#     # Report timing
#     total_time = time.time() - start_time
#     print(f"\nPipeline completed in {total_time:.1f} seconds ({total_time/60:.1f} minutes)")
#     print(f"Final memory usage: {memory_usage():.1f} MB")
    
#     return events_df

# # This function fixes the reference to 'verbose' in the load_site_depth_data call

# def run_memory_efficient_pipeline_fixed(feather_path, output_dir=None, 
#                                   site_batch_size=10, checkpoint_interval=5, 
#                                   max_gap_hours=8, interpolation_method='cubic', 
#                                   force_restart=False, include_moisture=True,
#                                   verbose=False):  # Keep verbose but don't use it
#     """
#     Run the complete memory-efficient zero curtain detection pipeline
#     with soil moisture integration
    
#     Parameters:
#     -----------
#     feather_path : str
#         Path to the feather file
#     output_dir : str
#         Directory to save results and checkpoints
#     site_batch_size : int
#         Number of sites to process in each batch (smaller = less memory)
#     checkpoint_interval : int
#         Number of sites between saving checkpoints
#     max_gap_hours : float
#         Maximum gap hours for interpolation (literature suggests 8 hours max)
#     interpolation_method : str
#         Interpolation method ('linear', 'cubic')
#     force_restart : bool
#         Whether to force restart from scratch
#     include_moisture : bool
#         Whether to include soil moisture data in the analysis
#     verbose : bool
#         Whether to print detailed progress (not used in subfunctions)
        
#     Returns:
#     --------
#     pandas.DataFrame
#         DataFrame with detected zero curtain events
#     """
#     print("=" * 80)
#     print("MEMORY-EFFICIENT ZERO CURTAIN DETECTION WITH MOISTURE INTEGRATION")
#     print("=" * 80)
#     print(f"Initial memory usage: {memory_usage():.1f} MB")
    
#     # Define a more aggressive cleanup function that stays in this function's scope
#     def aggressive_cleanup():
#         """Aggressively clean up memory"""
#         # Call garbage collection multiple times
#         for _ in range(3):
#             gc.collect()
    
#     # Set up directories
#     if output_dir is not None:
#         os.makedirs(output_dir, exist_ok=True)
#         checkpoint_dir = os.path.join(output_dir, 'checkpoints')
#         os.makedirs(checkpoint_dir, exist_ok=True)
#     else:
#         checkpoint_dir = None
    
#     # Start timing
#     start_time = time.time()

#     def save_checkpoint(data, name):
#         if checkpoint_dir:
#             backup_path = os.path.join(checkpoint_dir, f'{name}_backup.pkl')
#             target_path = os.path.join(checkpoint_dir, f'{name}.pkl')
            
#             # First save to backup file
#             with open(backup_path, 'wb') as f:
#                 pickle.dump(data, f)
            
#             # Then rename to target (atomic operation)
#             import shutil
#             shutil.move(backup_path, target_path)
            
#             print(f"Saved checkpoint to {target_path}")
            
#             # Force memory cleanup after saving checkpoints
#             aggressive_cleanup()
    
#     # Function to load checkpoint
#     def load_checkpoint(name):
#         if checkpoint_dir:
#             try:
#                 with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'rb') as f:
#                     data = pickle.load(f)
#                 print(f"Loaded checkpoint from {name}.pkl")
#                 return data
#             except:
#                 print(f"No checkpoint found for {name}.pkl")
#         return None
    
#     # Step 1: Get site-depth combinations
#     site_depths = None
#     if not force_restart:
#         site_depths = load_checkpoint('site_depths')
    
#     if site_depths is None:
#         print("\nStep 1: Finding unique site-depth combinations...")
#         # Make sure to pass include_moisture parameter here
#         site_depths = get_unique_site_depths(feather_path, include_moisture=include_moisture)
#         save_checkpoint(site_depths, 'site_depths')
    
#     # Force cleanup after loading site data
#     aggressive_cleanup()
    
#     total_combinations = len(site_depths)
#     print(f"Found {total_combinations} unique site-depth combinations")
    
#     # Report on moisture capabilities if column exists
#     if 'has_moisture_data' in site_depths.columns:
#         moisture_sites = site_depths['has_moisture_data'].sum()
#         print(f"  {moisture_sites} sites ({moisture_sites/total_combinations*100:.1f}%) have moist...
    
#     # Step 2: Initialize results
#     all_events = []
#     processed_indices = set()
    
#     if not force_restart:
#         saved_events = load_checkpoint('all_events')
#         if saved_events is not None:
#             if isinstance(saved_events, list):
#                 all_events = saved_events
#             else:
#                 # If it's a DataFrame, convert to list of dicts
#                 all_events = saved_events.to_dict('records') if len(saved_events) > 0 else []
        
#         saved_indices = load_checkpoint('processed_indices')
#         if saved_indices is not None:
#             processed_indices = set(saved_indices)
    
#     print(f"Starting from {len(processed_indices)}/{total_combinations} processed sites")
#     print(f"Current event count: {len(all_events)}")

#     # Step 3: Process in batches
#     print("\nProcessing site-depth combinations in batches")
    
#     # For tracking new events in this run
#     new_events_count = 0
    
#     # Create batches for processing
#     total_batches = (total_combinations + site_batch_size - 1) // site_batch_size
    
#     for batch_idx in range(total_batches):
#         batch_start = batch_idx * site_batch_size
#         batch_end = min(batch_start + site_batch_size, total_combinations)
        
#         # Skip if already processed
#         batch_indices = set(range(batch_start, batch_end))
#         if batch_indices.issubset(processed_indices):
#             print(f"Batch {batch_idx+1}/{total_batches} (sites {batch_start+1}-{batch_end}/{total_...
#             continue
        
#         print(f"\nProcessing batch {batch_idx+1}/{total_batches} (sites {batch_start+1}-{batch_end...
#         print(f"Memory before batch: {memory_usage():.1f} MB")
        
#         # Force garbage collection before each batch
#         aggressive_cleanup()
        
#         # Process each site in batch
#         for site_idx in range(batch_start, batch_end):
#             # Skip if already processed
#             if site_idx in processed_indices:
#                 continue
            
#             site = site_depths.iloc[site_idx]['source']
#             temp_depth = site_depths.iloc[site_idx]['soil_temp_depth']
            
#             # Only print detailed progress every 10 sites or if verbose mode is on
#             if verbose or (site_idx % 10 == 0):
#                 print(f"\nProcessing site {site_idx+1}/{total_combinations}: {site}, depth: {temp_...
            
#             try:
#                 # Check if site has moisture capabilities
#                 has_moisture = False
#                 if 'has_moisture_data' in site_depths.columns:
#                     has_moisture = site_depths.iloc[site_idx]['has_moisture_data']
                
#                 # Load site data efficiently - with soil moisture data if available
#                 # IMPORTANT: Don't pass verbose parameter here!
#                 site_df = load_site_depth_data(
#                     feather_path=feather_path, 
#                     site=site, 
#                     temp_depth=temp_depth, 
#                     include_moisture=(include_moisture and has_moisture)
#                 )
                
#                 # Skip if insufficient data
#                 if len(site_df) < 24:  # Require at least 24 observations (minimum 1 day)
#                     if verbose:
#                         print(f"  Insufficient data ({len(site_df)} rows), skipping")
#                     processed_indices.add(site_idx)
#                     continue
                
#                 # Process for zero curtain
#                 # IMPORTANT: Don't pass verbose parameter here!
#                 site_events = process_site_for_zero_curtain(
#                     site_df=site_df, 
#                     site=site, 
#                     temp_depth=temp_depth, 
#                     max_gap_hours=max_gap_hours, 
#                     interpolation_method=interpolation_method
#                 )
                
#                 # Add to all events
#                 new_events = len(site_events)
#                 all_events.extend(site_events)
#                 new_events_count += new_events
                
#                 if new_events > 0 or verbose:
#                     print(f"  Site {site_idx+1}: Found {new_events} events, total: {len(all_events...
                
#                 # Mark as processed
#                 processed_indices.add(site_idx)
                
#                 # Save checkpoint periodically
#                 if site_idx % checkpoint_interval == 0:
#                     save_checkpoint(all_events, 'all_events')
#                     save_checkpoint(list(processed_indices), 'processed_indices')
#                     if verbose:
#                         print(f"Memory after checkpoint: {memory_usage():.1f} MB")
                
#                 # Clean up after each site to prevent memory buildup
#                 del site_df, site_events
#                 aggressive_cleanup()

#             except Exception as e:
#                 print(f"  Error processing site {site_idx+1} ({site}, depth {temp_depth}): {str(e)...
#                 # Continue with next site even after error
#                 continue
        
#         # Save checkpoint after each batch
#         print(f"Saving checkpoint after batch {batch_idx+1}/{total_batches}")
#         save_checkpoint(all_events, 'all_events')
#         save_checkpoint(list(processed_indices), 'processed_indices')
        
#         # Also save intermediate results as CSV
#         if len(all_events) > 0:
#             interim_df = pd.DataFrame(all_events)
#             if output_dir is not None:
#                 interim_path = os.path.join(output_dir, 'events_checkpoint.csv')
#                 interim_df.to_csv(interim_path, index=False)
#                 print(f"Saved interim results to {interim_path}")
            
#             # Clean up the interim DataFrame immediately
#             del interim_df
#             aggressive_cleanup()
        
#         print(f"Memory after batch: {memory_usage():.1f} MB")
    
#     # Step 4: Create final dataframe
#     print("\nCreating final events dataframe")
    
#     # Force cleanup before creating final dataframe
#     aggressive_cleanup()
    
#     if len(all_events) > 0:
#         events_df = pd.DataFrame(all_events)
#         print(f"Created events dataframe with {len(events_df)} total events ({new_events_count} ne...
        
#         # Report on moisture data
#         if 'soil_moist_mean' in events_df.columns:
#             moisture_present = events_df['soil_moist_mean'].notna().sum()
#             print(f"  {moisture_present} events ({moisture_present/len(events_df)*100:.1f}%) have ...
#     else:
#         # Create empty dataframe with correct columns
#         events_df = pd.DataFrame(columns=[
#             'source', 'soil_temp_depth', 'soil_temp_depth_zone', 
#             'datetime_min', 'datetime_max', 'duration_hours',
#             'observation_count', 'observations_per_day',
#             'soil_temp_mean', 'soil_temp_min', 'soil_temp_max', 'soil_temp_std',
#             'season', 'latitude', 'longitude', 'year', 'month',
#             'soil_moist_mean', 'soil_moist_std', 'soil_moist_min', 'soil_moist_max', 
#             'soil_moist_change', 'soil_moist_depth', 
#             'soil_moist_gradient_mean', 'soil_moist_gradient_max',
#             'temp_gradient_mean', 'temp_stability',
#             'year_month', 'region', 'lat_band'
#         ])
#         print("No events found")
    
#     # Save final results
#     if output_dir is not None:
#         final_path = os.path.join(output_dir, 'zero_curtain_events.csv')

#         temp_path = os.path.join(output_dir, 'zero_curtain_events_temp.csv')
#         events_df.to_csv(temp_path, index=False)

#         import shutil
#         shutil.move(temp_path, final_path)
        
#         print(f"Saved final results to {final_path}")
    
#     # Report timing
#     total_time = time.time() - start_time
#     print(f"\nPipeline completed in {total_time:.1f} seconds ({total_time/60:.1f} minutes)")
#     print(f"Final memory usage: {memory_usage():.1f} MB")
    
#     return events_df

# # Modified load_site_depth_data function to handle moisture data properly
# def load_site_depth_data(feather_path, site, temp_depth, include_moisture=True):
#     """Load ONLY data for a specific site and depth with moisture data when available"""
#     print(f"Loading data for site: {site}, depth: {temp_depth}")
#     print(f"Memory before loading: {memory_usage():.1f} MB")
    
#     try:
#         # Use PyArrow Dataset API for efficient filtering
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Create filter expressions
#         site_filter = ds.field('source') == site
#         depth_filter = ds.field('soil_temp_depth') == float(temp_depth)
#         combined_filter = site_filter & depth_filter
        
#         # Apply filter and read only filtered data
#         table = dataset.to_table(filter=combined_filter)
#         filtered_df = table.to_pandas()
        
#         # Get soil moisture data if available and requested
#         if include_moisture and len(filtered_df) > 0:
#             # Check if we already have moisture in the filtered data
#             has_moisture = (
#                 'soil_moist_standardized' in filtered_df.columns and 
#                 not filtered_df['soil_moist_standardized'].isna().all()
#             )
            
#             if not has_moisture:
#                 # Try to get moisture data separately
#                 try:
#                     # First check if moisture data exists for this site
#                     moisture_filter = site_filter
                    
#                     # Sample to check for moisture data
#                     moisture_sample = dataset.to_table(
#                         filter=moisture_filter,
#                         columns=['soil_moist_depth', 'soil_moist_standardized'],
#                         limit=100
#                     ).to_pandas()
                    
#                     if (
#                         'soil_moist_standardized' in moisture_sample.columns and 
#                         not moisture_sample['soil_moist_standardized'].isna().all() and
#                         'soil_moist_depth' in moisture_sample.columns and
#                         not moisture_sample['soil_moist_depth'].isna().all()
#                     ):
#                         # Find closest moisture depth to temperature depth
#                         moist_depths = moisture_sample['soil_moist_depth'].dropna().unique()
#                         if len(moist_depths) > 0:
#                             # Get closest depth
#                             closest_depth = moist_depths[np.abs(moist_depths - temp_depth).argmin(...
#                             print(f"  Found closest moisture depth: {closest_depth}")
                            
#                             # Get moisture data at this depth
#                             moisture_filter = site_filter & (ds.field('soil_moist_depth') == float...
#                             moisture_table = dataset.to_table(
#                                 filter=moisture_filter,
#                                 columns=['datetime', 'soil_moist_standardized', 'soil_moist_depth'...
#                             )
#                             moisture_df = moisture_table.to_pandas()
                            
#                             if len(moisture_df) > 0:
#                                 # Merge with temperature data
#                                 filtered_df['datetime_key'] = filtered_df['datetime']
#                                 moisture_df['datetime_key'] = moisture_df['datetime']
                                
#                                 merged_df = pd.merge_asof(
#                                     filtered_df.sort_values('datetime_key'),
#                                     moisture_df[['datetime_key', 'soil_moist_standardized', 'soil_...
#                                     on='datetime_key',
#                                     direction='nearest',
#                                     tolerance=pd.Timedelta('6h')  # Match within 6 hours
#                                 )
                                
#                                 # Clean up merged data
#                                 merged_df.drop('datetime_key', axis=1, inplace=True)
#                                 filtered_df = merged_df
                                
#                                 # Add depth information
#                                 filtered_df['closest_moist_depth'] = closest_depth
                                
#                                 # Report on merge
#                                 valid_moisture = filtered_df['soil_moist_standardized'].notna().su...
#                                 print(f"  Added moisture data: {valid_moisture}/{len(filtered_df)}...
#                 except Exception as e:
#                     print(f"  Error adding moisture data: {str(e)}")
        
#         # Clean up
#         del table, dataset
#         gc.collect()
        
#     except Exception as e:
#         print(f"Error with PyArrow filtering: {str(e)}")
#         print("Falling back to pandas filtering...")
        
#         # Define chunk size based on available memory
#         available_mem = psutil.virtual_memory().available / (1024**2)  # MB
#         chunk_size = min(500000, max(100000, int(available_mem / 10)))
        
#         filtered_chunks = []
        
#         # Read and filter in chunks
#         for chunk in pd.read_feather(feather_path, columns=None, chunksize=chunk_size):
#             # Filter by site and depth
#             chunk_filtered = chunk[(chunk['source'] == site) & 
#                                    (chunk['soil_temp_depth'] == temp_depth)]
            
#             if len(chunk_filtered) > 0:
#                 filtered_chunks.append(chunk_filtered)
            
#             # Clean up
#             del chunk, chunk_filtered
#             gc.collect()
        
#         # Combine filtered chunks
#         if filtered_chunks:
#             filtered_df = pd.concat(filtered_chunks, ignore_index=True)
#             del filtered_chunks
#         else:
#             filtered_df = pd.DataFrame()
        
#         gc.collect()
    
#     # Ensure datetime is in datetime format
#     if 'datetime' in filtered_df.columns and not pd.api.types.is_datetime64_dtype(filtered_df['dat...
#         filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])
    
#     # Add depth zone classification
#     if 'soil_temp_depth' in filtered_df.columns:
#         filtered_df['soil_temp_depth_zone'] = pd.cut(
#             filtered_df['soil_temp_depth'],
#             bins=[-float('inf'), 5, 20, 50, 100, float('inf')],
#             labels=['surface', 'shallow', 'mid', 'deep', 'very_deep']
#         )
    
#     # Add year column if missing
#     if 'year' not in filtered_df.columns and 'datetime' in filtered_df.columns:
#         filtered_df['year'] = filtered_df['datetime'].dt.year
    
#     # Add season if missing
#     if 'season' not in filtered_df.columns and 'datetime' in filtered_df.columns:
#         # Define seasons - assume northern hemisphere
#         def get_season(month):
#             if month in [12, 1, 2]:
#                 return 'winter'
#             elif month in [3, 4, 5]:
#                 return 'spring'
#             elif month in [6, 7, 8]:
#                 return 'summer'
#             else:
#                 return 'fall'
        
#         filtered_df['season'] = filtered_df['datetime'].dt.month.apply(get_season)
    
#     print(f"Loaded {len(filtered_df)} rows for site-depth")
#     print(f"Memory after loading: {memory_usage():.1f} MB")
    
#     # Report moisture data
#     if include_moisture and 'soil_moist_standardized' in filtered_df.columns:
#         valid_moisture = filtered_df['soil_moist_standardized'].notna().sum()
#         pct = valid_moisture / len(filtered_df) * 100 if len(filtered_df) > 0 else 0
#         print(f"  Moisture data coverage: {valid_moisture}/{len(filtered_df)} ({pct:.1f}%)")
    
#     return filtered_df

# # Fix 4: Main execution function with enhanced moisture data handling
# # Replace your run_integrated_zero_curtain_detection function with this version
# # This version ONLY uses functions that already exist in your codebase

# # def run_integrated_zero_curtain_detection(
# #     feather_path, 
# #     output_dir='zero_curtain_output',
# #     max_gap_hours=8,
# #     site_batch_size=10,
# #     verbose=False  # We keep this parameter but don't pass it downstream
# # ):
# #     """Run zero curtain detection with proper soil moisture integration"""
# #     import time
# #     start_time = time.time()
    
# #     print(f"Starting soil moisture integrated zero curtain detection")
# #     print(f"Data source: {feather_path}")
# #     print(f"Output directory: {output_dir}")
# #     print(f"Maximum gap hours (literature-based): {max_gap_hours}")
# #     print(f"Site batch size: {site_batch_size}")
# #     print(f"Verbose output: {verbose}")
    
# #     # Create output directories
# #     os.makedirs(output_dir, exist_ok=True)
# #     events_dir = os.path.join(output_dir, 'events')
# #     ml_dir = os.path.join(output_dir, 'ml_features')
# #     os.makedirs(events_dir, exist_ok=True)
# #     os.makedirs(ml_dir, exist_ok=True)
    
# #     # Step 1: Run detection pipeline
# #     # IMPORTANT: Don't pass verbose parameter here!
# #     events_df = run_memory_efficient_pipeline(
# #         feather_path=feather_path,
# #         output_dir=events_dir,
# #         site_batch_size=site_batch_size,
# #         checkpoint_interval=10,
# #         max_gap_hours=max_gap_hours,
# #         interpolation_method='cubic',
# #         include_moisture=True  # This parameter should exist in your current implementation
# #     )
    
# #     # Force garbage collection
# #     for _ in range(3):
# #         gc.collect()
    
# #     # Check if we need to load from file (if run_memory_efficient_pipeline returned empty df)
# #     if len(events_df) == 0:
# #         events_path = os.path.join(events_dir, 'zero_curtain_events.csv')
# #         if os.path.exists(events_path):
# #             print(f"Loading events from {events_path}")
# #             events_df = pd.read_csv(events_path)
# #             print(f"Loaded {len(events_df)} events")
    
# #     # Step 2: Prepare ML features if events were found
# #     if len(events_df) > 0:
# #         print("Preparing ML features...")
        
# #         # Force cleanup before ML feature prep
# #         for _ in range(3):
# #             gc.collect()
        
# #         try:
# #             ml_features = prepare_ml_features(
# #                 events_df=events_df,
# #                 output_dir=ml_dir
# #             )
# #             print(f"ML feature preparation completed")
            
# #             # Force cleanup after ML feature prep
# #             del ml_features
# #             for _ in range(3):
# #                 gc.collect()
# #         except Exception as e:
# #             print(f"Error in ML feature preparation: {str(e)}")
# #     else:
# #         print("No events found, skipping ML feature preparation")
    
# #     # Report execution time
# #     execution_time = time.time() - start_time
# #     print(f"Execution completed in {execution_time:.1f} seconds ({execution_time/60:.1f} minutes...
# #     print(f"Final memory usage: {memory_usage():.1f} MB")
    
# #     return events_df


# # Updated run_integrated_zero_curtain_detection function to use the fixed pipeline
# def run_integrated_zero_curtain_detection_fixed(
#     feather_path, 
#     output_dir='zero_curtain_output',
#     max_gap_hours=8,
#     site_batch_size=10,
#     verbose=False
# ):
#     """Run zero curtain detection with proper soil moisture integration"""
#     import time
#     start_time = time.time()
    
#     print(f"Starting soil moisture integrated zero curtain detection")
#     print(f"Data source: {feather_path}")
#     print(f"Output directory: {output_dir}")
#     print(f"Maximum gap hours (literature-based): {max_gap_hours}")
#     print(f"Site batch size: {site_batch_size}")
#     print(f"Verbose output: {verbose}")
    
#     # Create output directories
#     os.makedirs(output_dir, exist_ok=True)
#     events_dir = os.path.join(output_dir, 'events')
#     ml_dir = os.path.join(output_dir, 'ml_features')
#     os.makedirs(events_dir, exist_ok=True)
#     os.makedirs(ml_dir, exist_ok=True)
    
#     # Step 1: Run detection pipeline with fixed function
#     # Uses the fixed version that doesn't pass verbose to subfunctions
#     events_df = run_memory_efficient_pipeline_fixed(
#         feather_path=feather_path,
#         output_dir=events_dir,
#         site_batch_size=site_batch_size,
#         checkpoint_interval=5,
#         max_gap_hours=max_gap_hours,
#         interpolation_method='cubic',
#         include_moisture=True,
#         verbose=verbose  # This parameter is handled properly in the fixed function
#     )
    
#     # Force aggressive cleanup
#     for _ in range(3):
#         gc.collect()
    
#     # Check if we need to load from file (if run_memory_efficient_pipeline returned empty df)
#     if len(events_df) == 0:
#         events_path = os.path.join(events_dir, 'zero_curtain_events.csv')
#         if os.path.exists(events_path):
#             print(f"Loading events from {events_path}")
#             events_df = pd.read_csv(events_path)
#             print(f"Loaded {len(events_df)} events")
    
#     # Step 2: Prepare ML features if events were found
#     if len(events_df) > 0:
#         print("Preparing ML features with optimized memory usage...")
        
#         # Force cleanup before ML feature prep
#         for _ in range(3):
#             gc.collect()
        
#         try:
#             ml_features = prepare_ml_features(
#                 events_df=events_df,
#                 output_dir=ml_dir
#             )
#             print(f"ML feature preparation completed")
            
#             # Force cleanup after ML feature prep
#             del ml_features
#             for _ in range(3):
#                 gc.collect()
#         except Exception as e:
#             print(f"Error in ML feature preparation: {str(e)}")
#     else:
#         print("No events found, skipping ML feature preparation")
    
#     # Report execution time
#     execution_time = time.time() - start_time
#     print(f"Execution completed in {execution_time:.1f} seconds ({execution_time/60:.1f} minutes)"...
#     print(f"Final memory usage: {memory_usage():.1f} MB")
    
#     return events_df


# #!/usr/bin/env python
# import os
# import numpy as np
# import pandas as pd
# import gc
# import pickle
# import psutil
# from datetime import datetime, timedelta
# import time
# import pyarrow as pa
# import pyarrow.feather as pf
# import pyarrow.dataset as ds
# import shutil
# import matplotlib.pyplot as plt
# import seaborn as sns

# def run_pipeline_with_fixed_moisture(feather_path, site_depths_path, output_dir):
#     """
#     Run the zero curtain pipeline using the fixed site_depths with correct moisture detection.
    
#     Parameters:
#     -----------
#     feather_path : str
#         Path to your feather data file
#     site_depths_path : str
#         Path to the site_depths.pkl file with correct moisture detection
#     output_dir : str
#         Directory to save results
#     """
#     print("=" * 80)
#     print("ZERO CURTAIN PIPELINE WITH FIXED MOISTURE DETECTION")
#     print("=" * 80)
    
#     # Create output directories
#     os.makedirs(output_dir, exist_ok=True)
#     events_dir = os.path.join(output_dir, 'events')
#     ml_dir = os.path.join(output_dir, 'ml_features')
#     checkpoint_dir = os.path.join(events_dir, 'checkpoints')
#     os.makedirs(events_dir, exist_ok=True)
#     os.makedirs(ml_dir, exist_ok=True)
#     os.makedirs(checkpoint_dir, exist_ok=True)
    
#     # Start timing
#     start_time = time.time()
    
#     # 1. Load the fixed site_depths file
#     print(f"Loading fixed site_depths from: {site_depths_path}")
#     with open(site_depths_path, 'rb') as f:
#         site_depths = pickle.load(f)
    
#     # Save a copy to the checkpoint directory
#     with open(os.path.join(checkpoint_dir, 'site_depths.pkl'), 'wb') as f:
#         pickle.dump(site_depths, f)
        
#     # Report moisture capabilities
#     total_sites = len(site_depths)
#     moisture_sites = site_depths['has_moisture_data'].sum()
#     print(f"Site depths loaded: {total_sites}")
#     print(f"Sites with moisture data: {moisture_sites} ({moisture_sites/total_sites*100:.1f}%)")
    
#     # 2. Process sites with optimized batch processing
#     site_batch_size = 10
#     max_gap_hours = 8
#     checkpoint_interval = 5
    
#     # For memory management
#     def aggressive_cleanup():
#         for _ in range(3):
#             gc.collect()
    
#     # Initialize results
#     all_events = []
#     processed_indices = set()
    
#     # Create batches for processing
#     total_batches = (total_sites + site_batch_size - 1) // site_batch_size
    
#     print(f"\nProcessing {total_sites} site-depth combinations in {total_batches} batches")
    
#     # Process in batches
#     for batch_idx in range(total_batches):
#         batch_start = batch_idx * site_batch_size
#         batch_end = min(batch_start + site_batch_size, total_sites)
        
#         print(f"\nProcessing batch {batch_idx+1}/{total_batches} (sites {batch_start+1}-{batch_end...
#         print(f"Memory before batch: {memory_usage():.1f} MB")
        
#         # Force garbage collection before each batch
#         aggressive_cleanup()
        
#         # Process each site in batch
#         for site_idx in range(batch_start, batch_end):
#             # Skip if already processed
#             if site_idx in processed_indices:
#                 continue
            
#             site = site_depths.iloc[site_idx]['source']
#             temp_depth = site_depths.iloc[site_idx]['soil_temp_depth']
            
#             print(f"\nProcessing site {site_idx+1}/{total_sites}: {site}, depth: {temp_depth}")
            
#             try:
#                 # Check if site has moisture capabilities - using corrected detection
#                 has_moisture = False
#                 if 'has_moisture_data' in site_depths.columns:
#                     has_moisture = site_depths.iloc[site_idx]['has_moisture_data']
                
#                 # Load site data - using your existing function
#                 site_df = load_site_depth_data(
#                     feather_path=feather_path, 
#                     site=site, 
#                     temp_depth=temp_depth, 
#                     include_moisture=has_moisture
#                 )
                
#                 # Skip if insufficient data
#                 if len(site_df) < 24:
#                     print(f"  Insufficient data ({len(site_df)} rows), skipping")
#                     processed_indices.add(site_idx)
#                     continue
                
#                 # Process for zero curtain - using your existing function
#                 site_events = process_site_for_zero_curtain(
#                     site_df=site_df, 
#                     site=site, 
#                     temp_depth=temp_depth, 
#                     max_gap_hours=max_gap_hours, 
#                     interpolation_method='cubic'
#                 )
                
#                 # Add to all events
#                 new_events = len(site_events)
#                 all_events.extend(site_events)
                
#                 print(f"  Site {site_idx+1}: Found {new_events} events, total: {len(all_events)}")
                
#                 # Mark as processed
#                 processed_indices.add(site_idx)
                
#                 # Save checkpoint periodically
#                 if site_idx % checkpoint_interval == 0:
#                     # Save events
#                     with open(os.path.join(checkpoint_dir, 'all_events.pkl'), 'wb') as f:
#                         pickle.dump(all_events, f)
#                     # Save processed indices
#                     with open(os.path.join(checkpoint_dir, 'processed_indices.pkl'), 'wb') as f:
#                         pickle.dump(list(processed_indices), f)
                
#                 # Clean up memory
#                 del site_df, site_events
#                 aggressive_cleanup()
                
#             except Exception as e:
#                 print(f"  Error processing site {site_idx+1} ({site}, depth {temp_depth}): {str(e)...
#                 continue
                
#         # Save checkpoint after each batch
#         print(f"Saving checkpoint after batch {batch_idx+1}/{total_batches}")
        
#         # Save events
#         with open(os.path.join(checkpoint_dir, 'all_events.pkl'), 'wb') as f:
#             pickle.dump(all_events, f)
        
#         # Save processed indices
#         with open(os.path.join(checkpoint_dir, 'processed_indices.pkl'), 'wb') as f:
#             pickle.dump(list(processed_indices), f)
        
#         # Also save intermediate results as CSV
#         if len(all_events) > 0:
#             interim_df = pd.DataFrame(all_events)
#             interim_path = os.path.join(events_dir, 'events_checkpoint.csv')
#             interim_df.to_csv(interim_path, index=False)
#             print(f"Saved interim results to {interim_path}")
            
#             # Check moisture data in intermediate results
#             moisture_cols = [col for col in interim_df.columns if 'moist' in col.lower()]
#             if moisture_cols:
#                 for col in moisture_cols:
#                     valid = pd.to_numeric(interim_df[col], errors='coerce').notna().sum()
#                     pct = valid / len(interim_df) * 100
#                     print(f"  {col}: {valid}/{len(interim_df)} values ({pct:.1f}%)")
            
#             # Clean up
#             del interim_df
#             gc.collect()
    
#     # Create final dataframe
#     print("\nCreating final events dataframe")
#     if len(all_events) > 0:
#         events_df = pd.DataFrame(all_events)
#         print(f"Created events dataframe with {len(events_df)} total events")
        
#         # Report on moisture data
#         moisture_cols = [col for col in events_df.columns if 'moist' in col.lower()]
#         if moisture_cols:
#             print("\nMoisture data in final results:")
#             for col in moisture_cols:
#                 valid = pd.to_numeric(events_df[col], errors='coerce').notna().sum()
#                 pct = valid / len(events_df) * 100
#                 print(f"  {col}: {valid}/{len(events_df)} values ({pct:.1f}%)")
        
#         # Save final results
#         final_path = os.path.join(events_dir, 'zero_curtain_events.csv')
#         temp_path = os.path.join(events_dir, 'zero_curtain_events_temp.csv')
#         events_df.to_csv(temp_path, index=False)
#         shutil.move(temp_path, final_path)
        
#         print(f"Saved final results to {final_path}")
        
#         # Prepare ML features
#         print("\nPreparing ML features...")
#         try:
#             prepare_ml_features(events_df, output_dir=ml_dir)
#             print("ML feature preparation completed")
#         except Exception as e:
#             print(f"Error in ML feature preparation: {str(e)}")
#     else:
#         print("No events found.")
    
#     # Report execution time
#     execution_time = time.time() - start_time
#     print(f"\nExecution completed in {execution_time:.1f} seconds ({execution_time/60:.1f} minutes...
#     print(f"Final memory usage: {memory_usage():.1f} MB")
    
# # Implement the optimized load_site_depth_data function specifically for moisture handling
# def load_site_depth_data(feather_path, site, temp_depth, include_moisture=True):
#     """Optimized function to load site data with proper moisture handling"""
#     print(f"Loading data for site: {site}, depth: {temp_depth}")
#     print(f"Memory before loading: {memory_usage():.1f} MB")
    
#     try:
#         # Use PyArrow for efficient filtering
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Create filter expressions
#         site_filter = ds.field('source') == site
#         depth_filter = ds.field('soil_temp_depth') == float(temp_depth)
#         combined_filter = site_filter & depth_filter
        
#         # Apply filter and read only filtered data
#         table = dataset.to_table(filter=combined_filter)
#         filtered_df = table.to_pandas()
        
#         # Get soil moisture data if available and requested
#         if include_moisture and len(filtered_df) > 0:
#             # Check if we already have moisture data
#             has_moisture = (
#                 'soil_moist_standardized' in filtered_df.columns and 
#                 not filtered_df['soil_moist_standardized'].isna().all()
#             )
            
#             if not has_moisture:
#                 # Try to get moisture data separately
#                 try:
#                     # Get all moisture data for this site
#                     moisture_filter = site_filter
#                     moisture_table = dataset.to_table(
#                         filter=moisture_filter,
#                         columns=['datetime', 'soil_moist_standardized', 'soil_moist_depth']
#                     )
#                     moisture_df = moisture_table.to_pandas()
                    
#                     # If we have moisture data
#                     if (len(moisture_df) > 0 and 
#                         'soil_moist_depth' in moisture_df.columns and 
#                         not moisture_df['soil_moist_depth'].isna().all()):
                        
#                         # Find optimal moisture depth
#                         optimal_depth = get_optimal_moisture_depth(moisture_df, temp_depth)
                        
#                         if not np.isnan(optimal_depth):
#                             print(f"  Found optimal moisture depth: {optimal_depth}")
                            
#                             # Filter to this depth
#                             depth_moisture = moisture_df[moisture_df['soil_moist_depth'] == optima...
                            
#                             if len(depth_moisture) > 0:
#                                 # Prepare for merge
#                                 filtered_df['datetime_key'] = filtered_df['datetime']
#                                 depth_moisture['datetime_key'] = depth_moisture['datetime']
                                
#                                 # Merge with temperature data
#                                 merged_df = pd.merge_asof(
#                                     filtered_df.sort_values('datetime_key'),
#                                     depth_moisture[['datetime_key', 'soil_moist_standardized', 'so...
#                                     on='datetime_key',
#                                     direction='nearest',
#                                     tolerance=pd.Timedelta('6h')
#                                 )
                                
#                                 # Clean up and update
#                                 merged_df.drop('datetime_key', axis=1, inplace=True)
#                                 filtered_df = merged_df
#                                 filtered_df['closest_moist_depth'] = optimal_depth
                                
#                                 # Report on merge
#                                 valid_moisture = filtered_df['soil_moist_standardized'].notna().su...
#                                 print(f"  Added moisture data: {valid_moisture}/{len(filtered_df)}...
#                 except Exception as e:
#                     print(f"  Error adding moisture data: {str(e)}")
        
#         # Clean up
#         del table, dataset
#         gc.collect()
        
#     except Exception as e:
#         print(f"Error with PyArrow filtering: {str(e)}")
#         # Fallback to your existing approach if needed
#         filtered_df = pd.DataFrame()
    
#     # Continue with the same post-processing as your existing code
#     # Ensure datetime format, add depth zone, year, season, etc.
#     if 'datetime' in filtered_df.columns and not pd.api.types.is_datetime64_dtype(filtered_df['dat...
#         filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])
    
#     # Add depth zone
#     if 'soil_temp_depth' in filtered_df.columns:
#         filtered_df['soil_temp_depth_zone'] = pd.cut(
#             filtered_df['soil_temp_depth'],
#             bins=[-float('inf'), 5, 20, 50, 100, float('inf')],
#             labels=['surface', 'shallow', 'mid', 'deep', 'very_deep']
#         )
    
#     # Add year if missing
#     if 'year' not in filtered_df.columns and 'datetime' in filtered_df.columns:
#         filtered_df['year'] = filtered_df['datetime'].dt.year
    
#     # Add season if missing
#     if 'season' not in filtered_df.columns and 'datetime' in filtered_df.columns:
#         def get_season(month):
#             if month in [12, 1, 2]:
#                 return 'winter'
#             elif month in [3, 4, 5]:
#                 return 'spring'
#             elif month in [6, 7, 8]:
#                 return 'summer'
#             else:
#                 return 'fall'
        
#         filtered_df['season'] = filtered_df['datetime'].dt.month.apply(get_season)
    
#     print(f"Loaded {len(filtered_df)} rows for site-depth")
#     print(f"Memory after loading: {memory_usage():.1f} MB")
    
#     # Report moisture data
#     if include_moisture and 'soil_moist_standardized' in filtered_df.columns:
#         valid_moisture = filtered_df['soil_moist_standardized'].notna().sum()
#         pct = valid_moisture / len(filtered_df) * 100 if len(filtered_df) > 0 else 0
#         print(f"  Moisture data coverage: {valid_moisture}/{len(filtered_df)} ({pct:.1f}%)")
    
#     return filtered_df

# # # Example usage
# # if __name__ == "__main__":
# #     # Replace with your actual file path
# #     feather_path = 'merged_compressed.feather'
# #     output_dir = 'zero_curtain_results_3'
    
# #     # Force cleanup before starting
# #     #aggressive_cleanup()
# #     for _ in range(3):
# #         gc.collect()
    
# #     # Run with optimized settings
# #     events = run_integrated_zero_curtain_detection_fixed(
# #         feather_path=feather_path,
# #         output_dir=output_dir,
# #         max_gap_hours=8,           # Literature-based parameter
# #         site_batch_size=10,        # Smaller batch size for lower memory usage
# #         verbose=False              # Only print essential messages
# #     )
    
# #     print(f"Detected {len(events)} zero curtain events with integrated soil moisture")

# #     print(f"  Starting memory: {memory_usage():.1f} MB")
    
# #     # Final cleanup
# #     del events
# #     #aggressive_cleanup()
# #     for _ in range(3):
# #         gc.collect()

# # Main execution
# if __name__ == "__main__":
#     # Define paths
#     feather_path = '/Users/bgay/Desktop/Research/Code/merged_compressed.feather'
#     site_depths_path = '/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling/scripts/z...
#     output_dir = 'zero_curtain_results_fixed'
    
#     # Run the pipeline with fixed moisture detection
#     run_pipeline_with_fixed_moisture(
#         feather_path=feather_path,
#         site_depths_path=site_depths_path,
#         output_dir=output_dir
#     )

# import os
# import numpy as np
# import pandas as pd
# import gc
# import pickle
# import psutil
# from datetime import datetime, timedelta
# import time
# from scipy.interpolate import interp1d
# import matplotlib.pyplot as plt
# import seaborn as sns

# def memory_usage():
#     """Monitor memory usage in MB"""
#     process = psutil.Process(os.getpid())
#     mem_info = process.memory_info()
#     return mem_info.rss / 1024**2  # Memory in MB

# def save_checkpoint(data, checkpoint_dir, name):
#     """Save checkpoint data to pickle file"""
#     os.makedirs(checkpoint_dir, exist_ok=True)
#     checkpoint_path = os.path.join(checkpoint_dir, f'{name}.pkl')
#     with open(checkpoint_path, 'wb') as f:
#         pickle.dump(data, f)
#     print(f"Saved checkpoint to {checkpoint_path}")

# def load_checkpoint(checkpoint_dir, name):
#     """Load checkpoint data from pickle file"""
#     checkpoint_path = os.path.join(checkpoint_dir, f'{name}.pkl')
#     try:
#         with open(checkpoint_path, 'rb') as f:
#             data = pickle.load(f)
#         print(f"Loaded checkpoint from {checkpoint_path}")
#         return data
#     except:
#         print(f"No checkpoint found at {checkpoint_path}")
#         return None

# # def aggressive_cleanup():
# #     """Aggressively clean up memory"""
# #     # Call garbage collection multiple times to ensure objects are freed
# #     for _ in range(3):
# #         gc.collect()
    
# #     # Force a memory compaction if running on Linux
# #     try:
# #         import ctypes
# #         libc = ctypes.CDLL('libc.so.6')
# #         # MADV_DONTNEED = 4
# #         # MADV_FREE = 8 (only on newer kernels)
# #         libc.malloc_trim(0)
# #     except:
# #         pass

# # data_loader.py
# import pyarrow as pa
# import pyarrow.feather as pf
# import pyarrow.compute as pc
# import pyarrow.dataset as ds
# import gc
# from tqdm.auto import tqdm

# def get_time_range(feather_path):
#     """Get min and max datetime from feather file efficiently"""
#     print("Determining dataset time range")
#     try:
#         # Use PyArrow for efficient metadata access
#         table = pf.read_table(feather_path, columns=['datetime'])
#         datetime_col = table['datetime']
#         min_date = pd.Timestamp(pc.min(datetime_col).as_py())
#         max_date = pd.Timestamp(pc.max(datetime_col).as_py())
#         del table, datetime_col
#         gc.collect()
#     except Exception as e:
#         print(f"Error with PyArrow min/max: {str(e)}")
#         print("Falling back to reading sample rows...")
        
#         # Read just first and last rows after sorting
#         # This is more efficient than reading all data
#         try:
#             dataset = ds.dataset(feather_path, format='feather')
            
#             # Get min date from sorted data (first row)
#             table_min = dataset.to_table(
#                 columns=['datetime'],
#                 filter=None,
#                 use_threads=True,
#                 limit=1
#             )
#             min_date = pd.Timestamp(table_min['datetime'][0].as_py())
            
#             # Get max date from reverse sorted data (first row)
#             # Note: PyArrow Dataset API doesn't support reverse sort directly
#             # So we read all datetime values and find max
#             table_all = dataset.to_table(columns=['datetime'])
#             max_date = pd.Timestamp(pc.max(table_all['datetime']).as_py())
            
#             del table_min, table_all, dataset
#             gc.collect()
#         except Exception as inner_e:
#             print(f"Error with optimized approach: {str(inner_e)}")
#             print("Using full scan method (slow)...")
            
#             # Last resort: read all timestamps in chunks
#             min_date = pd.Timestamp.max
#             max_date = pd.Timestamp.min
            
#             # Open file directly to avoid loading everything
#             table = pf.read_table(feather_path, columns=['datetime'])
#             datetime_values = table['datetime'].to_pandas()
            
#             min_date = datetime_values.min()
#             max_date = datetime_values.max()
            
#             del datetime_values, table
#             gc.collect()
    
#     print(f"Data timespan: {min_date} to {max_date}")
#     return min_date, max_date

# def get_site_metadata(feather_path):
#     """Get comprehensive site metadata including moisture capabilities"""
#     print("Extracting site metadata including moisture measurement capabilities")
#     try:
#         # Using PyArrow for efficient column scanning
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Read site metadata columns
#         table = dataset.to_table(columns=[
#             'source', 'latitude', 'longitude', 'elevation', 
#             'soil_temp_depth', 'soil_moist_depth'
#         ])
        
#         # Convert to pandas
#         site_meta_df = table.to_pandas()
        
#         # Get unique site information
#         sites = site_meta_df.drop_duplicates('source')
        
#         # Calculate moisture availability by site
#         site_moisture_avail = site_meta_df.groupby('source')['soil_moist_depth'].apply(
#             lambda x: not x.isna().all()
#         ).reset_index()
#         site_moisture_avail.columns = ['source', 'has_moisture_data']
        
#         # Merge site information with moisture availability
#         sites = sites.merge(site_moisture_avail, on='source', how='left')
        
#         # Clean up
#         del table, site_meta_df, site_moisture_avail, dataset
#         gc.collect()
        
#         print(f"Found {len(sites)} unique sites, {sites['has_moisture_data'].sum()} with moisture ...
        
#         return sites
        
#     except Exception as e:
#         print(f"Error extracting site metadata: {str(e)}")
#         return None

# def get_unique_site_depths(feather_path, include_moisture=True):
#     """Get unique site-depth combinations with both temperature and moisture capabilities"""
#     print("Finding unique site-depth combinations")
#     print(f"Memory before processing: {memory_usage():.1f} MB")
    
#     df = None  # Initialize df to avoid UnboundLocalError in except block
    
#     try:
#         # Use PyArrow for efficient operation
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Read only the columns we need
#         cols = ['source', 'soil_temp_depth']
#         if include_moisture:
#             cols.extend(['soil_moist_depth'])
            
#         table = dataset.to_table(columns=cols)
        
#         # Convert to pandas
#         df = table.to_pandas()
        
#         # Get valid combinations (where temp_depth is not NaN)
#         valid_df = df.dropna(subset=['soil_temp_depth'])
        
#         # Get unique combinations
#         site_temps = valid_df.drop_duplicates(['source', 'soil_temp_depth'])
        
#         # If moisture is included, mark sites with moisture capabilities
#         if include_moisture and 'soil_moist_depth' in df.columns:
#             # Group by source and check for moisture data availability
#             moisture_avail = df.groupby('source')['soil_moist_depth'].apply(
#                 lambda x: not x.isna().all()
#             ).reset_index()
#             moisture_avail.columns = ['source', 'has_moisture_data']
            
#             # Merge with site_temps
#             site_temps = site_temps.merge(moisture_avail, on='source', how='left')
            
#             # Fill NaN values
#             site_temps['has_moisture_data'] = site_temps['has_moisture_data'].fillna(False)
            
#             # Clean up
#             del moisture_avail
#         else:
#             site_temps['has_moisture_data'] = False
        
#         # Add latitude and longitude if available
#         if 'latitude' in df.columns and 'longitude' in df.columns:
#             # Get site coordinates
#             site_coords = df.drop_duplicates('source')[['source', 'latitude', 'longitude']]
#             # Merge with site_temps
#             site_temps = site_temps.merge(site_coords, on='source', how='left')
        
#         # Clean up
#         del table, df, valid_df, dataset
#         gc.collect()
        
#     except Exception as e:
#         print(f"Error with PyArrow approach: {str(e)}")
#         print("Falling back to chunked pandas approach...")
        
#         # Fallback: Process in chunks
#         chunk_size = 1000000
#         unique_combos = set()
#         has_moisture_by_site = {}
#         site_coords = {}
        
#         # Read feather in chunks (this is slower but more robust)
#         cols = ['source', 'soil_temp_depth']
#         if include_moisture:
#             cols.extend(['soil_moist_depth'])
            
#         # Add coordinate columns if we have them
#         # Important: Don't reference df here since it might not exist if the exception occurred ea...
#         try:
#             # Try to peek at the feather file to see what columns exist
#             peek_df = pd.read_feather(feather_path, nrows=1)
#             if 'latitude' in peek_df.columns and 'longitude' in peek_df.columns:
#                 cols.extend(['latitude', 'longitude'])
#             del peek_df
#         except:
#             pass  # If peek fails, just continue without coordinates
        
#         # Process in chunks - NOT using context manager
#         # Read the feather file in chunks instead of using 'with' statement
#         try:
#             # Calculate number of rows to estimate chunks
#             total_rows = 0
#             try:
#                 # Try to get row count using pyarrow if possible
#                 import pyarrow.parquet as pq
#                 dataset = ds.dataset(feather_path, format='feather')
#                 total_rows = dataset.count_rows()
#                 del dataset
#             except:
#                 try:
#                     # Fallback: Try to get row estimate from file size
#                     import os
#                     file_size = os.path.getsize(feather_path)
#                     # Rough estimate: 1 million rows per GB
#                     total_rows = int(file_size / (1024**3) * 1000000)
#                     if total_rows < 1000000:
#                         total_rows = 1000000  # Minimum estimate
#                 except:
#                     total_rows = 10000000  # Just guess if we can't determine
            
#             # Process in chunks
#             for chunk_start in range(0, total_rows, chunk_size):
#                 try:
#                     chunk = pd.read_feather(
#                         feather_path, 
#                         columns=cols,
#                         nthreads=1
#                     )
                    
#                     # If we got all data at once, just process it directly
#                     valid_rows = chunk.dropna(subset=['soil_temp_depth'])
#                     chunk_combinations = set(zip(valid_rows['source'], valid_rows['soil_temp_depth...
#                     unique_combos.update(chunk_combinations)
                    
#                     # Track moisture data availability by site
#                     if include_moisture and 'soil_moist_depth' in chunk.columns:
#                         for site in chunk['source'].unique():
#                             site_chunk = chunk[chunk['source'] == site]
#                             has_moisture = not site_chunk['soil_moist_depth'].isna().all()
#                             has_moisture_by_site[site] = has_moisture
                    
#                     # Track coordinates
#                     if 'latitude' in chunk.columns and 'longitude' in chunk.columns:
#                         for site in chunk['source'].unique():
#                             if site not in site_coords:
#                                 site_chunk = chunk[chunk['source'] == site].iloc[0]
#                                 site_coords[site] = {
#                                     'latitude': site_chunk['latitude'],
#                                     'longitude': site_chunk['longitude']
#                                 }
                    
#                     # If we read all data at once, break the loop
#                     break
                    
#                 except Exception as chunk_error:
#                     print(f"Error reading chunk: {str(chunk_error)}")
#                     # Try reading smaller chunks if full read fails
#                     try:
#                         smaller_chunk_size = min(chunk_size // 2, 100000)
#                         chunk = pd.read_feather(
#                             feather_path, 
#                             columns=cols,
#                             nthreads=1,
#                             nrows=smaller_chunk_size,
#                             skip=chunk_start
#                         )
                        
#                         # Process smaller chunk
#                         valid_rows = chunk.dropna(subset=['soil_temp_depth'])
#                         chunk_combinations = set(zip(valid_rows['source'], valid_rows['soil_temp_d...
#                         unique_combos.update(chunk_combinations)
                        
#                         # Track moisture data
#                         if include_moisture and 'soil_moist_depth' in chunk.columns:
#                             for site in chunk['source'].unique():
#                                 site_chunk = chunk[chunk['source'] == site]
#                                 has_moisture = not site_chunk['soil_moist_depth'].isna().all()
#                                 has_moisture_by_site[site] = has_moisture
                        
#                         # Track coordinates
#                         if 'latitude' in chunk.columns and 'longitude' in chunk.columns:
#                             for site in chunk['source'].unique():
#                                 if site not in site_coords:
#                                     site_chunk = chunk[chunk['source'] == site].iloc[0]
#                                     site_coords[site] = {
#                                         'latitude': site_chunk['latitude'],
#                                         'longitude': site_chunk['longitude']
#                                     }
#                     except Exception as small_chunk_error:
#                         print(f"Error reading smaller chunk: {str(small_chunk_error)}")
#                         # Skip this chunk if both attempts fail
                
#                 # Clean up regardless of success
#                 if 'chunk' in locals():
#                     del chunk
#                 if 'valid_rows' in locals():
#                     del valid_rows
#                 if 'chunk_combinations' in locals():
#                     del chunk_combinations
#                 gc.collect()
        
#         except Exception as e:
#             print(f"Error during chunked reading: {str(e)}")
#             # As a last resort, try to read just the first part of the file
#             try:
#                 chunk = pd.read_feather(feather_path, columns=cols, nrows=10000)
                
#                 # Process what we've got
#                 valid_rows = chunk.dropna(subset=['soil_temp_depth'])
#                 chunk_combinations = set(zip(valid_rows['source'], valid_rows['soil_temp_depth']))
#                 unique_combos.update(chunk_combinations)
                
#                 # Track moisture data
#                 if include_moisture and 'soil_moist_depth' in chunk.columns:
#                     for site in chunk['source'].unique():
#                         site_chunk = chunk[chunk['source'] == site]
#                         has_moisture = not site_chunk['soil_moist_depth'].isna().all()
#                         has_moisture_by_site[site] = has_moisture
                
#                 # Clean up
#                 del chunk, valid_rows, chunk_combinations
#                 gc.collect()
#             except Exception as final_error:
#                 print(f"All attempts to read feather file failed: {str(final_error)}")
#                 # Return empty DataFrame if everything fails
#                 return pd.DataFrame(columns=['source', 'soil_temp_depth', 'has_moisture_data'])
        
#         # Convert to DataFrame
#         site_temps = pd.DataFrame(list(unique_combos), columns=['source', 'soil_temp_depth'])
        
#         # Add moisture data availability
#         if include_moisture:
#             site_temps['has_moisture_data'] = site_temps['source'].map(
#                 lambda x: has_moisture_by_site.get(x, False)
#             )
#         else:
#             site_temps['has_moisture_data'] = False
            
#         # Add coordinates
#         if site_coords:
#             site_temps['latitude'] = site_temps['source'].map(
#                 lambda x: site_coords.get(x, {}).get('latitude', np.nan)
#             )
#             site_temps['longitude'] = site_temps['source'].map(
#                 lambda x: site_coords.get(x, {}).get('longitude', np.nan)
#             )
    
#     print(f"Found {len(site_temps)} unique site-depth combinations")
#     if 'has_moisture_data' in site_temps.columns:
#         print(f"{site_temps['has_moisture_data'].sum()} sites have moisture data")
#     print(f"Memory after processing: {memory_usage():.1f} MB")
    
#     return site_temps

# def get_optimal_moisture_depth(df, temp_depth):
#     """
#     Find the closest moisture depth measurement to a given temperature depth
#     This ensures we compare temperature and moisture at comparable depths
#     """
#     if 'soil_moist_depth' not in df.columns or df['soil_moist_depth'].isna().all():
#         return np.nan
    
#     # Get unique moisture depths
#     moist_depths = df['soil_moist_depth'].dropna().unique()
    
#     if len(moist_depths) == 0:
#         return np.nan
    
#     # Find closest depth
#     closest_depth = moist_depths[np.abs(moist_depths - temp_depth).argmin()]
    
#     return closest_depth

# def load_site_depth_data(feather_path, site, temp_depth, include_moisture=True, verbose=False):
#     """Load ONLY data for a specific site and depth with moisture data when available"""
#     if verbose:
#         print(f"Loading data for site: {site}, depth: {temp_depth}")
#         print(f"Memory before loading: {memory_usage():.1f} MB")
    
#     try:
#         # Use PyArrow Dataset API for efficient filtering
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Create filter expressions
#         site_filter = ds.field('source') == site
#         depth_filter = ds.field('soil_temp_depth') == float(temp_depth)
#         combined_filter = site_filter & depth_filter
        
#         # Apply filter and read only filtered data
#         table = dataset.to_table(filter=combined_filter)
#         filtered_df = table.to_pandas()
        
#         # Get soil moisture data if available and requested
#         if include_moisture and len(filtered_df) > 0:
#             # First check if we already have moisture in the filtered data
#             has_moisture = (
#                 'soil_moist_standardized' in filtered_df.columns and 
#                 not filtered_df['soil_moist_standardized'].isna().all()
#             )
            
#             if not has_moisture:
#                 # Need to get moisture data separately - find optimal depth first
#                 # Create a small sample to check for available moisture depths
#                 sample_moisture = dataset.to_table(
#                     filter=site_filter,
#                     columns=['soil_moist_depth', 'datetime'],
#                     limit=1000
#                 ).to_pandas()
                
#                 if 'soil_moist_depth' in sample_moisture.columns and not sample_moisture['soil_moi...
#                     # Find closest moisture depth to current temperature depth
#                     optimal_moist_depth = get_optimal_moisture_depth(sample_moisture, temp_depth)
                    
#                     if not np.isnan(optimal_moist_depth):
#                         if verbose:
#                             print(f"  Found optimal moisture depth: {optimal_moist_depth} (temp de...
                        
#                         # Get moisture data at optimal depth
#                         moist_filter = site_filter & (ds.field('soil_moist_depth') == float(optima...
#                         moist_table = dataset.to_table(
#                             filter=moist_filter,
#                             columns=['datetime', 'soil_moist_standardized', 'soil_moist_depth']
#                         )
#                         moist_df = moist_table.to_pandas()
                        
#                         if len(moist_df) > 0:
#                             # Merge moisture data with temperature data
#                             filtered_df['datetime_key'] = filtered_df['datetime']
#                             moist_df['datetime_key'] = moist_df['datetime']
                            
#                             # Create a merged dataset
#                             merged_df = pd.merge_asof(
#                                 filtered_df.sort_values('datetime_key'),
#                                 moist_df.sort_values('datetime_key')[['datetime_key', 'soil_moist_...
#                                 on='datetime_key',
#                                 direction='nearest',
#                                 tolerance=pd.Timedelta('6h')
#                             )
                            
#                             # Clean up the merge
#                             merged_df.drop('datetime_key', axis=1, inplace=True)
                            
#                             # Use the merged dataset
#                             filtered_df = merged_df
#                             filtered_df['closest_moist_depth'] = optimal_moist_depth
                            
#                             if verbose:
#                                 print(f"  Merged {len(moist_df)} moisture records with temperature...
                            
#                             # Clean up
#                             del moist_table, moist_df, merged_df
#                             gc.collect()
                
#                 # Clean up
#                 del sample_moisture
#                 gc.collect()
        
#         # Clean up
#         del table, dataset
#         gc.collect()
        
#     except Exception as e:
#         if verbose:
#             print(f"Error with PyArrow filtering: {str(e)}")
#             print("Falling back to pandas filtering...")
        
#         # Define chunk size based on available memory
#         available_mem = psutil.virtual_memory().available / (1024**2)  # MB
#         chunk_size = min(500000, max(100000, int(available_mem / 10)))
        
#         filtered_chunks = []
        
#         # Columns to load - always include basic columns
#         cols = ['source', 'soil_temp_depth', 'soil_temp_standardized', 'datetime']
        
#         # Add moisture columns if needed
#         if include_moisture:
#             cols.extend(['soil_moist_depth', 'soil_moist_standardized'])
        
#         # Add other useful columns if they exist
#         extra_cols = ['latitude', 'longitude', 'elevation', 'season', 'year']
#         for col in extra_cols:
#             cols.append(col)
        
#         # Read and filter in chunks
#         try:
#             chunk = pd.read_feather(feather_path, columns=cols)
#             chunk_filtered = chunk[(chunk['source'] == site) & 
#                                  (chunk['soil_temp_depth'] == temp_depth)]
            
#             if len(chunk_filtered) > 0:
#                 filtered_df = chunk_filtered.copy()
#             else:
#                 filtered_df = pd.DataFrame()
                
#             # Clean up
#             del chunk, chunk_filtered
#             gc.collect()
            
#         except Exception as inner_e:
#             if verbose:
#                 print(f"Error with direct read: {str(inner_e)}")
#                 print("Trying with chunks...")
            
#             # Try with chunks if full read fails
#             for i in range(0, 100000000, chunk_size):  # Arbitrary large number
#                 try:
#                     chunk = pd.read_feather(
#                         feather_path, 
#                         columns=cols,
#                         nrows=chunk_size,
#                         skip=i
#                     )
                    
#                     if len(chunk) == 0:
#                         # No more data to read
#                         break
                        
#                     # Filter by site and depth
#                     chunk_filtered = chunk[(chunk['source'] == site) & 
#                                          (chunk['soil_temp_depth'] == temp_depth)]
                    
#                     if len(chunk_filtered) > 0:
#                         filtered_chunks.append(chunk_filtered)
                    
#                     # Clean up
#                     del chunk, chunk_filtered
#                     gc.collect()
#                 except Exception as chunk_e:
#                     if verbose:
#                         print(f"Error reading chunk at position {i}: {str(chunk_e)}")
#                     break
            
#             # Combine filtered chunks
#             if filtered_chunks:
#                 filtered_df = pd.concat(filtered_chunks, ignore_index=True)
#                 del filtered_chunks
#             else:
#                 filtered_df = pd.DataFrame()
            
#             gc.collect()
    
#     # Ensure datetime is in datetime format
#     if 'datetime' in filtered_df.columns and not pd.api.types.is_datetime64_dtype(filtered_df['dat...
#         filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])
    
#     # Add depth zone categorization
#     if 'soil_temp_depth' in filtered_df.columns:
#         filtered_df['soil_temp_depth_zone'] = pd.cut(
#             filtered_df['soil_temp_depth'],
#             bins=[-float('inf'), 5, 20, 50, 100, float('inf')],
#             labels=['surface', 'shallow', 'mid', 'deep', 'very_deep']
#         )
    
#     # Add year column if missing
#     if 'year' not in filtered_df.columns and 'datetime' in filtered_df.columns:
#         filtered_df['year'] = filtered_df['datetime'].dt.year
    
#     # Add season if missing
#     if 'season' not in filtered_df.columns and 'datetime' in filtered_df.columns:
#         # Define seasons - assume northern hemisphere
#         def get_season(month):
#             if month in [12, 1, 2]:
#                 return 'winter'
#             elif month in [3, 4, 5]:
#                 return 'spring'
#             elif month in [6, 7, 8]:
#                 return 'summer'
#             else:
#                 return 'fall'
        
#         filtered_df['season'] = filtered_df['datetime'].dt.month.apply(get_season)
    
#     if verbose:
#         print(f"Loaded {len(filtered_df)} rows for site-depth")
#         print(f"Memory after loading: {memory_usage():.1f} MB")
    
#     # Report moisture data only if verbose
#     if verbose and include_moisture and 'soil_moist_standardized' in filtered_df.columns:
#         valid_moisture = filtered_df['soil_moist_standardized'].notna().sum()
#         pct = valid_moisture / len(filtered_df) * 100 if len(filtered_df) > 0 else 0
#         print(f"  Moisture data coverage: {valid_moisture}/{len(filtered_df)} ({pct:.1f}%)")
    
#     return filtered_df

# # zero_curtain_processing.py
# # Critical fixes to ensure soil moisture is preserved throughout the workflow
# # Fix 1: Update the process_site_for_zero_curtain function to properly use moisture data
# def process_site_for_zero_curtain(site_df, site, temp_depth, max_gap_hours=8, interpolation_method...
#     """Process a single site-depth for zero curtain events with moisture integration"""
#     # Initialize list to store events
#     site_events = []
    
#     # Skip if too few points
#     if len(site_df) < 24:  # Require at least 24 measurements for meaningful analysis
#         return []
    
#     # Sort by time
#     site_df = site_df.sort_values('datetime')
    
#     # Calculate time differences
#     site_df['time_diff'] = site_df['datetime'].diff().dt.total_seconds() / 3600
    
#     # Check for moisture data availability - CRITICAL FIX
#     has_moisture = ('soil_moist_standardized' in site_df.columns and 
#                    not site_df['soil_moist_standardized'].isna().all())
    
#     if verbose:
#         print(f"  Processing site with moisture data: {has_moisture}")
    
#     # Identify gaps for interpolation - literature suggests 8 hours max
#     interpolation_needed = (site_df['time_diff'] > 1.0) & (site_df['time_diff'] <= max_gap_hours)
    
#     # Perform interpolation if needed
#     if interpolation_needed.any():
#         interp_rows = []
#         gap_indices = site_df.index[interpolation_needed].tolist()
        
#         for idx in gap_indices:
#             try:
#                 # Get before and after rows
#                 before_row = site_df.loc[site_df.index[site_df.index.get_loc(idx) - 1]]
#                 after_row = site_df.loc[idx]
                
#                 # Calculate gap and intervals
#                 time_gap = after_row['time_diff']
#                 n_intervals = int(time_gap)
                
#                 if n_intervals > 0:
#                     # Create timestamps
#                     timestamps = pd.date_range(
#                         start=before_row['datetime'],
#                         end=after_row['datetime'],
#                         periods=n_intervals + 2  # Include endpoints
#                     )[1:-1]  # Exclude endpoints
                    
#                     # Create temperature values
#                     if interpolation_method == 'linear' or len(site_df) < 5:
#                         # Linear interpolation
#                         temp_start = before_row['soil_temp_standardized']
#                         temp_end = after_row['soil_temp_standardized']
#                         temp_values = np.linspace(temp_start, temp_end, n_intervals + 2)[1:-1]
#                     else:
#                         # Try cubic interpolation
#                         try:
#                             # Get neighboring points
#                             idx_loc = site_df.index.get_loc(idx)
#                             start_idx = max(0, idx_loc - 3)
#                             end_idx = min(len(site_df), idx_loc + 2)
                            
#                             temp_points = site_df.iloc[start_idx:end_idx]['soil_temp_standardized'...
#                             time_points = [(t - before_row['datetime']).total_seconds() / 3600 
#                                          for t in site_df.iloc[start_idx:end_idx]['datetime']]
                            
#                             interp_times = [(t - before_row['datetime']).total_seconds() / 3600 
#                                           for t in timestamps]
                            
#                             # Perform cubic interpolation if enough points
#                             if len(time_points) >= 4:
#                                 interp_func = interp1d(time_points, temp_points, 
#                                                      kind='cubic', bounds_error=False)
#                                 temp_values = interp_func(interp_times)
#                             else:
#                                 # Fallback to linear
#                                 temp_start = before_row['soil_temp_standardized']
#                                 temp_end = after_row['soil_temp_standardized']
#                                 temp_values = np.linspace(temp_start, temp_end, n_intervals + 2)[1...
#                         except:
#                             # Fallback to linear if cubic fails
#                             temp_start = before_row['soil_temp_standardized']
#                             temp_end = after_row['soil_temp_standardized']
#                             temp_values = np.linspace(temp_start, temp_end, n_intervals + 2)[1:-1]
                    
#                     # CRITICAL FIX: ALWAYS interpolate moisture data when available (not condition...
#                     moist_values = None
#                     if has_moisture:
#                         moist_start = before_row.get('soil_moist_standardized', np.nan)
#                         moist_end = after_row.get('soil_moist_standardized', np.nan)
                        
#                         if not pd.isna(moist_start) and not pd.isna(moist_end):
#                             moist_values = np.linspace(moist_start, moist_end, n_intervals + 2)[1:...
#                         elif not pd.isna(moist_start):
#                             moist_values = np.full(len(timestamps), moist_start)
#                         elif not pd.isna(moist_end):
#                             moist_values = np.full(len(timestamps), moist_end)
                    
#                     # Create new rows
#                     for j, timestamp in enumerate(timestamps):
#                         new_row = before_row.copy()
#                         new_row['datetime'] = timestamp
#                         new_row['soil_temp_standardized'] = temp_values[j]
#                         new_row['interpolated'] = True
                        
#                         if has_moisture and moist_values is not None:
#                             new_row['soil_moist_standardized'] = moist_values[j]
                        
#                         interp_rows.append(new_row)
#             except Exception as e:
#                 if verbose:
#                     print(f"  Error during interpolation at idx {idx}: {str(e)}")
#                 continue
        
#         # Add interpolated rows
#         if interp_rows:
#             interp_df = pd.DataFrame(interp_rows)
#             site_df = pd.concat([site_df, interp_df], ignore_index=True)
#             site_df = site_df.sort_values('datetime')
            
#             # Clean up
#             del interp_df, interp_rows
#             gc.collect()
    
#     # Calculate temperature gradient
#     site_df['temp_gradient'] = site_df['soil_temp_standardized'].diff() / \
#                              (site_df['datetime'].diff().dt.total_seconds() / 3600)
    
#     # CRITICAL FIX: Always calculate moisture gradient when moisture data exists
#     if has_moisture:
#         site_df['moist_gradient'] = site_df['soil_moist_standardized'].diff() / \
#                                    (site_df['datetime'].diff().dt.total_seconds() / 3600)
    
#     # Literature-based detection criteria
#     mask_temp = (site_df['soil_temp_standardized'].abs() <= 0.5)  # Temperature near freezing
#     mask_gradient = (site_df['temp_gradient'].abs() <= 0.02)  # Stable temperature
    
#     # CRITICAL FIX: Properly integrate moisture in zero curtain detection
#     if has_moisture:
#         # Use moisture gradient for phase change detection
#         mask_moisture = (site_df['moist_gradient'].abs() >= 0.0005)  # Moisture changing during ph...
        
#         # Combined detection criteria
#         combined_mask = mask_temp & (mask_gradient | mask_moisture)
#     else:
#         # Use only temperature if no moisture data
#         combined_mask = mask_temp & mask_gradient
    
#     # Find continuous events
#     site_df['zero_curtain_flag'] = combined_mask
#     site_df['event_start'] = combined_mask & ~combined_mask.shift(1, fill_value=False)
#     site_df['event_end'] = combined_mask & ~combined_mask.shift(-1, fill_value=False)
    
#     # Get event starts and ends
#     event_starts = site_df[site_df['event_start']]['datetime'].tolist()
#     event_ends = site_df[site_df['event_end']]['datetime'].tolist()
    
#     if len(event_starts) == 0 or len(event_ends) == 0:
#         return []
    
#     # Handle mismatched starts/ends
#     if len(event_starts) > len(event_ends):
#         event_starts = event_starts[:len(event_ends)]
#     elif len(event_ends) > len(event_starts):
#         event_ends = event_ends[:len(event_starts)]
    
#     # Process each event
#     for start, end in zip(event_starts, event_ends):
#         event_duration = (end - start).total_seconds() / 3600
        
#         # Literature-based minimum duration (24 hours)
#         if event_duration < 24:
#             continue
        
#         # Get event data
#         event_data = site_df[(site_df['datetime'] >= start) & (site_df['datetime'] <= end)]
        
#         if len(event_data) < 5:
#             continue
        
#         # Extract event info
#         try:
#             event_info = extract_event_info(event_data, site, temp_depth, start, end, 
#                                            event_duration, has_moisture)
#             site_events.append(event_info)
#         except Exception as e:
#             print(f"  Error extracting event info: {str(e)}")
    
#     # Clean up to free memory
#     del site_df
#     gc.collect()
    
#     return site_events
    

# # Fix 2: Properly extract moisture metrics in event_info extraction
# def extract_event_info(event_data, site, temp_depth, start, end, event_duration, has_moisture):
#     """Extract comprehensive event info with proper moisture integration"""
#     # Basic event info
#     event_info = {
#         'source': site,
#         'soil_temp_depth': temp_depth,
#         'soil_temp_depth_zone': event_data['soil_temp_depth_zone'].iloc[0] if 'soil_temp_depth_zon...
#         'datetime_min': start,
#         'datetime_max': end,
#         'duration_hours': event_duration,
#         'observation_count': len(event_data),
#         'observations_per_day': len(event_data) / (event_duration / 24) if event_duration > 0 else...
#         'soil_temp_mean': event_data['soil_temp_standardized'].mean(),
#         'soil_temp_min': event_data['soil_temp_standardized'].min(),
#         'soil_temp_max': event_data['soil_temp_standardized'].max(),
#         'soil_temp_std': event_data['soil_temp_standardized'].std(),
#         'season': event_data['season'].iloc[0] if 'season' in event_data.columns else None,
#         'latitude': event_data['latitude'].iloc[0] if 'latitude' in event_data.columns else None,
#         'longitude': event_data['longitude'].iloc[0] if 'longitude' in event_data.columns else Non...
#         'year': event_data['year'].iloc[0] if 'year' in event_data.columns else None,
#         'month': start.month
#     }
    
#     # CRITICAL FIX: Always include moisture fields to avoid suppression
#     # This ensures consistent data structure even when moisture data isn't available
    
#     # Add moisture metrics when available
#     if has_moisture and not event_data['soil_moist_standardized'].isna().all():
#         event_info['soil_moist_mean'] = event_data['soil_moist_standardized'].mean()
#         event_info['soil_moist_std'] = event_data['soil_moist_standardized'].std()
#         event_info['soil_moist_min'] = event_data['soil_moist_standardized'].min()
#         event_info['soil_moist_max'] = event_data['soil_moist_standardized'].max()
#         event_info['soil_moist_change'] = event_data['soil_moist_standardized'].max() - event_data...
        
#         # Moisture depth information
#         if 'closest_moist_depth' in event_data.columns and not event_data['closest_moist_depth'].i...
#             event_info['soil_moist_depth'] = event_data['closest_moist_depth'].iloc[0]
#         elif 'soil_moist_depth' in event_data.columns and not event_data['soil_moist_depth'].isna(...
#             event_info['soil_moist_depth'] = event_data['soil_moist_depth'].iloc[0]
#         else:
#             event_info['soil_moist_depth'] = np.nan
            
#         # Add moisture gradient metrics
#         if 'moist_gradient' in event_data.columns:
#             event_info['soil_moist_gradient_mean'] = event_data['moist_gradient'].mean()
#             event_info['soil_moist_gradient_max'] = event_data['moist_gradient'].abs().max()
#     else:
#         # Set empty moisture values BUT PRESERVE THE COLUMNS
#         # This is critical - we don't want to suppress the moisture data structure
#         event_info['soil_moist_mean'] = np.nan
#         event_info['soil_moist_std'] = np.nan
#         event_info['soil_moist_min'] = np.nan
#         event_info['soil_moist_max'] = np.nan
#         event_info['soil_moist_change'] = np.nan
#         event_info['soil_moist_depth'] = np.nan
#         event_info['soil_moist_gradient_mean'] = np.nan
#         event_info['soil_moist_gradient_max'] = np.nan
    
#     # Temperature gradient info
#     if 'temp_gradient' in event_data.columns and not event_data['temp_gradient'].isna().all():
#         event_info['temp_gradient_mean'] = event_data['temp_gradient'].mean()
#         event_info['temp_stability'] = event_data['temp_gradient'].abs().mean()
    
#     # Add year-month
#     event_info['year_month'] = f"{event_info['year']}-{event_info['month']:02d}"
    
#     # Add region classification
#     if 'latitude' in event_info and event_info['latitude'] is not None:
#         lat = event_info['latitude']
#         if lat >= 66.5:
#             event_info['region'] = 'Arctic'
#         elif lat >= 60:
#             event_info['region'] = 'Subarctic'
#         elif lat >= 49:
#             event_info['region'] = 'Boreal'
#         else:
#             event_info['region'] = 'Other'
            
#         # Add latitude band
#         if lat < 49:
#             event_info['lat_band'] = '<49°N'
#         elif lat < 55:
#             event_info['lat_band'] = '<49-55°N (Boreal)'
#         elif lat < 60:
#             event_info['lat_band'] = '55-60°N (Boreal)'
#         elif lat < 66.5:
#             event_info['lat_band'] = '60-66.5°N (Sub_Arctic)'
#         elif lat < 70:
#             event_info['lat_band'] = '66.5-70°N (Arctic)'
#         elif lat < 75:
#             event_info['lat_band'] = '70-75°N (Arctic)'
#         elif lat < 80:
#             event_info['lat_band'] = '75-80°N (Arctic)'
#         else:
#             event_info['lat_band'] = '>80°N (Arctic)'
#     else:
#         event_info['region'] = None
#         event_info['lat_band'] = None
    
#     return event_info

# # Fix 3: Function to prepare ML features with moisture preservation
# def prepare_ml_features(events_df, output_dir=None):
#     """
#     Prepare features and labels for ML ensuring soil moisture features are preserved
#     """
#     print("Preparing ML features with moisture data integration")
    
#     # Ensure datetime columns are properly formatted
#     if isinstance(events_df, str):
#         events_df = pd.read_csv(events_df)
#         # Convert datetime columns to datetime
#         if 'datetime_min' in events_df.columns:
#             events_df['datetime_min'] = pd.to_datetime(events_df['datetime_min'])
#         if 'datetime_max' in events_df.columns:
#             events_df['datetime_max'] = pd.to_datetime(events_df['datetime_max'])
    
#     # Get all moisture columns - this is critical for not suppressing data
#     moisture_cols = [col for col in events_df.columns if 'moist' in col]
#     print(f"Found {len(moisture_cols)} moisture-related columns: {moisture_cols}")
    
#     # Create feature columns - KEEPING ALL MOISTURE COLUMNS
#     feature_cols = []
    
#     # Add all standard columns except non-numeric ones
#     exclude_cols = ['datetime_min', 'datetime_max', 'source', 'year_month']
#     for col in events_df.columns:
#         if col not in exclude_cols and col != 'duration_hours':  # duration is our target
#             feature_cols.append(col)
    
#     # Create a training dataset
#     X = events_df[feature_cols].copy()
#     y = events_df['duration_hours'].copy()
    
#     # Create spatial stratification
#     X['spatial_group'] = 'other'
#     if 'region' in X.columns and 'soil_temp_depth_zone' in X.columns:
#         X['spatial_group'] = X['region'] + '_' + X['soil_temp_depth_zone'].astype(str)
#     elif 'source' in events_df.columns:
#         X['spatial_group'] = events_df['source']
    
#     # Create train/val/test split that respects spatial dependencies
#     from sklearn.model_selection import train_test_split
    
#     # Split off test set first
#     train_val_idx, test_idx = train_test_split(
#         range(len(X)), test_size=0.2, 
#         stratify=X['spatial_group'], random_state=42
#     )
    
#     # Then split train/val
#     train_idx, val_idx = train_test_split(
#         train_val_idx, test_size=0.25,  # 25% of 80% = 20% of total
#         stratify=X.iloc[train_val_idx]['spatial_group'], random_state=42
#     )
    
#     # Create split datasets
#     X_train = X.iloc[train_idx].drop('spatial_group', axis=1)
#     X_val = X.iloc[val_idx].drop('spatial_group', axis=1)
#     X_test = X.iloc[test_idx].drop('spatial_group', axis=1)
    
#     y_train = y.iloc[train_idx]
#     y_val = y.iloc[val_idx]
#     y_test = y.iloc[test_idx]
    
#     # Report on moisture features
#     print("\nMoisture feature statistics:")
#     for col in moisture_cols:
#         if col in X_train.columns:
#             non_null = X_train[col].notna().sum()
#             pct = non_null / len(X_train) * 100
#             print(f"  {col}: {non_null}/{len(X_train)} non-null values ({pct:.1f}%)")
    
#     # Save datasets if output directory provided
#     if output_dir:
#         os.makedirs(output_dir, exist_ok=True)
        
#         # Save the features
#         X_train.to_csv(os.path.join(output_dir, 'X_train.csv'), index=False)
#         X_val.to_csv(os.path.join(output_dir, 'X_val.csv'), index=False)
#         X_test.to_csv(os.path.join(output_dir, 'X_test.csv'), index=False)
        
#         # Save the targets
#         y_train.to_csv(os.path.join(output_dir, 'y_train.csv'), index=False)
#         y_val.to_csv(os.path.join(output_dir, 'y_val.csv'), index=False)
#         y_test.to_csv(os.path.join(output_dir, 'y_test.csv'), index=False)
        
#         # Save complete dataset
#         full_dataset = events_df.copy()
#         full_dataset['split'] = 'train'
#         full_dataset.iloc[val_idx, full_dataset.columns.get_loc('split')] = 'val'
#         full_dataset.iloc[test_idx, full_dataset.columns.get_loc('split')] = 'test'
#         full_dataset.to_csv(os.path.join(output_dir, 'full_dataset.csv'), index=False)
        
#         print(f"Saved ML datasets to {output_dir}")
    
#     return {
#         'X_train': X_train,
#         'X_val': X_val,
#         'X_test': X_test,
#         'y_train': y_train,
#         'y_val': y_val,
#         'y_test': y_test,
#         'feature_cols': feature_cols,
#         'moisture_cols': moisture_cols
#     }

# def run_memory_efficient_pipeline_fixed(feather_path, output_dir=None, 
#                                   site_batch_size=10, checkpoint_interval=5, 
#                                   max_gap_hours=8, interpolation_method='cubic', 
#                                   force_restart=False, include_moisture=True,
#                                   verbose=False):  # Keep verbose but don't use it
#     """
#     Run the complete memory-efficient zero curtain detection pipeline
#     with soil moisture integration
    
#     Parameters:
#     -----------
#     feather_path : str
#         Path to the feather file
#     output_dir : str
#         Directory to save results and checkpoints
#     site_batch_size : int
#         Number of sites to process in each batch (smaller = less memory)
#     checkpoint_interval : int
#         Number of sites between saving checkpoints
#     max_gap_hours : float
#         Maximum gap hours for interpolation (literature suggests 8 hours max)
#     interpolation_method : str
#         Interpolation method ('linear', 'cubic')
#     force_restart : bool
#         Whether to force restart from scratch
#     include_moisture : bool
#         Whether to include soil moisture data in the analysis
#     verbose : bool
#         Whether to print detailed progress (not used in subfunctions)
        
#     Returns:
#     --------
#     pandas.DataFrame
#         DataFrame with detected zero curtain events
#     """
#     print("=" * 80)
#     print("MEMORY-EFFICIENT ZERO CURTAIN DETECTION WITH MOISTURE INTEGRATION")
#     print("=" * 80)
#     print(f"Initial memory usage: {memory_usage():.1f} MB")
    
#     # Define a more aggressive cleanup function that stays in this function's scope
#     def aggressive_cleanup():
#         """Aggressively clean up memory"""
#         # Call garbage collection multiple times
#         for _ in range(3):
#             gc.collect()
    
#     # Set up directories
#     if output_dir is not None:
#         os.makedirs(output_dir, exist_ok=True)
#         checkpoint_dir = os.path.join(output_dir, 'checkpoints')
#         os.makedirs(checkpoint_dir, exist_ok=True)
#     else:
#         checkpoint_dir = None
    
#     # Start timing
#     start_time = time.time()

#     def save_checkpoint(data, name):
#         if checkpoint_dir:
#             backup_path = os.path.join(checkpoint_dir, f'{name}_backup.pkl')
#             target_path = os.path.join(checkpoint_dir, f'{name}.pkl')
            
#             # First save to backup file
#             with open(backup_path, 'wb') as f:
#                 pickle.dump(data, f)
            
#             # Then rename to target (atomic operation)
#             import shutil
#             shutil.move(backup_path, target_path)
            
#             print(f"Saved checkpoint to {target_path}")
            
#             # Force memory cleanup after saving checkpoints
#             aggressive_cleanup()
    
#     # Function to load checkpoint
#     def load_checkpoint(name):
#         if checkpoint_dir:
#             try:
#                 with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'rb') as f:
#                     data = pickle.load(f)
#                 print(f"Loaded checkpoint from {name}.pkl")
#                 return data
#             except:
#                 print(f"No checkpoint found for {name}.pkl")
#         return None
    
#     # Step 1: Get site-depth combinations
#     site_depths = None
#     if not force_restart:
#         site_depths = load_checkpoint('site_depths')
    
#     if site_depths is None:
#         print("\nStep 1: Finding unique site-depth combinations...")
#         # Make sure to pass include_moisture parameter here
#         site_depths = get_unique_site_depths(feather_path, include_moisture=include_moisture)
#         save_checkpoint(site_depths, 'site_depths')
    
#     # Force cleanup after loading site data
#     aggressive_cleanup()
    
#     total_combinations = len(site_depths)
#     print(f"Found {total_combinations} unique site-depth combinations")
    
#     # Report on moisture capabilities if column exists
#     if 'has_moisture_data' in site_depths.columns:
#         moisture_sites = site_depths['has_moisture_data'].sum()
#         print(f"  {moisture_sites} sites ({moisture_sites/total_combinations*100:.1f}%) have moist...
    
#     # Step 2: Initialize results
#     all_events = []
#     processed_indices = set()
    
#     if not force_restart:
#         saved_events = load_checkpoint('all_events')
#         if saved_events is not None:
#             if isinstance(saved_events, list):
#                 all_events = saved_events
#             else:
#                 # If it's a DataFrame, convert to list of dicts
#                 all_events = saved_events.to_dict('records') if len(saved_events) > 0 else []
        
#         saved_indices = load_checkpoint('processed_indices')
#         if saved_indices is not None:
#             processed_indices = set(saved_indices)
    
#     print(f"Starting from {len(processed_indices)}/{total_combinations} processed sites")
#     print(f"Current event count: {len(all_events)}")

#     # Step 3: Process in batches
#     print("\nProcessing site-depth combinations in batches")
    
#     # For tracking new events in this run
#     new_events_count = 0
    
#     # Create batches for processing
#     total_batches = (total_combinations + site_batch_size - 1) // site_batch_size
    
#     for batch_idx in range(total_batches):
#         batch_start = batch_idx * site_batch_size
#         batch_end = min(batch_start + site_batch_size, total_combinations)
        
#         # Skip if already processed
#         batch_indices = set(range(batch_start, batch_end))
#         if batch_indices.issubset(processed_indices):
#             print(f"Batch {batch_idx+1}/{total_batches} (sites {batch_start+1}-{batch_end}/{total_...
#             continue
        
#         print(f"\nProcessing batch {batch_idx+1}/{total_batches} (sites {batch_start+1}-{batch_end...
#         print(f"Memory before batch: {memory_usage():.1f} MB")
        
#         # Force garbage collection before each batch
#         aggressive_cleanup()
        
#         # Process each site in batch
#         for site_idx in range(batch_start, batch_end):
#             # Skip if already processed
#             if site_idx in processed_indices:
#                 continue
            
#             site = site_depths.iloc[site_idx]['source']
#             temp_depth = site_depths.iloc[site_idx]['soil_temp_depth']
            
#             # Only print detailed progress every 10 sites or if verbose mode is on
#             if verbose or (site_idx % 10 == 0):
#                 print(f"\nProcessing site {site_idx+1}/{total_combinations}: {site}, depth: {temp_...
            
#             try:
#                 # Check if site has moisture capabilities
#                 has_moisture = False
#                 if 'has_moisture_data' in site_depths.columns:
#                     has_moisture = site_depths.iloc[site_idx]['has_moisture_data']
                
#                 # Load site data efficiently - with soil moisture data if available
#                 # IMPORTANT: Don't pass verbose parameter here!
#                 site_df = load_site_depth_data(
#                     feather_path=feather_path, 
#                     site=site, 
#                     temp_depth=temp_depth, 
#                     include_moisture=(include_moisture and has_moisture)
#                 )
                
#                 # Skip if insufficient data
#                 if len(site_df) < 24:  # Require at least 24 observations (minimum 1 day)
#                     if verbose:
#                         print(f"  Insufficient data ({len(site_df)} rows), skipping")
#                     processed_indices.add(site_idx)
#                     continue
                
#                 # Process for zero curtain
#                 # IMPORTANT: Don't pass verbose parameter here!
#                 site_events = process_site_for_zero_curtain(
#                     site_df=site_df, 
#                     site=site, 
#                     temp_depth=temp_depth, 
#                     max_gap_hours=max_gap_hours, 
#                     interpolation_method=interpolation_method
#                 )
                
#                 # Add to all events
#                 new_events = len(site_events)
#                 all_events.extend(site_events)
#                 new_events_count += new_events
                
#                 if new_events > 0 or verbose:
#                     print(f"  Site {site_idx+1}: Found {new_events} events, total: {len(all_events...
                
#                 # Mark as processed
#                 processed_indices.add(site_idx)
                
#                 # Save checkpoint periodically
#                 if site_idx % checkpoint_interval == 0:
#                     save_checkpoint(all_events, 'all_events')
#                     save_checkpoint(list(processed_indices), 'processed_indices')
#                     if verbose:
#                         print(f"Memory after checkpoint: {memory_usage():.1f} MB")
                
#                 # Clean up after each site to prevent memory buildup
#                 del site_df, site_events
#                 aggressive_cleanup()

#             except Exception as e:
#                 print(f"  Error processing site {site_idx+1} ({site}, depth {temp_depth}): {str(e)...
#                 # Continue with next site even after error
#                 continue
        
#         # Save checkpoint after each batch
#         print(f"Saving checkpoint after batch {batch_idx+1}/{total_batches}")
#         save_checkpoint(all_events, 'all_events')
#         save_checkpoint(list(processed_indices), 'processed_indices')
        
#         # Also save intermediate results as CSV
#         if len(all_events) > 0:
#             interim_df = pd.DataFrame(all_events)
#             if output_dir is not None:
#                 interim_path = os.path.join(output_dir, 'events_checkpoint.csv')
#                 interim_df.to_csv(interim_path, index=False)
#                 print(f"Saved interim results to {interim_path}")
            
#             # Clean up the interim DataFrame immediately
#             del interim_df
#             aggressive_cleanup()
        
#         print(f"Memory after batch: {memory_usage():.1f} MB")
    
#     # Step 4: Create final dataframe
#     print("\nCreating final events dataframe")
    
#     # Force cleanup before creating final dataframe
#     aggressive_cleanup()
    
#     if len(all_events) > 0:
#         events_df = pd.DataFrame(all_events)
#         print(f"Created events dataframe with {len(events_df)} total events ({new_events_count} ne...
        
#         # Report on moisture data
#         if 'soil_moist_mean' in events_df.columns:
#             moisture_present = events_df['soil_moist_mean'].notna().sum()
#             print(f"  {moisture_present} events ({moisture_present/len(events_df)*100:.1f}%) have ...
#     else:
#         # Create empty dataframe with correct columns
#         events_df = pd.DataFrame(columns=[
#             'source', 'soil_temp_depth', 'soil_temp_depth_zone', 
#             'datetime_min', 'datetime_max', 'duration_hours',
#             'observation_count', 'observations_per_day',
#             'soil_temp_mean', 'soil_temp_min', 'soil_temp_max', 'soil_temp_std',
#             'season', 'latitude', 'longitude', 'year', 'month',
#             'soil_moist_mean', 'soil_moist_std', 'soil_moist_min', 'soil_moist_max', 
#             'soil_moist_change', 'soil_moist_depth', 
#             'soil_moist_gradient_mean', 'soil_moist_gradient_max',
#             'temp_gradient_mean', 'temp_stability',
#             'year_month', 'region', 'lat_band'
#         ])
#         print("No events found")
    
#     # Save final results
#     if output_dir is not None:
#         final_path = os.path.join(output_dir, 'zero_curtain_events.csv')

#         temp_path = os.path.join(output_dir, 'zero_curtain_events_temp.csv')
#         events_df.to_csv(temp_path, index=False)

#         import shutil
#         shutil.move(temp_path, final_path)
        
#         print(f"Saved final results to {final_path}")
    
#     # Report timing
#     total_time = time.time() - start_time
#     print(f"\nPipeline completed in {total_time:.1f} seconds ({total_time/60:.1f} minutes)")
#     print(f"Final memory usage: {memory_usage():.1f} MB")
    
#     return events_df

# # Modified load_site_depth_data function to handle moisture data properly
# def load_site_depth_data(feather_path, site, temp_depth, include_moisture=True):
#     """Load ONLY data for a specific site and depth with moisture data when available"""
#     print(f"Loading data for site: {site}, depth: {temp_depth}")
#     print(f"Memory before loading: {memory_usage():.1f} MB")
    
#     try:
#         # Use PyArrow Dataset API for efficient filtering
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Create filter expressions
#         site_filter = ds.field('source') == site
#         depth_filter = ds.field('soil_temp_depth') == float(temp_depth)
#         combined_filter = site_filter & depth_filter
        
#         # Apply filter and read only filtered data
#         table = dataset.to_table(filter=combined_filter)
#         filtered_df = table.to_pandas()
        
#         # Get soil moisture data if available and requested
#         if include_moisture and len(filtered_df) > 0:
#             # Check if we already have moisture in the filtered data
#             has_moisture = (
#                 'soil_moist_standardized' in filtered_df.columns and 
#                 not filtered_df['soil_moist_standardized'].isna().all()
#             )
            
#             if not has_moisture:
#                 # Try to get moisture data separately
#                 try:
#                     # First check if moisture data exists for this site
#                     moisture_filter = site_filter
                    
#                     # Sample to check for moisture data
#                     moisture_sample = dataset.to_table(
#                         filter=moisture_filter,
#                         columns=['soil_moist_depth', 'soil_moist_standardized'],
#                         limit=100
#                     ).to_pandas()
                    
#                     if (
#                         'soil_moist_standardized' in moisture_sample.columns and 
#                         not moisture_sample['soil_moist_standardized'].isna().all() and
#                         'soil_moist_depth' in moisture_sample.columns and
#                         not moisture_sample['soil_moist_depth'].isna().all()
#                     ):
#                         # Find closest moisture depth to temperature depth
#                         moist_depths = moisture_sample['soil_moist_depth'].dropna().unique()
#                         if len(moist_depths) > 0:
#                             # Get closest depth
#                             closest_depth = moist_depths[np.abs(moist_depths - temp_depth).argmin(...
#                             print(f"  Found closest moisture depth: {closest_depth}")
                            
#                             # Get moisture data at this depth
#                             moisture_filter = site_filter & (ds.field('soil_moist_depth') == float...
#                             moisture_table = dataset.to_table(
#                                 filter=moisture_filter,
#                                 columns=['datetime', 'soil_moist_standardized', 'soil_moist_depth'...
#                             )
#                             moisture_df = moisture_table.to_pandas()
                            
#                             if len(moisture_df) > 0:
#                                 # Merge with temperature data
#                                 filtered_df['datetime_key'] = filtered_df['datetime']
#                                 moisture_df['datetime_key'] = moisture_df['datetime']
                                
#                                 merged_df = pd.merge_asof(
#                                     filtered_df.sort_values('datetime_key'),
#                                     moisture_df[['datetime_key', 'soil_moist_standardized', 'soil_...
#                                     on='datetime_key',
#                                     direction='nearest',
#                                     tolerance=pd.Timedelta('6h')  # Match within 6 hours
#                                 )
                                
#                                 # Clean up merged data
#                                 merged_df.drop('datetime_key', axis=1, inplace=True)
#                                 filtered_df = merged_df
                                
#                                 # Add depth information
#                                 filtered_df['closest_moist_depth'] = closest_depth
                                
#                                 # Report on merge
#                                 valid_moisture = filtered_df['soil_moist_standardized'].notna().su...
#                                 print(f"  Added moisture data: {valid_moisture}/{len(filtered_df)}...
#                 except Exception as e:
#                     print(f"  Error adding moisture data: {str(e)}")
        
#         # Clean up
#         del table, dataset
#         gc.collect()
        
#     except Exception as e:
#         print(f"Error with PyArrow filtering: {str(e)}")
#         print("Falling back to pandas filtering...")
        
#         # Define chunk size based on available memory
#         available_mem = psutil.virtual_memory().available / (1024**2)  # MB
#         chunk_size = min(500000, max(100000, int(available_mem / 10)))
        
#         filtered_chunks = []
        
#         # Read and filter in chunks
#         for chunk in pd.read_feather(feather_path, columns=None, chunksize=chunk_size):
#             # Filter by site and depth
#             chunk_filtered = chunk[(chunk['source'] == site) & 
#                                    (chunk['soil_temp_depth'] == temp_depth)]
            
#             if len(chunk_filtered) > 0:
#                 filtered_chunks.append(chunk_filtered)
            
#             # Clean up
#             del chunk, chunk_filtered
#             gc.collect()
        
#         # Combine filtered chunks
#         if filtered_chunks:
#             filtered_df = pd.concat(filtered_chunks, ignore_index=True)
#             del filtered_chunks
#         else:
#             filtered_df = pd.DataFrame()
        
#         gc.collect()
    
#     # Ensure datetime is in datetime format
#     if 'datetime' in filtered_df.columns and not pd.api.types.is_datetime64_dtype(filtered_df['dat...
#         filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])
    
#     # Add depth zone classification
#     if 'soil_temp_depth' in filtered_df.columns:
#         filtered_df['soil_temp_depth_zone'] = pd.cut(
#             filtered_df['soil_temp_depth'],
#             bins=[-float('inf'), 5, 20, 50, 100, float('inf')],
#             labels=['surface', 'shallow', 'mid', 'deep', 'very_deep']
#         )
    
#     # Add year column if missing
#     if 'year' not in filtered_df.columns and 'datetime' in filtered_df.columns:
#         filtered_df['year'] = filtered_df['datetime'].dt.year
    
#     # Add season if missing
#     if 'season' not in filtered_df.columns and 'datetime' in filtered_df.columns:
#         # Define seasons - assume northern hemisphere
#         def get_season(month):
#             if month in [12, 1, 2]:
#                 return 'winter'
#             elif month in [3, 4, 5]:
#                 return 'spring'
#             elif month in [6, 7, 8]:
#                 return 'summer'
#             else:
#                 return 'fall'
        
#         filtered_df['season'] = filtered_df['datetime'].dt.month.apply(get_season)
    
#     print(f"Loaded {len(filtered_df)} rows for site-depth")
#     print(f"Memory after loading: {memory_usage():.1f} MB")
    
#     # Report moisture data
#     if include_moisture and 'soil_moist_standardized' in filtered_df.columns:
#         valid_moisture = filtered_df['soil_moist_standardized'].notna().sum()
#         pct = valid_moisture / len(filtered_df) * 100 if len(filtered_df) > 0 else 0
#         print(f"  Moisture data coverage: {valid_moisture}/{len(filtered_df)} ({pct:.1f}%)")
    
#     return filtered_df

# import os
# import pandas as pd
# import pickle
# import gc
# import numpy as np
# import pyarrow as pa
# import pyarrow.feather as pf
# import pyarrow.dataset as ds
# import psutil

# # Define critical utility function
# def memory_usage():
#     """Monitor memory usage in MB"""
#     process = psutil.Process(os.getpid())
#     mem_info = process.memory_info()
#     return mem_info.rss / 1024**2  # Memory in MB

# # 1. FIX: IMPROVED MOISTURE DATA DETECTION
# def detect_moisture_capability(feather_path):
#     """Enhanced function to detect sites with soil moisture capabilities"""
#     print("Performing enhanced moisture capability detection...")
    
#     try:
#         # Using PyArrow for efficient column scanning
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Sample data to check structure (limit to reasonable size)
#         table = dataset.to_table(columns=['source', 'soil_moist_depth', 'soil_moist_standardized']...
        
#         # Convert to pandas for analysis
#         df_sample = table.to_pandas()
        
#         # Check which sites have any non-null moisture data
#         sites_with_moisture = df_sample.dropna(subset=['soil_moist_standardized']).groupby('source...
#         sites_with_moisture.columns = ['source', 'moisture_count']
#         sites_with_moisture = sites_with_moisture[sites_with_moisture['moisture_count'] > 0]
        
#         # Create a dictionary for faster lookup
#         moisture_capable_sites = set(sites_with_moisture['source'].unique())
        
#         print(f"Found {len(moisture_capable_sites)} sites with moisture capability in sample")
#         return moisture_capable_sites
        
#     except Exception as e:
#         print(f"Error in enhanced moisture detection: {str(e)}")
#         return set()  # Return empty set if detection fails

# # 2. FIX: IMPROVED SITE DEPTH EXTRACTION WITH BETTER MOISTURE DETECTION
# def get_improved_site_depths(feather_path, moisture_capable_sites=None):
#     """Get unique site-depth combinations with reliable moisture data detection"""
#     print("Finding unique site-depth combinations with improved moisture detection")
    
#     try:
#         # Use PyArrow for efficient operation
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Read essential columns only
#         table = dataset.to_table(columns=['source', 'soil_temp_depth', 'latitude', 'longitude'])
        
#         # Convert to pandas
#         df = table.to_pandas()
        
#         # Get valid combinations (where temp_depth is not NaN)
#         valid_df = df.dropna(subset=['soil_temp_depth'])
        
#         # Get unique combinations
#         site_temps = valid_df.drop_duplicates(['source', 'soil_temp_depth'])
        
#         # Mark sites with moisture capabilities
#         if moisture_capable_sites:
#             site_temps['has_moisture_data'] = site_temps['source'].isin(moisture_capable_sites)
#         else:
#             site_temps['has_moisture_data'] = False
        
#         # Add latitude and longitude if available
#         if 'latitude' in df.columns and 'longitude' in df.columns:
#             # Get site coordinates
#             site_coords = df.drop_duplicates('source')[['source', 'latitude', 'longitude']]
#             # Merge with site_temps
#             site_temps = site_temps.merge(site_coords, on='source', how='left')
        
#         # Clean up
#         del table, df, valid_df
#         gc.collect()
        
#         moisture_count = site_temps['has_moisture_data'].sum()
#         print(f"Found {len(site_temps)} unique site-depth combinations")
#         print(f"  {moisture_count} site-depth combinations ({moisture_count/len(site_temps)*100:.1...
        
#         return site_temps
        
#     except Exception as e:
#         print(f"Error in improved site depth extraction: {str(e)}")
#         return pd.DataFrame(columns=['source', 'soil_temp_depth', 'has_moisture_data'])

# # 3. FIX: ENHANCED SITE DATA LOADING WITH BETTER MOISTURE INTEGRATION
# def load_site_depth_data_improved(feather_path, site, temp_depth, include_moisture=True, moisture_...
#     """Load data for specific site and depth with improved moisture handling"""
#     print(f"Loading data for site: {site}, depth: {temp_depth}")
    
#     # First determine if this site has moisture capability
#     has_moisture_capability = False
#     if moisture_capable_sites and site in moisture_capable_sites:
#         has_moisture_capability = True
#         print(f"  Site {site} has moisture capability")
    
#     try:
#         # Use PyArrow Dataset API for filtering
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Create filter expressions
#         site_filter = ds.field('source') == site
#         depth_filter = ds.field('soil_temp_depth') == float(temp_depth)
#         combined_filter = site_filter & depth_filter
        
#         # Determine columns to load
#         columns = ['source', 'soil_temp_depth', 'soil_temp_standardized', 'datetime', 
#                   'latitude', 'longitude', 'elevation']
        
#         # Always try to get moisture if the site is moisture-capable
#         if include_moisture and has_moisture_capability:
#             columns.extend(['soil_moist_depth', 'soil_moist_standardized'])
        
#         # Apply filter and read data
#         table = dataset.to_table(filter=combined_filter, columns=columns)
#         filtered_df = table.to_pandas()
        
#         # If we need moisture data and don't have it yet, try to fetch it separately
#         if include_moisture and has_moisture_capability and len(filtered_df) > 0:
#             # Check if we already have adequate moisture data
#             has_sufficient_moisture = (
#                 'soil_moist_standardized' in filtered_df.columns and 
#                 filtered_df['soil_moist_standardized'].notna().sum() > len(filtered_df) * 0.1  # A...
#             )
            
#             if not has_sufficient_moisture:
#                 # Get moisture data for this site at any depth
#                 moisture_filter = site_filter & ~ds.field('soil_moist_standardized').is_null()
                
#                 try:
#                     # Read moisture data
#                     moisture_table = dataset.to_table(
#                         filter=moisture_filter,
#                         columns=['datetime', 'soil_moist_depth', 'soil_moist_standardized']
#                     )
                    
#                     if moisture_table.num_rows > 0:
#                         moisture_df = moisture_table.to_pandas()
                        
#                         # Find unique moisture depths
#                         moist_depths = moisture_df['soil_moist_depth'].dropna().unique()
                        
#                         if len(moist_depths) > 0:
#                             # Find closest moisture depth to temperature depth
#                             closest_depth_idx = np.abs(moist_depths - temp_depth).argmin()
#                             closest_depth = moist_depths[closest_depth_idx]
                            
#                             print(f"  Found closest moisture depth: {closest_depth} (temp depth: {...
                            
#                             # Filter to just that depth
#                             moisture_depth_df = moisture_df[moisture_df['soil_moist_depth'] == clo...
                            
#                             if len(moisture_depth_df) > 0:
#                                 # Prepare for merge
#                                 if not pd.api.types.is_datetime64_dtype(filtered_df['datetime']):
#                                     filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime...
                                
#                                 if not pd.api.types.is_datetime64_dtype(moisture_depth_df['datetim...
#                                     moisture_depth_df['datetime'] = pd.to_datetime(moisture_depth_...
                                
#                                 # Sort both DataFrames by datetime
#                                 filtered_df = filtered_df.sort_values('datetime')
#                                 moisture_depth_df = moisture_depth_df.sort_values('datetime')
                                
#                                 # Merge with temperature data using merge_asof for nearest time ma...
#                                 merged_df = pd.merge_asof(
#                                     filtered_df,
#                                     moisture_depth_df[['datetime', 'soil_moist_standardized', 'soi...
#                                     on='datetime',
#                                     direction='nearest',
#                                     tolerance=pd.Timedelta('6h')  # Match within 6 hours
#                                 )
                                
#                                 # Use merged dataset and add depth info
#                                 filtered_df = merged_df
#                                 filtered_df['closest_moist_depth'] = closest_depth
                                
#                                 # Report moisture coverage
#                                 valid_moisture = filtered_df['soil_moist_standardized'].notna().su...
#                                 pct = valid_moisture / len(filtered_df) * 100
#                                 print(f"  Merged moisture data: {valid_moisture}/{len(filtered_df)...
#                 except Exception as e:
#                     print(f"  Error fetching moisture data: {str(e)}")
        
#         # Ensure datetime column is datetime type
#         if 'datetime' in filtered_df.columns and not pd.api.types.is_datetime64_dtype(filtered_df[...
#             filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])
        
#         # Add auxiliary columns
#         # Add depth zone
#         if 'soil_temp_depth' in filtered_df.columns:
#             filtered_df['soil_temp_depth_zone'] = pd.cut(
#                 filtered_df['soil_temp_depth'],
#                 bins=[-float('inf'), 5, 20, 50, 100, float('inf')],
#                 labels=['surface', 'shallow', 'mid', 'deep', 'very_deep']
#             )
        
#         # Add year
#         if 'datetime' in filtered_df.columns:
#             filtered_df['year'] = filtered_df['datetime'].dt.year
        
#         # Add season
#         if 'datetime' in filtered_df.columns:
#             def get_season(month):
#                 if month in [12, 1, 2]: return 'winter'
#                 elif month in [3, 4, 5]: return 'spring'
#                 elif month in [6, 7, 8]: return 'summer'
#                 else: return 'fall'
            
#             filtered_df['season'] = filtered_df['datetime'].dt.month.apply(get_season)
        
#         print(f"Loaded {len(filtered_df)} rows for site-depth")
        
#         # Report on moisture data if present
#         if 'soil_moist_standardized' in filtered_df.columns:
#             valid_moisture = filtered_df['soil_moist_standardized'].notna().sum()
#             pct = valid_moisture / len(filtered_df) * 100 if len(filtered_df) > 0 else 0
#             print(f"  Final moisture data coverage: {valid_moisture}/{len(filtered_df)} rows ({pct...
        
#         return filtered_df
    
#     except Exception as e:
#         print(f"Error loading site data: {str(e)}")
#         return pd.DataFrame()  # Return empty DataFrame on error

# # 4. MAIN PIPELINE FIX
# def run_fixed_moisture_pipeline(feather_path, output_dir=None, 
#                               site_batch_size=10, checkpoint_interval=5,
#                               max_gap_hours=8, interpolation_method='cubic',
#                               force_restart=False):
#     """
#     Improved pipeline with reliable moisture data integration
#     """
#     print("=" * 80)
#     print("IMPROVED ZERO CURTAIN DETECTION WITH RELIABLE MOISTURE INTEGRATION")
#     print("=" * 80)
#     print(f"Initial memory usage: {memory_usage():.1f} MB")
    
#     # Set up directories
#     if output_dir is not None:
#         os.makedirs(output_dir, exist_ok=True)
#         checkpoint_dir = os.path.join(output_dir, 'checkpoints')
#         os.makedirs(checkpoint_dir, exist_ok=True)
#     else:
#         checkpoint_dir = None
    
#     # Define checkpoint functions
#     def save_checkpoint(data, name):
#         if checkpoint_dir:
#             backup_path = os.path.join(checkpoint_dir, f'{name}_backup.pkl')
#             target_path = os.path.join(checkpoint_dir, f'{name}.pkl')
            
#             # First save to backup file
#             with open(backup_path, 'wb') as f:
#                 pickle.dump(data, f)
            
#             # Then rename to target (atomic operation)
#             import shutil
#             shutil.move(backup_path, target_path)
            
#             print(f"Saved checkpoint to {target_path}")
            
#             # Force memory cleanup after saving checkpoints
#             for _ in range(3):
#                 gc.collect()
    
#     def load_checkpoint(name):
#         if checkpoint_dir:
#             try:
#                 with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'rb') as f:
#                     data = pickle.load(f)
#                 print(f"Loaded checkpoint from {name}.pkl")
#                 return data
#             except:
#                 print(f"No checkpoint found for {name}.pkl")
#         return None
    
#     # Step 1: Enhanced moisture capability detection
#     moisture_capable_sites = None
#     if not force_restart:
#         moisture_capable_sites = load_checkpoint('moisture_capable_sites')
    
#     if moisture_capable_sites is None:
#         print("\nStep 1: Performing enhanced moisture detection...")
#         moisture_capable_sites = detect_moisture_capability(feather_path)
#         if moisture_capable_sites:
#             save_checkpoint(moisture_capable_sites, 'moisture_capable_sites')
    
#     if moisture_capable_sites:
#         print(f"Working with {len(moisture_capable_sites)} moisture-capable sites")
#     else:
#         moisture_capable_sites = set()
#         print("No moisture-capable sites detected")
    
#     # Step 2: Get site-depth combinations with improved moisture detection
#     site_depths = None
#     if not force_restart:
#         site_depths = load_checkpoint('site_depths')
    
#     if site_depths is None:
#         print("\nStep 2: Finding unique site-depth combinations...")
#         site_depths = get_improved_site_depths(feather_path, moisture_capable_sites)
#         save_checkpoint(site_depths, 'site_depths')
    
#     # Force cleanup after loading site data
#     for _ in range(3):
#         gc.collect()
    
#     total_combinations = len(site_depths)
#     print(f"Found {total_combinations} unique site-depth combinations")
    
#     # Report on moisture capabilities
#     if 'has_moisture_data' in site_depths.columns:
#         moisture_sites = site_depths['has_moisture_data'].sum()
#         print(f"  {moisture_sites} sites ({moisture_sites/total_combinations*100:.1f}%) have moist...
    
#     # Step 3: Initialize or load event tracking
#     all_events = []
#     processed_indices = set()
    
#     if not force_restart:
#         saved_events = load_checkpoint('all_events')
#         if saved_events is not None:
#             if isinstance(saved_events, list):
#                 all_events = saved_events
#             else:
#                 # If it's a DataFrame, convert to list of dicts
#                 all_events = saved_events.to_dict('records') if len(saved_events) > 0 else []
        
#         saved_indices = load_checkpoint('processed_indices')
#         if saved_indices is not None:
#             processed_indices = set(saved_indices)
    
#     print(f"Starting from {len(processed_indices)}/{total_combinations} processed sites")
#     print(f"Current event count: {len(all_events)}")
    
#     # Step 4: Process in batches with improved data loading
#     print("\nProcessing site-depth combinations in batches")
    
#     # For tracking new events in this run
#     new_events_count = 0
    
#     # Create batches for processing
#     total_batches = (total_combinations + site_batch_size - 1) // site_batch_size
    
#     # Process 10 batches for testing
#     for batch_idx in range(min(10, total_batches)):
#         batch_start = batch_idx * site_batch_size
#         batch_end = min(batch_start + site_batch_size, total_combinations)
        
#         # Skip if already processed
#         batch_indices = set(range(batch_start, batch_end))
#         if batch_indices.issubset(processed_indices):
#             print(f"Batch {batch_idx+1}/{total_batches} already processed, skipping")
#             continue
        
#         print(f"\nProcessing batch {batch_idx+1}/{total_batches} (sites {batch_start+1}-{batch_end...
#         print(f"Memory before batch: {memory_usage():.1f} MB")
        
#         # Force garbage collection before each batch
#         for _ in range(3):
#             gc.collect()
        
#         # Process each site in batch
#         for site_idx in range(batch_start, batch_end):
#             # Skip if already processed
#             if site_idx in processed_indices:
#                 continue
            
#             site = site_depths.iloc[site_idx]['source']
#             temp_depth = site_depths.iloc[site_idx]['soil_temp_depth']
            
#             print(f"\nProcessing site {site_idx+1}/{total_combinations}: {site}, depth: {temp_dept...
            
#             # Check if site has moisture capabilities
#             has_moisture = False
#             if 'has_moisture_data' in site_depths.columns:
#                 has_moisture = site_depths.iloc[site_idx]['has_moisture_data']
            
#             print(f"Site has moisture capability: {has_moisture}")
            
#             # Load site data with improved moisture handling
#             site_df = load_site_depth_data_improved(
#                 feather_path=feather_path, 
#                 site=site, 
#                 temp_depth=temp_depth, 
#                 include_moisture=has_moisture,
#                 moisture_capable_sites=moisture_capable_sites
#             )
            
#             # Add to processed sites
#             processed_indices.add(site_idx)
            
#             # Save simple event summary for demonstration
#             if len(site_df) > 0 and 'soil_moist_standardized' in site_df.columns:
#                 moist_avail = site_df['soil_moist_standardized'].notna().sum()
#                 if moist_avail > 0:
#                     # Create a sample event
#                     event = {
#                         'source': site,
#                         'soil_temp_depth': temp_depth,
#                         'datetime_min': site_df['datetime'].min(),
#                         'datetime_max': site_df['datetime'].max(),
#                         'duration_hours': (site_df['datetime'].max() - site_df['datetime'].min())....
#                         'soil_moist_mean': site_df['soil_moist_standardized'].mean(),
#                         'soil_moist_coverage': moist_avail / len(site_df) * 100,
#                         'moisture_demo': True  # Mark as demo event
#                     }
#                     all_events.append(event)
#                     new_events_count += 1
#                     print(f"  Created moisture data sample event, moisture coverage: {moist_avail}...
            
#             # Save checkpoints periodically
#             if site_idx % checkpoint_interval == 0:
#                 save_checkpoint(all_events, 'all_events')
#                 save_checkpoint(list(processed_indices), 'processed_indices')
#                 print(f"Memory after checkpoint: {memory_usage():.1f} MB")
            
#             # Clean up
#             del site_df
#             for _ in range(3):
#                 gc.collect()
        
#         # Save checkpoint after each batch
#         print(f"Saving checkpoint after batch {batch_idx+1}/{total_batches}")
#         save_checkpoint(all_events, 'all_events')
#         save_checkpoint(list(processed_indices), 'processed_indices')
        
#         # Also save intermediate results as CSV
#         if len(all_events) > 0:
#             interim_df = pd.DataFrame(all_events)
#             if output_dir is not None:
#                 interim_path = os.path.join(output_dir, 'moisture_events_demo.csv')
#                 interim_df.to_csv(interim_path, index=False)
#                 print(f"Saved interim results to {interim_path}")
            
#             # Clean up the interim DataFrame immediately
#             del interim_df
#             for _ in range(3):
#                 gc.collect()
        
#         print(f"Memory after batch: {memory_usage():.1f} MB")
    
#     # Final summary
#     print("\n" + "=" * 80)
#     print(f"Pipeline demonstration completed with {new_events_count} moisture data samples")
#     print(f"Final memory usage: {memory_usage():.1f} MB")
    
#     return pd.DataFrame(all_events) if all_events else pd.DataFrame()

# import os
# import pandas as pd
# import pickle
# import gc
# import numpy as np
# import pyarrow.feather as pf
# import psutil

# # Define critical utility function
# def memory_usage():
#     """Monitor memory usage in MB"""
#     process = psutil.Process(os.getpid())
#     mem_info = process.memory_info()
#     return mem_info.rss / 1024**2  # Memory in MB

# # 1. FIX: ROBUST MOISTURE DATA DETECTION - NO PYARROW LIMIT PARAMETER
# def detect_moisture_capability(feather_path):
#     """Robust moisture capability detection without PyArrow limit parameter"""
#     print("Performing robust moisture capability detection...")
    
#     try:
#         # Read just the necessary columns - no PyArrow limit parameter needed
#         print("Reading moisture data columns...")
#         df_sample = pd.read_feather(feather_path, columns=['source', 'soil_moist_standardized'])
        
#         # Check which sites have any non-null moisture data
#         moisture_sites = df_sample.dropna(subset=['soil_moist_standardized'])['source'].unique()
#         moisture_capable_sites = set(moisture_sites)
        
#         print(f"Found {len(moisture_capable_sites)} sites with moisture capability")
#         return moisture_capable_sites
        
#     except Exception as e:
#         print(f"Error in robust moisture detection: {str(e)}")
        
#         # Fallback: Try chunked reading if direct column access fails
#         try:
#             print("Trying chunked approach for moisture detection...")
#             # Process in smaller chunks to avoid memory issues
#             chunk_size = 500000
#             moisture_capable_sites = set()
            
#             # Read feather file in chunks
#             for chunk in pd.read_feather(feather_path, columns=['source', 'soil_moist_standardized...
#                 # Identify sites with non-null moisture data in this chunk
#                 chunk_moisture_sites = chunk.dropna(subset=['soil_moist_standardized'])['source']....
#                 moisture_capable_sites.update(chunk_moisture_sites)
                
#                 # Clean up to avoid memory issues
#                 del chunk
#                 gc.collect()
                
#             print(f"Found {len(moisture_capable_sites)} sites with moisture capability (chunked ap...
#             return moisture_capable_sites
            
#         except Exception as chunk_error:
#             print(f"Error in chunked moisture detection: {str(chunk_error)}")
#             return set()  # Return empty set if all detection methods fail

# # 2. FIX: SIMPLIFIED SITE DEPTH EXTRACTION WITHOUT USING EXTRA COLUMNS
# def get_simplified_site_depths(feather_path, moisture_capable_sites=None):
#     """Get unique site-depth combinations with simplified approach"""
#     print("Finding unique site-depth combinations with simplified detection")
    
#     try:
#         # Direct read of only necessary columns
#         print("Reading site-depth data...")
#         site_df = pd.read_feather(feather_path, columns=['source', 'soil_temp_depth'])
        
#         # Get valid combinations (where temp_depth is not NaN)
#         valid_df = site_df.dropna(subset=['soil_temp_depth'])
        
#         # Get unique combinations
#         site_temps = valid_df.drop_duplicates(['source', 'soil_temp_depth'])
        
#         # Mark sites with moisture capabilities
#         if moisture_capable_sites:
#             site_temps['has_moisture_data'] = site_temps['source'].isin(moisture_capable_sites)
#         else:
#             site_temps['has_moisture_data'] = False
        
#         # Clean up
#         del site_df, valid_df
#         gc.collect()
        
#         moisture_count = site_temps['has_moisture_data'].sum()
#         print(f"Found {len(site_temps)} unique site-depth combinations")
#         print(f"  {moisture_count} site-depth combinations ({moisture_count/len(site_temps)*100:.1...
        
#         return site_temps
        
#     except Exception as e:
#         print(f"Error in simplified site depth extraction: {str(e)}")
        
#         # Fallback: Try chunked reading approach
#         try:
#             print("Trying chunked approach for site-depth extraction...")
#             chunk_size = 500000
#             all_combinations = set()
            
#             # Read in chunks
#             for chunk in pd.read_feather(feather_path, columns=['source', 'soil_temp_depth'], chun...
#                 valid_rows = chunk.dropna(subset=['soil_temp_depth'])
#                 site_depth_tuples = list(zip(valid_rows['source'], valid_rows['soil_temp_depth']))
#                 all_combinations.update(site_depth_tuples)
                
#                 # Clean up
#                 del chunk, valid_rows
#                 gc.collect()
            
#             # Convert to DataFrame
#             site_temps = pd.DataFrame(list(all_combinations), columns=['source', 'soil_temp_depth'...
            
#             # Mark moisture capability
#             if moisture_capable_sites:
#                 site_temps['has_moisture_data'] = site_temps['source'].isin(moisture_capable_sites...
#             else:
#                 site_temps['has_moisture_data'] = False
            
#             moisture_count = site_temps['has_moisture_data'].sum()
#             print(f"Found {len(site_temps)} unique site-depth combinations (chunked approach)")
#             print(f"  {moisture_count} site-depth combinations ({moisture_count/len(site_temps)*10...
            
#             return site_temps
            
#         except Exception as chunk_error:
#             print(f"Error in chunked site depth extraction: {str(chunk_error)}")
#             return pd.DataFrame(columns=['source', 'soil_temp_depth', 'has_moisture_data'])

# # 3. FIX: SIMPLIFIED DATA LOADING WITHOUT ELEVATION REFERENCES
# def load_site_data_simplified(feather_path, site, temp_depth, include_moisture=True, moisture_capa...
#     """Load site data with simplified approach avoiding unnecessary columns"""
#     print(f"Loading data for site: {site}, depth: {temp_depth}")
    
#     # First determine if this site has moisture capability
#     has_moisture_capability = False
#     if moisture_capable_sites and site in moisture_capable_sites:
#         has_moisture_capability = True
#         print(f"  Site {site} has moisture capability")
    
#     try:
#         # Only request columns we know exist
#         columns = ['source', 'soil_temp_depth', 'soil_temp_standardized', 'datetime']
        
#         # Add moisture columns only if site has moisture capability
#         if has_moisture_capability and include_moisture:
#             columns.extend(['soil_moist_depth', 'soil_moist_standardized'])
        
#         # Read data with direct filtering in pandas
#         df = pd.read_feather(feather_path, columns=columns)
#         filtered_df = df[(df['source'] == site) & (df['soil_temp_depth'] == temp_depth)].copy()
        
#         # Clean up the main dataframe immediately
#         del df
#         gc.collect()
        
#         # If we need moisture data and don't have it yet, try to fetch it separately
#         if include_moisture and has_moisture_capability and len(filtered_df) > 0:
#             # Check if we already have adequate moisture data
#             has_sufficient_moisture = (
#                 'soil_moist_standardized' in filtered_df.columns and 
#                 filtered_df['soil_moist_standardized'].notna().sum() > len(filtered_df) * 0.1  # A...
#             )
            
#             if not has_sufficient_moisture:
#                 try:
#                     # Read moisture data separately
#                     moist_df = pd.read_feather(
#                         feather_path, 
#                         columns=['source', 'datetime', 'soil_moist_depth', 'soil_moist_standardize...
#                     )
                    
#                     # Filter to current site only
#                     moist_df = moist_df[(moist_df['source'] == site) & 
#                                          moist_df['soil_moist_standardized'].notna()]
                    
#                     if len(moist_df) > 0:
#                         # Find unique moisture depths
#                         moist_depths = moist_df['soil_moist_depth'].dropna().unique()
                        
#                         if len(moist_depths) > 0:
#                             # Find closest moisture depth to temperature depth
#                             closest_depth_idx = np.abs(moist_depths - temp_depth).argmin()
#                             closest_depth = moist_depths[closest_depth_idx]
                            
#                             print(f"  Found closest moisture depth: {closest_depth} (temp depth: {...
                            
#                             # Filter to just that depth
#                             moisture_depth_df = moist_df[moist_df['soil_moist_depth'] == closest_d...
                            
#                             if len(moisture_depth_df) > 0:
#                                 # Prepare for merge
#                                 if not pd.api.types.is_datetime64_dtype(filtered_df['datetime']):
#                                     filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime...
                                
#                                 if not pd.api.types.is_datetime64_dtype(moisture_depth_df['datetim...
#                                     moisture_depth_df['datetime'] = pd.to_datetime(moisture_depth_...
                                
#                                 # Sort both DataFrames by datetime
#                                 filtered_df = filtered_df.sort_values('datetime')
#                                 moisture_depth_df = moisture_depth_df.sort_values('datetime')
                                
#                                 # Merge with temperature data using merge_asof for nearest time ma...
#                                 merged_df = pd.merge_asof(
#                                     filtered_df,
#                                     moisture_depth_df[['datetime', 'soil_moist_standardized', 'soi...
#                                     on='datetime',
#                                     direction='nearest',
#                                     tolerance=pd.Timedelta('6h')  # Match within 6 hours
#                                 )
                                
#                                 # Use merged dataset and add depth info
#                                 filtered_df = merged_df
#                                 filtered_df['closest_moist_depth'] = closest_depth
                                
#                                 # Report moisture coverage
#                                 valid_moisture = filtered_df['soil_moist_standardized'].notna().su...
#                                 pct = valid_moisture / len(filtered_df) * 100
#                                 print(f"  Merged moisture data: {valid_moisture}/{len(filtered_df)...
                    
#                     # Clean up
#                     del moist_df
#                     if 'moisture_depth_df' in locals():
#                         del moisture_depth_df
#                     gc.collect()
                    
#                 except Exception as e:
#                     print(f"  Error fetching separate moisture data: {str(e)}")
        
#         # Ensure datetime column is datetime type
#         if 'datetime' in filtered_df.columns and not pd.api.types.is_datetime64_dtype(filtered_df[...
#             filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])
        
#         # Add depth zone
#         if 'soil_temp_depth' in filtered_df.columns:
#             filtered_df['soil_temp_depth_zone'] = pd.cut(
#                 filtered_df['soil_temp_depth'],
#                 bins=[-float('inf'), 5, 20, 50, 100, float('inf')],
#                 labels=['surface', 'shallow', 'mid', 'deep', 'very_deep']
#             )
        
#         # Add year
#         if 'datetime' in filtered_df.columns:
#             filtered_df['year'] = filtered_df['datetime'].dt.year
        
#         # Add season
#         if 'datetime' in filtered_df.columns:
#             def get_season(month):
#                 if month in [12, 1, 2]: return 'winter'
#                 elif month in [3, 4, 5]: return 'spring'
#                 elif month in [6, 7, 8]: return 'summer'
#                 else: return 'fall'
            
#             filtered_df['season'] = filtered_df['datetime'].dt.month.apply(get_season)
        
#         print(f"Loaded {len(filtered_df)} rows for site-depth")
        
#         # Report on moisture data if present
#         if 'soil_moist_standardized' in filtered_df.columns:
#             valid_moisture = filtered_df['soil_moist_standardized'].notna().sum()
#             pct = valid_moisture / len(filtered_df) * 100 if len(filtered_df) > 0 else 0
#             print(f"  Final moisture data coverage: {valid_moisture}/{len(filtered_df)} rows ({pct...
        
#         return filtered_df
    
#     except Exception as e:
#         print(f"Error loading site data: {str(e)}")
        
#         # Try even simpler approach with chunking if direct reading fails
#         try:
#             print("  Attempting simplified chunked data loading...")
#             chunk_size = 100000
#             filtered_chunks = []
            
#             # Define columns conservatively
#             columns = ['source', 'soil_temp_depth', 'soil_temp_standardized', 'datetime']
            
#             # Loop through chunks
#             for chunk in pd.read_feather(feather_path, columns=columns, chunksize=chunk_size):
#                 chunk_filtered = chunk[(chunk['source'] == site) & 
#                                       (chunk['soil_temp_depth'] == temp_depth)]
                
#                 if len(chunk_filtered) > 0:
#                     filtered_chunks.append(chunk_filtered)
                
#                 # Clean up
#                 del chunk
#                 gc.collect()
            
#             # Combine filtered chunks
#             if filtered_chunks:
#                 filtered_df = pd.concat(filtered_chunks, ignore_index=True)
                
#                 # Add basic columns
#                 if 'datetime' in filtered_df.columns:
#                     if not pd.api.types.is_datetime64_dtype(filtered_df['datetime']):
#                         filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])
#                     filtered_df['year'] = filtered_df['datetime'].dt.year
#                     filtered_df['season'] = filtered_df['datetime'].dt.month.apply(
#                         lambda m: 'winter' if m in [12, 1, 2] else
#                                  'spring' if m in [3, 4, 5] else
#                                  'summer' if m in [6, 7, 8] else 'fall'
#                     )
                
#                 if 'soil_temp_depth' in filtered_df.columns:
#                     filtered_df['soil_temp_depth_zone'] = pd.cut(
#                         filtered_df['soil_temp_depth'],
#                         bins=[-float('inf'), 5, 20, 50, 100, float('inf')],
#                         labels=['surface', 'shallow', 'mid', 'deep', 'very_deep']
#                     )
                
#                 print(f"  Loaded {len(filtered_df)} rows using chunked approach")
#                 return filtered_df
#             else:
#                 return pd.DataFrame()  # Return empty DataFrame if no data found
                
#         except Exception as chunk_error:
#             print(f"  Error in chunked loading: {str(chunk_error)}")
#             return pd.DataFrame()  # Return empty DataFrame on all errors

# # 4. ULTRA-ROBUST PIPELINE
# def run_ultra_robust_pipeline(feather_path, output_dir=None, 
#                              site_batch_size=10, checkpoint_interval=5,
#                              max_gap_hours=8):
#     """
#     Ultra-robust pipeline with maximum compatibility for moisture data integration
#     Avoids PyArrow API dependencies and missing column references
#     """
#     print("=" * 80)
#     print("ULTRA-ROBUST ZERO CURTAIN DETECTION WITH MOISTURE INTEGRATION")
#     print("=" * 80)
#     print(f"Initial memory usage: {memory_usage():.1f} MB")
    
#     # Set up directories
#     if output_dir is not None:
#         os.makedirs(output_dir, exist_ok=True)
#         checkpoint_dir = os.path.join(output_dir, 'checkpoints')
#         os.makedirs(checkpoint_dir, exist_ok=True)
#     else:
#         checkpoint_dir = None
    
#     # Define checkpoint functions
#     def save_checkpoint(data, name):
#         if checkpoint_dir:
#             backup_path = os.path.join(checkpoint_dir, f'{name}_backup.pkl')
#             target_path = os.path.join(checkpoint_dir, f'{name}.pkl')
            
#             # First save to backup file
#             with open(backup_path, 'wb') as f:
#                 pickle.dump(data, f)
            
#             # Then rename to target (atomic operation)
#             import shutil
#             shutil.move(backup_path, target_path)
            
#             print(f"Saved checkpoint to {target_path}")
            
#             # Force memory cleanup after saving checkpoints
#             for _ in range(3):
#                 gc.collect()
    
#     def load_checkpoint(name):
#         if checkpoint_dir:
#             try:
#                 with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'rb') as f:
#                     data = pickle.load(f)
#                 print(f"Loaded checkpoint from {name}.pkl")
#                 return data
#             except:
#                 print(f"No checkpoint found for {name}.pkl")
#         return None
    
#     # Step 1: Ultra-robust moisture capability detection
#     print("\nStep 1: Performing ultra-robust moisture detection...")
#     moisture_capable_sites = detect_moisture_capability(feather_path)
    
#     if moisture_capable_sites:
#         save_checkpoint(moisture_capable_sites, 'moisture_capable_sites')
#         print(f"Working with {len(moisture_capable_sites)} moisture-capable sites")
#     else:
#         moisture_capable_sites = set()
#         print("No moisture-capable sites detected")
    
#     # Step 2: Get site-depth combinations with simplified approach
#     print("\nStep 2: Finding unique site-depth combinations...")
#     site_depths = get_simplified_site_depths(feather_path, moisture_capable_sites)
#     save_checkpoint(site_depths, 'site_depths')
    
#     # Force cleanup after loading site data
#     for _ in range(3):
#         gc.collect()
    
#     total_combinations = len(site_depths)
#     print(f"Found {total_combinations} unique site-depth combinations")
    
#     # Report on moisture capabilities
#     if 'has_moisture_data' in site_depths.columns:
#         moisture_sites = site_depths['has_moisture_data'].sum()
#         print(f"  {moisture_sites} sites ({moisture_sites/total_combinations*100:.1f}%) have moist...
    
#     # Step 3: Initialize event tracking
#     all_events = []
#     processed_indices = set()
    
#     # Step 4: Process in batches with simplified data loading
#     print("\nProcessing site-depth combinations in batches")
    
#     # For tracking new events in this run
#     new_events_count = 0
    
#     # Create batches for processing
#     total_batches = (total_combinations + site_batch_size - 1) // site_batch_size
    
#     # Process just a few batches for demonstration
#     # Change this to total_batches for full processing
#     max_batches = min(5, total_batches)
    
#     for batch_idx in range(max_batches):
#         batch_start = batch_idx * site_batch_size
#         batch_end = min(batch_start + site_batch_size, total_combinations)
        
#         print(f"\nProcessing batch {batch_idx+1}/{total_batches} (sites {batch_start+1}-{batch_end...
#         print(f"Memory before batch: {memory_usage():.1f} MB")
        
#         # Force garbage collection before each batch
#         for _ in range(3):
#             gc.collect()
        
#         # Process each site in batch
#         for site_idx in range(batch_start, batch_end):
#             site = site_depths.iloc[site_idx]['source']
#             temp_depth = site_depths.iloc[site_idx]['soil_temp_depth']
            
#             print(f"\nProcessing site {site_idx+1}/{total_combinations}: {site}, depth: {temp_dept...
            
#             # Check if site has moisture capabilities
#             has_moisture = False
#             if 'has_moisture_data' in site_depths.columns:
#                 has_moisture = site_depths.iloc[site_idx]['has_moisture_data']
            
#             print(f"Site has moisture capability: {has_moisture}")
            
#             # Load site data with simplified approach
#             site_df = load_site_data_simplified(
#                 feather_path=feather_path, 
#                 site=site, 
#                 temp_depth=temp_depth, 
#                 include_moisture=has_moisture,
#                 moisture_capable_sites=moisture_capable_sites
#             )
            
#             # Add to processed sites
#             processed_indices.add(site_idx)
            
#             # Create synthetic event for demonstration
#             if len(site_df) > 10:  # Only if we have some data
#                 # Check if this site has moisture data
#                 has_moisture_data = (
#                     'soil_moist_standardized' in site_df.columns and 
#                     site_df['soil_moist_standardized'].notna().sum() > 0
#                 )
                
#                 # Create a simple demo event
#                 event = {
#                     'source': site,
#                     'soil_temp_depth': temp_depth,
#                     'soil_temp_depth_zone': site_df['soil_temp_depth_zone'].iloc[0] 
#                                           if 'soil_temp_depth_zone' in site_df else None,
#                     'datetime_min': site_df['datetime'].min(),
#                     'datetime_max': site_df['datetime'].max(),
#                     'duration_hours': (site_df['datetime'].max() - site_df['datetime'].min())
#                                       .total_seconds() / 3600,
#                     'soil_temp_mean': site_df['soil_temp_standardized'].mean(),
#                     'year': site_df['year'].iloc[0] if 'year' in site_df else None,
#                     'month': site_df['datetime'].dt.month.iloc[0],
#                     'season': site_df['season'].iloc[0] if 'season' in site_df else None,
#                 }
                
#                 # Add moisture data if available
#                 if has_moisture_data:
#                     event['soil_moist_mean'] = site_df['soil_moist_standardized'].mean()
#                     event['soil_moist_std'] = site_df['soil_moist_standardized'].std()
#                     event['soil_moist_min'] = site_df['soil_moist_standardized'].min()
#                     event['soil_moist_max'] = site_df['soil_moist_standardized'].max()
#                     event['soil_moist_depth'] = site_df['soil_moist_depth'].iloc[0] if 'soil_moist...
                    
#                     # Report moisture coverage
#                     valid_moisture = site_df['soil_moist_standardized'].notna().sum()
#                     coverage_pct = valid_moisture / len(site_df) * 100
#                     print(f"  Has moisture data: {valid_moisture}/{len(site_df)} rows ({coverage_p...
#                     event['moisture_coverage_pct'] = coverage_pct
#                 else:
#                     # Add empty moisture fields for consistency
#                     event['soil_moist_mean'] = np.nan
#                     event['soil_moist_std'] = np.nan
#                     event['soil_moist_min'] = np.nan
#                     event['soil_moist_max'] = np.nan
#                     event['soil_moist_depth'] = np.nan
#                     event['moisture_coverage_pct'] = 0.0
                
#                 all_events.append(event)
#                 new_events_count += 1
#                 print(f"  Created demo event {new_events_count}")
            
#             # Save checkpoints periodically
#             if site_idx % checkpoint_interval == 0:
#                 save_checkpoint(all_events, 'all_events')
#                 save_checkpoint(list(processed_indices), 'processed_indices')
#                 print(f"Memory after checkpoint: {memory_usage():.1f} MB")
            
#             # Clean up
#             del site_df
#             for _ in range(3):
#                 gc.collect()
        
#         # Save checkpoint after each batch
#         print(f"Saving checkpoint after batch {batch_idx+1}/{total_batches}")
#         save_checkpoint(all_events, 'all_events')
#         save_checkpoint(list(processed_indices), 'processed_indices')
        
#         # Also save intermediate results as CSV
#         if len(all_events) > 0:
#             interim_df = pd.DataFrame(all_events)
#             if output_dir is not None:
#                 interim_path = os.path.join(output_dir, 'moisture_events_demo.csv')
#                 interim_df.to_csv(interim_path, index=False)
#                 print(f"Saved interim results to {interim_path}")
            
#             # Clean up the interim DataFrame immediately
#             del interim_df
#             for _ in range(3):
#                 gc.collect()
        
#         print(f"Memory after batch: {memory_usage():.1f} MB")
    
#     # Final summary
#     print("\n" + "=" * 80)
#     print(f"Ultra-robust pipeline demonstration completed with {new_events_count} events")
#     print(f"Final memory usage: {memory_usage():.1f} MB")
    
#     return pd.DataFrame(all_events) if all_events else pd.DataFrame()

# import os
# import tensorflow as tf

# # Configure memory growth to avoid pre-allocating all GPU memory
# physical_devices = tf.config.list_physical_devices('GPU')
# if physical_devices:
#     for device in physical_devices:
#         tf.config.experimental.set_memory_growth(device, True)

# # Optional: Set TensorFlow logging level
# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # 0=debug, 1=info, 2=warning, 3=error

# # TensorFlow Performance Troubleshooting Guide for Apple Silicon (M1/M2)

# """
# This guide contains functions and tips to resolve common TensorFlow performance issues,
# especially on Apple Silicon (M1/M2) Macs.

# Functions:
# - diagnose_tf_environment: Print detailed information about TensorFlow and system
# - enable_metal_gpu: Configure TensorFlow to use Metal GPU acceleration
# - optimize_model_build: Test and optimize model building with progressive complexity
# - check_memory_leak: Test for memory leaks during model training
# - benchmark_performance: Benchmark model performance with different configurations
# """

# import os
# import sys
# import time
# import platform
# import tensorflow as tf
# import numpy as np

# def diagnose_tf_environment():
#     """
#     Print detailed information about the TensorFlow environment
#     and system configuration to help diagnose performance issues.
#     """
#     print("\n===== TensorFlow Environment Diagnosis =====")
    
#     # System information
#     print("\n----- System Information -----")
#     print(f"Python version: {sys.version}")
#     print(f"TensorFlow version: {tf.__version__}")
#     print(f"Platform: {platform.platform()}")
#     print(f"Processor: {platform.processor()}")
    
#     # Check if running on Apple Silicon
#     is_apple_silicon = (platform.system() == 'Darwin' and 
#                        ('arm64' in platform.machine() or 'ARM64' in platform.machine()))
#     print(f"Apple Silicon detected: {is_apple_silicon}")
    
#     # Check if Metal plugin is available (for Apple Silicon)
#     try:
#         from tensorflow.python.framework.errors_impl import NotFoundError
#         try:
#             tf.config.list_physical_devices('GPU')
#             metal_available = True
#         except NotFoundError:
#             metal_available = False
#         print(f"Metal plugin available: {metal_available}")
#     except:
#         print("Could not determine Metal plugin availability")
    
#     # Available devices
#     print("\n----- Available Devices -----")
#     devices = tf.config.list_physical_devices()
#     if not devices:
#         print("No devices found.")
#     else:
#         for device in devices:
#             print(f"Device name: {device.name}, type: {device.device_type}")
    
#     # Memory settings
#     print("\n----- Memory Settings -----")
#     gpu_devices = tf.config.list_physical_devices('GPU')
#     if gpu_devices:
#         try:
#             for gpu in gpu_devices:
#                 memory_details = tf.config.experimental.get_memory_info(gpu.name)
#                 print(f"Device: {gpu.name}")
#                 print(f"  Current memory allocation: {memory_details['current'] / 1e9:.2f} GB")
#                 print(f"  Peak memory allocation: {memory_details['peak'] / 1e9:.2f} GB")
#         except:
#             print("Memory information not available")
    
#     # Environment variables
#     print("\n----- TensorFlow-Related Environment Variables -----")
#     tf_env_vars = [var for var in os.environ if 'TF_' in var]
#     if tf_env_vars:
#         for var in tf_env_vars:
#             print(f"{var}={os.environ[var]}")
#     else:
#         print("No TensorFlow-specific environment variables set")
    
#     return is_apple_silicon

# def enable_metal_gpu(debug_level=0):
#     """
#     Configure TensorFlow to use Metal GPU acceleration on Apple Silicon.
    
#     Parameters:
#     -----------
#     debug_level : int
#         0 = minimal logging
#         1 = info logging
#         2 = verbose logging
    
#     Returns:
#     --------
#     bool
#         Whether Metal GPU was successfully enabled
#     """
#     # Check if running on MacOS with Apple Silicon
#     is_apple_silicon = (platform.system() == 'Darwin' and 
#                        ('arm64' in platform.machine() or 'ARM64' in platform.machine()))
    
#     if not is_apple_silicon:
#         print("Not running on Apple Silicon - skipping Metal GPU configuration")
#         return False
    
#     # Set environment variables for TensorFlow Metal plugin
#     os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
    
#     # Set debug level
#     if debug_level == 0:
#         os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress all logs
#     elif debug_level == 1:
#         os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # Show INFO logs
#     else:
#         os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # Show all logs
#         os.environ['TF_METAL_DEVICE_DELEGATE_DEBUG'] = '1'  # Enable Metal debug
    
#     # Try to detect Metal GPU
#     try:
#         physical_devices = tf.config.list_physical_devices('GPU')
#         if not physical_devices:
#             print("No Metal GPU found. TensorFlow will use CPU.")
#             return False
        
#         print(f"Found {len(physical_devices)} Metal GPU(s)")
        
#         # Configure memory growth
#         for gpu in physical_devices:
#             tf.config.experimental.set_memory_growth(gpu, True)
#             print(f"Enabled memory growth for {gpu}")
        
#         # Run a simple test on GPU
#         print("Testing Metal GPU with a simple operation...")
#         with tf.device('/GPU:0'):
#             start_time = time.time()
#             a = tf.random.normal((1000, 1000))
#             b = tf.random.normal((1000, 1000))
#             c = tf.matmul(a, b)
#             # Force execution
#             result = c.numpy()
#             execution_time = time.time() - start_time
#             print(f"Test completed in {execution_time:.4f}s")
        
#         return True
    
#     except Exception as e:
#         print(f"Error configuring Metal GPU: {str(e)}")
#         print("Falling back to CPU execution")
#         return False
        
# diagnose_tf_environment()

# tf.config.optimizer.set_jit(True)  # Enable XLA

# from tensorflow.keras.mixed_precision import set_global_policy
# set_global_policy('mixed_float16')

# import os
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# import cartopy.crs as ccrs
# import cartopy.feature as cfeature
# from matplotlib.colors import PowerNorm, Normalize
# from datetime import timedelta
# from tqdm.auto import tqdm

# def enhanced_zero_curtain_detection_with_interpolation(df, output_dir=None, max_gap_hours=6, inter...
#     """
#     Physics-based zero curtain detection with interpolation to improve transition precision.
    
#     Parameters:
#     -----------
#     df : pandas.DataFrame
#         The merged dataframe containing soil temperature and moisture data
#     output_dir : str, optional
#         Directory to save any diagnostic outputs
#     max_gap_hours : float
#         Maximum gap in hours to interpolate (reduced from 24 in original code)
#     interpolation_method : str
#         Method for interpolation ('linear', 'cubic', or 'spline')
        
#     Returns:
#     --------
#     pandas.DataFrame
#         DataFrame containing detected zero curtain events with comprehensive statistics
#     """
#     print("Starting enhanced zero curtain detection with interpolation...")
    
#     # Create a copy to avoid modifying the original
#     df = df.copy()
    
#     # Prepare dataset - group by site, depth, and time
#     df = df.sort_values(['source', 'soil_temp_depth', 'datetime'])
    
#     # Get site-depth combinations for progress tracking
#     site_depths = df.groupby(['source', 'soil_temp_depth']).size().reset_index()[['source', 'soil_...
#     total_combinations = len(site_depths)
#     print(f"Processing {total_combinations} site-depth combinations...")
    
#     # Calculate derived variables
#     site_data = []
    
#     # Create progress bar for site-depth combinations
#     for i, (site, temp_depth) in enumerate(tqdm(
#         zip(site_depths['source'], site_depths['soil_temp_depth']), 
#         total=total_combinations,
#         desc="Processing sites"
#     )):
#         group = df[(df['source'] == site) & (df['soil_temp_depth'] == temp_depth)]
        
#         if len(group) < 10:  # Skip sites with insufficient data
#             continue
            
#         # Extract time series
#         group = group.sort_values('datetime')
        
#         # Check for gaps that need interpolation
#         group['time_diff'] = group['datetime'].diff().dt.total_seconds() / 3600
        
#         # Identify gaps that need interpolation (less than max_gap_hours)
#         interpolation_needed = (group['time_diff'] > 1.0) & (group['time_diff'] <= max_gap_hours)
        
#         # Perform interpolation if needed
#         if interpolation_needed.any():
#             # Get indices of gaps to interpolate
#             gap_indices = group.index[interpolation_needed].tolist()
            
#             # Create new rows for interpolation
#             interp_rows = []
            
#             for idx in gap_indices:
#                 # Get before and after rows
#                 before_row = group.loc[group.index[group.index.get_loc(idx) - 1]]
#                 after_row = group.loc[idx]
                
#                 # Calculate time gap
#                 time_gap = after_row['time_diff']
                
#                 # Calculate number of intervals for interpolation
#                 # (One point per hour is usually sufficient)
#                 n_intervals = int(time_gap)
                
#                 if n_intervals > 0:
#                     # Create interpolated timestamps
#                     timestamps = pd.date_range(
#                         start=before_row['datetime'],
#                         end=after_row['datetime'],
#                         periods=n_intervals + 2  # Include endpoints
#                     )[1:-1]  # Exclude endpoints (they already exist)
                    
#                     # Create interpolated values for soil temperature
#                     if interpolation_method == 'linear':
#                         temp_start = before_row['soil_temp_standardized']
#                         temp_end = after_row['soil_temp_standardized']
#                         temp_values = np.linspace(temp_start, temp_end, n_intervals + 2)[1:-1]
#                     elif interpolation_method == 'cubic' or interpolation_method == 'spline':
#                         # Need more points for cubic/spline interpolation
#                         if len(group) >= 5:
#                             # Get a few points before and after for better interpolation
#                             idx_loc = group.index.get_loc(idx)
#                             start_idx = max(0, idx_loc - 3)
#                             end_idx = min(len(group), idx_loc + 2)
                            
#                             temp_points = group.iloc[start_idx:end_idx]['soil_temp_standardized']....
#                             time_points = [(t - before_row['datetime']).total_seconds() / 3600 
#                                          for t in group.iloc[start_idx:end_idx]['datetime']]
                            
#                             # New time points for interpolation
#                             interp_times = [(t - before_row['datetime']).total_seconds() / 3600 
#                                         for t in timestamps]
                            
#                             # Perform cubic interpolation
#                             from scipy.interpolate import interp1d
#                             if len(time_points) >= 4:  # Need at least 4 points for cubic
#                                 try:
#                                     interp_func = interp1d(time_points, temp_points, 
#                                                          kind='cubic', bounds_error=False)
#                                     temp_values = interp_func(interp_times)
#                                 except:
#                                     # Fall back to linear if cubic fails
#                                     temp_start = before_row['soil_temp_standardized']
#                                     temp_end = after_row['soil_temp_standardized']
#                                     temp_values = np.linspace(temp_start, temp_end, n_intervals + ...
#                             else:
#                                 # Fall back to linear if not enough points
#                                 temp_start = before_row['soil_temp_standardized']
#                                 temp_end = after_row['soil_temp_standardized']
#                                 temp_values = np.linspace(temp_start, temp_end, n_intervals + 2)[1...
#                         else:
#                             # Fall back to linear if not enough points
#                             temp_start = before_row['soil_temp_standardized']
#                             temp_end = after_row['soil_temp_standardized']
#                             temp_values = np.linspace(temp_start, temp_end, n_intervals + 2)[1:-1]
#                     else:
#                         # Default to linear
#                         temp_start = before_row['soil_temp_standardized']
#                         temp_end = after_row['soil_temp_standardized']
#                         temp_values = np.linspace(temp_start, temp_end, n_intervals + 2)[1:-1]
                    
#                     # Also interpolate soil moisture if available
#                     moist_values = None
#                     if 'soil_moist_standardized' in group.columns and not pd.isna(before_row['soil...
#                         moist_start = before_row['soil_moist_standardized']
#                         moist_end = after_row['soil_moist_standardized']
#                         moist_values = np.linspace(moist_start, moist_end, n_intervals + 2)[1:-1]
                    
#                     # Create new interpolated rows
#                     for j, timestamp in enumerate(timestamps):
#                         # Create a copy of the before row as a template
#                         new_row = before_row.copy()
                        
#                         # Update the timestamp and interpolated values
#                         new_row['datetime'] = timestamp
#                         new_row['soil_temp_standardized'] = temp_values[j]
                        
#                         # Mark as interpolated
#                         new_row['interpolated'] = True
                        
#                         # Update moisture values if available
#                         if moist_values is not None:
#                             new_row['soil_moist_standardized'] = moist_values[j]
                        
#                         interp_rows.append(new_row)
            
#             # Add interpolated rows to the dataset
#             if interp_rows:
#                 interp_df = pd.DataFrame(interp_rows)
#                 group = pd.concat([group, interp_df], ignore_index=True)
#                 # Re-sort by time
#                 group = group.sort_values('datetime')
        
#         # Calculate temperature derivatives with improved precision from interpolation
#         group['temp_gradient'] = group['soil_temp_standardized'].diff() / \
#                                (group['datetime'].diff().dt.total_seconds() / 3600)
        
#         # The rest of the processing remains the same, with improved precision due to interpolatio...
#         # Identify potential phase change periods
#         mask_temp = (group['soil_temp_standardized'].abs() <= 0.5)
#         mask_gradient = (group['temp_gradient'].abs() <= 0.02)  # Reduced rate of change
        
#         # Find corresponding soil moisture data for this site, if available
#         use_moisture = False
#         moisture_data = None
        
#         # Check if soil moisture data exists for this site
#         if 'soil_moist_standardized' in group.columns and not group['soil_moist_standardized'].isn...
#             use_moisture = True
            
#             # Calculate moisture gradient with improved precision
#             group['moist_gradient'] = group['soil_moist_standardized'].diff() / \
#                                     (group['datetime'].diff().dt.total_seconds() / 3600)
            
#             # Moisture changes during phase change
#             mask_moisture = (group['moist_gradient'].abs() >= 0.0005)
            
#             # Combined mask with moisture information
#             combined_mask = mask_temp & (mask_gradient | mask_moisture)
#         else:
#             # Fall back to just temperature if no moisture data
#             combined_mask = mask_temp & mask_gradient
        
#         # Find continuous events with improved temporal precision
#         group['zero_curtain_flag'] = combined_mask
#         group['event_start'] = combined_mask & ~combined_mask.shift(1, fill_value=False)
#         group['event_end'] = combined_mask & ~combined_mask.shift(-1, fill_value=False)
        
#         # Extract events
#         event_starts = group[group['event_start']]['datetime'].tolist()
#         event_ends = group[group['event_end']]['datetime'].tolist()
        
#         if len(event_starts) == 0 or len(event_ends) == 0:
#             continue
            
#         # Handle unmatched start/end points
#         if len(event_starts) > len(event_ends):
#             event_starts = event_starts[:len(event_ends)]
#         elif len(event_ends) > len(event_starts):
#             event_ends = event_ends[:len(event_starts)]
        
#         # Calculate duration and other event properties
#         for start, end in zip(event_starts, event_ends):
#             event_duration = (end - start).total_seconds() / 3600
            
#             # Filter for minimum duration
#             if event_duration < 12:
#                 continue
                
#             # Get event data
#             event_data = group[(group['datetime'] >= start) & (group['datetime'] <= end)]
            
#             if len(event_data) < 3:
#                 continue
            
#             # Create event information dictionary with comprehensive soil temperature stats
#             event_info = {
#                 'source': site,
#                 'soil_temp_depth': temp_depth,
#                 'soil_temp_depth_zone': event_data['soil_temp_depth_zone'].iloc[0] if 'soil_temp_d...
#                 'datetime_min': start,
#                 'datetime_max': end,
#                 'duration_hours': event_duration,
#                 'observation_count': len(event_data),
#                 'observations_per_day': len(event_data) / (event_duration / 24) if event_duration ...
#                 'soil_temp_mean': event_data['soil_temp_standardized'].mean(),
#                 'soil_temp_min': event_data['soil_temp_standardized'].min(),
#                 'soil_temp_max': event_data['soil_temp_standardized'].max(),
#                 'soil_temp_std': event_data['soil_temp_standardized'].std(),
#                 'season': event_data['season'].mode().iloc[0] if 'season' in event_data.columns el...
#                 'latitude': event_data['latitude'].iloc[0] if 'latitude' in event_data.columns els...
#                 'longitude': event_data['longitude'].iloc[0] if 'longitude' in event_data.columns ...
#                 'year': event_data['year'].iloc[0] if 'year' in event_data.columns else None,
#                 'month': start.month
#             }
            
#             # Add soil moisture metrics if available
#             if use_moisture and not event_data['soil_moist_standardized'].isna().all():
#                 # Calculate comprehensive moisture statistics
#                 event_info['soil_moist_mean'] = event_data['soil_moist_standardized'].mean()
#                 event_info['soil_moist_std'] = event_data['soil_moist_standardized'].std()
#                 event_info['soil_moist_min'] = event_data['soil_moist_standardized'].min()
#                 event_info['soil_moist_max'] = event_data['soil_moist_standardized'].max()
#                 event_info['soil_moist_change'] = event_info['soil_moist_max'] - event_info['soil_...
                
#                 # Add moisture depth information
#                 if not event_data['closest_moist_depth'].isna().all():
#                     # Use the most common moisture depth during the event
#                     event_info['soil_moist_depth'] = event_data['closest_moist_depth'].mode().iloc...
#                 else:
#                     event_info['soil_moist_depth'] = np.nan
#             else:
#                 # Include placeholder values if moisture data not available
#                 event_info['soil_moist_mean'] = np.nan
#                 event_info['soil_moist_std'] = np.nan
#                 event_info['soil_moist_min'] = np.nan
#                 event_info['soil_moist_max'] = np.nan
#                 event_info['soil_moist_change'] = np.nan
#                 event_info['soil_moist_depth'] = np.nan
            
#             # Calculate latent heat effects (if sufficient data exists)
#             # This captures the energy involved in the phase change
#             has_temp_gradient = not event_data['temp_gradient'].isna().all()
#             if has_temp_gradient:
#                 event_info['temp_gradient_mean'] = event_data['temp_gradient'].mean()
#                 event_info['temp_stability'] = event_data['temp_gradient'].abs().mean()
            
#             # Add year-month identifier for temporal analysis
#             event_info['year_month'] = f"{event_info['year']}-{event_info['month']:02d}"
            
#             # Add region classification based on latitude
#             if 'latitude' in event_info and event_info['latitude'] is not None:
#                 if event_info['latitude'] >= 66.5:
#                     event_info['region'] = 'Arctic'
#                 elif event_info['latitude'] >= 60:
#                     event_info['region'] = 'Subarctic'
#                 elif event_info['latitude'] >= 50:
#                     event_info['region'] = 'Northern Boreal'
#                 else:
#                     event_info['region'] = 'Other'
#             else:
#                 event_info['region'] = None
                
#             # Calculate latitude band for detailed spatial analysis
#             if 'latitude' in event_info and event_info['latitude'] is not None:
#                 lat = event_info['latitude']
#                 if lat < 55:
#                     event_info['lat_band'] = '<55°N'
#                 elif lat < 60:
#                     event_info['lat_band'] = '55-60°N'
#                 elif lat < 66.5:
#                     event_info['lat_band'] = '60-66.5°N'
#                 elif lat < 70:
#                     event_info['lat_band'] = '66.5-70°N'
#                 elif lat < 75:
#                     event_info['lat_band'] = '70-75°N'
#                 elif lat < 80:
#                     event_info['lat_band'] = '75-80°N'
#                 else:
#                     event_info['lat_band'] = '>80°N'
#             else:
#                 event_info['lat_band'] = None
            
#             site_data.append(event_info)
    
#     # Create events dataframe
#     print(f"Creating events dataframe with {len(site_data)} detected events...")
#     events_df = pd.DataFrame(site_data)
    
#     # Save to file if output directory specified
#     if output_dir is not None:
#         os.makedirs(output_dir, exist_ok=True)
#         events_df.to_csv(os.path.join(output_dir, 'enhanced_zero_curtain_events.csv'), index=False...
#         print(f"Results saved to {os.path.join(output_dir, 'enhanced_zero_curtain_events.csv')}")
    
#     return events_df


# def prepare_data_for_deep_learning(merged_df, enhanced_events, sequence_length=6):
#     """
#     Prepare time series data for deep learning model by creating sliding window sequences
#     that incorporate both soil temperature and moisture information.
    
#     Parameters:
#     -----------
#     merged_df : pandas.DataFrame
#         Raw data containing all measurements
#     enhanced_events : pandas.DataFrame
#         Events detected by the enhanced_zero_curtain_detection function
#     sequence_length : int
#         Length of sequences to create (e.g., 30 time steps)
        
#     Returns:
#     --------
#     tuple
#         (X_features, y_labels, metadata) where features are input sequences,
#         labels are binary zero curtain indicators, and metadata contains
#         information about each sequence
#     """
#     import numpy as np
#     from datetime import timedelta
#     from tqdm.auto import tqdm
    
#     features = []
#     labels = []
#     metadata = []  # Store site, depth, and time information for each sequence
    
#     # Create a mapping of detected events for labeling
#     print("Creating event mapping...")
#     event_map = {}
#     for _, event in enhanced_events.iterrows():
#         site = event['source']
#         depth = event['soil_temp_depth']
#         start = event['datetime_min']
#         end = event['datetime_max']
        
#         if (site, depth) not in event_map:
#             event_map[(site, depth)] = []
        
#         event_map[(site, depth)].append((start, end))
    
#     # Get site-depth combinations for progress tracking
#     site_depths = merged_df.groupby(['source', 'soil_temp_depth']).size().reset_index()[['source',...
#     total_combinations = len(site_depths)
#     print(f"Preparing sequences from {total_combinations} site-depth combinations...")
    
#     # Process each site and depth combination with progress bar
#     for (site, temp_depth) in tqdm(
#         zip(site_depths['source'], site_depths['soil_temp_depth']),
#         total=total_combinations,
#         desc="Creating sequences"
#     ):
#         group = merged_df[(merged_df['source'] == site) & (merged_df['soil_temp_depth'] == temp_de...
        
#         if len(group) < sequence_length + 1:
#             continue
            
#         # Sort by time
#         group = group.sort_values('datetime')
        
#         # Create feature set
#         feature_cols = ['soil_temp_standardized']
        
#         # Calculate gradient features
#         group['temp_gradient'] = group['soil_temp_standardized'].diff()
#         feature_cols.append('temp_gradient')
        
#         # Add soil depth as feature
#         group['depth_normalized'] = temp_depth / 10.0  # Normalize depth
#         feature_cols.append('depth_normalized')
        
#         # Add soil moisture if available
#         has_moisture = False
#         if 'soil_moist_standardized' in group.columns and not group['soil_moist_standardized'].isn...
#             has_moisture = True
#             feature_cols.append('soil_moist_standardized')
#             group['moist_gradient'] = group['soil_moist_standardized'].diff()
#             feature_cols.append('moist_gradient')
        
#         # Fill missing values to ensure consistent sequences
#         group[feature_cols] = group[feature_cols].fillna(0)
        
#         # Create sequences with sliding window
#         for i in range(len(group) - sequence_length):
#             # Get time window
#             start_time = group.iloc[i]['datetime']
#             end_time = group.iloc[i + sequence_length - 1]['datetime']
            
#             # Extract sequence data
#             sequence = group.iloc[i:i+sequence_length][feature_cols].values
            
#             # Check if this sequence overlaps with any known zero curtain event
#             is_zero_curtain = 0
#             if (site, temp_depth) in event_map:
#                 for event_start, event_end in event_map[(site, temp_depth)]:
#                     # Check for significant overlap (at least 50% of sequence)
#                     if (min(end_time, event_end) - max(start_time, event_start)).total_seconds() >...
#                        0.5 * (end_time - start_time).total_seconds():
#                         is_zero_curtain = 1
#                         break
            
#             # Use detected events as labels
#             features.append(sequence)
#             labels.append(is_zero_curtain)
            
#             # Store metadata for this sequence
#             meta = {
#                 'source': site,
#                 'soil_temp_depth': temp_depth,
#                 'start_time': start_time,
#                 'end_time': end_time,
#                 'latitude': group.iloc[i]['latitude'] if 'latitude' in group.columns else None,
#                 'longitude': group.iloc[i]['longitude'] if 'longitude' in group.columns else None,
#                 'has_moisture_data': has_moisture
#             }
#             metadata.append(meta)
    
#     print(f"Created {len(features)} sequences with {len(labels)} labels")
    
#     # Convert to arrays
#     X = np.array(features)
#     y = np.array(labels)
    
#     return X, y, metadata

# def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
#     """
#     Advanced model architecture combining ConvLSTM, Transformers, and 
#     Variational Autoencoder components to better capture complex zero curtain dynamics.
    
#     Parameters:
#     -----------
#     input_shape : tuple
#         Shape of input data (sequence_length, num_features)
#     include_moisture : bool
#         Whether soil moisture features are included
        
#     Returns:
#     --------
#     tensorflow.keras.Model
#         Compiled model ready for training
#     """
#     import tensorflow as tf
#     from tensorflow.keras.models import Model
#     from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
#     from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPoo...
#     from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
#     from tensorflow.keras.optimizers import Adam
    
#     # Input layer
#     inputs = Input(shape=input_shape)
    
#     # Reshape for ConvLSTM (add spatial dimension)
#     # From (sequence_length, features) to (sequence_length, 1, features)
    
#     #x = Reshape((input_shape[0], 1, input_shape[1]))(inputs)
#     x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
#     # ConvLSTM layer to capture spatiotemporal patterns
#     from tensorflow.keras.layers import ConvLSTM2D
#     convlstm = ConvLSTM2D(
#         filters=64,
#         kernel_size=(3, 1),
#         padding='same',
#         return_sequences=True,
#         activation='tanh',
#         recurrent_dropout=0.2
#     )(x)
    
#     # Reshape back to (sequence_length, features)
#     convlstm = Reshape((input_shape[0], 64))(convlstm)
    
#     # Add positional encoding for transformer
#     def positional_encoding(length, depth):
#         positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
#         depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
#         angle_rates = 1 / tf.pow(10000.0, depths)
#         angle_rads = positions * angle_rates
        
#         pos_encoding = tf.concat(
#             [tf.sin(angle_rads), tf.cos(angle_rads)], axis=-1)
        
#         return pos_encoding
    
#     # Add positional encoding
#     pos_encoding = positional_encoding(input_shape[0], 64)
#     transformer_input = convlstm + pos_encoding
    
#     # Transformer encoder block
#     def transformer_encoder(x, num_heads=8, key_dim=64, ff_dim=128):
#         # Multi-head attention
#         attention_output = MultiHeadAttention(
#             num_heads=num_heads, key_dim=key_dim
#         )(x, x)
        
#         # Skip connection 1
#         x1 = Add()([attention_output, x])
#         x1 = LayerNormalization(epsilon=1e-6)(x1)
        
#         # Feed-forward network
#         ff_output = Dense(ff_dim, activation='relu')(x1)
#         ff_output = Dropout(0.1)(ff_output)
#         ff_output = Dense(64)(ff_output)
        
#         # Skip connection 2
#         x2 = Add()([ff_output, x1])
#         return LayerNormalization(epsilon=1e-6)(x2)
    
#     # Apply transformer encoder
#     transformer_output = transformer_encoder(transformer_input)
    
#     # Parallel CNN paths for multi-scale feature extraction
#     cnn_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(inputs)
#     cnn_1 = BatchNormalization()(cnn_1)
    
#     cnn_2 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(inputs)
#     cnn_2 = BatchNormalization()(cnn_2)
    
#     cnn_3 = Conv1D(filters=32, kernel_size=7, padding='same', activation='relu')(inputs)
#     cnn_3 = BatchNormalization()(cnn_3)
    
#     # Variational Autoencoder components
#     def sampling(args):
#         z_mean, z_log_var = args
#         batch = tf.shape(z_mean)[0]
#         dim = tf.shape(z_mean)[1]
#         epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
#         return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
#     # Global temporal features
#     global_max = GlobalMaxPooling1D()(transformer_output)
#     global_avg = GlobalAveragePooling1D()(transformer_output)
    
#     # VAE encoding
#     z_mean = Dense(32)(Concatenate()([global_max, global_avg]))
#     z_log_var = Dense(32)(Concatenate()([global_max, global_avg]))
#     z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
#     # Combine all features
#     merged_features = Concatenate()(
#         [
#             GlobalMaxPooling1D()(cnn_1),
#             GlobalMaxPooling1D()(cnn_2),
#             GlobalMaxPooling1D()(cnn_3),
#             global_max,
#             global_avg,
#             z
#         ]
#     )
    
#     # Final classification layers
#     x = Dense(128, activation='relu')(merged_features)
#     x = Dropout(0.3)(x)
#     x = BatchNormalization()(x)
#     x = Dense(64, activation='relu')(x)
#     x = Dropout(0.2)(x)
    
#     # Output layer
#     outputs = Dense(1, activation='sigmoid')(x)
    
#     # Create model
#     model = Model(inputs=inputs, outputs=outputs)
    
#     # Add VAE loss
#     kl_loss = -0.5 * tf.reduce_mean(
#         z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
#     )
#     model.add_loss(0.001 * kl_loss)  # Small weight for VAE loss
    
#     # Compile model with appropriate metrics
#     model.compile(
#         optimizer=Adam(learning_rate=0.001),
#         loss='binary_crossentropy',
#         metrics=[
#             'accuracy',
#             tf.keras.metrics.AUC(name='auc'),
#             tf.keras.metrics.Precision(name='precision'),
#             tf.keras.metrics.Recall(name='recall')
#         ]
#     )
    
#     return model

# # def build_zero_curtain_lstm_model(input_shape, include_moisture=True):
# #     """
# #     Build a bidirectional LSTM model for zero curtain detection
# #     that captures temporal dynamics in soil temperature and moisture.
    
# #     Parameters:
# #     -----------
# #     input_shape : tuple
# #         Shape of input data (sequence_length, num_features)
# #     include_moisture : bool
# #         Whether soil moisture features are included
        
# #     Returns:
# #     --------
# #     tensorflow.keras.Model
# #         Compiled model ready for training
# #     """
# #     import tensorflow as tf
# #     from tensorflow.keras.models import Model
# #     from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Dropout
# #     from tensorflow.keras.layers import BatchNormalization, Conv1D, GlobalMaxPooling1D, Concaten...
# #     from tensorflow.keras.optimizers import Adam
    
# #     # Input layer
# #     inputs = Input(shape=input_shape)
    
# #     # CNN branch for feature extraction
# #     conv1 = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(inputs)
# #     conv2 = Conv1D(filters=32, kernel_size=5, activation='relu', padding='same')(inputs)
# #     conv3 = Conv1D(filters=32, kernel_size=7, activation='relu', padding='same')(inputs)
    
# #     # Bidirectional LSTM to capture temporal dependencies in both directions
# #     lstm = Bidirectional(LSTM(64, return_sequences=True))(inputs)
# #     lstm = Dropout(0.2)(lstm)
# #     lstm = BatchNormalization()(lstm)
    
# #     # Second LSTM layer
# #     lstm = Bidirectional(LSTM(32, return_sequences=True))(lstm)
# #     lstm = Dropout(0.2)(lstm)
# #     lstm = BatchNormalization()(lstm)
    
# #     # Combine CNN and LSTM features
# #     concat = Concatenate()([conv1, conv2, conv3, lstm])
    
# #     # Global pooling to reduce sequence dimension
# #     pooled = GlobalMaxPooling1D()(concat)
    
# #     # Dense layers for classification
# #     x = Dense(64, activation='relu')(pooled)
# #     x = Dropout(0.3)(x)
# #     x = BatchNormalization()(x)
    
# #     # Output layer - probability of zero curtain
# #     outputs = Dense(1, activation='sigmoid')(x)
    
# #     model = Model(inputs=inputs, outputs=outputs)
    
# #     # Use AUC as a metric since this is an imbalanced classification problem
# #     model.compile(
# #         optimizer=Adam(learning_rate=0.001),
# #         loss='binary_crossentropy',
# #         metrics=[
# #             'accuracy',
# #             tf.keras.metrics.AUC(name='auc'),
# #             tf.keras.metrics.Precision(name='precision'),
# #             tf.keras.metrics.Recall(name='recall')
# #         ]
# #     )
    
# #     return model


# def train_zero_curtain_model(merged_df, enhanced_events, output_dir=None):
#     """
#     Train a deep learning model to detect zero curtain events based on
#     multivariate time series data.
    
#     Parameters:
#     -----------
#     merged_df : pandas.DataFrame
#         Raw data containing all measurements
#     enhanced_events : pandas.DataFrame
#         Events detected by the enhanced_zero_curtain_detection function
#     output_dir : str, optional
#         Directory to save model and results
        
#     Returns:
#     --------
#     tuple
#         (trained_model, training_history, evaluation_results)
#     """
#     import tensorflow as tf
#     from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
#     from sklearn.model_selection import train_test_split
#     import matplotlib.pyplot as plt
#     import os
#     from tqdm.keras import TqdmCallback  # Special tqdm callback for Keras
    
#     print("Preparing data for deep learning model...")
    
#     # Prepare sequences with sliding window
#     sequence_length = 24  # 24 time steps - adjust based on your data frequency
#     X, y, metadata = prepare_data_for_deep_learning(merged_df, enhanced_events, sequence_length)
    
#     print(f"Prepared {len(X)} sequences with {X.shape[1]} time steps and {X.shape[2]} features eac...
#     print(f"Positive examples (zero curtain): {sum(y)}/{len(y)} ({sum(y)/len(y)*100:.1f}%)")
    
#     # Split data into train, validation, and test sets
#     print("Splitting data into train/validation/test sets...")
#     X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stra...
#     X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42...
    
#     # Build model
#     print("Building model...")
#     input_shape = (X.shape[1], X.shape[2])
#     model = build_advanced_zero_curtain_model(input_shape)
    
#     # Set up callbacks
#     callbacks = [
#         EarlyStopping(patience=10, restore_best_weights=True, monitor='val_auc', mode='max'),
#         ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6, monitor='val_auc', mode='max'),
#         TqdmCallback(verbose=1)  # Add tqdm progress bar for epochs
#     ]
    
#     if output_dir:
#         os.makedirs(output_dir, exist_ok=True)
#         callbacks.append(
#             ModelCheckpoint(
#                 os.path.join(output_dir, 'zero_curtain_model.h5'),
#                 save_best_only=True,
#                 monitor='val_auc',
#                 mode='max'
#             )
#         )
    
#     # Train model
#     print("Training model...")
#     batch_size = 32
#     epochs = 100
    
#     history = model.fit(
#         X_train, y_train,
#         validation_data=(X_val, y_val),
#         epochs=epochs,
#         batch_size=batch_size,
#         callbacks=callbacks,
#         class_weight={0: 1, 1: len(y_train)/sum(y_train) if sum(y_train) > 0 else 1},  # Handle cl...
#         verbose=0  # Set to 0 because we're using TqdmCallback
#     )
    
#     # Evaluate on test set
#     print("Evaluating model on test set...")
#     evaluation = model.evaluate(X_test, y_test)
#     print("Test performance:")
#     for metric, value in zip(model.metrics_names, evaluation):
#         print(f"  {metric}: {value:.4f}")
    
#     # Generate predictions
#     y_pred_prob = model.predict(X_test)
#     y_pred = (y_pred_prob > 0.5).astype(int).flatten()
    
#     # Plot training history
#     if output_dir:
#         plt.figure(figsize=(12, 4))
        
#         plt.subplot(1, 2, 1)
#         plt.plot(history.history['auc'])
#         plt.plot(history.history['val_auc'])
#         plt.title('Model AUC')
#         plt.ylabel('AUC')
#         plt.xlabel('Epoch')
#         plt.legend(['Train', 'Validation'], loc='lower right')
        
#         plt.subplot(1, 2, 2)
#         plt.plot(history.history['loss'])
#         plt.plot(history.history['val_loss'])
#         plt.title('Model Loss')
#         plt.ylabel('Loss')
#         plt.xlabel('Epoch')
#         plt.legend(['Train', 'Validation'], loc='upper right')
        
#         plt.tight_layout()
#         plt.savefig(os.path.join(output_dir, 'training_history.png'))
        
#         # Save detailed model summary
#         from contextlib import redirect_stdout
#         with open(os.path.join(output_dir, 'model_summary.txt'), 'w') as f:
#             with redirect_stdout(f):
#                 model.summary()
    
#     return model, history, evaluation


# def apply_model_to_new_data(model, new_data, sequence_length=6):
#     """
#     Apply a trained model to detect zero curtain events in new data.
    
#     Parameters:
#     -----------
#     model : tensorflow.keras.Model
#         Trained zero curtain detection model
#     new_data : pandas.DataFrame
#         New data to analyze, with similar structure to training data
#     sequence_length : int
#         Length of sequences used for model input
        
#     Returns:
#     --------
#     pandas.DataFrame
#         DataFrame with predictions and probabilities for zero curtain events
#     """
#     import numpy as np
#     from tqdm.auto import tqdm
    
#     predictions = []
    
#     # Get site-depth combinations for progress tracking
#     site_depths = new_data.groupby(['source', 'soil_temp_depth']).size().reset_index()[['source', ...
#     total_combinations = len(site_depths)
#     print(f"Applying model to {total_combinations} site-depth combinations...")
    
#     # Process each site and depth combination with progress bar
#     for (site, temp_depth) in tqdm(
#         zip(site_depths['source'], site_depths['soil_temp_depth']),
#         total=total_combinations,
#         desc="Making predictions"
#     ):
#         group = new_data[(new_data['source'] == site) & (new_data['soil_temp_depth'] == temp_depth...
        
#         if len(group) < sequence_length + 1:
#             continue
            
#         # Sort by time
#         group = group.sort_values('datetime')
        
#         # Prepare features (same as in training)
#         feature_cols = ['soil_temp_standardized']
#         group['temp_gradient'] = group['soil_temp_standardized'].diff()
#         feature_cols.append('temp_gradient')
#         group['depth_normalized'] = temp_depth / 10.0
#         feature_cols.append('depth_normalized')
        
#         # Add soil moisture if available
#         if 'soil_moist_standardized' in group.columns and not group['soil_moist_standardized'].isn...
#             feature_cols.append('soil_moist_standardized')
#             group['moist_gradient'] = group['soil_moist_standardized'].diff()
#             feature_cols.append('moist_gradient')
        
#         # Fill missing values
#         group[feature_cols] = group[feature_cols].fillna(0)
        
#         # Create sequences
#         for i in range(len(group) - sequence_length):
#             # Get time window
#             start_time = group.iloc[i]['datetime']
#             end_time = group.iloc[i + sequence_length - 1]['datetime']
            
#             # Extract features
#             sequence = group.iloc[i:i+sequence_length][feature_cols].values
            
#             # Make prediction
#             try:
#                 # Reshape for model input
#                 X = np.array([sequence])
#                 pred_prob = model.predict(X, verbose=0)[0][0]
                
#                 # Store result
#                 prediction = {
#                     'source': site,
#                     'soil_temp_depth': temp_depth,
#                     'datetime_min': start_time,
#                     'datetime_max': end_time,
#                     'zero_curtain_probability': pred_prob,
#                     'is_zero_curtain': 1 if pred_prob > 0.5 else 0,
#                     'soil_temp_mean': group.iloc[i:i+sequence_length]['soil_temp_standardized'].me...
#                     'latitude': group.iloc[i]['latitude'] if 'latitude' in group.columns else None...
#                     'longitude': group.iloc[i]['longitude'] if 'longitude' in group.columns else N...
#                 }
#                 predictions.append(prediction)
#             except Exception as e:
#                 print(f"Error making prediction for {site} at depth {temp_depth}: {e}")
    
#     # Convert to DataFrame
#     predictions_df = pd.DataFrame(predictions)
#     print(f"Generated {len(predictions_df)} predictions")
    
#     # Consolidate overlapping events
#     if len(predictions_df) > 0:
#         print("Consolidating overlapping events...")
#         consolidated_events = consolidate_overlapping_events(predictions_df)
#         print(f"Consolidated into {len(consolidated_events)} events")
#         return consolidated_events
#     else:
#         return predictions_df


# def consolidate_overlapping_events(predictions_df, probability_threshold=0.5, gap_threshold=6):
#     """
#     Consolidate overlapping zero curtain events from model predictions.
    
#     Parameters:
#     -----------
#     predictions_df : pandas.DataFrame
#         DataFrame with model predictions
#     probability_threshold : float
#         Minimum probability to consider as zero curtain
#     gap_threshold : float
#         Maximum gap in hours to consider events as continuous
        
#     Returns:
#     --------
#     pandas.DataFrame
#         DataFrame with consolidated zero curtain events
#     """
#     consolidated_events = []
    
#     # Filter to likely zero curtain events
#     zero_curtain_events = predictions_df[predictions_df['zero_curtain_probability'] >= probability...
    
#     # Process each site and depth separately
#     for (site, depth), group in zero_curtain_events.groupby(['source', 'soil_temp_depth']):
#         # Sort by time
#         group = group.sort_values('datetime_min')
        
#         current_event = None
        
#         for _, event in group.iterrows():
#             if current_event is None:
#                 # Start a new event
#                 current_event = {
#                     'source': site,
#                     'soil_temp_depth': depth,
#                     'datetime_min': event['datetime_min'],
#                     'datetime_max': event['datetime_max'],
#                     'zero_curtain_probability': [event['zero_curtain_probability']],
#                     'soil_temp_values': [event['soil_temp_mean']],
#                     'latitude': event['latitude'],
#                     'longitude': event['longitude']
#                 }
#             else:
#                 # Check if this event overlaps or is close to the current event
#                 time_gap = (event['datetime_min'] - current_event['datetime_max']).total_seconds()...
                
#                 if time_gap <= gap_threshold:
#                     # Extend the current event
#                     current_event['datetime_max'] = max(current_event['datetime_max'], event['date...
#                     current_event['zero_curtain_probability'].append(event['zero_curtain_probabili...
#                     current_event['soil_temp_values'].append(event['soil_temp_mean'])
#                 else:
#                     # Finalize the current event
#                     duration_hours = (current_event['datetime_max'] - current_event['datetime_min'...
                    
#                     if duration_hours >= 12:  # Minimum duration threshold
#                         final_event = {
#                             'source': current_event['source'],
#                             'soil_temp_depth': current_event['soil_temp_depth'],
#                             'datetime_min': current_event['datetime_min'],
#                             'datetime_max': current_event['datetime_max'],
#                             'duration_hours': duration_hours,
#                             'zero_curtain_probability': np.mean(current_event['zero_curtain_probab...
#                             'soil_temp_mean': np.mean(current_event['soil_temp_values']),
#                             'latitude': current_event['latitude'],
#                             'longitude': current_event['longitude']
#                         }
#                         consolidated_events.append(final_event)
                    
#                     # Start a new event
#                     current_event = {
#                         'source': site,
#                         'soil_temp_depth': depth,
#                         'datetime_min': event['datetime_min'],
#                         'datetime_max': event['datetime_max'],
#                         'zero_curtain_probability': [event['zero_curtain_probability']],
#                         'soil_temp_values': [event['soil_temp_mean']],
#                         'latitude': event['latitude'],
#                         'longitude': event['longitude']
#                     }
        
#         # Handle the last event
#         if current_event is not None:
#             duration_hours = (current_event['datetime_max'] - current_event['datetime_min']).total...
            
#             if duration_hours >= 12:  # Minimum duration threshold
#                 final_event = {
#                     'source': current_event['source'],
#                     'soil_temp_depth': current_event['soil_temp_depth'],
#                     'datetime_min': current_event['datetime_min'],
#                     'datetime_max': current_event['datetime_max'],
#                     'duration_hours': duration_hours,
#                     'zero_curtain_probability': np.mean(current_event['zero_curtain_probability'])...
#                     'soil_temp_mean': np.mean(current_event['soil_temp_values']),
#                     'latitude': current_event['latitude'],
#                     'longitude': current_event['longitude']
#                 }
#                 consolidated_events.append(final_event)
    
#     # Convert to DataFrame
#     consolidated_df = pd.DataFrame(consolidated_events)
    
#     # Add region and latitude band classifications
#     if len(consolidated_df) > 0 and 'latitude' in consolidated_df.columns:
#         # Add region classification
#         consolidated_df['region'] = consolidated_df['latitude'].apply(lambda lat: 
#             'Arctic' if lat >= 66.5 else
#             'Subarctic' if lat >= 60 else
#             'Northern Boreal' if lat >= 50 else
#             'Other'
#         )
        
#         # Add latitude band
#         consolidated_df['lat_band'] = consolidated_df['latitude'].apply(lambda lat:
#             '<55°N' if lat < 55 else
#             '55-60°N' if lat < 60 else
#             '60-66.5°N' if lat < 66.5 else
#             '66.5-70°N' if lat < 70 else
#             '70-75°N' if lat < 75 else
#             '75-80°N' if lat < 80 else
#             '>80°N'
#         )
    
#     return consolidated_df


# def create_final_visualization(events_df, output_filename=None):
#     """
#     Create an optimized visualization for zero curtain events with correct handling
#     of the duration distribution.
    
#     Parameters:
#     -----------
#     events_df : pandas.DataFrame
#         DataFrame containing zero curtain events
#     output_filename : str, optional
#         If provided, save figure to this filename
        
#     Returns:
#     --------
#     tuple
#         (figure, statistics_dict)
#     """
#     # Calculate percentile boundaries instead of quartiles
#     p10 = np.percentile(events_df['duration_hours'], 10)
#     p25 = np.percentile(events_df['duration_hours'], 25)
#     p50 = np.percentile(events_df['duration_hours'], 50)
#     p75 = np.percentile(events_df['duration_hours'], 75)
#     p90 = np.percentile(events_df['duration_hours'], 90)
    
#     # Use consistent figure size and configuration
#     fig, axes = plt.subplots(1, 2, figsize=(14, 7), 
#                            subplot_kw={'projection': ccrs.NorthPolarStereo()})
    
#     # Aggregate by site for visualization
#     site_data = events_df.groupby(['source', 'latitude', 'longitude']).agg({
#         'duration_hours': ['count', 'mean', 'median', 'min', 'max'],
#         'soil_temp_depth_zone': lambda x: x.mode()[0] if not x.mode().empty else None
#     }).reset_index()
    
#     # Flatten column names
#     site_data.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col 
#                         for col in site_data.columns]
    
#     # Set map features
#     for ax in axes:
#         ax.set_extent([-180, 180, 45, 90], ccrs.PlateCarree())
#         ax.add_feature(cfeature.LAND, facecolor='whitesmoke')
#         ax.add_feature(cfeature.OCEAN, facecolor='aliceblue')
#         ax.add_feature(cfeature.COASTLINE, linewidth=0.5)
        
#         # Add Arctic Circle with label
#         ax.plot(
#             np.linspace(-180, 180, 60),
#             np.ones(60) * 66.5,
#             transform=ccrs.PlateCarree(),
#             linestyle='-',
#             color='gray',
#             linewidth=1.0,
#             alpha=0.7
#         )
        
#         ax.text(
#             0, 66.5 + 2,
#             "Arctic Circle",
#             transform=ccrs.PlateCarree(),
#             horizontalalignment='center',
#             verticalalignment='bottom',
#             fontsize=9,
#             bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.2')
#         )
    
#     # Plot 1: Event count
#     count_max = site_data['duration_hours_count'].quantile(0.95)
#     scatter1 = axes[0].scatter(
#         site_data['longitude'],
#         site_data['latitude'],
#         transform=ccrs.PlateCarree(),
#         c=site_data['duration_hours_count'],
#         s=30,
#         cmap='viridis',
#         vmin=1,
#         vmax=count_max,
#         alpha=0.8,
#         edgecolor='none'
#     )
#     plt.colorbar(scatter1, ax=axes[0], shrink=0.7, pad=0.05, label='Event Count')
#     axes[0].set_title('Zero Curtain Event Count', fontsize=12)
    
#     # Plot 2: Mean duration using percentile bounds
#     # Use percentile-based bounds instead of IQR
#     lower_bound = p10
#     upper_bound = p90
    
#     # Non-linear scaling for better color differentiation
#     scatter2 = axes[1].scatter(
#         site_data['longitude'],
#         site_data['latitude'],
#         transform=ccrs.PlateCarree(),
#         c=site_data['duration_hours_mean'],
#         s=30,
#         cmap='RdYlBu_r',
#         norm=PowerNorm(gamma=0.7, vmin=lower_bound, vmax=upper_bound),
#         alpha=0.8,
#         edgecolor='none'
#     )
    
#     # Create better colorbar with percentile markers
#     cbar = plt.colorbar(scatter2, ax=axes[1], shrink=0.7, pad=0.05, 
#                        label='Mean Duration (hours)')
    
#     # Show percentile ticks
#     percentile_ticks = [p10, p25, p50, p75, p90]
#     cbar.set_ticks(percentile_ticks)
#     cbar.set_ticklabels([f"{h:.0f}h\n({h/24:.1f}d)" for h in percentile_ticks])
    
#     axes[1].set_title('Mean Zero Curtain Duration', fontsize=12)
    
#     # Add comprehensive title with percentile information
#     plt.suptitle(
#         f'Zero Curtain Analysis: {len(site_data)} Sites, {len(events_df)} Events\n' +
#         f'Duration: median={p50:.1f}h ({p50/24:.1f}d), 10-90%={p10:.1f}-{p90:.1f}h',
#         fontsize=14
#     )
    
#     plt.tight_layout(rect=[0, 0, 1, 0.93])
    
#     # Save if requested
#     if output_filename:
#         try:
#             plt.savefig(output_filename, dpi=200, bbox_inches='tight')
#             print(f"Figure saved to {output_filename}")
#         except Exception as e:
#             print(f"Error saving figure: {e}")
    
#     return fig, {
#         'p10': p10,
#         'p25': p25,
#         'p50': p50,
#         'p75': p75,
#         'p90': p90,
#         'mean': events_df['duration_hours'].mean(),
#         'std': events_df['duration_hours'].std(),
#         'min': events_df['duration_hours'].min(),
#         'max': events_df['duration_hours'].max()
#     }


# def compare_detection_methods(physical_events, model_events, output_dir=None):
#     """
#     Compare zero curtain events detected by different methods.
    
#     Parameters:
#     -----------
#     physical_events : pandas.DataFrame
#         Events detected by the physics-based method
#     model_events : pandas.DataFrame
#         Events detected by the deep learning model
#     output_dir : str, optional
#         Directory to save comparison results
        
#     Returns:
#     --------
#     dict
#         Comparison statistics and metrics
#     """
#     # Calculate basic statistics for each method
#     physical_stats = {
#         'total_events': len(physical_events),
#         'unique_sites': physical_events['source'].nunique(),
#         'median_duration': physical_events['duration_hours'].median(),
#         'mean_duration': physical_events['duration_hours'].mean()
#     }
    
#     model_stats = {
#         'total_events': len(model_events),
#         'unique_sites': model_events['source'].nunique(),
#         'median_duration': model_events['duration_hours'].median(),
#         'mean_duration': model_events['duration_hours'].mean()
#     }
    
#     # Create a site-day matching table for overlap analysis
#     physical_days = set()
#     model_days = set()
    
#     for _, event in physical_events.iterrows():
#         site = event['source']
#         depth = event['soil_temp_depth']
#         start_day = event['datetime_min'].date()
#         end_day = event['datetime_max'].date()
        
#         # Add each day of the event
#         current_day = start_day
#         while current_day <= end_day:
#             physical_days.add((site, depth, current_day))
#             current_day += timedelta(days=1)
    
#     for _, event in model_events.iterrows():
#         site = event['source']
#         depth = event['soil_temp_depth']
#         start_day = event['datetime_min'].date()
#         end_day = event['datetime_max'].date()
        
#         # Add each day of the event
#         current_day = start_day
#         while current_day <= end_day:
#             model_days.add((site, depth, current_day))
#             current_day += timedelta(days=1)
    
#     # Calculate overlap metrics
#     overlap_days = physical_days.intersection(model_days)
    
#     overlap_metrics = {
#         'physical_only_days': len(physical_days - model_days),
#         'model_only_days': len(model_days - physical_days),
#         'overlap_days': len(overlap_days),
#         'jaccard_index': len(overlap_days) / len(physical_days.union(model_days)) if len(physical_...
#     }
    
#     # Print comparison results
#     print("\n=== DETECTION METHOD COMPARISON ===\n")
    
#     print("Physics-based Detection:")
#     print(f"  Total Events: {physical_stats['total_events']}")
#     print(f"  Unique Sites: {physical_stats['unique_sites']}")
#     print(f"  Median Duration: {physical_stats['median_duration']:.1f} hours ({physical_stats['med...
#     print(f"  Mean Duration: {physical_stats['mean_duration']:.1f} hours ({physical_stats['mean_du...
    
#     print("\nDeep Learning Model Detection:")
#     print(f"  Total Events: {model_stats['total_events']}")
#     print(f"  Unique Sites: {model_stats['unique_sites']}")
#     print(f"  Median Duration: {model_stats['median_duration']:.1f} hours ({model_stats['median_du...
#     print(f"  Mean Duration: {model_stats['mean_duration']:.1f} hours ({model_stats['mean_duration...
    
#     print("\nOverlap Analysis:")
#     print(f"  Days with Events (Physics-based): {len(physical_days)}")
#     print(f"  Days with Events (Deep Learning): {len(model_days)}")
#     print(f"  Days detected by both methods: {overlap_metrics['overlap_days']}")
#     print(f"  Days detected only by Physics-based: {overlap_metrics['physical_only_days']}")
#     print(f"  Days detected only by Deep Learning: {overlap_metrics['model_only_days']}")
#     print(f"  Jaccard Index (overlap): {overlap_metrics['jaccard_index']:.4f}")
    
#     # Generate comparison visualizations
#     if output_dir:
#         os.makedirs(output_dir, exist_ok=True)
        
#         # Create a Venn diagram of detection overlap
#         try:
#             from matplotlib_venn import venn2
            
#             plt.figure(figsize=(8, 6))
#             venn2(subsets=(len(physical_days - model_days), 
#                           len(model_days - physical_days), 
#                           len(overlap_days)),
#                  set_labels=('Physics-based', 'Deep Learning'))
#             plt.title('Overlap between Detection Methods', fontsize=14)
#             plt.savefig(os.path.join(output_dir, 'detection_overlap.png'), dpi=200, bbox_inches='t...
#         except ImportError:
#             print("matplotlib_venn not installed. Skipping Venn diagram.")
        
#         # Compare duration distributions
#         plt.figure(figsize=(10, 6))
        
#         sns.histplot(physical_events['duration_hours'], kde=True, alpha=0.5, 
#                     label='Physics-based', color='blue', bins=50)
#         sns.histplot(model_events['duration_hours'], kde=True, alpha=0.5, 
#                     label='Deep Learning', color='red', bins=50)
        
#         plt.xlabel('Duration (hours)')
#         plt.ylabel('Frequency')
#         plt.title('Comparison of Zero Curtain Duration Distributions')
#         plt.legend()
#         plt.grid(alpha=0.3)
        
#         plt.savefig(os.path.join(output_dir, 'duration_comparison.png'), dpi=200, bbox_inches='tig...
    
#     comparison_results = {
#         'physical_stats': physical_stats,
#         'model_stats': model_stats,
#         'overlap_metrics': overlap_metrics
#     }
    
#     return comparison_results


# def diagnose_zero_curtain_durations(events_df, output_dir=None):
#     """
#     Comprehensive analysis of zero curtain duration patterns to identify
#     the source of value clustering and recommend fixes.
    
#     Parameters:
#     -----------
#     events_df : pandas.DataFrame
#         DataFrame containing zero curtain events
#     output_dir : str, optional
#         Directory to save diagnostic outputs
        
#     Returns:
#     --------
#     dict
#         Dictionary containing diagnostic results
#     """
#     print("=" * 80)
#     print("ZERO CURTAIN DURATION PATTERN ANALYSIS")
#     print("=" * 80)
    
#     # 1. Basic statistics
#     duration_values = events_df['duration_hours'].values
#     n_events = len(duration_values)
#     n_unique = len(np.unique(duration_values))
    
#     print(f"\nTotal events: {n_events}")
#     print(f"Unique duration values: {n_unique} ({n_unique/n_events*100:.1f}%)")
    
#     # Calculate full statistics
#     stats_df = pd.DataFrame({
#         'Statistic': ['Mean', 'Median', 'Std Dev', 'Min', 'Max', 'Q1', 'Q3', 'IQR'],
#         'Value': [
#             duration_values.mean(),
#             np.median(duration_values),
#             duration_values.std(),
#             duration_values.min(),
#             duration_values.max(),
#             np.percentile(duration_values, 25),
#             np.percentile(duration_values, 75),
#             np.percentile(duration_values, 75) - np.percentile(duration_values, 25)
#         ]
#     })
    
#     print("\nBasic Statistics:")
#     for _, row in stats_df.iterrows():
#         print(f"  {row['Statistic']}: {row['Value']:.2f}")
    
#     # 2. Value frequency analysis
#     from collections import Counter
#     value_counts = Counter(duration_values)
#     common_values = pd.Series(value_counts).sort_values(ascending=False).head(20)
    
#     print("\nMost common duration values:")
#     for val, count in common_values.items():
#         print(f"  {val:.2f} hours: {count} events ({count/n_events*100:.1f}%)")
    
#     # 3. Check for patterns in the values
#     # Are values clustering at specific intervals?
#     rounded_to_hour = np.round(duration_values)
#     rounded_to_day = np.round(duration_values / 24) * 24
    
#     print("\nRounding patterns:")
#     print(f"  Events matching exact hours: {np.sum(duration_values == rounded_to_hour)} ({np.sum(d...
#     print(f"  Events matching exact days: {np.sum(duration_values == rounded_to_day)} ({np.sum(dur...
    
#     # Check for specific hour intervals (6h, 12h, 24h)
#     for interval in [1, 6, 12, 24]:
#         rounded = np.round(duration_values / interval) * interval
#         match_pct = np.sum(duration_values == rounded) / n_events * 100
#         print(f"  Events at {interval}h intervals: {np.sum(duration_values == rounded)} ({match_pc...
    
#     # 4. Create visualizations
#     # Distribution plot
#     plt.figure(figsize=(12, 5))
    
#     plt.subplot(121)
#     sns.histplot(duration_values, bins=50, kde=True)
#     plt.title('Distribution of Duration Values')
#     plt.xlabel('Duration (hours)')
#     plt.ylabel('Frequency')
    
#     # Log scale for better visibility
#     plt.subplot(122)
#     sns.histplot(duration_values, bins=50, kde=True, log_scale=(False, True))
#     plt.title('Distribution (Log Scale)')
#     plt.xlabel('Duration (hours)')
#     plt.ylabel('Log Frequency')
    
#     if output_dir:
#         os.makedirs(output_dir, exist_ok=True)
#         plt.tight_layout()
#         plt.savefig(os.path.join(output_dir, 'distribution.png'), dpi=150)
    
#     # Duration value heatmap
#     # Create a 2D histogram based on:
#     # X-axis: Duration value
#     # Y-axis: Remainder when divided by 24 (to check for daily patterns)
    
#     # Filter to reasonable range for visualization
#     filtered_durations = duration_values[duration_values <= np.percentile(duration_values, 95)]
    
#     plt.figure(figsize=(14, 6))
    
#     # 2D histogram for patterns
#     hours_remainder = filtered_durations % 24
#     plt.subplot(121)
#     plt.hist2d(
#         filtered_durations, 
#         hours_remainder, 
#         bins=[50, 24],
#         cmap='viridis'
#     )
#     plt.colorbar(label='Count')
#     plt.title('Duration Patterns')
#     plt.xlabel('Duration (hours)')
#     plt.ylabel('Hours Remainder (duration % 24)')
    
#     # Plot the relationship between durations and observation count
#     plt.subplot(122)
    
#     # Group by source and calculate stats
#     site_durations = events_df.groupby('source').agg({
#         'duration_hours': ['count', 'mean', 'median']
#     })
    
#     site_durations.columns = ['_'.join(col).strip('_') for col in site_durations.columns]
    
#     plt.scatter(
#         site_durations['duration_hours_count'],
#         site_durations['duration_hours_mean'],
#         alpha=0.5
#     )
#     plt.title('Relationship: Event Count vs Duration')
#     plt.xlabel('Number of Events per Site')
#     plt.ylabel('Mean Duration (hours)')
#     plt.grid(alpha=0.3)
    
#     if output_dir:
#         plt.tight_layout()
#         plt.savefig(os.path.join(output_dir, 'patterns.png'), dpi=150)
    
#     # 5. Check for algorithm artifacts
#     # Look for signs of temporal aliasing or measurement effects
#     temporal_patterns = events_df.copy()
    
#     # Extract detection algorithm artifacts if datetime columns exist
#     if 'datetime_min' in events_df.columns and 'datetime_max' in events_df.columns:
#         if not pd.api.types.is_datetime64_dtype(events_df['datetime_min']):
#             temporal_patterns['datetime_min'] = pd.to_datetime(events_df['datetime_min'])
#         if not pd.api.types.is_datetime64_dtype(events_df['datetime_max']):
#             temporal_patterns['datetime_max'] = pd.to_datetime(events_df['datetime_max'])
            
#         # Extract time of day and day of week
#         temporal_patterns['start_hour'] = temporal_patterns['datetime_min'].dt.hour
#         temporal_patterns['end_hour'] = temporal_patterns['datetime_max'].dt.hour
#         temporal_patterns['start_day'] = temporal_patterns['datetime_min'].dt.dayofweek
#         temporal_patterns['end_day'] = temporal_patterns['datetime_max'].dt.dayofweek
        
#         # Plot patterns
#         fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        
#         sns.histplot(data=temporal_patterns, x='start_hour', kde=True, ax=axes[0, 0])
#         axes[0, 0].set_title('Event Start Hour')
        
#         sns.histplot(data=temporal_patterns, x='end_hour', kde=True, ax=axes[0, 1])
#         axes[0, 1].set_title('Event End Hour')
        
#         sns.histplot(data=temporal_patterns, x='start_day', kde=True, ax=axes[1, 0])
#         axes[1, 0].set_title('Event Start Day')
#         axes[1, 0].set_xticks(range(7))
#         axes[1, 0].set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])
        
#         sns.histplot(data=temporal_patterns, x='end_day', kde=True, ax=axes[1, 1])
#         axes[1, 1].set_title('Event End Day')
#         axes[1, 1].set_xticks(range(7))
#         axes[1, 1].set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])
        
#         if output_dir:
#             plt.tight_layout()
#             plt.savefig(os.path.join(output_dir, 'temporal.png'), dpi=150)
    
#     # 6. Correlation with other variables
#     # Check if duration is correlated with geographic location
#     correlation_data = events_df.copy()
    
#     plt.figure(figsize=(14, 5))
    
#     plt.subplot(121)
#     plt.scatter(
#         correlation_data['latitude'],
#         correlation_data['duration_hours'],
#         alpha=0.1,
#         s=3
#     )
#     plt.title('Duration vs Latitude')
#     plt.xlabel('Latitude')
#     plt.ylabel('Duration (hours)')
#     plt.grid(alpha=0.3)
    
#     plt.subplot(122)
#     plt.scatter(
#         correlation_data['soil_temp_depth'],
#         correlation_data['duration_hours'],
#         alpha=0.1,
#         s=3
#     )
#     plt.title('Duration vs Soil Depth')
#     plt.xlabel('Soil Temperature Depth')
#     plt.ylabel('Duration (hours)')
#     plt.grid(alpha=0.3)
    
#     if output_dir:
#         plt.tight_layout()
#         plt.savefig(os.path.join(output_dir, 'correlations.png'), dpi=150)
    
#     # 7. Generate recommendations based on findings
#     print("\n" + "=" * 80)
#     print("DIAGNOSIS AND RECOMMENDATIONS")
#     print("=" * 80)
    
#     # Check for daily measurement effects
#     day_effect = np.sum(duration_values % 24 == 0) / n_events * 100
#     if day_effect > 50:
#         print("\nDiagnosis: Strong daily measurement effect detected.")
#         print(f"  {day_effect:.1f}% of durations are exact multiples of 24 hours.")
#         print("  This suggests temporal aliasing due to measurement frequency.")
#         print("\nRecommendation:")
#         print("  1. Review zero_curtain_detection temporal parameters")
#         print("  2. Decrease 'max_gap_hours' to allow for more fine-grained detection")
#         print("  3. Apply interpolation to estimate more precise event transitions")
    
#     # Check for binning or rounding
#     if n_unique / n_events < 0.1:
#         print("\nDiagnosis: Severe value binning or rounding detected.")
#         print(f"  Only {n_unique} unique values for {n_events} events ({n_unique/n_events*100:.1f}...
#         print("\nRecommendation:")
#         print("  1. Check for explicit rounding in duration calculations")
#         print("  2. Use higher precision timestamps for event boundaries")
#         print("  3. Consider continuous time representation instead of discrete bins")
    
#     # Check for IQR issues
#     q1 = np.percentile(duration_values, 25)
#     q3 = np.percentile(duration_values, 75)
#     iqr = q3 - q1
    
#     if iqr < 1e-6:
#         print("\nDiagnosis: Zero or near-zero IQR detected.")
#         print(f"  Q1 and Q3 are both {q1:.2f}, creating visualization challenges.")
#         print("\nRecommendation:")
#         print("  1. Use percentile-based visualization bounds instead of IQR")
#         print("  2. For visualization, force a minimum range based on domain knowledge")
#         print("  3. Consider a non-linear transformation of duration values")
    
#     # Return diagnostic results
#     return {
#         'n_events': n_events,
#         'n_unique': n_unique,
#         'statistics': stats_df,
#         'common_values': common_values,
#         'day_effect_pct': day_effect,
#         'q1': q1,
#         'q3': q3,
#         'iqr': iqr
#     }


# def run_full_analysis_pipeline(merged_df, output_base_dir='results'):
#     """
#     Run the complete zero curtain analysis pipeline with progress tracking.
    
#     Parameters:
#     -----------
#     merged_df : pandas.DataFrame
#         The input merged dataframe containing all measurements
#     output_base_dir : str
#         Base directory for saving all outputs
        
#     Returns:
#     --------
#     dict
#         Dictionary containing all analysis results
#     """
#     from tqdm.auto import tqdm
#     import time
#     import os
    
#     # Create output directories
#     os.makedirs(output_base_dir, exist_ok=True)
    
#     results = {}
    
#     # Add a progress indicator for the overall workflow
#     stages = ['Enhanced Detection', 'Model Training', 'Model Application', 'Visualization', 'Compa...
    
#     with tqdm(total=len(stages), desc="Overall Progress") as pbar:
#         # Stage 1: Enhanced physical detection
#         start_time = time.time()
#         pbar.set_description(f"Stage 1/5: {stages[0]}")
        
#         enhanced_events = enhanced_zero_curtain_detection(
#             merged_df,
#             output_dir=os.path.join(output_base_dir, 'enhanced')
#         )
        
#         results['enhanced_events'] = enhanced_events
#         results['enhanced_time'] = time.time() - start_time
#         pbar.update(1)
        
#         # Diagnostic analysis before proceeding
#         diagnostics = diagnose_zero_curtain_durations(
#             enhanced_events,
#             output_dir=os.path.join(output_base_dir, 'diagnostics')
#         )
#         results['diagnostics'] = diagnostics
        
#         # Stage 2: Train deep learning model
#         start_time = time.time()
#         pbar.set_description(f"Stage 2/5: {stages[1]}")
        
#         try:
#             model, history, evaluation = train_zero_curtain_model(
#                 merged_df,
#                 enhanced_events,
#                 output_dir=os.path.join(output_base_dir, 'model')
#             )
            
#             results['model'] = model
#             results['model_history'] = history
#             results['model_evaluation'] = evaluation
#             results['model_training_time'] = time.time() - start_time
#         except Exception as e:
#             print(f"Error in model training: {str(e)}")
#             results['model_error'] = str(e)
        
#         pbar.update(1)
        
#         # Stage 3: Apply model to data
#         start_time = time.time()
#         pbar.set_description(f"Stage 3/5: {stages[2]}")
        
#         try:
#             if 'model' in results:
#                 model_predictions = apply_model_to_new_data(
#                     results['model'],
#                     merged_df,
#                     sequence_length=24
#                 )
                
#                 results['model_predictions'] = model_predictions
#                 results['model_application_time'] = time.time() - start_time
#             else:
#                 print("Skipping model application due to training error")
#         except Exception as e:
#             print(f"Error in model application: {str(e)}")
#             results['model_application_error'] = str(e)
        
#         pbar.update(1)
        
#         # Stage 4: Create visualizations
#         start_time = time.time()
#         pbar.set_description(f"Stage 4/5: {stages[3]}")
        
#         # Visualize the enhanced detection results
#         fig1, stats1 = create_final_visualization(
#             enhanced_events,
#             os.path.join(output_base_dir, 'enhanced_events_visualization.png')
#         )
#         results['enhanced_visualization'] = stats1
        
#         # Visualize model predictions if available
#         if 'model_predictions' in results and len(results['model_predictions']) > 0:
#             fig2, stats2 = create_final_visualization(
#                 results['model_predictions'],
#                 os.path.join(output_base_dir, 'model_predictions_visualization.png')
#             )
#             results['model_visualization'] = stats2
        
#         results['visualization_time'] = time.time() - start_time
#         pbar.update(1)
        
#         # Stage 5: Compare methods
#         start_time = time.time()
#         pbar.set_description(f"Stage 5/5: {stages[4]}")
        
#         if 'model_predictions' in results and len(results['model_predictions']) > 0:
#             comparison = compare_detection_methods(
#                 enhanced_events,
#                 results['model_predictions'],
#                 output_dir=os.path.join(output_base_dir, 'comparison')
#             )
#             results['comparison'] = comparison
        
#         results['comparison_time'] = time.time() - start_time
#         pbar.update(1)
    
#     # Generate final summary report
#     total_time = (results.get('enhanced_time', 0) + 
#                  results.get('model_training_time', 0) + 
#                  results.get('model_application_time', 0) +
#                  results.get('visualization_time', 0) +
#                  results.get('comparison_time', 0))
    
#     print("\n" + "=" * 80)
#     print("ZERO CURTAIN ANALYSIS COMPLETE")
#     print("=" * 80)
    
#     print(f"\nTotal runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
#     print(f"Output directory: {output_base_dir}")
    
#     print("\nSummary of Results:")
#     print(f"  Enhanced Detection: {len(enhanced_events)} events detected")
    
#     if 'model_predictions' in results:
#         print(f"  Deep Learning Model: {len(results['model_predictions'])} events detected")
    
#     if 'comparison' in results:
#         overlap = results['comparison']['overlap_metrics']['jaccard_index']
#         print(f"  Method Agreement: {overlap*100:.1f}% overlap between methods")
    
#     # Save summary to file
#     with open(os.path.join(output_base_dir, 'summary.txt'), 'w') as f:
#         f.write("ZERO CURTAIN ANALYSIS SUMMARY\n")
#         f.write("=" * 30 + "\n\n")
        
#         f.write(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\n")
#         f.write(f"Enhanced Detection: {len(enhanced_events)} events detected\n")
        
#         if 'model_predictions' in results:
#             f.write(f"Deep Learning Model: {len(results['model_predictions'])} events detected\n")
        
#         if 'comparison' in results:
#             overlap = results['comparison']['overlap_metrics']['jaccard_index']
#             f.write(f"Method Agreement: {overlap*100:.1f}% overlap between methods\n")
    
#     return results

# def process_single_site(site_df, site, temp_depth):
#     """
#     Process a single site-depth combination to detect zero curtain events.
    
#     Parameters:
#     -----------
#     site_df : pandas.DataFrame
#         DataFrame containing data for a single site and depth
#     site : str
#         Site identifier
#     temp_depth : float
#         Soil temperature depth value
        
#     Returns:
#     --------
#     list
#         List of detected zero curtain events for this site-depth
#     """
#     # Initialize list to hold events
#     site_data = []
    
#     # Sort by datetime
#     site_df = site_df.sort_values('datetime')
    
#     # Calculate temperature derivatives
#     site_df['temp_gradient'] = site_df['soil_temp_standardized'].diff() / \
#                              (site_df['datetime'].diff().dt.total_seconds() / 3600)
    
#     # Identify potential phase change periods
#     # Zero curtain = temp near 0°C AND reduced temperature gradient
#     mask_temp = (site_df['soil_temp_standardized'].abs() <= 0.5)
#     mask_gradient = (site_df['temp_gradient'].abs() <= 0.02)  # Reduced rate of change
    
#     # Find corresponding soil moisture data for this site, if available
#     use_moisture = False
    
#     # Check if soil moisture data exists for this site
#     if 'soil_moist_standardized' in site_df.columns and not site_df['soil_moist_standardized'].isn...
#         use_moisture = True
        
#         # Calculate moisture gradient - helpful for phase change detection
#         site_df['moist_gradient'] = site_df['soil_moist_standardized'].diff() / \
#                                   (site_df['datetime'].diff().dt.total_seconds() / 3600)
        
#         # Find closest moisture depth to current temperature depth
#         if 'soil_moist_depth' in site_df.columns and not site_df['soil_moist_depth'].isna().all():
#             # Record moisture depth for each record
#             site_df['closest_moist_depth'] = site_df['soil_moist_depth']
#         else:
#             # If explicit depth not available, note as missing
#             site_df['closest_moist_depth'] = np.nan
        
#         # Moisture changes during phase change (phase change affects moisture readings)
#         mask_moisture = (site_df['moist_gradient'].abs() >= 0.0005)
        
#         # Combined mask with moisture information
#         combined_mask = mask_temp & (mask_gradient | mask_moisture)
#     else:
#         # Fall back to just temperature if no moisture data
#         combined_mask = mask_temp & mask_gradient
    
#     # Find continuous events
#     site_df['zero_curtain_flag'] = combined_mask
#     site_df['event_start'] = combined_mask & ~combined_mask.shift(1, fill_value=False)
#     site_df['event_end'] = combined_mask & ~combined_mask.shift(-1, fill_value=False)
    
#     # Extract events
#     event_starts = site_df[site_df['event_start']]['datetime'].tolist()
#     event_ends = site_df[site_df['event_end']]['datetime'].tolist()
    
#     if len(event_starts) == 0 or len(event_ends) == 0:
#         return []  # No events found
        
#     # Handle unmatched start/end points
#     if len(event_starts) > len(event_ends):
#         event_starts = event_starts[:len(event_ends)]
#     elif len(event_ends) > len(event_starts):
#         event_ends = event_ends[:len(event_starts)]
    
#     # Calculate duration and other event properties
#     for start, end in zip(event_starts, event_ends):
#         event_duration = (end - start).total_seconds() / 3600
        
#         # Filter for minimum duration
#         if event_duration < 12:
#             continue
            
#         # Get event data
#         event_data = site_df[(site_df['datetime'] >= start) & (site_df['datetime'] <= end)]
        
#         if len(event_data) < 3:
#             continue
        
#         # Create event information dictionary with comprehensive soil temperature stats
#         event_info = {
#             'source': site,
#             'soil_temp_depth': temp_depth,
#             'soil_temp_depth_zone': event_data['soil_temp_depth_zone'].iloc[0] if 'soil_temp_depth...
#             'datetime_min': start,
#             'datetime_max': end,
#             'duration_hours': event_duration,
#             'observation_count': len(event_data),
#             'observations_per_day': len(event_data) / (event_duration / 24) if event_duration > 0 ...
#             'soil_temp_mean': event_data['soil_temp_standardized'].mean(),
#             'soil_temp_min': event_data['soil_temp_standardized'].min(),
#             'soil_temp_max': event_data['soil_temp_standardized'].max(),
#             'soil_temp_std': event_data['soil_temp_standardized'].std(),
#             'season': event_data['season'].mode().iloc[0] if 'season' in event_data.columns else N...
#             'latitude': event_data['latitude'].iloc[0] if 'latitude' in event_data.columns else No...
#             'longitude': event_data['longitude'].iloc[0] if 'longitude' in event_data.columns else...
#             'year': event_data['year'].iloc[0] if 'year' in event_data.columns else None,
#             'month': start.month
#         }
        
#         # Add soil moisture metrics if available
#         if use_moisture and not event_data['soil_moist_standardized'].isna().all():
#             # Calculate comprehensive moisture statistics
#             event_info['soil_moist_mean'] = event_data['soil_moist_standardized'].mean()
#             event_info['soil_moist_std'] = event_data['soil_moist_standardized'].std()
#             event_info['soil_moist_min'] = event_data['soil_moist_standardized'].min()
#             event_info['soil_moist_max'] = event_data['soil_moist_standardized'].max()
#             event_info['soil_moist_change'] = event_info['soil_moist_max'] - event_info['soil_mois...
            
#             # Add moisture depth information
#             if not event_data['closest_moist_depth'].isna().all():
#                 # Use the most common moisture depth during the event
#                 event_info['soil_moist_depth'] = event_data['closest_moist_depth'].mode().iloc[0]
#             else:
#                 event_info['soil_moist_depth'] = np.nan
#         else:
#             # Include placeholder values if moisture data not available
#             event_info['soil_moist_mean'] = np.nan
#             event_info['soil_moist_std'] = np.nan
#             event_info['soil_moist_min'] = np.nan
#             event_info['soil_moist_max'] = np.nan
#             event_info['soil_moist_change'] = np.nan
#             event_info['soil_moist_depth'] = np.nan
        
#         # Calculate latent heat effects (if sufficient data exists)
#         # This captures the energy involved in the phase change
#         has_temp_gradient = not event_data['temp_gradient'].isna().all()
#         if has_temp_gradient:
#             event_info['temp_gradient_mean'] = event_data['temp_gradient'].mean()
#             event_info['temp_stability'] = event_data['temp_gradient'].abs().mean()
        
#         # Add year-month identifier for temporal analysis
#         event_info['year_month'] = f"{event_info['year']}-{event_info['month']:02d}"
        
#         # Add region classification based on latitude
#         if 'latitude' in event_info and event_info['latitude'] is not None:
#             if event_info['latitude'] >= 66.5:
#                 event_info['region'] = 'Arctic'
#             elif event_info['latitude'] >= 60:
#                 event_info['region'] = 'Subarctic'
#             elif event_info['latitude'] >= 50:
#                 event_info['region'] = 'Northern Boreal'
#             else:
#                 event_info['region'] = 'Other'
#         else:
#             event_info['region'] = None
            
#         # Calculate latitude band for detailed spatial analysis
#         if 'latitude' in event_info and event_info['latitude'] is not None:
#             lat = event_info['latitude']
#             if lat < 55:
#                 event_info['lat_band'] = '<55°N'
#             elif lat < 60:
#                 event_info['lat_band'] = '55-60°N'
#             elif lat < 66.5:
#                 event_info['lat_band'] = '60-66.5°N'
#             elif lat < 70:
#                 event_info['lat_band'] = '66.5-70°N'
#             elif lat < 75:
#                 event_info['lat_band'] = '70-75°N'
#             elif lat < 80:
#                 event_info['lat_band'] = '75-80°N'
#             else:
#                 event_info['lat_band'] = '>80°N'
#         else:
#             event_info['lat_band'] = None
        
#         site_data.append(event_info)
    
#     return site_data


# def run_full_analysis_pipeline_with_memory_management(merged_df, output_base_dir='results', use_ch...
#     """
#     Run the complete zero curtain analysis pipeline with progress tracking,
#     memory management, and checkpoint saving.
    
#     Parameters:
#     -----------
#     merged_df : pandas.DataFrame
#         The input merged dataframe containing all measurements
#     output_base_dir : str
#         Base directory for saving all outputs
#     use_checkpoints : bool
#         Whether to use checkpointing for the enhanced detection
#     batch_size : int
#         Number of site-depth combinations to process per batch
        
#     Returns:
#     --------
#     dict
#         Dictionary containing all analysis results
#     """
#     from tqdm.auto import tqdm
#     import time
#     import os
#     import gc
#     import pickle
#     import psutil  # For memory monitoring
    
#     # Create output directories
#     os.makedirs(output_base_dir, exist_ok=True)
#     checkpoint_dir = os.path.join(output_base_dir, 'checkpoints')
#     os.makedirs(checkpoint_dir, exist_ok=True)
    
#     # Function to monitor memory usage
#     def memory_usage():
#         process = psutil.Process(os.getpid())
#         mem_info = process.memory_info()
#         return mem_info.rss / 1024**2  # Memory in MB
    
#     # Function to save checkpoint
#     def save_checkpoint(data, name):
#         with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'wb') as f:
#             pickle.dump(data, f)
    
#     # Function to load checkpoint
#     def load_checkpoint(name):
#         try:
#             with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'rb') as f:
#                 return pickle.load(f)
#         except:
#             return None
    
#     # Initialize results
#     results = load_checkpoint('pipeline_results') or {}
    
#     # Check for completed stages
#     completed_stages = set(results.get('completed_stages', []))
#     print(f"Found {len(completed_stages)} completed stages: {completed_stages}")
    
#     # Add a progress indicator for the overall workflow
#     stages = ['Enhanced Detection', 'Model Training', 'Model Application', 'Visualization', 'Compa...
    
#     with tqdm(total=len(stages), desc="Overall Progress", initial=len(completed_stages)) as pbar:
#         # Stage 1: Enhanced physical detection
#         if 'Enhanced Detection' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 1/5: {stages[0]}")
            
#             print(f"Current memory usage: {memory_usage():.1f} MB")
            
#             if use_checkpoints:
#                 # Check if we have cached results
#                 enhanced_events = load_checkpoint('enhanced_events')
                
#                 if enhanced_events is None:
#                     # Use batch processing to manage memory
#                     enhanced_events = []
                    
#                     # Get site-depth combinations
#                     site_depths = merged_df.groupby(['source', 'soil_temp_depth']).size().reset_in...
#                     total_combinations = len(site_depths)
                    
#                     for batch_start in range(0, total_combinations, batch_size):
#                         batch_end = min(batch_start + batch_size, total_combinations)
#                         print(f"\nProcessing batch: sites {batch_start+1} to {batch_end} of {total...
                        
#                         batch_results = []
                        
#                         for i in tqdm(range(batch_start, batch_end), desc="Processing sites"):
#                             site = site_depths.iloc[i]['source']
#                             temp_depth = site_depths.iloc[i]['soil_temp_depth']
                            
#                             # Filter data for this site-depth
#                             site_df = merged_df[(merged_df['source'] == site) & 
#                                               (merged_df['soil_temp_depth'] == temp_depth)].copy()
                            
#                             if len(site_df) < 10:  # Skip sites with insufficient data
#                                 continue
                            
#                             # Process this site using process_single_site function
#                             try:
#                                 site_events = process_single_site(site_df, site, temp_depth)
#                                 batch_results.extend(site_events)
#                             except Exception as e:
#                                 print(f"Error processing site {site}, depth {temp_depth}: {str(e)}...
                            
#                             # Clear memory
#                             del site_df
#                             gc.collect()
                        
#                         # Add batch results
#                         enhanced_events.extend(batch_results)
                        
#                         # Save checkpoint after each batch
#                         save_checkpoint(enhanced_events, 'enhanced_events')
                        
#                         # Also save as CSV
#                         if batch_results:  # Only if we have results
#                             temp_df = pd.DataFrame(batch_results)
#                             os.makedirs(os.path.join(output_base_dir, 'enhanced'), exist_ok=True)
#                             temp_df.to_csv(os.path.join(output_base_dir, 'enhanced', 
#                                           f'events_batch_{batch_start}_{batch_end}.csv'), index=Fa...
                        
#                         # Clear memory
#                         del batch_results
#                         gc.collect()
                        
#                         print(f"Memory usage after batch: {memory_usage():.1f} MB")
                    
#                     # Convert final results to DataFrame if not empty
#                     if enhanced_events:
#                         enhanced_events = pd.DataFrame(enhanced_events)
#                     else:
#                         enhanced_events = pd.DataFrame()  # Empty DataFrame
#                 else:
#                     # If enhanced_events was loaded from checkpoint and it's a list, convert to Da...
#                     if isinstance(enhanced_events, list):
#                         if enhanced_events:
#                             enhanced_events = pd.DataFrame(enhanced_events)
#                         else:
#                             enhanced_events = pd.DataFrame()
#             else:
#                 # Use regular detection without batching
#                 enhanced_events = enhanced_zero_curtain_detection(
#                     merged_df,
#                     output_dir=os.path.join(output_base_dir, 'enhanced')
#                 )
            
#             results['enhanced_events'] = enhanced_events
#             results['enhanced_time'] = time.time() - start_time
            
#             # Save progress
#             completed_stages.add('Enhanced Detection')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             # Run diagnostics only if we have events
#             if len(enhanced_events) > 0:
#                 diagnostics = diagnose_zero_curtain_durations(
#                     enhanced_events,
#                     output_dir=os.path.join(output_base_dir, 'diagnostics')
#                 )
#                 results['diagnostics'] = diagnostics
#                 save_checkpoint(results, 'pipeline_results')
#             else:
#                 print("No events detected, skipping diagnostics.")
            
#             pbar.update(1)
            
#             # Clear memory before next stage
#             gc.collect()
#             print(f"Memory usage after Stage 1: {memory_usage():.1f} MB")
#         else:
#             # Skip this stage if already done
#             if 'enhanced_events' not in results:
#                 enhanced_events = load_checkpoint('enhanced_events')
#                 if isinstance(enhanced_events, list) and enhanced_events:
#                     enhanced_events = pd.DataFrame(enhanced_events)
#                 elif enhanced_events is None:
#                     enhanced_events = pd.DataFrame()
#                 results['enhanced_events'] = enhanced_events
        
#         # Stage 2: Train deep learning model
#         if 'Model Training' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 2/5: {stages[1]}")
            
#             try:
#                 # Get enhanced events if needed
#                 enhanced_events = results.get('enhanced_events')
#                 if enhanced_events is None or len(enhanced_events) == 0:
#                     enhanced_events = load_checkpoint('enhanced_events')
#                     if isinstance(enhanced_events, list) and enhanced_events:
#                         enhanced_events = pd.DataFrame(enhanced_events)
#                     elif enhanced_events is None:
#                         # Try loading from CSV
#                         csv_path = os.path.join(output_base_dir, 'enhanced', 'enhanced_zero_curtai...
#                         if os.path.exists(csv_path):
#                             enhanced_events = pd.read_csv(csv_path, parse_dates=['datetime_min', '...
                
#                 if enhanced_events is not None and len(enhanced_events) > 0:
#                     model, history, evaluation = train_zero_curtain_model(
#                         merged_df,
#                         enhanced_events,
#                         output_dir=os.path.join(output_base_dir, 'model')
#                     )
                    
#                     results['model'] = model
#                     results['model_history'] = history
#                     results['model_evaluation'] = evaluation
#                     results['model_training_time'] = time.time() - start_time
                    
#                     # Save model separately (it's large)
#                     try:
#                         import tensorflow as tf
#                         model_path = os.path.join(output_base_dir, 'model', 'zero_curtain_model.h5...
#                         os.makedirs(os.path.dirname(model_path), exist_ok=True)
#                         model.save(model_path)
                        
#                         # Remove model from results dictionary to save memory
#                         results.pop('model', None)
#                     except Exception as e:
#                         print(f"Error saving model: {str(e)}")
#                 else:
#                     print("Skipping model training: No enhanced events available")
#                     results['model_error'] = "No enhanced events available"
#             except Exception as e:
#                 print(f"Error in model training: {str(e)}")
#                 results['model_error'] = str(e)
            
#             # Save progress
#             completed_stages.add('Model Training')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             pbar.update(1)
            
#             # Clear memory before next stage
#             gc.collect()
#             print(f"Memory usage after Stage 2: {memory_usage():.1f} MB")
        
#         # Stage 3: Apply model to data
#         if 'Model Application' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 3/5: {stages[2]}")
            
#             try:
#                 # Load model if needed
#                 model = results.get('model', None)
#                 if model is None:
#                     model_path = os.path.join(output_base_dir, 'model', 'zero_curtain_model.h5')
#                     if os.path.exists(model_path):
#                         try:
#                             import tensorflow as tf
#                             model = tf.keras.models.load_model(model_path)
#                         except Exception as e:
#                             print(f"Error loading model: {str(e)}")
#                             model = None
                
#                 if model is not None:
#                     # Create output directory for predictions
#                     os.makedirs(os.path.join(output_base_dir, 'predictions'), exist_ok=True)
                    
#                     # Use batched processing for model application
#                     site_depths = merged_df.groupby(['source', 'soil_temp_depth']).size().reset_in...
#                     total_combinations = len(site_depths)
                    
#                     # Initialize empty list for predictions
#                     all_predictions = []
                    
#                     for batch_start in range(0, total_combinations, batch_size):
#                         batch_end = min(batch_start + batch_size, total_combinations)
#                         print(f"\nApplying model to batch: sites {batch_start+1} to {batch_end} of...
                        
#                         batch_sites = site_depths.iloc[batch_start:batch_end]
                        
#                         # Filter merged_df for just these sites/depths
#                         batch_df = merged_df[
#                             merged_df['source'].isin(batch_sites['source']) & 
#                             merged_df['soil_temp_depth'].isin(batch_sites['soil_temp_depth'])
#                         ].copy()
                        
#                         if len(batch_df) > 0:
#                             try:
#                                 batch_predictions = apply_model_to_new_data(
#                                     model,
#                                     batch_df,
#                                     sequence_length=24
#                                 )
                                
#                                 if len(batch_predictions) > 0:
#                                     all_predictions.append(batch_predictions)
                                    
#                                     # Save batch results
#                                     batch_predictions.to_csv(
#                                         os.path.join(output_base_dir, 'predictions', 
#                                                   f'predictions_batch_{batch_start}_{batch_end}.cs...
#                                         index=False
#                                     )
#                             except Exception as e:
#                                 print(f"Error processing batch {batch_start}-{batch_end}: {str(e)}...
                        
#                         # Clear memory
#                         del batch_df
#                         gc.collect()
                    
#                     # Combine all predictions if we have any
#                     if all_predictions:
#                         model_predictions = pd.concat(all_predictions, ignore_index=True)
#                         model_predictions.to_csv(os.path.join(output_base_dir, 'model_predictions....
                        
#                         results['model_predictions'] = model_predictions
#                         results['model_application_time'] = time.time() - start_time
                        
#                         # Save checkpoint
#                         save_checkpoint(model_predictions, 'model_predictions')
#                     else:
#                         print("No predictions generated.")
#                         results['model_predictions'] = pd.DataFrame()
#                 else:
#                     print("Skipping model application: model not available")
#             except Exception as e:
#                 print(f"Error in model application: {str(e)}")
#                 results['model_application_error'] = str(e)
            
#             # Save progress
#             completed_stages.add('Model Application')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             pbar.update(1)
            
#             # Clear memory before next stage
#             gc.collect()
#             print(f"Memory usage after Stage 3: {memory_usage():.1f} MB")
        
#         # Stage 4: Create visualizations
#         if 'Visualization' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 4/5: {stages[3]}")
            
#             # Get required data
#             enhanced_events = results.get('enhanced_events')
#             if enhanced_events is None or len(enhanced_events) == 0:
#                 enhanced_events = load_checkpoint('enhanced_events')
#                 if isinstance(enhanced_events, list) and enhanced_events:
#                     enhanced_events = pd.DataFrame(enhanced_events)
#                 elif enhanced_events is None:
#                     # Try loading from CSV
#                     csv_path = os.path.join(output_base_dir, 'enhanced', 'enhanced_zero_curtain_ev...
#                     if os.path.exists(csv_path):
#                         enhanced_events = pd.read_csv(csv_path, parse_dates=['datetime_min', 'date...
#                     else:
#                         enhanced_events = pd.DataFrame()
            
#             if enhanced_events is not None and len(enhanced_events) > 0:
#                 try:
#                     # Visualize the enhanced detection results
#                     fig1, stats1 = create_final_visualization(
#                         enhanced_events,
#                         os.path.join(output_base_dir, 'enhanced_events_visualization.png')
#                     )
#                     results['enhanced_visualization'] = stats1
#                 except Exception as e:
#                     print(f"Error creating enhanced visualization: {str(e)}")
#             else:
#                 print("Skipping enhanced visualization: No events available")
            
#             # Visualize model predictions if available
#             model_predictions = results.get('model_predictions')
#             if model_predictions is None or len(model_predictions) == 0:
#                 model_predictions = load_checkpoint('model_predictions')
#                 if model_predictions is None:
#                     # Try loading from CSV
#                     csv_path = os.path.join(output_base_dir, 'model_predictions.csv')
#                     if os.path.exists(csv_path):
#                         model_predictions = pd.read_csv(csv_path, parse_dates=['datetime_min', 'da...
            
#             if model_predictions is not None and len(model_predictions) > 0:
#                 try:
#                     fig2, stats2 = create_final_visualization(
#                         model_predictions,
#                         os.path.join(output_base_dir, 'model_predictions_visualization.png')
#                     )
#                     results['model_visualization'] = stats2
#                 except Exception as e:
#                     print(f"Error creating model predictions visualization: {str(e)}")
            
#             results['visualization_time'] = time.time() - start_time
            
#             # Save progress
#             completed_stages.add('Visualization')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             pbar.update(1)
            
#             # Clear memory before next stage
#             gc.collect()
#             print(f"Memory usage after Stage 4: {memory_usage():.1f} MB")
        
#         # Stage 5: Compare methods
#         if 'Comparison' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 5/5: {stages[4]}")
            
#             # Get required data
#             enhanced_events = results.get('enhanced_events')
#             if enhanced_events is None or len(enhanced_events) == 0:
#                 enhanced_events = load_checkpoint('enhanced_events')
#                 if isinstance(enhanced_events, list) and enhanced_events:
#                     enhanced_events = pd.DataFrame(enhanced_events)
#                 elif enhanced_events is None:
#                     # Try loading from CSV
#                     csv_path = os.path.join(output_base_dir, 'enhanced', 'enhanced_zero_curtain_ev...
#                     if os.path.exists(csv_path):
#                         enhanced_events = pd.read_csv(csv_path, parse_dates=['datetime_min', 'date...
#                     else:
#                         enhanced_events = pd.DataFrame()
            
#             model_predictions = results.get('model_predictions')
#             if model_predictions is None or len(model_predictions) == 0:
#                 model_predictions = load_checkpoint('model_predictions')
#                 if model_predictions is None:
#                     # Try loading from CSV
#                     csv_path = os.path.join(output_base_dir, 'model_predictions.csv')
#                     if os.path.exists(csv_path):
#                         model_predictions = pd.read_csv(csv_path, parse_dates=['datetime_min', 'da...
#                     else:
#                         model_predictions = pd.DataFrame()
            
#             if (enhanced_events is not None and len(enhanced_events) > 0 and 
#                 model_predictions is not None and len(model_predictions) > 0):
#                 try:
#                     os.makedirs(os.path.join(output_base_dir, 'comparison'), exist_ok=True)
#                     comparison = compare_detection_methods(
#                         enhanced_events,
#                         model_predictions,
#                         output_dir=os.path.join(output_base_dir, 'comparison')
#                     )
#                     results['comparison'] = comparison
#                 except Exception as e:
#                     print(f"Error in comparison: {str(e)}")
#             else:
#                 print("Skipping comparison: Either enhanced events or model predictions not availa...
            
#             results['comparison_time'] = time.time() - start_time
            
#             # Save progress
#             completed_stages.add('Comparison')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             pbar.update(1)
            
#             # Clear memory
#             gc.collect()
#             print(f"Memory usage after Stage 5: {memory_usage():.1f} MB")
    
#     # Generate final summary report
#     total_time = (results.get('enhanced_time', 0) + 
#                  results.get('model_training_time', 0) + 
#                  results.get('model_application_time', 0) +
#                  results.get('visualization_time', 0) +
#                  results.get('comparison_time', 0))
    
#     print("\n" + "=" * 80)
#     print("ZERO CURTAIN ANALYSIS COMPLETE")
#     print("=" * 80)
    
#     print(f"\nTotal runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
#     print(f"Output directory: {output_base_dir}")
    
#     # Get enhanced events count
#     enhanced_count = 0
#     enhanced_events = results.get('enhanced_events')
#     if enhanced_events is not None:
#         enhanced_count = len(enhanced_events)
    
#     # Get model predictions count
#     model_count = 0
#     model_predictions = results.get('model_predictions')
#     if model_predictions is not None:
#         model_count = len(model_predictions)
    
#     print("\nSummary of Results:")
#     print(f"  Enhanced Detection: {enhanced_count} events detected")
    
#     if model_count > 0:
#         print(f"  Deep Learning Model: {model_count} events detected")
    
#     comparison = results.get('comparison')
#     if comparison is not None and 'overlap_metrics' in comparison and 'jaccard_index' in compariso...
#         overlap = comparison['overlap_metrics']['jaccard_index']
#         print(f"  Method Agreement: {overlap*100:.1f}% overlap between methods")
    
#     # Save summary to file
#     with open(os.path.join(output_base_dir, 'summary.txt'), 'w') as f:
#         f.write("ZERO CURTAIN ANALYSIS SUMMARY\n")
#         f.write("=" * 30 + "\n\n")
        
#         f.write(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\n")
#         f.write(f"Enhanced Detection: {enhanced_count} events detected\n")
        
#         if model_count > 0:
#             f.write(f"Deep Learning Model: {model_count} events detected\n")
        
#         if comparison is not None and 'overlap_metrics' in comparison and 'jaccard_index' in compa...
#             overlap = comparison['overlap_metrics']['jaccard_index']
#             f.write(f"Method Agreement: {overlap*100:.1f}% overlap between methods\n")
    
#     return results

# # Run with memory management and checkpointing
# results = run_full_analysis_pipeline_with_memory_management(
#     merged_df,
#     output_base_dir='zero_curtain_results',
#     use_checkpoints=True,
#     batch_size=500  # Adjust based on memory constraints
# )

# data_loader.py
import os
import pandas as pd
import numpy as np
import pyarrow as pa
import pyarrow.feather as pf
import pyarrow.compute as pc
import pyarrow.dataset as ds
import gc
from tqdm.auto import tqdm

def get_time_range(feather_path):
    """Get min and max datetime from feather file efficiently"""
    print("Determining dataset time range")
    try:
        # Use PyArrow for efficient metadata access
        table = pf.read_table(feather_path, columns=['datetime'])
        datetime_col = table['datetime']
        min_date = pd.Timestamp(pc.min(datetime_col).as_py())
        max_date = pd.Timestamp(pc.max(datetime_col).as_py())
        del table, datetime_col
        gc.collect()
    except Exception as e:
        print(f"Error with PyArrow min/max: {str(e)}")
        print("Falling back to reading sample rows...")
        
        # Read just first and last rows after sorting
        # This is more efficient than reading all data
        try:
            dataset = ds.dataset(feather_path, format='feather')
            
            # Get min date from sorted data (first row)
            table_min = dataset.to_table(
                columns=['datetime'],
                filter=None,
                use_threads=True,
                limit=1
            )
            min_date = pd.Timestamp(table_min['datetime'][0].as_py())
            
            # Get max date from reverse sorted data (first row)
            # Note: PyArrow Dataset API doesn't support reverse sort directly
            # So we read all datetime values and find max
            table_all = dataset.to_table(columns=['datetime'])
            max_date = pd.Timestamp(pc.max(table_all['datetime']).as_py())
            
            del table_min, table_all, dataset
            gc.collect()
        except Exception as inner_e:
            print(f"Error with optimized approach: {str(inner_e)}")
            print("Using full scan method (slow)...")
            
            # Last resort: read all timestamps in chunks
            min_date = pd.Timestamp.max
            max_date = pd.Timestamp.min
            
            # Open file directly to avoid loading everything
            table = pf.read_table(feather_path, columns=['datetime'])
            datetime_values = table['datetime'].to_pandas()
            
            min_date = datetime_values.min()
            max_date = datetime_values.max()
            
            del datetime_values, table
            gc.collect()
    
    print(f"Data timespan: {min_date} to {max_date}")
    return min_date, max_date

def get_unique_site_depths(feather_path):
    """Get unique site-depth combinations efficiently"""
    print("Finding unique site-depth combinations")
    print(f"Memory before processing: {memory_usage():.1f} MB")
    
    try:
        # Use PyArrow for efficient operation
        dataset = ds.dataset(feather_path, format='feather')
        
        # Read only the columns we need
        table = dataset.to_table(columns=['source', 'soil_temp_depth'])
        
        # Convert to pandas
        df = table.to_pandas()
        
        # Get valid combinations (where temp_depth is not NaN)
        valid_df = df.dropna(subset=['soil_temp_depth'])
        
        # Get unique combinations
        site_depths = valid_df.drop_duplicates(['source', 'soil_temp_depth'])
        
        # Clean up
        del table, df, valid_df, dataset
        gc.collect()
        
    except Exception as e:
        print(f"Error with PyArrow approach: {str(e)}")
        print("Falling back to chunked pandas approach...")
        
        # Fallback: Process in chunks
        chunk_size = 1000000
        unique_combos = set()
        
        # Read feather in chunks (this is slower but more robust)
        with pd.read_feather(feather_path, columns=['source', 'soil_temp_depth']) as reader:
            for chunk in reader:
                # Get valid rows and unique combinations
                valid_rows = chunk.dropna(subset=['soil_temp_depth'])
                chunk_combinations = set(zip(valid_rows['source'], valid_rows['soil_temp_depth']))
                unique_combos.update(chunk_combinations)
                
                # Clean up
                del chunk, valid_rows, chunk_combinations
                gc.collect()
        
        # Convert to DataFrame
        site_depths = pd.DataFrame(list(unique_combos), columns=['source', 'soil_temp_depth'])
    
    print(f"Found {len(site_depths)} unique site-depth combinations")
    print(f"Memory after processing: {memory_usage():.1f} MB")
    
    return site_depths[['source', 'soil_temp_depth']]

def load_site_depth_data(feather_path, site, temp_depth):
    """Load ONLY data for a specific site and depth using PyArrow filtering"""
    print(f"Loading data for site: {site}, depth: {temp_depth}")
    print(f"Memory before loading: {memory_usage():.1f} MB")
    
    try:
        # Use PyArrow Dataset API for efficient filtering
        dataset = ds.dataset(feather_path, format='feather')
        
        # Create filter expressions
        site_filter = ds.field('source') == site
        depth_filter = ds.field('soil_temp_depth') == float(temp_depth)
        combined_filter = site_filter & depth_filter
        
        # Apply filter and read only filtered data
        table = dataset.to_table(filter=combined_filter)
        filtered_df = table.to_pandas()
        
        # Clean up
        del table, dataset
        gc.collect()
        
    except Exception as e:
        print(f"Error with PyArrow filtering: {str(e)}")
        print("Falling back to pandas filtering...")
        
        # Define chunk size based on available memory
        available_mem = psutil.virtual_memory().available / (1024**2)  # MB
        chunk_size = min(500000, max(100000, int(available_mem / 10)))
        
        filtered_chunks = []
        
        # Read and filter in chunks
        for chunk in pd.read_feather(feather_path, columns=None, chunksize=chunk_size):
            # Filter by site and depth
            chunk_filtered = chunk[(chunk['source'] == site) & 
                                   (chunk['soil_temp_depth'] == temp_depth)]
            
            if len(chunk_filtered) > 0:
                filtered_chunks.append(chunk_filtered)
            
            # Clean up
            del chunk, chunk_filtered
            gc.collect()
        
        # Combine filtered chunks
        if filtered_chunks:
            filtered_df = pd.concat(filtered_chunks, ignore_index=True)
            del filtered_chunks
        else:
            filtered_df = pd.DataFrame()
        
        gc.collect()
    
    # Ensure datetime is in datetime format
    if 'datetime' in filtered_df.columns and not pd.api.types.is_datetime64_dtype(filtered_df['datetime']):
        filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])
    
    print(f"Loaded {len(filtered_df)} rows for site-depth")
    print(f"Memory after loading: {memory_usage():.1f} MB")
    
    return filtered_df

def prepare_data_for_deep_learning_efficiently(feather_path, events_df, sequence_length=6, 
                                               output_dir=None, batch_size=500, start_batch=0):
    """
    Memory-efficient version of prepare_data_for_deep_learning that processes
    site-depths in batches and saves intermediate results without accumulating all data in memory.
    """
    import numpy as np
    from tqdm.auto import tqdm
    import os
    import gc
    import pandas as pd
    
    print("Preparing data for deep learning model...")
    print(f"Memory before preparation: {memory_usage():.1f} MB")
    
    # Ensure datetime columns are proper datetime objects
    if not pd.api.types.is_datetime64_dtype(events_df['datetime_min']):
        events_df['datetime_min'] = pd.to_datetime(events_df['datetime_min'], format='mixed')
    if not pd.api.types.is_datetime64_dtype(events_df['datetime_max']):
        events_df['datetime_max'] = pd.to_datetime(events_df['datetime_max'], format='mixed')
    
    # Create a mapping of detected events for labeling
    print("Creating event mapping...")
    event_map = {}
    for _, event in events_df.iterrows():
        site = event['source']
        depth = event['soil_temp_depth']
        start = event['datetime_min']
        end = event['datetime_max']
        
        if (site, depth) not in event_map:
            event_map[(site, depth)] = []
        
        event_map[(site, depth)].append((start, end))
    
    # Get site-depth combinations for progress tracking
    print("Finding unique site-depth combinations")
    print(f"Memory before processing: {memory_usage():.1f} MB")
    
    # Use a function that efficiently gets unique site-depths without loading all data
    site_depths = get_unique_site_depths(feather_path)
    total_combinations = len(site_depths)
    
    print(f"Found {total_combinations} unique site-depth combinations")
    print(f"Memory after processing: {memory_usage():.1f} MB")
    print(f"Preparing sequences from {total_combinations} site-depth combinations...")
    
    # Check for existing batch files to support resuming
    if output_dir is not None:
        os.makedirs(output_dir, exist_ok=True)
        import glob
        existing_batches = glob.glob(os.path.join(output_dir, 'X_batch_*_*.npy'))
        if existing_batches and start_batch == 0:
            # Extract batch numbers from filenames
            batch_ends = [int(os.path.basename(f).split('_')[-1].split('.')[0]) for f in existing_batches]
            if batch_ends:
                last_processed_batch = max(batch_ends)
                # Start from the next batch
                start_batch = (last_processed_batch // batch_size) * batch_size + batch_size
                print(f"Found existing batch files, resuming from batch {start_batch}")
    
    # Track total counts for reporting
    total_sequences = 0
    total_positive = 0
    
    # Process in batches to manage memory
    for batch_start in range(start_batch, total_combinations, batch_size):
        batch_end = min(batch_start + batch_size, total_combinations)
        print(f"\nProcessing batch {batch_start+1}-{batch_end}/{total_combinations}")
        
        # Initialize lists for this batch only
        batch_features = []
        batch_labels = []
        batch_metadata = []
        
        # Process each site-depth in the batch
        for i in tqdm(range(batch_start, batch_end), desc="Creating sequences"):
            site = site_depths.iloc[i]['source']
            temp_depth = site_depths.iloc[i]['soil_temp_depth']
            
            # Skip if no events exist for this site-depth
            if (site, temp_depth) not in event_map and len(event_map) > 0:
                continue
            
            try:
                # Load only the data for this site-depth
                print(f"Loading data for site: {site}, depth: {temp_depth}")
                print(f"Memory before loading: {memory_usage():.1f} MB")
                
                group = load_site_depth_data(feather_path, site, temp_depth)
                
                print(f"Loaded {len(group)} rows for site-depth")
                print(f"Memory after loading: {memory_usage():.1f} MB")
                
                if len(group) < sequence_length + 1:
                    continue
                
                # Ensure datetime is in datetime format
                if not pd.api.types.is_datetime64_dtype(group['datetime']):
                    group['datetime'] = pd.to_datetime(group['datetime'], format='mixed')
                
                # Sort by time
                group = group.sort_values('datetime')
                
                # Create feature set
                feature_cols = ['soil_temp_standardized']
                
                # Calculate gradient features
                group['temp_gradient'] = group['soil_temp_standardized'].diff()
                feature_cols.append('temp_gradient')
                
                # Add soil depth as feature
                group['depth_normalized'] = temp_depth / 10.0
                feature_cols.append('depth_normalized')
                
                # Add soil moisture if available
                has_moisture = False
                if 'soil_moist_standardized' in group.columns and not group['soil_moist_standardized'].isna().all():
                    has_moisture = True
                    feature_cols.append('soil_moist_standardized')
                    group['moist_gradient'] = group['soil_moist_standardized'].diff()
                    feature_cols.append('moist_gradient')
                
                # Fill missing values
                group[feature_cols] = group[feature_cols].fillna(0)
                
                # Create sequences with sliding window
                for j in range(len(group) - sequence_length):
                    # Get time window
                    start_time = group.iloc[j]['datetime']
                    end_time = group.iloc[j + sequence_length - 1]['datetime']
                    
                    # Extract sequence data
                    sequence = group.iloc[j:j+sequence_length][feature_cols].values
                    
                    # Check if this sequence overlaps with known zero curtain event
                    is_zero_curtain = 0
                    if (site, temp_depth) in event_map:
                        for event_start, event_end in event_map[(site, temp_depth)]:
                            # Ensure proper datetime comparison
                            # Check for significant overlap (at least 50% of sequence)
                            if (min(end_time, event_end) - max(start_time, event_start)).total_seconds() > \
                               0.5 * (end_time - start_time).total_seconds():
                                is_zero_curtain = 1
                                break
                    
                    # Store features and labels
                    batch_features.append(sequence)
                    batch_labels.append(is_zero_curtain)
                    total_positive += is_zero_curtain
                    total_sequences += 1
                    
                    # Store metadata
                    meta = {
                        'source': site,
                        'soil_temp_depth': temp_depth,
                        'start_time': start_time,
                        'end_time': end_time,
                        'latitude': group.iloc[j]['latitude'] if 'latitude' in group.columns else None,
                        'longitude': group.iloc[j]['longitude'] if 'longitude' in group.columns else None,
                        'has_moisture_data': has_moisture
                    }
                    batch_metadata.append(meta)
                
                # Clean up to free memory
                del group
                gc.collect()
                
            except Exception as e:
                print(f"Error processing {site}, depth {temp_depth}: {str(e)}")
                import traceback
                traceback.print_exc()
                continue
        
        # Save batch results if output directory provided and we have data
        if output_dir is not None and batch_features:
            os.makedirs(output_dir, exist_ok=True)
            
            # Save batch as numpy files
            batch_X = np.array(batch_features)
            batch_y = np.array(batch_labels)
            
            np.save(os.path.join(output_dir, f'X_batch_{batch_start}_{batch_end}.npy'), batch_X)
            np.save(os.path.join(output_dir, f'y_batch_{batch_start}_{batch_end}.npy'), batch_y)
            
            # Save metadata as pickle
            import pickle
            with open(os.path.join(output_dir, f'metadata_batch_{batch_start}_{batch_end}.pkl'), 'wb') as f:
                pickle.dump(batch_metadata, f)
            
            # Save progress marker
            with open(os.path.join(output_dir, 'progress.txt'), 'w') as f:
                f.write(f"Last processed batch: {batch_start}-{batch_end}\n")
                f.write(f"Total sequences: {total_sequences}\n")
                f.write(f"Positive examples: {total_positive} ({total_positive/total_sequences*100:.1f}%)\n")
        
        # Clean up batch variables - this is key for memory efficiency
        del batch_features, batch_labels, batch_metadata
        if 'batch_X' in locals(): del batch_X
        if 'batch_y' in locals(): del batch_y
        gc.collect()
        
        print(f"Memory after batch: {memory_usage():.1f} MB")
        #print(f"Progress: {total_sequences} sequences processed, {total_positive} positive examples...
        positive_percentage = (total_positive/total_sequences*100) if total_sequences > 0 else 0
        print(f"Progress: {total_sequences} sequences processed, {total_positive} positive examples ({positive_percentage:.1f}%)")

    print("Data preparation complete!")
    print(f"Total sequences: {total_sequences}")
    #print(f"Positive examples: {total_positive} ({total_positive/total_sequences*100:.1f}% if total...
    positive_percentage = (total_positive/total_sequences*100) if total_sequences > 0 else 0
    print(f"Positive examples: {total_positive} ({positive_percentage:.1f}%)")
    
    if output_dir is not None:
        print(f"Results saved to {output_dir}")
        print("To merge batch files into final dataset, use merge_batch_files(output_dir)")
    
    print(f"Memory after preparation: {memory_usage():.1f} MB")
    
    # Return info instead of data - prevents memory issues
    return {
        'total_sequences': total_sequences,
        'total_positive': total_positive,
        'positive_percentage': total_positive/total_sequences*100 if total_sequences > 0 else 0,
        'output_dir': output_dir
    }


def merge_batch_files(output_dir):
    """
    Merge all batch files into single X_features.npy and y_labels.npy files.
    """
    import numpy as np
    import os
    import glob
    import pickle
    import gc
    
    print(f"Memory before merging: {memory_usage():.1f} MB")
    
    # Find all batch files
    x_batch_files = sorted(glob.glob(os.path.join(output_dir, 'X_batch_*_*.npy')))
    y_batch_files = sorted(glob.glob(os.path.join(output_dir, 'y_batch_*_*.npy')))
    metadata_batch_files = sorted(glob.glob(os.path.join(output_dir, 'metadata_batch_*_*.pkl')))
    
    print(f"Found {len(x_batch_files)} X batches, {len(y_batch_files)} y batches, and {len(metadata_batch_files)} metadata batches")
    
    # Get dimensions of first batch to initialize arrays
    if x_batch_files:
        first_batch = np.load(x_batch_files[0])
        shape = first_batch.shape
        del first_batch
        gc.collect()
        
        # Count total sequences
        total_sequences = 0
        for batch_file in x_batch_files:
            batch = np.load(batch_file)
            total_sequences += batch.shape[0]
            del batch
            gc.collect()
        
        # Pre-allocate arrays
        X = np.zeros((total_sequences, shape[1], shape[2]), dtype=np.float32)
        y = np.zeros(total_sequences, dtype=np.int32)
        
        # Load and copy batches
        idx = 0
        for i, (x_file, y_file) in enumerate(zip(x_batch_files, y_batch_files)):
            print(f"Processing batch {i+1}/{len(x_batch_files)}")
            print(f"Memory: {memory_usage():.1f} MB")
            
            batch_x = np.load(x_file)
            batch_y = np.load(y_file)
            
            batch_size = batch_x.shape[0]
            X[idx:idx+batch_size] = batch_x
            y[idx:idx+batch_size] = batch_y
            
            idx += batch_size
            
            # Clean up
            del batch_x, batch_y
            gc.collect()
        
        # Save merged X and y
        print(f"Saving merged arrays: X.shape={X.shape}, y.shape={y.shape}")
        np.save(os.path.join(output_dir, 'X_features.npy'), X)
        np.save(os.path.join(output_dir, 'y_labels.npy'), y)
        
        # Clean up
        del X, y
        gc.collect()
        
        # Load and concatenate metadata batches
        all_metadata = []
        for i, batch_file in enumerate(metadata_batch_files):
            print(f"Processing metadata batch {i+1}/{len(metadata_batch_files)}")
            print(f"Memory: {memory_usage():.1f} MB")
            
            with open(batch_file, 'rb') as f:
                batch_metadata = pickle.load(f)
            all_metadata.extend(batch_metadata)
            
            # Clean up
            del batch_metadata
            gc.collect()
        
        # Save merged metadata
        print(f"Saving merged metadata: {len(all_metadata)} entries")
        with open(os.path.join(output_dir, 'metadata.pkl'), 'wb') as f:
            pickle.dump(all_metadata, f)
        
        # Final cleanup
        del all_metadata
        gc.collect()
        
        print(f"Memory after merging: {memory_usage():.1f} MB")
        return {
            'total_sequences': total_sequences,
            'output_dir': output_dir
        }
    else:
        print("No batch files found to merge")
        return None

def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
    """
    Advanced model architecture combining ConvLSTM, Transformers, and 
    Variational Autoencoder components to better capture complex zero curtain dynamics.
    
    Parameters:
    -----------
    input_shape : tuple
        Shape of input data (sequence_length, num_features)
    include_moisture : bool
        Whether soil moisture features are included
        
    Returns:
    --------
    tensorflow.keras.Model
        Compiled model ready for training
    """
    import os
    os.environ["DEVICE_COUNT_GPU"] = "0"
    import tensorflow as tf
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
    from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D
    from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
    from tensorflow.keras.optimizers import Adam
    
    # Input layer
    inputs = Input(shape=input_shape)
    
    # Reshape for ConvLSTM (add spatial dimension)
    # From (sequence_length, features) to (sequence_length, 1, features)
    
    #x = Reshape((input_shape[0], 1, input_shape[1]))(inputs)
    x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
    # ConvLSTM layer to capture spatiotemporal patterns
    from tensorflow.keras.layers import ConvLSTM2D
    convlstm = ConvLSTM2D(
        filters=64,
        kernel_size=(3, 1),
        padding='same',
        return_sequences=True,
        activation='tanh',
        recurrent_dropout=0.2
    )(x)
    
    # Reshape back to (sequence_length, features)
    convlstm = Reshape((input_shape[0], 64))(convlstm)
    
    # Add positional encoding for transformer
    def positional_encoding(length, depth):
        positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
        angle_rates = 1 / tf.pow(10000.0, depths)
        angle_rads = positions * angle_rates
        
        pos_encoding = tf.concat(
            [tf.sin(angle_rads), tf.cos(angle_rads)], axis=-1)
        
        return pos_encoding
    
    # Add positional encoding
    pos_encoding = positional_encoding(input_shape[0], 64)
    transformer_input = convlstm + pos_encoding
    
    # Transformer encoder block
    def transformer_encoder(x, num_heads=8, key_dim=64, ff_dim=128):
        # Multi-head attention
        attention_output = MultiHeadAttention(
            num_heads=num_heads, key_dim=key_dim
        )(x, x)
        
        # Skip connection 1
        x1 = Add()([attention_output, x])
        x1 = LayerNormalization(epsilon=1e-6)(x1)
        
        # Feed-forward network
        ff_output = Dense(ff_dim, activation='relu')(x1)
        ff_output = Dropout(0.1)(ff_output)
        ff_output = Dense(64)(ff_output)
        
        # Skip connection 2
        x2 = Add()([ff_output, x1])
        return LayerNormalization(epsilon=1e-6)(x2)
    
    # Apply transformer encoder
    transformer_output = transformer_encoder(transformer_input)
    
    # Parallel CNN paths for multi-scale feature extraction
    cnn_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(inputs)
    cnn_1 = BatchNormalization()(cnn_1)
    
    cnn_2 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(inputs)
    cnn_2 = BatchNormalization()(cnn_2)
    
    cnn_3 = Conv1D(filters=32, kernel_size=7, padding='same', activation='relu')(inputs)
    cnn_3 = BatchNormalization()(cnn_3)
    
    # Variational Autoencoder components
    def sampling(args):
        z_mean, z_log_var = args
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
    # Global temporal features
    global_max = GlobalMaxPooling1D()(transformer_output)
    global_avg = GlobalAveragePooling1D()(transformer_output)
    
    # VAE encoding
    z_mean = Dense(32)(Concatenate()([global_max, global_avg]))
    z_log_var = Dense(32)(Concatenate()([global_max, global_avg]))
    z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
    # Combine all features
    merged_features = Concatenate()(
        [
            GlobalMaxPooling1D()(cnn_1),
            GlobalMaxPooling1D()(cnn_2),
            GlobalMaxPooling1D()(cnn_3),
            global_max,
            global_avg,
            z
        ]
    )
    
    # Final classification layers
    x = Dense(128, activation='relu')(merged_features)
    x = Dropout(0.3)(x)
    x = BatchNormalization()(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.2)(x)
    
    # Output layer
    outputs = Dense(1, activation='sigmoid')(x)
    
    # Create model
    model = Model(inputs=inputs, outputs=outputs)
    
    # Add VAE loss
    kl_loss = -0.5 * tf.reduce_mean(
        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
    )
    model.add_loss(0.001 * kl_loss)  # Small weight for VAE loss
    
    # Compile model with appropriate metrics
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=[
            'accuracy',
            tf.keras.metrics.AUC(name='auc'),
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall')
        ]
    )
    
    return model

# Time-based split rather than random split
def temporal_train_test_split(X, y, metadata, val_ratio=0.2, test_ratio=0.1):
    """
    Split data temporally for time series modeling.
    
    Parameters:
    -----------
    X : numpy.ndarray
        Input features
    y : numpy.ndarray
        Output labels
    metadata : list
        Metadata containing timestamps for each sequence
    val_ratio : float
        Proportion of data for validation
    test_ratio : float
        Proportion of data for testing
        
    Returns:
    --------
    tuple
        (X_train, X_val, X_test, y_train, y_val, y_test)
    """
    # Extract timestamps from metadata
    timestamps = [meta['start_time'] for meta in metadata]
    
    # Sort indices by timestamp
    sorted_indices = sorted(range(len(timestamps)), key=lambda i: timestamps[i])
    
    # Calculate split points
    n_samples = len(sorted_indices)
    test_start = int(n_samples * (1 - test_ratio))
    val_start = int(n_samples * (1 - test_ratio - val_ratio))
    
    # Split indices into train, validation, and test sets
    train_indices = sorted_indices[:val_start]
    val_indices = sorted_indices[val_start:test_start]
    test_indices = sorted_indices[test_start:]
    
    # Create the splits
    X_train = X[train_indices]
    y_train = y[train_indices]
    
    X_val = X[val_indices]
    y_val = y[val_indices]
    
    X_test = X[test_indices]
    y_test = y[test_indices]
    
    return X_train, X_val, X_test, y_train, y_val, y_test

def train_zero_curtain_model_efficiently(X, y, metadata=None, output_dir=None):
    """
    Memory-efficient version of train_zero_curtain_model that implements
    batch training and model checkpointing with temporal data splitting.
    
    Parameters:
    -----------
    X : numpy.ndarray
        Input features
    y : numpy.ndarray
        Output labels
    metadata : list, optional
        Metadata about each sequence (must contain timestamps)
    output_dir : str, optional
        Directory to save model and results
        
    Returns:
    --------
    tuple
        (trained_model, training_history, evaluation_results)
    """
    import tensorflow as tf
    from tensorflow.keras.callbacks import (
        EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, 
        CSVLogger, TensorBoard
    )
    import matplotlib.pyplot as plt
    import os
    import gc
    import numpy as np
    
    # Enable memory growth to avoid pre-allocating all GPU memory
    physical_devices = tf.config.list_physical_devices('GPU')
    if physical_devices:
        for device in physical_devices:
            tf.config.experimental.set_memory_growth(device, True)
            print(f"Enabled memory growth for {device}")
    
    print("Training zero curtain model...")
    print(f"Memory before training: {memory_usage():.1f} MB")
    
    # Create output directory
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    # Temporal split for time series data
    print("Performing temporal split for train/validation/test sets...")
    if metadata is None:
        raise ValueError("Metadata with timestamps is required for temporal splitting")
    
    # Extract timestamps from metadata
    timestamps = np.array([meta['start_time'] for meta in metadata])
    
    # Sort indices by timestamp
    sorted_indices = np.argsort(timestamps)
    
    # Calculate split points (70% train, 15% validation, 15% test)
    n_samples = len(sorted_indices)
    test_ratio = 0.15
    val_ratio = 0.15
    
    test_start = int(n_samples * (1 - test_ratio))
    val_start = int(n_samples * (1 - test_ratio - val_ratio))
    
    # Split indices into train, validation, and test sets
    train_indices = sorted_indices[:val_start]
    val_indices = sorted_indices[val_start:test_start]
    test_indices = sorted_indices[test_start:]
    
    print(f"Training on data from {timestamps[train_indices[0]]} to {timestamps[train_indices[-1]]}")
    print(f"Validating on data from {timestamps[val_indices[0]]} to {timestamps[val_indices[-1]]}")
    print(f"Testing on data from {timestamps[test_indices[0]]} to {timestamps[test_indices[-1]]}")
    
    # Create the splits
    X_train = X[train_indices]
    y_train = y[train_indices]
    
    X_val = X[val_indices]
    y_val = y[val_indices]
    
    X_test = X[test_indices]
    y_test = y[test_indices]
    
    print(f"Split sizes: Train={len(X_train)}, Validation={len(X_val)}, Test={len(X_test)}")
    
    # Check class balance in each split
    train_pos = np.sum(y_train)
    val_pos = np.sum(y_val)
    test_pos = np.sum(y_test)
    
    print(f"Positive examples: Train={train_pos} ({train_pos/len(y_train)*100:.1f}%), " +
          f"Val={val_pos} ({val_pos/len(y_val)*100:.1f}%), " +
          f"Test={test_pos} ({test_pos/len(y_test)*100:.1f}%)")
    
    # Clean up to free memory
    del sorted_indices, timestamps
    gc.collect()
    
    # Build model with appropriate input shape
    print("Building model...")
    input_shape = (X_train.shape[1], X_train.shape[2])
    
    model = build_advanced_zero_curtain_model(input_shape)
    
    # If output directory exists, check for existing model checkpoint
    model_checkpoint_path = None
    if output_dir:
        model_checkpoint_path = os.path.join(output_dir, 'checkpoint.h5')
        if os.path.exists(model_checkpoint_path):
            print(f"Loading existing model checkpoint from {model_checkpoint_path}")
            try:
                model = tf.keras.models.load_model(model_checkpoint_path)
                print("Checkpoint loaded successfully")
            except Exception as e:
                print(f"Error loading checkpoint: {str(e)}")
    
    # Set up callbacks with additional memory management
    callbacks = [
        # Stop early if validation performance plateaus
        EarlyStopping(
            patience=15,  # Increased patience for temporal data
            restore_best_weights=True, 
            monitor='val_auc', 
            mode='max'
        ),
        # Reduce learning rate when improvement slows
        ReduceLROnPlateau(
            factor=0.5, 
            patience=7,  # Increased patience for temporal data
            min_lr=1e-6, 
            monitor='val_auc', 
            mode='max'
        ),
        # Manual garbage collection after each epoch
        tf.keras.callbacks.LambdaCallback(
            on_epoch_end=lambda epoch, logs: gc.collect()
        )
    ]
    
    # Add additional callbacks if output directory provided
    if output_dir:
        callbacks.extend([
            # Save best model
            ModelCheckpoint(
                os.path.join(output_dir, 'checkpoint.h5'),
                save_best_only=True,
                monitor='val_auc',
                mode='max'
            ),
            # Log training progress to CSV
            CSVLogger(
                os.path.join(output_dir, 'training_log.csv'),
                append=True
            ),
            # TensorBoard visualization
            TensorBoard(
                log_dir=os.path.join(output_dir, 'tensorboard_logs'),
                histogram_freq=1,
                profile_batch=0  # Disable profiling to save memory
            )
        ])
    
    # Calculate class weights to handle imbalance
    pos_weight = len(y_train) / max(sum(y_train), 1)
    class_weight = {0: 1, 1: pos_weight}
    print(f"Using class weight {pos_weight:.2f} for positive class")
    
    # Train model with memory-efficient settings
    print("Training model...")
    batch_size = 32  # Adjust based on available memory
    epochs = 100
    
    # Use fit with appropriate memory settings
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=callbacks,
        class_weight=class_weight,
        verbose=1,
        # Memory efficiency settings
        shuffle=True,  # Still shuffle within the temporal train split
        use_multiprocessing=False,  # Avoid extra memory overhead
        workers=1  # Reduce parallel processing to save memory
    )
    
    # Clean up to free memory
    del X_train, y_train, X_val, y_val
    gc.collect()
    
    # Evaluate on test set
    print("Evaluating model on test set...")
    evaluation = model.evaluate(X_test, y_test, batch_size=batch_size)
    print("Test performance:")
    for metric, value in zip(model.metrics_names, evaluation):
        print(f"  {metric}: {value:.4f}")
    
    # Generate predictions for visualization and further analysis
    y_pred_prob = model.predict(X_test, batch_size=batch_size)
    y_pred = (y_pred_prob > 0.5).astype(int).flatten()
    
    # Calculate and save additional evaluation metrics
    from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
    report = classification_report(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    
    print("Classification Report:")
    print(report)
    
    print("Confusion Matrix:")
    print(conf_matrix)
    
    # Plot and save training history
    if output_dir:
        # Save evaluation metrics
        with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w') as f:
            f.write("Classification Report:\n")
            f.write(report)
            f.write("\n\nConfusion Matrix:\n")
            f.write(str(conf_matrix))
            f.write("\n\nTest Metrics:\n")
            for metric, value in zip(model.metrics_names, evaluation):
                f.write(f"{metric}: {value:.4f}\n")
        
        # Plot training history
        plt.figure(figsize=(16, 6))
        
        plt.subplot(1, 3, 1)
        plt.plot(history.history['auc'])
        plt.plot(history.history['val_auc'])
        plt.title('Model AUC')
        plt.ylabel('AUC')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'], loc='lower right')
        
        plt.subplot(1, 3, 2)
        plt.plot(history.history['loss'])
        plt.plot(history.history['val_loss'])
        plt.title('Model Loss')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'], loc='upper right')
        
        # Plot ROC curve
        plt.subplot(1, 3, 3)
        fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve (Test Set)')
        plt.legend(loc='lower right')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'training_performance.png'), dpi=300)
        
        # Save detailed model summary
        from contextlib import redirect_stdout
        with open(os.path.join(output_dir, 'model_summary.txt'), 'w') as f:
            with redirect_stdout(f):
                model.summary()
    
    # Clean up to free memory
    del X_test, y_test, y_pred, y_pred_prob
    gc.collect()
    
    print(f"Memory after training: {memory_usage():.1f} MB")
    return model, history, evaluation

def run_full_analysis_pipeline(feather_path, output_base_dir='results', batch_size=50):
    """
    Run the complete zero curtain analysis pipeline with progress tracking
    and memory efficiency.
    
    Parameters:
    -----------
    feather_path : str
        Path to the feather file with merged data
    output_base_dir : str
        Base directory for saving outputs
    batch_size : int
        Number of site-depths to process per batch
        
    Returns:
    --------
    dict
        Dictionary containing analysis results
    """
    from tqdm.auto import tqdm
    import time
    import os
    import gc
    import pickle
    
    # Create output directories
    os.makedirs(output_base_dir, exist_ok=True)
    checkpoint_dir = os.path.join(output_base_dir, 'checkpoints')
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # Function to save checkpoint
    def save_checkpoint(data, name):
        with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'wb') as f:
            pickle.dump(data, f)
    
    # Function to load checkpoint
    def load_checkpoint(name):
        try:
            with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'rb') as f:
                return pickle.load(f)
        except:
            return None
    
    # Initialize results
    results = load_checkpoint('pipeline_results') or {}
    
    # Check for completed stages
    completed_stages = set(results.get('completed_stages', []))
    print(f"Found {len(completed_stages)} completed stages: {completed_stages}")
    
    # Add a progress indicator for the overall workflow
    stages = ['Enhanced Detection', 'Data Preparation', 'Model Training', 
              'Model Application', 'Visualization', 'Comparison']
    
    with tqdm(total=len(stages), desc="Overall Progress", initial=len(completed_stages)) as pbar:
        # Stage 1: Enhanced physical detection
        if 'Enhanced Detection' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 1/{len(stages)}: {stages[0]}")
            
            print(f"Current memory usage: {memory_usage():.1f} MB")
            
            # Run memory-efficient detection using your implementation
            # This part is already implemented in your code
            #from zero_curtain_pipeline import run_memory_efficient_pipeline
            
            enhanced_events = run_memory_efficient_pipeline(
                feather_path=feather_path,
                output_dir=os.path.join(output_base_dir, 'enhanced'),
                site_batch_size=batch_size,
                checkpoint_interval=5,
                max_gap_hours=6,
                interpolation_method='cubic'
            )
            
            results['enhanced_events'] = enhanced_events
            results['enhanced_time'] = time.time() - start_time
            
            # Save progress
            completed_stages.add('Enhanced Detection')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            # Clean up memory
            gc.collect()
            print(f"Memory after Stage 1: {memory_usage():.1f} MB")
            pbar.update(1)
        else:
            # Load enhanced events if needed
            if 'enhanced_events' not in results:
                enhanced_events = load_checkpoint('enhanced_events')
                if enhanced_events is None:
                    # Try loading from CSV
                    csv_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events.csv')
                    if os.path.exists(csv_path):
                        enhanced_events = pd.read_csv(csv_path, parse_dates=['datetime_min', 'datetime_max'])
                    else:
                        print("Warning: No enhanced events found, cannot proceed with deep learning")
                        enhanced_events = pd.DataFrame()
                results['enhanced_events'] = enhanced_events
        
        # Stage 2: Data Preparation for Deep Learning
        if 'Data Preparation' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 2/{len(stages)}: {stages[1]}")
            
            # Get enhanced events
            enhanced_events = results.get('enhanced_events')
            if enhanced_events is not None and len(enhanced_events) > 0:
                try:
                    # Prepare data for deep learning with memory efficiency
                    X, y, metadata = prepare_data_for_deep_learning_efficiently(
                        feather_path=feather_path,
                        events_df=enhanced_events,
                        sequence_length=24,  # Use 24 time steps as in your original code
                        output_dir=os.path.join(output_base_dir, 'ml_data'),
                        batch_size=batch_size
                    )
                    
                    results['X'] = X.shape  # Store only shape to save memory
                    results['y'] = y.shape
                    results['data_preparation_time'] = time.time() - start_time
                    
                    # Clean up to free memory
                    del X, y
                    gc.collect()
                except Exception as e:
                    print(f"Error in data preparation: {str(e)}")
                    results['data_preparation_error'] = str(e)
            else:
                print("Skipping data preparation: No enhanced events available")
            
            # Save progress
            completed_stages.add('Data Preparation')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            # Clean up memory
            gc.collect()
            print(f"Memory after Stage 2: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 3: Model Training
        if 'Model Training' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 3/{len(stages)}: {stages[2]}")
            
            try:
                # Load prepared data
                data_dir = os.path.join(output_base_dir, 'ml_data')
                X = np.load(os.path.join(data_dir, 'X_features.npy'))
                y = np.load(os.path.join(data_dir, 'y_labels.npy'))
                
                # Load metadata if needed
                with open(os.path.join(data_dir, 'metadata.pkl'), 'rb') as f:
                    metadata = pickle.load(f)
                
                # Train model
                model, history, evaluation = train_zero_curtain_model_efficiently(
                    X=X, 
                    y=y,
                    metadata=metadata,
                    output_dir=os.path.join(output_base_dir, 'model')
                )
                
                # Store minimal results to save memory
                results['model_evaluation'] = evaluation
                results['model_training_time'] = time.time() - start_time
                
                # Clean up to free memory
                del X, y, metadata, model, history
                gc.collect()
            except Exception as e:
                print(f"Error in model training: {str(e)}")
                results['model_training_error'] = str(e)
            
            # Save progress
            completed_stages.add('Model Training')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            # Clean up memory
            gc.collect()
            print(f"Memory after Stage 3: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 4: Model Application
        if 'Model Application' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 4/{len(stages)}: {stages[3]}")
            
            try:
                # Load model
                model_path = os.path.join(output_base_dir, 'model', 'checkpoint.h5')
                if os.path.exists(model_path):
                    import tensorflow as tf
                    model = tf.keras.models.load_model(model_path)
                    
                    # Create directory for predictions
                    pred_dir = os.path.join(output_base_dir, 'predictions')
                    os.makedirs(pred_dir, exist_ok=True)
                    
                    # Apply model with memory efficiency (batched processing)
                    #from apply_model_efficiently import apply_model_to_new_data_efficiently
                    
                    predictions = apply_model_to_new_data_efficiently(
                        model=model,
                        feather_path=feather_path,
                        sequence_length=24,
                        output_dir=pred_dir,
                        batch_size=batch_size
                    )
                    
                    results['model_predictions_count'] = len(predictions)
                    results['model_application_time'] = time.time() - start_time
                    
                    # Clean up
                    del model, predictions
                    gc.collect()
                else:
                    print("Skipping model application: No model checkpoint found")
            except Exception as e:
                print(f"Error in model application: {str(e)}")
                results['model_application_error'] = str(e)
            
            # Save progress
            completed_stages.add('Model Application')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            # Clean up memory
            gc.collect()
            print(f"Memory after Stage 4: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stages 5 and 6: Visualization and Comparison
        # (Follow the same pattern - load data, process, clean up memory)
        
    # Generate final summary report
    total_time = sum([
        results.get('enhanced_time', 0),
        results.get('data_preparation_time', 0),
        results.get('model_training_time', 0),
        results.get('model_application_time', 0),
        results.get('visualization_time', 0),
        results.get('comparison_time', 0)
    ])
    
    print("\n" + "=" * 80)
    print("ZERO CURTAIN ANALYSIS COMPLETE")
    print("=" * 80)
    print(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
    
    return results

def apply_model_to_new_data_efficiently(model, feather_path, sequence_length=6, 
                                        output_dir=None, batch_size=50):
    """
    Apply a trained model to detect zero curtain events in new data with memory efficiency.
    
    Parameters:
    -----------
    model : tensorflow.keras.Model
        Trained zero curtain detection model
    feather_path : str
        Path to the feather file
    sequence_length : int
        Length of sequences used for model input
    output_dir : str, optional
        Output directory for results
    batch_size : int
        Number of site-depths to process per batch
        
    Returns:
    --------
    pandas.DataFrame
        DataFrame with predictions and probabilities
    """
    #from data_loader import get_unique_site_depths, load_site_depth_data
    import numpy as np
    from tqdm.auto import tqdm
    import os
    import gc
    import pandas as pd
    
    print("Applying model to new data...")
    print(f"Memory before application: {memory_usage():.1f} MB")
    
    # Create output directory
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    # Get site-depth combinations
    site_depths = get_unique_site_depths(feather_path)
    total_combinations = len(site_depths)
    print(f"Applying model to {total_combinations} site-depth combinations...")
    
    # Initialize list for all predictions
    all_predictions = []
    
    # Process in batches
    for batch_start in range(0, total_combinations, batch_size):
        batch_end = min(batch_start + batch_size, total_combinations)
        print(f"\nProcessing batch {batch_start+1}-{batch_end}/{total_combinations}")
        
        batch_predictions = []
        
        # Process each site-depth in batch
        for i in tqdm(range(batch_start, batch_end), desc="Making predictions"):
            site = site_depths.iloc[i]['source']
            temp_depth = site_depths.iloc[i]['soil_temp_depth']
            
            try:
                # Load data for this site-depth
                group = load_site_depth_data(feather_path, site, temp_depth)
                
                if len(group) < sequence_length + 1:
                    continue
                
                # Sort by time
                group = group.sort_values('datetime')
                
                # Prepare features (same as in training)
                feature_cols = ['soil_temp_standardized']
                group['temp_gradient'] = group['soil_temp_standardized'].diff()
                feature_cols.append('temp_gradient')
                group['depth_normalized'] = temp_depth / 10.0
                feature_cols.append('depth_normalized')
                
                # Add soil moisture if available
                if 'soil_moist_standardized' in group.columns and not group['soil_moist_standardized'].isna().all():
                    feature_cols.append('soil_moist_standardized')
                    group['moist_gradient'] = group['soil_moist_standardized'].diff()
                    feature_cols.append('moist_gradient')
                
                # Fill missing values
                group[feature_cols] = group[feature_cols].fillna(0)
                
                # Create sequences and predict in mini-batches to save memory
                sequences = []
                sequence_meta = []
                
                for j in range(len(group) - sequence_length):
                    # Get time window
                    start_time = group.iloc[j]['datetime']
                    end_time = group.iloc[j + sequence_length - 1]['datetime']
                    
                    # Extract sequence
                    sequence = group.iloc[j:j+sequence_length][feature_cols].values
                    sequences.append(sequence)
                    
                    # Store metadata
                    meta = {
                        'source': site,
                        'soil_temp_depth': temp_depth,
                        'datetime_min': start_time,
                        'datetime_max': end_time,
                        'latitude': group.iloc[j]['latitude'] if 'latitude' in group.columns else None,
                        'longitude': group.iloc[j]['longitude'] if 'longitude' in group.columns else None
                    }
                    sequence_meta.append(meta)
                    
                    # Process in mini-batches of 1000 sequences
                    if len(sequences) >= 1000:
                        # Make predictions
                        X_batch = np.array(sequences)
                        pred_probs = model.predict(X_batch, verbose=0)
                        
                        # Store results
                        for k, prob in enumerate(pred_probs):
                            meta = sequence_meta[k]
                            prediction = {
                                'source': meta['source'],
                                'soil_temp_depth': meta['soil_temp_depth'],
                                'datetime_min': meta['datetime_min'],
                                'datetime_max': meta['datetime_max'],
                                'zero_curtain_probability': float(prob[0]),
                                'is_zero_curtain': 1 if prob[0] > 0.5 else 0,
                                'latitude': meta['latitude'],
                                'longitude': meta['longitude']
                            }
                            batch_predictions.append(prediction)
                        
                        # Clear mini-batch to free memory
                        sequences = []
                        sequence_meta = []
                
                # Process any remaining sequences
                if sequences:
                    X_batch = np.array(sequences)
                    pred_probs = model.predict(X_batch, verbose=0)
                    
                    for k, prob in enumerate(pred_probs):
                        meta = sequence_meta[k]
                        prediction = {
                            'source': meta['source'],
                            'soil_temp_depth': meta['soil_temp_depth'],
                            'datetime_min': meta['datetime_min'],
                            'datetime_max': meta['datetime_max'],
                            'zero_curtain_probability': float(prob[0]),
                            'is_zero_curtain': 1 if prob[0] > 0.5 else 0,
                            'latitude': meta['latitude'],
                            'longitude': meta['longitude']
                        }
                        batch_predictions.append(prediction)
                
                # Clean up to free memory
                del group, sequences, sequence_meta, X_batch, pred_probs
                gc.collect()
                
            except Exception as e:
                print(f"Error processing {site}, depth {temp_depth}: {str(e)}")
                continue
        
        # Add batch predictions to all predictions
        all_predictions.extend(batch_predictions)
        
        # Save batch predictions
        if output_dir and batch_predictions:
            batch_df = pd.DataFrame(batch_predictions)
            batch_df.to_csv(os.path.join(output_dir, f'predictions_batch_{batch_start}_{batch_end}.csv'), index=False)
        
        # Clear batch to free memory
        del batch_predictions
        gc.collect()
        
        print(f"Memory after batch: {memory_usage():.1f} MB")
    
    # Consolidate all predictions
    print(f"Generated {len(all_predictions)} raw predictions")
    
    # Convert to DataFrame
    predictions_df = pd.DataFrame(all_predictions)
    
    # Save all predictions
    if output_dir and len(predictions_df) > 0:
        predictions_df.to_csv(os.path.join(output_dir, 'all_predictions.csv'), index=False)
    
    # Consolidate overlapping events to get final events
    if len(predictions_df) > 0:
        print("Consolidating overlapping events...")
        consolidated_events = consolidate_overlapping_events(predictions_df)
        print(f"Consolidated into {len(consolidated_events)} events")
        
        # Save consolidated events
        if output_dir:
            consolidated_events.to_csv(os.path.join(output_dir, 'consolidated_events.csv'), index=False)
        
        print(f"Memory after application: {memory_usage():.1f} MB")
        return consolidated_events
    else:
        print("No predictions generated")
        print(f"Memory after application: {memory_usage():.1f} MB")
        return pd.DataFrame()

def consolidate_overlapping_events(predictions_df, probability_threshold=0.5, gap_threshold=6):
    """
    Consolidate overlapping zero curtain events from model predictions.
    
    Parameters:
    -----------
    predictions_df : pandas.DataFrame
        DataFrame with model predictions
    probability_threshold : float
        Minimum probability to consider as zero curtain
    gap_threshold : float
        Maximum gap in hours to consider events as continuous
        
    Returns:
    --------
    pandas.DataFrame
        DataFrame with consolidated zero curtain events
    """
    import pandas as pd
    import numpy as np
    from tqdm.auto import tqdm
    
    print(f"Consolidating {len(predictions_df)} predictions...")
    
    # Filter to likely zero curtain events
    zero_curtain_events = predictions_df[predictions_df['zero_curtain_probability'] >= probability_threshold].copy()
    
    # Ensure datetime columns are datetime type
    if not pd.api.types.is_datetime64_dtype(zero_curtain_events['datetime_min']):
        zero_curtain_events['datetime_min'] = pd.to_datetime(zero_curtain_events['datetime_min'])
    if not pd.api.types.is_datetime64_dtype(zero_curtain_events['datetime_max']):
        zero_curtain_events['datetime_max'] = pd.to_datetime(zero_curtain_events['datetime_max'])
    
    # Process each site and depth separately
    consolidated_events = []
    
    # Get unique site-depth combinations
    site_depths = zero_curtain_events[['source', 'soil_temp_depth']].drop_duplicates()
    
    # Process each site-depth
    for _, row in tqdm(site_depths.iterrows(), total=len(site_depths), desc="Consolidating events"):
        site = row['source']
        depth = row['soil_temp_depth']
        
        # Get events for this site-depth
        group = zero_curtain_events[
            (zero_curtain_events['source'] == site) & 
            (zero_curtain_events['soil_temp_depth'] == depth)
        ].sort_values('datetime_min')
        
        current_event = None
        
        for _, event in group.iterrows():
            if current_event is None:
                # Start a new event
                current_event = {
                    'source': site,
                    'soil_temp_depth': depth,
                    'datetime_min': event['datetime_min'],
                    'datetime_max': event['datetime_max'],
                    'zero_curtain_probability': [event['zero_curtain_probability']],
                    'latitude': event['latitude'],
                    'longitude': event['longitude']
                }
            else:
                # Check if this event overlaps or is close to the current event
                time_gap = (event['datetime_min'] - current_event['datetime_max']).total_seconds() / 3600
                
                if time_gap <= gap_threshold:
                    # Extend the current event
                    current_event['datetime_max'] = max(current_event['datetime_max'], event['datetime_max'])
                    current_event['zero_curtain_probability'].append(event['zero_curtain_probability'])
                else:
                    # Finalize the current event
                    duration_hours = (current_event['datetime_max'] - current_event['datetime_min']).total_seconds() / 3600
                    
                    if duration_hours >= 12:  # Minimum duration threshold
                        final_event = {
                            'source': current_event['source'],
                            'soil_temp_depth': current_event['soil_temp_depth'],
                            'datetime_min': current_event['datetime_min'],
                            'datetime_max': current_event['datetime_max'],
                            'duration_hours': duration_hours,
                            'zero_curtain_probability': np.mean(current_event['zero_curtain_probability']),
                            'latitude': current_event['latitude'],
                            'longitude': current_event['longitude']
                        }
                        consolidated_events.append(final_event)
                    
                    # Start a new event
                    current_event = {
                        'source': site,
                        'soil_temp_depth': depth,
                        'datetime_min': event['datetime_min'],
                        'datetime_max': event['datetime_max'],
                        'zero_curtain_probability': [event['zero_curtain_probability']],
                        'latitude': event['latitude'],
                        'longitude': event['longitude']
                    }
        
        # Handle the last event
        if current_event is not None:
            duration_hours = (current_event['datetime_max'] - current_event['datetime_min']).total_seconds() / 3600
            
            if duration_hours >= 12:  # Minimum duration threshold
                final_event = {
                    'source': current_event['source'],
                    'soil_temp_depth': current_event['soil_temp_depth'],
                    'datetime_min': current_event['datetime_min'],
                    'datetime_max': current_event['datetime_max'],
                    'duration_hours': duration_hours,
                    'zero_curtain_probability': np.mean(current_event['zero_curtain_probability']),
                    'latitude': current_event['latitude'],
                    'longitude': current_event['longitude']
                }
                consolidated_events.append(final_event)
    
    # Convert to DataFrame
    consolidated_df = pd.DataFrame(consolidated_events)
    
    # Add region and latitude band classifications if latitude is available
    if len(consolidated_df) > 0 and 'latitude' in consolidated_df.columns:
        # Add region classification
        def assign_region(lat):
            if lat is None or pd.isna(lat):
                return None
            elif lat >= 66.5:
                return 'Arctic'
            elif lat >= 60:
                return 'Subarctic'
            elif lat >= 50:
                return 'Boreal'
            else:
                return 'Other'
        
        consolidated_df['region'] = consolidated_df['latitude'].apply(assign_region)
        
        # Add latitude band
        def assign_lat_band(lat):
            if lat is None or pd.isna(lat):
                return None
            elif lat < 49:
                return '<49°N'
            elif lat < 55:
                return '49-55°N (Boreal)'
            elif lat < 60:
                return '55-60°N (Boreal)'
            elif lat < 66.5:
                return '60-66.5°N (Sub-Arctic)'
            elif lat < 70:
                return '66.5-70°N (Arctic)'
            elif lat < 75:
                return '70-75°N (Arctic)'
            elif lat < 80:
                return '75-80°N (Arctic)'
            else:
                return '>80°N (Arctic)'
        
        consolidated_df['lat_band'] = consolidated_df['latitude'].apply(assign_lat_band)
    
    return consolidated_df

def visualize_events_efficiently(events_df, output_file=None):
    """
    Create visualizations for zero curtain events with memory efficiency.
    
    Parameters:
    -----------
    events_df : pandas.DataFrame
        DataFrame containing zero curtain events
    output_file : str, optional
        Path to save the visualization
        
    Returns:
    --------
    dict
        Statistics about the visualized events
    """
    import matplotlib.pyplot as plt
    import cartopy.crs as ccrs
    import cartopy.feature as cfeature
    import numpy as np
    from matplotlib.colors import PowerNorm
    import gc
    
    print(f"Creating visualization for {len(events_df)} events...")
    print(f"Memory before visualization: {memory_usage():.1f} MB")
    
    # Calculate percentile boundaries for better scaling
    p10 = np.percentile(events_df['duration_hours'], 10)
    p25 = np.percentile(events_df['duration_hours'], 25)
    p50 = np.percentile(events_df['duration_hours'], 50)  # median
    p75 = np.percentile(events_df['duration_hours'], 75)
    p90 = np.percentile(events_df['duration_hours'], 90)
    
    # Aggregate by site to reduce memory usage and plotting overhead
    site_data = events_df.groupby(['source', 'latitude', 'longitude']).agg({
        'duration_hours': ['count', 'mean', 'median', 'min', 'max']
    }).reset_index()
    
    # Flatten column names
    site_data.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col 
                        for col in site_data.columns]
    
    # Create figure
    fig, axes = plt.subplots(1, 2, figsize=(14, 7), 
                           subplot_kw={'projection': ccrs.NorthPolarStereo()})
    
    # Set map features
    for ax in axes:
        ax.set_extent([-180, 180, 45, 90], ccrs.PlateCarree())
        ax.add_feature(cfeature.LAND, facecolor='whitesmoke')
        ax.add_feature(cfeature.OCEAN, facecolor='aliceblue')
        ax.add_feature(cfeature.COASTLINE, linewidth=0.5)
        
        # Add Arctic Circle with label
        ax.plot(
            np.linspace(-180, 180, 60),
            np.ones(60) * 66.5,
            transform=ccrs.PlateCarree(),
            linestyle='-',
            color='gray',
            linewidth=1.0,
            alpha=0.7
        )
        
        ax.text(
            0, 66.5 + 2,
            "Arctic Circle",
            transform=ccrs.PlateCarree(),
            horizontalalignment='center',
            verticalalignment='bottom',
            fontsize=9,
            bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.2')
        )
    
    # Plot 1: Event count
    count_max = site_data['duration_hours_count'].quantile(0.95)
    scatter1 = axes[0].scatter(
        site_data['longitude'],
        site_data['latitude'],
        transform=ccrs.PlateCarree(),
        c=site_data['duration_hours_count'],
        s=30,
        cmap='viridis',
        vmin=1,
        vmax=count_max,
        alpha=0.8,
        edgecolor='none'
    )
    plt.colorbar(scatter1, ax=axes[0], shrink=0.7, pad=0.05, label='Event Count')
    axes[0].set_title('Zero Curtain Event Count', fontsize=12)
    
    # Plot 2: Mean duration using percentile bounds
    lower_bound = p10
    upper_bound = p90
    
    # Non-linear scaling for better color differentiation
    scatter2 = axes[1].scatter(
        site_data['longitude'],
        site_data['latitude'],
        transform=ccrs.PlateCarree(),
        c=site_data['duration_hours_mean'],
        s=30,
        cmap='RdYlBu_r',
        norm=PowerNorm(gamma=0.7, vmin=lower_bound, vmax=upper_bound),
        alpha=0.8,
        edgecolor='none'
    )
    
    # Create better colorbar with percentile markers
    cbar = plt.colorbar(scatter2, ax=axes[1], shrink=0.7, pad=0.05, 
                       label='Mean Duration (hours)')
    
    # Show percentile ticks
    percentile_ticks = [p10, p25, p50, p75, p90]
    cbar.set_ticks(percentile_ticks)
    cbar.set_ticklabels([f"{h:.0f}h\n({h/24:.1f}d)" for h in percentile_ticks])
    
    axes[1].set_title('Mean Zero Curtain Duration', fontsize=12)
    
    # Add comprehensive title with statistics
    plt.suptitle(
        f'Zero Curtain Analysis: {len(site_data)} Sites, {len(events_df)} Events\n' +
        f'Duration: median={p50:.1f}h ({p50/24:.1f}d), 10-90%={p10:.1f}-{p90:.1f}h',
        fontsize=14
    )
    
    plt.tight_layout(rect=[0, 0, 1, 0.93])
    
    # Save if requested
    if output_file:
        plt.savefig(output_file, dpi=200, bbox_inches='tight')
        print(f"Visualization saved to {output_file}")
    
    # Clean up to free memory
    plt.close(fig)
    del site_data, fig, axes
    gc.collect()
    
    print(f"Memory after visualization: {memory_usage():.1f} MB")
    
    # Return statistics
    return {
        'p10': p10,
        'p25': p25,
        'p50': p50,
        'p75': p75,
        'p90': p90,
        'mean': events_df['duration_hours'].mean(),
        'std': events_df['duration_hours'].std(),
        'min': events_df['duration_hours'].min(),
        'max': events_df['duration_hours'].max()
    }

def compare_detection_methods_efficiently(physical_events_file, model_events_file, output_dir=None):
    """
    Compare zero curtain events detected by different methods with memory efficiency.
    
    Parameters:
    -----------
    physical_events_file : str
        Path to CSV file with events detected by the physics-based method
    model_events_file : str
        Path to CSV file with events detected by the deep learning model
    output_dir : str, optional
        Directory to save comparison results
        
    Returns:
    --------
    dict
        Comparison statistics and metrics
    """
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    from datetime import timedelta
    import os
    import gc
    
    print("Comparing detection methods...")
    print(f"Memory before comparison: {memory_usage():.1f} MB")
    
    # Load events
    physical_events = pd.read_csv(physical_events_file, parse_dates=['datetime_min', 'datetime_max'])
    model_events = pd.read_csv(model_events_file, parse_dates=['datetime_min', 'datetime_max'])
    
    # Calculate basic statistics for each method
    physical_stats = {
        'total_events': len(physical_events),
        'unique_sites': physical_events['source'].nunique(),
        'median_duration': physical_events['duration_hours'].median(),
        'mean_duration': physical_events['duration_hours'].mean()
    }
    
    model_stats = {
        'total_events': len(model_events),
        'unique_sites': model_events['source'].nunique(),
        'median_duration': model_events['duration_hours'].median(),
        'mean_duration': model_events['duration_hours'].mean()
    }
    
    # Create a site-day matching table for overlap analysis
    # Process in batches to save memory
    physical_days = set()
    model_days = set()
    
    # Process physical events in batches
    batch_size = 1000
    for i in range(0, len(physical_events), batch_size):
        batch = physical_events.iloc[i:i+batch_size]
        
        for _, event in batch.iterrows():
            site = event['source']
            depth = event['soil_temp_depth']
            start_day = event['datetime_min'].date()
            end_day = event['datetime_max'].date()
            
            # Add each day of the event
            current_day = start_day
            while current_day <= end_day:
                physical_days.add((site, depth, current_day))
                current_day += timedelta(days=1)
        
        # Clear batch to free memory
        del batch
        gc.collect()
    
    # Process model events in batches
    for i in range(0, len(model_events), batch_size):
        batch = model_events.iloc[i:i+batch_size]
        
        for _, event in batch.iterrows():
            site = event['source']
            depth = event['soil_temp_depth']
            start_day = event['datetime_min'].date()
            end_day = event['datetime_max'].date()
            
            # Add each day of the event
            current_day = start_day
            while current_day <= end_day:
                model_days.add((site, depth, current_day))
                current_day += timedelta(days=1)
        
        # Clear batch to free memory
        del batch
        gc.collect()
    
    # Calculate overlap metrics
    overlap_days = physical_days.intersection(model_days)
    
    overlap_metrics = {
        'physical_only_days': len(physical_days - model_days),
        'model_only_days': len(model_days - physical_days),
        'overlap_days': len(overlap_days),
        'jaccard_index': len(overlap_days) / len(physical_days.union(model_days)) if len(physical_days.union(model_days)) > 0 else 0
    }
    
    # Print comparison results
    print("\n=== DETECTION METHOD COMPARISON ===\n")
    
    print("Physics-based Detection:")
    print(f"  Total Events: {physical_stats['total_events']}")
    print(f"  Unique Sites: {physical_stats['unique_sites']}")
    print(f"  Median Duration: {physical_stats['median_duration']:.1f} hours ({physical_stats['median_duration']/24:.1f} days)")
    print(f"  Mean Duration: {physical_stats['mean_duration']:.1f} hours ({physical_stats['mean_duration']/24:.1f} days)")
    
    print("\nDeep Learning Model Detection:")
    print(f"  Total Events: {model_stats['total_events']}")
    print(f"  Unique Sites: {model_stats['unique_sites']}")
    print(f"  Median Duration: {model_stats['median_duration']:.1f} hours ({model_stats['median_duration']/24:.1f} days)")
    print(f"  Mean Duration: {model_stats['mean_duration']:.1f} hours ({model_stats['mean_duration']/24:.1f} days)")
    
    print("\nOverlap Analysis:")
    print(f"  Days with Events (Physics-based): {len(physical_days)}")
    print(f"  Days with Events (Deep Learning): {len(model_days)}")
    print(f"  Days detected by both methods: {overlap_metrics['overlap_days']}")
    print(f"  Days detected only by Physics-based: {overlap_metrics['physical_only_days']}")
    print(f"  Days detected only by Deep Learning: {overlap_metrics['model_only_days']}")
    print(f"  Jaccard Index (overlap): {overlap_metrics['jaccard_index']:.4f}")
    
    # Generate comparison visualizations
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        
        # Create a Venn diagram of detection overlap
        try:
            from matplotlib_venn import venn2
            
            plt.figure(figsize=(8, 6))
            venn2(subsets=(len(physical_days - model_days), 
                          len(model_days - physical_days), 
                          len(overlap_days)),
                 set_labels=('Physics-based', 'Deep Learning'))
            plt.title('Overlap between Detection Methods', fontsize=14)
            plt.savefig(os.path.join(output_dir, 'detection_overlap.png'), dpi=200, bbox_inches='tight')
            plt.close()
        except ImportError:
            print("matplotlib_venn not installed. Skipping Venn diagram.")
        
        # Compare duration distributions
        plt.figure(figsize=(10, 6))
        
        sns.histplot(physical_events['duration_hours'], kde=True, alpha=0.5, 
                    label='Physics-based', color='blue', bins=50)
        sns.histplot(model_events['duration_hours'], kde=True, alpha=0.5, 
                    label='Deep Learning', color='red', bins=50)
        
        plt.xlabel('Duration (hours)')
        plt.ylabel('Frequency')
        plt.title('Comparison of Zero Curtain Duration Distributions')
        plt.legend()
        plt.grid(alpha=0.3)
        
        plt.savefig(os.path.join(output_dir, 'duration_comparison.png'), dpi=200, bbox_inches='tight')
        plt.close()
    
    # Clean up to free memory
    del physical_events, model_events, physical_days, model_days, overlap_days
    gc.collect()
    
    print(f"Memory after comparison: {memory_usage():.1f} MB")
    
    comparison_results = {
        'physical_stats': physical_stats,
        'model_stats': model_stats,
        'overlap_metrics': overlap_metrics
    }
    
    return comparison_results

def run_complete_pipeline(feather_path, output_base_dir='results'):
    """
    Run the complete zero curtain analysis pipeline with memory efficiency.
    
    Parameters:
    -----------
    feather_path : str
        Path to the feather file
    output_base_dir : str
        Base directory for outputs
        
    Returns:
    --------
    dict
        Summary of results
    """
    import os
    import time
    import gc
    import pickle
    from tqdm.auto import tqdm
    
    # Create output directories
    os.makedirs(output_base_dir, exist_ok=True)
    checkpoint_dir = os.path.join(output_base_dir, 'checkpoints')
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # Function to save/load checkpoint
    def save_checkpoint(data, name):
        with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'wb') as f:
            pickle.dump(data, f)
    
    def load_checkpoint(name):
        try:
            with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'rb') as f:
                return pickle.load(f)
        except:
            return None
    
    # Initialize results
    results = load_checkpoint('pipeline_results') or {}
    
    # Check for completed stages
    completed_stages = set(results.get('completed_stages', []))
    print(f"Found {len(completed_stages)} completed stages: {completed_stages}")
    
    # Define stages
    stages = ['Zero Curtain Detection', 'Data Preparation', 'Model Training', 
              'Model Application', 'Visualization', 'Comparison']
    
    # Overall progress bar
    with tqdm(total=len(stages), desc="Overall Progress", initial=len(completed_stages)) as pbar:
        # Stage 1: Zero Curtain Detection
        if 'Zero Curtain Detection' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 1/{len(stages)}: {stages[0]}")
            
            #from zero_curtain_pipeline import run_memory_efficient_pipeline
            
            enhanced_events = run_memory_efficient_pipeline(
                feather_path=feather_path,
                output_dir=os.path.join(output_base_dir, 'enhanced'),
                site_batch_size=20,
                checkpoint_interval=5,
                max_gap_hours=6,
                interpolation_method='cubic'
            )
            
            results['enhanced_events_count'] = len(enhanced_events)
            results['enhanced_time'] = time.time() - start_time
            
            # Save checkpoint
            completed_stages.add('Zero Curtain Detection')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            # Free memory
            del enhanced_events
            gc.collect()
            
            print(f"Memory after stage 1: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 2: Data Preparation
        if 'Data Preparation' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 2/{len(stages)}: {stages[1]}")
            
            # Load events
            enhanced_events_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events.csv')
            if os.path.exists(enhanced_events_path):
                import pandas as pd
                enhanced_events = pd.read_csv(enhanced_events_path, parse_dates=['datetime_min', 'datetime_max'])
                
                # Prepare data for model
                X, y, metadata = prepare_data_for_deep_learning_efficiently(
                    feather_path=feather_path,
                    events_df=enhanced_events,
                    sequence_length=24,
                    output_dir=os.path.join(output_base_dir, 'ml_data'),
                    batch_size=20
                )
                
                results['data_prep_time'] = time.time() - start_time
                results['data_shape'] = X.shape
                results['positive_examples'] = int(sum(y))
                results['positive_percentage'] = float(sum(y)/len(y)*100)
                
                # Clean up
                del X, y, metadata, enhanced_events
                gc.collect()
            else:
                print("No enhanced events file found, cannot proceed with data preparation")
                results['data_prep_error'] = "No enhanced events file found"
            
            # Save checkpoint
            completed_stages.add('Data Preparation')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            print(f"Memory after stage 2: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 3: Model Training
        if 'Model Training' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 3/{len(stages)}: {stages[2]}")
            
            ml_data_dir = os.path.join(output_base_dir, 'ml_data')
            x_path = os.path.join(ml_data_dir, 'X_features.npy')
            y_path = os.path.join(ml_data_dir, 'y_labels.npy')
            
            if os.path.exists(x_path) and os.path.exists(y_path):
                import numpy as np
                X = np.load(x_path)
                y = np.load(y_path)
                
                # Train model
                model, history, evaluation = train_zero_curtain_model_efficiently(
                    X=X,
                    y=y,
                    output_dir=os.path.join(output_base_dir, 'model')
                )
                
                results['model_training_time'] = time.time() - start_time
                results['model_evaluation'] = evaluation
                
                # Clean up
                del X, y, model, history, evaluation
                gc.collect()
            else:
                print("No prepared data found, cannot proceed with model training")
                results['model_training_error'] = "No prepared data found"
            
            # Save checkpoint
            completed_stages.add('Model Training')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            print(f"Memory after stage 3: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 4: Model Application
        if 'Model Application' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 4/{len(stages)}: {stages[3]}")
            
            model_path = os.path.join(output_base_dir, 'model', 'checkpoint.h5')
            
            if os.path.exists(model_path):
                import tensorflow as tf
                model = tf.keras.models.load_model(model_path)
                
                # Apply model
                predictions = apply_model_to_new_data_efficiently(
                    model=model,
                    feather_path=feather_path,
                    sequence_length=24,
                    output_dir=os.path.join(output_base_dir, 'predictions'),
                    batch_size=20
                )
                
                results['model_application_time'] = time.time() - start_time
                results['predictions_count'] = len(predictions)
                
                # Clean up
                del model, predictions
                gc.collect()
            else:
                print("No model checkpoint found, cannot proceed with model application")
                results['model_application_error'] = "No model checkpoint found"
            
            # Save checkpoint
            completed_stages.add('Model Application')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            print(f"Memory after stage 4: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 5: Visualization
        if 'Visualization' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 5/{len(stages)}: {stages[4]}")
            
            import pandas as pd
            
            # Visualize enhanced events
            enhanced_events_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events.csv')
            if os.path.exists(enhanced_events_path):
                enhanced_events = pd.read_csv(enhanced_events_path, parse_dates=['datetime_min', 'datetime_max'])
                
                enhanced_stats = visualize_events_efficiently(
                    events_df=enhanced_events,
                    output_file=os.path.join(output_base_dir, 'enhanced_visualization.png')
                )
                
                results['enhanced_visualization_stats'] = enhanced_stats
                
                # Clean up
                del enhanced_events, enhanced_stats
                gc.collect()
            
            # Visualize model predictions
            predictions_path = os.path.join(output_base_dir, 'predictions', 'consolidated_events.csv')
            if os.path.exists(predictions_path):
                model_events = pd.read_csv(predictions_path, parse_dates=['datetime_min', 'datetime_max'])
                
                model_stats = visualize_events_efficiently(
                    events_df=model_events,
                    output_file=os.path.join(output_base_dir, 'model_visualization.png')
                )
                
                results['model_visualization_stats'] = model_stats
                
                # Clean up
                del model_events, model_stats
                gc.collect()
            
            results['visualization_time'] = time.time() - start_time
            
            # Save checkpoint
            completed_stages.add('Visualization')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            print(f"Memory after stage 5: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 6: Comparison
        if 'Comparison' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 6/{len(stages)}: {stages[5]}")
            
            enhanced_events_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events.csv')
            predictions_path = os.path.join(output_base_dir, 'predictions', 'consolidated_events.csv')
            
            if os.path.exists(enhanced_events_path) and os.path.exists(predictions_path):
                comparison_results = compare_detection_methods_efficiently(
                    physical_events_file=enhanced_events_path,
                    model_events_file=predictions_path,
                    output_dir=os.path.join(output_base_dir, 'comparison')
                )
                
                results['comparison'] = comparison_results
            else:
                print("Missing events files, cannot perform comparison")
                results['comparison_error'] = "Missing events files"
            
            results['comparison_time'] = time.time() - start_time
            
            # Save checkpoint
            completed_stages.add('Comparison')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            print(f"Memory after stage 6: {memory_usage():.1f} MB")
            pbar.update(1)
    
    # Generate summary report
    total_time = (
        results.get('enhanced_time', 0) +
        results.get('data_prep_time', 0) +
        results.get('model_training_time', 0) +
        results.get('model_application_time', 0) +
        results.get('visualization_time', 0) +
        results.get('comparison_time', 0)
    )
    
    # Save summary to file
    with open(os.path.join(output_base_dir, 'summary.txt'), 'w') as f:
        f.write("ZERO CURTAIN ANALYSIS SUMMARY\n")
        f.write("=" * 30 + "\n\n")
        
        f.write(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\n\n")
        
        f.write("STAGE TIMINGS:\n")
        f.write(f"  Zero Curtain Detection: {results.get('enhanced_time', 0):.2f} seconds\n")
        f.write(f"  Data Preparation: {results.get('data_prep_time', 0):.2f} seconds\n")
        f.write(f"  Model Training: {results.get('model_training_time', 0):.2f} seconds\n")
        f.write(f"  Model Application: {results.get('model_application_time', 0):.2f} seconds\n")
        f.write(f"  Visualization: {results.get('visualization_time', 0):.2f} seconds\n")
        f.write(f"  Comparison: {results.get('comparison_time', 0):.2f} seconds\n\n")
        
        f.write("RESULTS SUMMARY:\n")
        f.write(f"  Enhanced Detection: {results.get('enhanced_events_count', 0)} events\n")
        f.write(f"  Model Predictions: {results.get('predictions_count', 0)} events\n")
        
        if 'comparison' in results and 'overlap_metrics' in results['comparison']:
            overlap = results['comparison']['overlap_metrics']['jaccard_index']
            f.write(f"  Method Agreement: {overlap*100:.1f}% overlap\n")
    
    print("\n" + "=" * 80)
    print("ZERO CURTAIN ANALYSIS COMPLETE")
    print("=" * 80)
    print(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
    print(f"Results saved to {output_base_dir}")
    
    return results

# # Combine results from all batches
# distances = np.vstack(all_distances)  # Efficient concatenation

# # Compute final geographic density
# geo_density = np.exp(-distances[:, 1:].mean(axis=1))

# #ALL OF THE CODE THROUGH THIS ITERATION OF SPLITTING
# import cartopy.crs as ccrs
# import cartopy.feature as cfeature
# import cmocean
# import gc
# import glob
# import json
# import logging
# import matplotlib.gridspec as gridspec
# import matplotlib.pyplot as plt
# import numpy as np
# import os
# import pandas as pd
# import pathlib
# import pickle
# import psutil
# import re
# import gc
# import dask
# import dask.dataframe as dd
# import scipy.interpolate as interpolate
# import scipy.stats as stats
# import seaborn as sns
# from pyproj import Proj
# import sklearn.experimental
# import sklearn.impute
# import sklearn.linear_model
# import sklearn.preprocessing
# import tqdm
# import xarray as xr
# import warnings
# warnings.filterwarnings('ignore')

# from osgeo import gdal, osr
# from matplotlib.colors import LinearSegmentedColormap
# from concurrent.futures import ThreadPoolExecutor
# from datetime import datetime, timedelta
# from pathlib import Path
# from scipy.spatial import cKDTree
# from tqdm import tqdm
# from tqdm.notebook import tqdm

# import tensorflow as tf
# try:
#     physical_devices = tf.config.list_physical_devices('GPU')
#     for device in physical_devices:
#         tf.config.experimental.set_memory_growth(device, True)
# except:
#     pass

# print("TensorFlow version:", tf.__version__)

# import keras_tuner as kt
# from keras_tuner.tuners import BayesianOptimization
# import tensorflow as tf
# import json
# import glob
# import keras
# from keras import layers
# from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
# import seaborn as sns
# from sklearn.metrics import confusion_matrix, classification_report

# import os
# import numpy as np
# import pandas as pd
# import gc
# import pickle
# import psutil
# from datetime import datetime, timedelta
# import time
# from scipy.interpolate import interp1d

# def memory_usage():
#     """Monitor memory usage in MB"""
#     process = psutil.Process(os.getpid())
#     mem_info = process.memory_info()
#     return mem_info.rss / 1024**2  # Memory in MB

# def save_checkpoint(data, checkpoint_dir, name):
#     """Save checkpoint data to pickle file"""
#     os.makedirs(checkpoint_dir, exist_ok=True)
#     checkpoint_path = os.path.join(checkpoint_dir, f'{name}.pkl')
#     with open(checkpoint_path, 'wb') as f:
#         pickle.dump(data, f)
#     print(f"Saved checkpoint to {checkpoint_path}")

# def load_checkpoint(checkpoint_dir, name):
#     """Load checkpoint data from pickle file"""
#     checkpoint_path = os.path.join(checkpoint_dir, f'{name}.pkl')
#     try:
#         with open(checkpoint_path, 'rb') as f:
#             data = pickle.load(f)
#         print(f"Loaded checkpoint from {checkpoint_path}")
#         return data
#     except:
#         print(f"No checkpoint found at {checkpoint_path}")
#         return None

# # data_loader.py
# import os
# import pandas as pd
# import numpy as np
# import pyarrow as pa
# import pyarrow.feather as pf
# import pyarrow.compute as pc
# import pyarrow.dataset as ds
# import gc
# from tqdm.auto import tqdm

# def get_time_range(feather_path):
#     """Get min and max datetime from feather file efficiently"""
#     print("Determining dataset time range")
#     try:
#         # Use PyArrow for efficient metadata access
#         table = pf.read_table(feather_path, columns=['datetime'])
#         datetime_col = table['datetime']
#         min_date = pd.Timestamp(pc.min(datetime_col).as_py())
#         max_date = pd.Timestamp(pc.max(datetime_col).as_py())
#         del table, datetime_col
#         gc.collect()
#     except Exception as e:
#         print(f"Error with PyArrow min/max: {str(e)}")
#         print("Falling back to reading sample rows...")
        
#         # Read just first and last rows after sorting
#         # This is more efficient than reading all data
#         try:
#             dataset = ds.dataset(feather_path, format='feather')
            
#             # Get min date from sorted data (first row)
#             table_min = dataset.to_table(
#                 columns=['datetime'],
#                 filter=None,
#                 use_threads=True,
#                 limit=1
#             )
#             min_date = pd.Timestamp(table_min['datetime'][0].as_py())
            
#             # Get max date from reverse sorted data (first row)
#             # Note: PyArrow Dataset API doesn't support reverse sort directly
#             # So we read all datetime values and find max
#             table_all = dataset.to_table(columns=['datetime'])
#             max_date = pd.Timestamp(pc.max(table_all['datetime']).as_py())
            
#             del table_min, table_all, dataset
#             gc.collect()
#         except Exception as inner_e:
#             print(f"Error with optimized approach: {str(inner_e)}")
#             print("Using full scan method (slow)...")
            
#             # Last resort: read all timestamps in chunks
#             min_date = pd.Timestamp.max
#             max_date = pd.Timestamp.min
            
#             # Open file directly to avoid loading everything
#             table = pf.read_table(feather_path, columns=['datetime'])
#             datetime_values = table['datetime'].to_pandas()
            
#             min_date = datetime_values.min()
#             max_date = datetime_values.max()
            
#             del datetime_values, table
#             gc.collect()
    
#     print(f"Data timespan: {min_date} to {max_date}")
#     return min_date, max_date

# def get_unique_site_depths(feather_path):
#     """Get unique site-depth combinations efficiently"""
#     print("Finding unique site-depth combinations")
#     print(f"Memory before processing: {memory_usage():.1f} MB")
    
#     try:
#         # Use PyArrow for efficient operation
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Read only the columns we need
#         table = dataset.to_table(columns=['source', 'soil_temp_depth'])
        
#         # Convert to pandas
#         df = table.to_pandas()
        
#         # Get valid combinations (where temp_depth is not NaN)
#         valid_df = df.dropna(subset=['soil_temp_depth'])
        
#         # Get unique combinations
#         site_depths = valid_df.drop_duplicates(['source', 'soil_temp_depth'])
        
#         # Clean up
#         del table, df, valid_df, dataset
#         gc.collect()
        
#     except Exception as e:
#         print(f"Error with PyArrow approach: {str(e)}")
#         print("Falling back to chunked pandas approach...")
        
#         # Fallback: Process in chunks
#         chunk_size = 1000000
#         unique_combos = set()
        
#         # Read feather in chunks (this is slower but more robust)
#         with pd.read_feather(feather_path, columns=['source', 'soil_temp_depth']) as reader:
#             for chunk in reader:
#                 # Get valid rows and unique combinations
#                 valid_rows = chunk.dropna(subset=['soil_temp_depth'])
#                 chunk_combinations = set(zip(valid_rows['source'], valid_rows['soil_temp_depth']))
#                 unique_combos.update(chunk_combinations)
                
#                 # Clean up
#                 del chunk, valid_rows, chunk_combinations
#                 gc.collect()
        
#         # Convert to DataFrame
#         site_depths = pd.DataFrame(list(unique_combos), columns=['source', 'soil_temp_depth'])
    
#     print(f"Found {len(site_depths)} unique site-depth combinations")
#     print(f"Memory after processing: {memory_usage():.1f} MB")
    
#     return site_depths[['source', 'soil_temp_depth']]

# def load_site_depth_data(feather_path, site, temp_depth):
#     """Load ONLY data for a specific site and depth using PyArrow filtering"""
#     print(f"Loading data for site: {site}, depth: {temp_depth}")
#     print(f"Memory before loading: {memory_usage():.1f} MB")
    
#     try:
#         # Use PyArrow Dataset API for efficient filtering
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Create filter expressions
#         site_filter = ds.field('source') == site
#         depth_filter = ds.field('soil_temp_depth') == float(temp_depth)
#         combined_filter = site_filter & depth_filter
        
#         # Apply filter and read only filtered data
#         table = dataset.to_table(filter=combined_filter)
#         filtered_df = table.to_pandas()
        
#         # Clean up
#         del table, dataset
#         gc.collect()
        
#     except Exception as e:
#         print(f"Error with PyArrow filtering: {str(e)}")
#         print("Falling back to pandas filtering...")
        
#         # Define chunk size based on available memory
#         available_mem = psutil.virtual_memory().available / (1024**2)  # MB
#         chunk_size = min(500000, max(100000, int(available_mem / 10)))
        
#         filtered_chunks = []
        
#         # Read and filter in chunks
#         for chunk in pd.read_feather(feather_path, columns=None, chunksize=chunk_size):
#             # Filter by site and depth
#             chunk_filtered = chunk[(chunk['source'] == site) & 
#                                    (chunk['soil_temp_depth'] == temp_depth)]
            
#             if len(chunk_filtered) > 0:
#                 filtered_chunks.append(chunk_filtered)
            
#             # Clean up
#             del chunk, chunk_filtered
#             gc.collect()
        
#         # Combine filtered chunks
#         if filtered_chunks:
#             filtered_df = pd.concat(filtered_chunks, ignore_index=True)
#             del filtered_chunks
#         else:
#             filtered_df = pd.DataFrame()
        
#         gc.collect()
    
#     # Ensure datetime is in datetime format
#     if 'datetime' in filtered_df.columns and not pd.api.types.is_datetime64_dtype(filtered_df['dat...
#         filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])
    
#     print(f"Loaded {len(filtered_df)} rows for site-depth")
#     print(f"Memory after loading: {memory_usage():.1f} MB")
    
#     return filtered_df

# # data_loader.py
# import os
# import pandas as pd
# import numpy as np
# import pyarrow as pa
# import pyarrow.feather as pf
# import pyarrow.compute as pc
# import pyarrow.dataset as ds
# import gc
# from tqdm.auto import tqdm

# def get_time_range(feather_path):
#     """Get min and max datetime from feather file efficiently"""
#     print("Determining dataset time range")
#     try:
#         # Use PyArrow for efficient metadata access
#         table = pf.read_table(feather_path, columns=['datetime'])
#         datetime_col = table['datetime']
#         min_date = pd.Timestamp(pc.min(datetime_col).as_py())
#         max_date = pd.Timestamp(pc.max(datetime_col).as_py())
#         del table, datetime_col
#         gc.collect()
#     except Exception as e:
#         print(f"Error with PyArrow min/max: {str(e)}")
#         print("Falling back to reading sample rows...")
        
#         # Read just first and last rows after sorting
#         # This is more efficient than reading all data
#         try:
#             dataset = ds.dataset(feather_path, format='feather')
            
#             # Get min date from sorted data (first row)
#             table_min = dataset.to_table(
#                 columns=['datetime'],
#                 filter=None,
#                 use_threads=True,
#                 limit=1
#             )
#             min_date = pd.Timestamp(table_min['datetime'][0].as_py())
            
#             # Get max date from reverse sorted data (first row)
#             # Note: PyArrow Dataset API doesn't support reverse sort directly
#             # So we read all datetime values and find max
#             table_all = dataset.to_table(columns=['datetime'])
#             max_date = pd.Timestamp(pc.max(table_all['datetime']).as_py())
            
#             del table_min, table_all, dataset
#             gc.collect()
#         except Exception as inner_e:
#             print(f"Error with optimized approach: {str(inner_e)}")
#             print("Using full scan method (slow)...")
            
#             # Last resort: read all timestamps in chunks
#             min_date = pd.Timestamp.max
#             max_date = pd.Timestamp.min
            
#             # Open file directly to avoid loading everything
#             table = pf.read_table(feather_path, columns=['datetime'])
#             datetime_values = table['datetime'].to_pandas()
            
#             min_date = datetime_values.min()
#             max_date = datetime_values.max()
            
#             del datetime_values, table
#             gc.collect()
    
#     print(f"Data timespan: {min_date} to {max_date}")
#     return min_date, max_date

# def get_unique_site_depths(feather_path):
#     """Get unique site-depth combinations efficiently"""
#     print("Finding unique site-depth combinations")
#     print(f"Memory before processing: {memory_usage():.1f} MB")
    
#     try:
#         # Use PyArrow for efficient operation
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Read only the columns we need
#         table = dataset.to_table(columns=['source', 'soil_temp_depth'])
        
#         # Convert to pandas
#         df = table.to_pandas()
        
#         # Get valid combinations (where temp_depth is not NaN)
#         valid_df = df.dropna(subset=['soil_temp_depth'])
        
#         # Get unique combinations
#         site_depths = valid_df.drop_duplicates(['source', 'soil_temp_depth'])
        
#         # Clean up
#         del table, df, valid_df, dataset
#         gc.collect()
        
#     except Exception as e:
#         print(f"Error with PyArrow approach: {str(e)}")
#         print("Falling back to chunked pandas approach...")
        
#         # Fallback: Process in chunks
#         chunk_size = 1000000
#         unique_combos = set()
        
#         # Read feather in chunks (this is slower but more robust)
#         with pd.read_feather(feather_path, columns=['source', 'soil_temp_depth']) as reader:
#             for chunk in reader:
#                 # Get valid rows and unique combinations
#                 valid_rows = chunk.dropna(subset=['soil_temp_depth'])
#                 chunk_combinations = set(zip(valid_rows['source'], valid_rows['soil_temp_depth']))
#                 unique_combos.update(chunk_combinations)
                
#                 # Clean up
#                 del chunk, valid_rows, chunk_combinations
#                 gc.collect()
        
#         # Convert to DataFrame
#         site_depths = pd.DataFrame(list(unique_combos), columns=['source', 'soil_temp_depth'])
    
#     print(f"Found {len(site_depths)} unique site-depth combinations")
#     print(f"Memory after processing: {memory_usage():.1f} MB")
    
#     return site_depths[['source', 'soil_temp_depth']]

# def load_site_depth_data(feather_path, site, temp_depth):
#     """Load ONLY data for a specific site and depth using PyArrow filtering"""
#     print(f"Loading data for site: {site}, depth: {temp_depth}")
#     print(f"Memory before loading: {memory_usage():.1f} MB")
    
#     try:
#         # Use PyArrow Dataset API for efficient filtering
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Create filter expressions
#         site_filter = ds.field('source') == site
#         depth_filter = ds.field('soil_temp_depth') == float(temp_depth)
#         combined_filter = site_filter & depth_filter
        
#         # Apply filter and read only filtered data
#         table = dataset.to_table(filter=combined_filter)
#         filtered_df = table.to_pandas()
        
#         # Clean up
#         del table, dataset
#         gc.collect()
        
#     except Exception as e:
#         print(f"Error with PyArrow filtering: {str(e)}")
#         print("Falling back to pandas filtering...")
        
#         # Define chunk size based on available memory
#         available_mem = psutil.virtual_memory().available / (1024**2)  # MB
#         chunk_size = min(500000, max(100000, int(available_mem / 10)))
        
#         filtered_chunks = []
        
#         # Read and filter in chunks
#         for chunk in pd.read_feather(feather_path, columns=None, chunksize=chunk_size):
#             # Filter by site and depth
#             chunk_filtered = chunk[(chunk['source'] == site) & 
#                                    (chunk['soil_temp_depth'] == temp_depth)]
            
#             if len(chunk_filtered) > 0:
#                 filtered_chunks.append(chunk_filtered)
            
#             # Clean up
#             del chunk, chunk_filtered
#             gc.collect()
        
#         # Combine filtered chunks
#         if filtered_chunks:
#             filtered_df = pd.concat(filtered_chunks, ignore_index=True)
#             del filtered_chunks
#         else:
#             filtered_df = pd.DataFrame()
        
#         gc.collect()
    
#     # Ensure datetime is in datetime format
#     if 'datetime' in filtered_df.columns and not pd.api.types.is_datetime64_dtype(filtered_df['dat...
#         filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])
    
#     print(f"Loaded {len(filtered_df)} rows for site-depth")
#     print(f"Memory after loading: {memory_usage():.1f} MB")
    
#     return filtered_df

# def prepare_data_for_deep_learning_efficiently(feather_path, events_df, sequence_length=6, 
#                                                output_dir=None, batch_size=500, start_batch=0):
#     """
#     Memory-efficient version of prepare_data_for_deep_learning that processes
#     site-depths in batches and saves intermediate results without accumulating all data in memory.
#     """
#     import numpy as np
#     from tqdm.auto import tqdm
#     import os
#     import gc
#     import pandas as pd
    
#     print("Preparing data for deep learning model...")
#     print(f"Memory before preparation: {memory_usage():.1f} MB")
    
#     # Ensure datetime columns are proper datetime objects
#     if not pd.api.types.is_datetime64_dtype(events_df['datetime_min']):
#         events_df['datetime_min'] = pd.to_datetime(events_df['datetime_min'], format='mixed')
#     if not pd.api.types.is_datetime64_dtype(events_df['datetime_max']):
#         events_df['datetime_max'] = pd.to_datetime(events_df['datetime_max'], format='mixed')
    
#     # Create a mapping of detected events for labeling
#     print("Creating event mapping...")
#     event_map = {}
#     for _, event in events_df.iterrows():
#         site = event['source']
#         depth = event['soil_temp_depth']
#         start = event['datetime_min']
#         end = event['datetime_max']
        
#         if (site, depth) not in event_map:
#             event_map[(site, depth)] = []
        
#         event_map[(site, depth)].append((start, end))
    
#     # Get site-depth combinations for progress tracking
#     print("Finding unique site-depth combinations")
#     print(f"Memory before processing: {memory_usage():.1f} MB")
    
#     # Use a function that efficiently gets unique site-depths without loading all data
#     site_depths = get_unique_site_depths(feather_path)
#     total_combinations = len(site_depths)
    
#     print(f"Found {total_combinations} unique site-depth combinations")
#     print(f"Memory after processing: {memory_usage():.1f} MB")
#     print(f"Preparing sequences from {total_combinations} site-depth combinations...")
    
#     # Check for existing batch files to support resuming
#     if output_dir is not None:
#         os.makedirs(output_dir, exist_ok=True)
#         import glob
#         existing_batches = glob.glob(os.path.join(output_dir, 'X_batch_*_*.npy'))
#         if existing_batches and start_batch == 0:
#             # Extract batch numbers from filenames
#             batch_ends = [int(os.path.basename(f).split('_')[-1].split('.')[0]) for f in existing_...
#             if batch_ends:
#                 last_processed_batch = max(batch_ends)
#                 # Start from the next batch
#                 start_batch = (last_processed_batch // batch_size) * batch_size + batch_size
#                 print(f"Found existing batch files, resuming from batch {start_batch}")
    
#     # Track total counts for reporting
#     total_sequences = 0
#     total_positive = 0
    
#     # Process in batches to manage memory
#     for batch_start in range(start_batch, total_combinations, batch_size):
#         batch_end = min(batch_start + batch_size, total_combinations)
#         print(f"\nProcessing batch {batch_start+1}-{batch_end}/{total_combinations}")
        
#         # Initialize lists for this batch only
#         batch_features = []
#         batch_labels = []
#         batch_metadata = []
        
#         # Process each site-depth in the batch
#         for i in tqdm(range(batch_start, batch_end), desc="Creating sequences"):
#             site = site_depths.iloc[i]['source']
#             temp_depth = site_depths.iloc[i]['soil_temp_depth']
            
#             # Skip if no events exist for this site-depth
#             if (site, temp_depth) not in event_map and len(event_map) > 0:
#                 continue
            
#             try:
#                 # Load only the data for this site-depth
#                 print(f"Loading data for site: {site}, depth: {temp_depth}")
#                 print(f"Memory before loading: {memory_usage():.1f} MB")
                
#                 group = load_site_depth_data(feather_path, site, temp_depth)
                
#                 print(f"Loaded {len(group)} rows for site-depth")
#                 print(f"Memory after loading: {memory_usage():.1f} MB")
                
#                 if len(group) < sequence_length + 1:
#                     continue
                
#                 # Ensure datetime is in datetime format
#                 if not pd.api.types.is_datetime64_dtype(group['datetime']):
#                     group['datetime'] = pd.to_datetime(group['datetime'], format='mixed')
                
#                 # Sort by time
#                 group = group.sort_values('datetime')
                
#                 # Create feature set
#                 feature_cols = ['soil_temp_standardized']
                
#                 # Calculate gradient features
#                 group['temp_gradient'] = group['soil_temp_standardized'].diff()
#                 feature_cols.append('temp_gradient')
                
#                 # Add soil depth as feature
#                 group['depth_normalized'] = temp_depth / 10.0
#                 feature_cols.append('depth_normalized')
                
#                 # Add soil moisture if available
#                 has_moisture = False
#                 if 'soil_moist_standardized' in group.columns and not group['soil_moist_standardiz...
#                     has_moisture = True
#                     feature_cols.append('soil_moist_standardized')
#                     group['moist_gradient'] = group['soil_moist_standardized'].diff()
#                     feature_cols.append('moist_gradient')
                
#                 # Fill missing values
#                 group[feature_cols] = group[feature_cols].fillna(0)
                
#                 # Create sequences with sliding window
#                 for j in range(len(group) - sequence_length):
#                     # Get time window
#                     start_time = group.iloc[j]['datetime']
#                     end_time = group.iloc[j + sequence_length - 1]['datetime']
                    
#                     # Extract sequence data
#                     sequence = group.iloc[j:j+sequence_length][feature_cols].values
                    
#                     # Check if this sequence overlaps with known zero curtain event
#                     is_zero_curtain = 0
#                     if (site, temp_depth) in event_map:
#                         for event_start, event_end in event_map[(site, temp_depth)]:
#                             # Ensure proper datetime comparison
#                             # Check for significant overlap (at least 50% of sequence)
#                             if (min(end_time, event_end) - max(start_time, event_start)).total_sec...
#                                0.5 * (end_time - start_time).total_seconds():
#                                 is_zero_curtain = 1
#                                 break
                    
#                     # Store features and labels
#                     batch_features.append(sequence)
#                     batch_labels.append(is_zero_curtain)
#                     total_positive += is_zero_curtain
#                     total_sequences += 1
                    
#                     # Store metadata
#                     meta = {
#                         'source': site,
#                         'soil_temp_depth': temp_depth,
#                         'start_time': start_time,
#                         'end_time': end_time,
#                         'latitude': group.iloc[j]['latitude'] if 'latitude' in group.columns else ...
#                         'longitude': group.iloc[j]['longitude'] if 'longitude' in group.columns el...
#                         'has_moisture_data': has_moisture
#                     }
#                     batch_metadata.append(meta)
                
#                 # Clean up to free memory
#                 del group
#                 gc.collect()
                
#             except Exception as e:
#                 print(f"Error processing {site}, depth {temp_depth}: {str(e)}")
#                 import traceback
#                 traceback.print_exc()
#                 continue
        
#         # Save batch results if output directory provided and we have data
#         if output_dir is not None and batch_features:
#             os.makedirs(output_dir, exist_ok=True)
            
#             # Save batch as numpy files
#             batch_X = np.array(batch_features)
#             batch_y = np.array(batch_labels)
            
#             np.save(os.path.join(output_dir, f'X_batch_{batch_start}_{batch_end}.npy'), batch_X)
#             np.save(os.path.join(output_dir, f'y_batch_{batch_start}_{batch_end}.npy'), batch_y)
            
#             # Save metadata as pickle
#             import pickle
#             with open(os.path.join(output_dir, f'metadata_batch_{batch_start}_{batch_end}.pkl'), '...
#                 pickle.dump(batch_metadata, f)
            
#             # Save progress marker
#             with open(os.path.join(output_dir, 'progress.txt'), 'w') as f:
#                 f.write(f"Last processed batch: {batch_start}-{batch_end}\n")
#                 f.write(f"Total sequences: {total_sequences}\n")
#                 f.write(f"Positive examples: {total_positive} ({total_positive/total_sequences*100...
        
#         # Clean up batch variables - this is key for memory efficiency
#         del batch_features, batch_labels, batch_metadata
#         if 'batch_X' in locals(): del batch_X
#         if 'batch_y' in locals(): del batch_y
#         gc.collect()
        
#         print(f"Memory after batch: {memory_usage():.1f} MB")
#         #print(f"Progress: {total_sequences} sequences processed, {total_positive} positive exampl...
#         positive_percentage = (total_positive/total_sequences*100) if total_sequences > 0 else 0
#         print(f"Progress: {total_sequences} sequences processed, {total_positive} positive example...

#     print("Data preparation complete!")
#     print(f"Total sequences: {total_sequences}")
#     #print(f"Positive examples: {total_positive} ({total_positive/total_sequences*100:.1f}% if tot...
#     positive_percentage = (total_positive/total_sequences*100) if total_sequences > 0 else 0
#     print(f"Positive examples: {total_positive} ({positive_percentage:.1f}%)")
    
#     if output_dir is not None:
#         print(f"Results saved to {output_dir}")
#         print("To merge batch files into final dataset, use merge_batch_files(output_dir)")
    
#     print(f"Memory after preparation: {memory_usage():.1f} MB")
    
#     # Return info instead of data - prevents memory issues
#     return {
#         'total_sequences': total_sequences,
#         'total_positive': total_positive,
#         'positive_percentage': total_positive/total_sequences*100 if total_sequences > 0 else 0,
#         'output_dir': output_dir
#     }


# def merge_batch_files(output_dir):
#     """
#     Merge all batch files into single X_features.npy and y_labels.npy files.
#     """
#     import numpy as np
#     import os
#     import glob
#     import pickle
#     import gc
    
#     print(f"Memory before merging: {memory_usage():.1f} MB")
    
#     # Find all batch files
#     x_batch_files = sorted(glob.glob(os.path.join(output_dir, 'X_batch_*_*.npy')))
#     y_batch_files = sorted(glob.glob(os.path.join(output_dir, 'y_batch_*_*.npy')))
#     metadata_batch_files = sorted(glob.glob(os.path.join(output_dir, 'metadata_batch_*_*.pkl')))
    
#     print(f"Found {len(x_batch_files)} X batches, {len(y_batch_files)} y batches, and {len(metadat...
    
#     # Get dimensions of first batch to initialize arrays
#     if x_batch_files:
#         first_batch = np.load(x_batch_files[0])
#         shape = first_batch.shape
#         del first_batch
#         gc.collect()
        
#         # Count total sequences
#         total_sequences = 0
#         for batch_file in x_batch_files:
#             batch = np.load(batch_file)
#             total_sequences += batch.shape[0]
#             del batch
#             gc.collect()
        
#         # Pre-allocate arrays
#         X = np.zeros((total_sequences, shape[1], shape[2]), dtype=np.float32)
#         y = np.zeros(total_sequences, dtype=np.int32)
        
#         # Load and copy batches
#         idx = 0
#         for i, (x_file, y_file) in enumerate(zip(x_batch_files, y_batch_files)):
#             print(f"Processing batch {i+1}/{len(x_batch_files)}")
#             print(f"Memory: {memory_usage():.1f} MB")
            
#             batch_x = np.load(x_file)
#             batch_y = np.load(y_file)
            
#             batch_size = batch_x.shape[0]
#             X[idx:idx+batch_size] = batch_x
#             y[idx:idx+batch_size] = batch_y
            
#             idx += batch_size
            
#             # Clean up
#             del batch_x, batch_y
#             gc.collect()
        
#         # Save merged X and y
#         print(f"Saving merged arrays: X.shape={X.shape}, y.shape={y.shape}")
#         np.save(os.path.join(output_dir, 'X_features.npy'), X)
#         np.save(os.path.join(output_dir, 'y_labels.npy'), y)
        
#         # Clean up
#         del X, y
#         gc.collect()
        
#         # Load and concatenate metadata batches
#         all_metadata = []
#         for i, batch_file in enumerate(metadata_batch_files):
#             print(f"Processing metadata batch {i+1}/{len(metadata_batch_files)}")
#             print(f"Memory: {memory_usage():.1f} MB")
            
#             with open(batch_file, 'rb') as f:
#                 batch_metadata = pickle.load(f)
#             all_metadata.extend(batch_metadata)
            
#             # Clean up
#             del batch_metadata
#             gc.collect()
        
#         # Save merged metadata
#         print(f"Saving merged metadata: {len(all_metadata)} entries")
#         with open(os.path.join(output_dir, 'metadata.pkl'), 'wb') as f:
#             pickle.dump(all_metadata, f)
        
#         # Final cleanup
#         del all_metadata
#         gc.collect()
        
#         print(f"Memory after merging: {memory_usage():.1f} MB")
#         return {
#             'total_sequences': total_sequences,
#             'output_dir': output_dir
#         }
#     else:
#         print("No batch files found to merge")
#         return None
        
# output_base_dir = '/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling'
# data_dir = os.path.join(output_base_dir, 'ml_data')
# X = np.load(os.path.join(data_dir, 'X_features.npy'))
# y = np.load(os.path.join(data_dir, 'y_labels.npy'))
# with open(os.path.join(data_dir, 'metadata.pkl'), 'rb') as f:
#     metadata = pickle.load(f)

# def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
#     """
#     Advanced model architecture combining ConvLSTM, Transformers, and 
#     Variational Autoencoder components to better capture complex zero curtain dynamics.
    
#     Parameters:
#     -----------
#     input_shape : tuple
#         Shape of input data (sequence_length, num_features)
#     include_moisture : bool
#         Whether soil moisture features are included
        
#     Returns:
#     --------
#     tensorflow.keras.Model
#         Compiled model ready for training
#     """
#     import tensorflow as tf
#     from tensorflow.keras.models import Model
#     from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
#     from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPoo...
#     from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
#     from tensorflow.keras.optimizers import Adam
    
#     # Input layer
#     inputs = Input(shape=input_shape)
    
#     # Reshape for ConvLSTM (add spatial dimension)
#     # From (sequence_length, features) to (sequence_length, 1, features)
    
#     #x = Reshape((input_shape[0], 1, input_shape[1]))(inputs)
#     x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
#     # ConvLSTM layer to capture spatiotemporal patterns
#     from tensorflow.keras.layers import ConvLSTM2D
#     convlstm = ConvLSTM2D(
#         filters=64,
#         kernel_size=(3, 1),
#         padding='same',
#         return_sequences=True,
#         activation='tanh',
#         recurrent_dropout=0.2
#     )(x)
    
#     # Reshape back to (sequence_length, features)
#     convlstm = Reshape((input_shape[0], 64))(convlstm)
    
#     # Add positional encoding for transformer
#     def positional_encoding(length, depth):
#         positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
#         depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
#         angle_rates = 1 / tf.pow(10000.0, depths)
#         angle_rads = positions * angle_rates
        
#         pos_encoding = tf.concat(
#             [tf.sin(angle_rads), tf.cos(angle_rads)], axis=-1)
        
#         return pos_encoding
    
#     # Add positional encoding
#     pos_encoding = positional_encoding(input_shape[0], 64)
#     transformer_input = convlstm + pos_encoding
    
#     # Transformer encoder block
#     def transformer_encoder(x, num_heads=8, key_dim=64, ff_dim=128):
#         # Multi-head attention
#         attention_output = MultiHeadAttention(
#             num_heads=num_heads, key_dim=key_dim
#         )(x, x)
        
#         # Skip connection 1
#         x1 = Add()([attention_output, x])
#         x1 = LayerNormalization(epsilon=1e-6)(x1)
        
#         # Feed-forward network
#         ff_output = Dense(ff_dim, activation='relu')(x1)
#         ff_output = Dropout(0.1)(ff_output)
#         ff_output = Dense(64)(ff_output)
        
#         # Skip connection 2
#         x2 = Add()([ff_output, x1])
#         return LayerNormalization(epsilon=1e-6)(x2)
    
#     # Apply transformer encoder
#     transformer_output = transformer_encoder(transformer_input)
    
#     # Parallel CNN paths for multi-scale feature extraction
#     cnn_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(inputs)
#     cnn_1 = BatchNormalization()(cnn_1)
    
#     cnn_2 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(inputs)
#     cnn_2 = BatchNormalization()(cnn_2)
    
#     cnn_3 = Conv1D(filters=32, kernel_size=7, padding='same', activation='relu')(inputs)
#     cnn_3 = BatchNormalization()(cnn_3)
    
#     # Variational Autoencoder components
#     def sampling(args):
#         z_mean, z_log_var = args
#         batch = tf.shape(z_mean)[0]
#         dim = tf.shape(z_mean)[1]
#         epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
#         return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
#     # Global temporal features
#     global_max = GlobalMaxPooling1D()(transformer_output)
#     global_avg = GlobalAveragePooling1D()(transformer_output)
    
#     # VAE encoding
#     z_mean = Dense(32)(Concatenate()([global_max, global_avg]))
#     z_log_var = Dense(32)(Concatenate()([global_max, global_avg]))
#     z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
#     # Combine all features
#     merged_features = Concatenate()(
#         [
#             GlobalMaxPooling1D()(cnn_1),
#             GlobalMaxPooling1D()(cnn_2),
#             GlobalMaxPooling1D()(cnn_3),
#             global_max,
#             global_avg,
#             z
#         ]
#     )
    
#     # Final classification layers
#     x = Dense(128, activation='relu')(merged_features)
#     x = Dropout(0.3)(x)
#     x = BatchNormalization()(x)
#     x = Dense(64, activation='relu')(x)
#     x = Dropout(0.2)(x)
    
#     # Output layer
#     outputs = Dense(1, activation='sigmoid')(x)
    
#     # Create model
#     model = Model(inputs=inputs, outputs=outputs)
    
#     # Add VAE loss
#     kl_loss = -0.5 * tf.reduce_mean(
#         z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
#     )
#     model.add_loss(0.001 * kl_loss)  # Small weight for VAE loss
    
#     # Compile model with appropriate metrics
#     model.compile(
#         optimizer=Adam(learning_rate=0.001),
#         loss='binary_crossentropy',
#         metrics=[
#             'accuracy',
#             tf.keras.metrics.AUC(name='auc'),
#             tf.keras.metrics.Precision(name='precision'),
#             tf.keras.metrics.Recall(name='recall')
#         ]
#     )
    
#     return model

# # Time-based split rather than random split
# def temporal_train_test_split(X, y, metadata, val_ratio=0.2, test_ratio=0.1):
#     """
#     Split data temporally for time series modeling.
    
#     Parameters:
#     -----------
#     X : numpy.ndarray
#         Input features
#     y : numpy.ndarray
#         Output labels
#     metadata : list
#         Metadata containing timestamps for each sequence
#     val_ratio : float
#         Proportion of data for validation
#     test_ratio : float
#         Proportion of data for testing
        
#     Returns:
#     --------
#     tuple
#         (X_train, X_val, X_test, y_train, y_val, y_test)
#     """
#     # Extract timestamps from metadata
#     timestamps = [meta['start_time'] for meta in metadata]
    
#     # Sort indices by timestamp
#     sorted_indices = sorted(range(len(timestamps)), key=lambda i: timestamps[i])
    
#     # Calculate split points
#     n_samples = len(sorted_indices)
#     test_start = int(n_samples * (1 - test_ratio))
#     val_start = int(n_samples * (1 - test_ratio - val_ratio))
    
#     # Split indices into train, validation, and test sets
#     train_indices = sorted_indices[:val_start]
#     val_indices = sorted_indices[val_start:test_start]
#     test_indices = sorted_indices[test_start:]
    
#     # Create the splits
#     X_train = X[train_indices]
#     y_train = y[train_indices]
    
#     X_val = X[val_indices]
#     y_val = y[val_indices]
    
#     X_test = X[test_indices]
#     y_test = y[test_indices]
    
#     return X_train, X_val, X_test, y_train, y_val, y_test

# def train_zero_curtain_model_efficiently(X, y, metadata=None, output_dir=None):
#     """
#     Memory-efficient version of train_zero_curtain_model that implements
#     batch training and model checkpointing with temporal data splitting.
    
#     Parameters:
#     -----------
#     X : numpy.ndarray
#         Input features
#     y : numpy.ndarray
#         Output labels
#     metadata : list, optional
#         Metadata about each sequence (must contain timestamps)
#     output_dir : str, optional
#         Directory to save model and results
        
#     Returns:
#     --------
#     tuple
#         (trained_model, training_history, evaluation_results)
#     """
#     import tensorflow as tf
#     from tensorflow.keras.callbacks import (
#         EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, 
#         CSVLogger, TensorBoard
#     )
#     import matplotlib.pyplot as plt
#     import os
#     import gc
#     import numpy as np
    
#     # Enable memory growth to avoid pre-allocating all GPU memory
#     physical_devices = tf.config.list_physical_devices('GPU')
#     if physical_devices:
#         for device in physical_devices:
#             tf.config.experimental.set_memory_growth(device, True)
#             print(f"Enabled memory growth for {device}")
    
#     print("Training zero curtain model...")
#     print(f"Memory before training: {memory_usage():.1f} MB")
    
#     # Create output directory
#     if output_dir:
#         os.makedirs(output_dir, exist_ok=True)
    
#     # Temporal split for time series data
#     print("Performing temporal split for train/validation/test sets...")
#     if metadata is None:
#         raise ValueError("Metadata with timestamps is required for temporal splitting")
    
#     # Extract timestamps from metadata
#     timestamps = np.array([meta['start_time'] for meta in metadata])
    
#     # Sort indices by timestamp
#     sorted_indices = np.argsort(timestamps)
    
#     # Calculate split points (70% train, 15% validation, 15% test)
#     n_samples = len(sorted_indices)
#     test_ratio = 0.15
#     val_ratio = 0.15
    
#     test_start = int(n_samples * (1 - test_ratio))
#     val_start = int(n_samples * (1 - test_ratio - val_ratio))
    
#     # Split indices into train, validation, and test sets
#     train_indices = sorted_indices[:val_start]
#     val_indices = sorted_indices[val_start:test_start]
#     test_indices = sorted_indices[test_start:]
    
#     print(f"Training on data from {timestamps[train_indices[0]]} to {timestamps[train_indices[-1]]...
#     print(f"Validating on data from {timestamps[val_indices[0]]} to {timestamps[val_indices[-1]]}"...
#     print(f"Testing on data from {timestamps[test_indices[0]]} to {timestamps[test_indices[-1]]}")
    
#     # Create the splits
#     X_train = X[train_indices]
#     y_train = y[train_indices]
    
#     X_val = X[val_indices]
#     y_val = y[val_indices]
    
#     X_test = X[test_indices]
#     y_test = y[test_indices]
    
#     print(f"Split sizes: Train={len(X_train)}, Validation={len(X_val)}, Test={len(X_test)}")
    
#     # Check class balance in each split
#     train_pos = np.sum(y_train)
#     val_pos = np.sum(y_val)
#     test_pos = np.sum(y_test)
    
#     print(f"Positive examples: Train={train_pos} ({train_pos/len(y_train)*100:.1f}%), " +
#           f"Val={val_pos} ({val_pos/len(y_val)*100:.1f}%), " +
#           f"Test={test_pos} ({test_pos/len(y_test)*100:.1f}%)")
    
#     # Clean up to free memory
#     del sorted_indices, timestamps
#     gc.collect()
    
#     # Build model with appropriate input shape
#     print("Building model...")
#     input_shape = (X_train.shape[1], X_train.shape[2])
    
#     model = build_advanced_zero_curtain_model(input_shape)
    
#     # If output directory exists, check for existing model checkpoint
#     model_checkpoint_path = None
#     if output_dir:
#         model_checkpoint_path = os.path.join(output_dir, 'checkpoint.h5')
#         if os.path.exists(model_checkpoint_path):
#             print(f"Loading existing model checkpoint from {model_checkpoint_path}")
#             try:
#                 model = tf.keras.models.load_model(model_checkpoint_path)
#                 print("Checkpoint loaded successfully")
#             except Exception as e:
#                 print(f"Error loading checkpoint: {str(e)}")
    
#     # Set up callbacks with additional memory management
#     callbacks = [
#         # Stop early if validation performance plateaus
#         EarlyStopping(
#             patience=15,  # Increased patience for temporal data
#             restore_best_weights=True, 
#             monitor='val_auc', 
#             mode='max'
#         ),
#         # Reduce learning rate when improvement slows
#         ReduceLROnPlateau(
#             factor=0.5, 
#             patience=7,  # Increased patience for temporal data
#             min_lr=1e-6, 
#             monitor='val_auc', 
#             mode='max'
#         ),
#         # Manual garbage collection after each epoch
#         tf.keras.callbacks.LambdaCallback(
#             on_epoch_end=lambda epoch, logs: gc.collect()
#         )
#     ]
    
#     # Add additional callbacks if output directory provided
#     if output_dir:
#         callbacks.extend([
#             # Save best model
#             ModelCheckpoint(
#                 os.path.join(output_dir, 'checkpoint.h5'),
#                 save_best_only=True,
#                 monitor='val_auc',
#                 mode='max'
#             ),
#             # Log training progress to CSV
#             CSVLogger(
#                 os.path.join(output_dir, 'training_log.csv'),
#                 append=True
#             ),
#             # TensorBoard visualization
#             TensorBoard(
#                 log_dir=os.path.join(output_dir, 'tensorboard_logs'),
#                 histogram_freq=1,
#                 profile_batch=0  # Disable profiling to save memory
#             )
#         ])
    
#     # Calculate class weights to handle imbalance
#     pos_weight = len(y_train) / max(sum(y_train), 1)
#     class_weight = {0: 1, 1: pos_weight}
#     print(f"Using class weight {pos_weight:.2f} for positive class")
    
#     # Train model with memory-efficient settings
#     print("Training model...")
#     batch_size = 32  # Adjust based on available memory
#     epochs = 100
    
#     # Use fit with appropriate memory settings
#     history = model.fit(
#         X_train, y_train,
#         validation_data=(X_val, y_val),
#         epochs=epochs,
#         batch_size=batch_size,
#         callbacks=callbacks,
#         class_weight=class_weight,
#         verbose=1,
#         # Memory efficiency settings
#         shuffle=True,  # Still shuffle within the temporal train split
#         use_multiprocessing=False,  # Avoid extra memory overhead
#         workers=1  # Reduce parallel processing to save memory
#     )
    
#     # Clean up to free memory
#     del X_train, y_train, X_val, y_val
#     gc.collect()
    
#     # Evaluate on test set
#     print("Evaluating model on test set...")
#     evaluation = model.evaluate(X_test, y_test, batch_size=batch_size)
#     print("Test performance:")
#     for metric, value in zip(model.metrics_names, evaluation):
#         print(f"  {metric}: {value:.4f}")
    
#     # Generate predictions for visualization and further analysis
#     y_pred_prob = model.predict(X_test, batch_size=batch_size)
#     y_pred = (y_pred_prob > 0.5).astype(int).flatten()
    
#     # Calculate and save additional evaluation metrics
#     from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
#     report = classification_report(y_test, y_pred)
#     conf_matrix = confusion_matrix(y_test, y_pred)
    
#     print("Classification Report:")
#     print(report)
    
#     print("Confusion Matrix:")
#     print(conf_matrix)
    
#     # Plot and save training history
#     if output_dir:
#         # Save evaluation metrics
#         with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w') as f:
#             f.write("Classification Report:\n")
#             f.write(report)
#             f.write("\n\nConfusion Matrix:\n")
#             f.write(str(conf_matrix))
#             f.write("\n\nTest Metrics:\n")
#             for metric, value in zip(model.metrics_names, evaluation):
#                 f.write(f"{metric}: {value:.4f}\n")
        
#         # Plot training history
#         plt.figure(figsize=(16, 6))
        
#         plt.subplot(1, 3, 1)
#         plt.plot(history.history['auc'])
#         plt.plot(history.history['val_auc'])
#         plt.title('Model AUC')
#         plt.ylabel('AUC')
#         plt.xlabel('Epoch')
#         plt.legend(['Train', 'Validation'], loc='lower right')
        
#         plt.subplot(1, 3, 2)
#         plt.plot(history.history['loss'])
#         plt.plot(history.history['val_loss'])
#         plt.title('Model Loss')
#         plt.ylabel('Loss')
#         plt.xlabel('Epoch')
#         plt.legend(['Train', 'Validation'], loc='upper right')
        
#         # Plot ROC curve
#         plt.subplot(1, 3, 3)
#         fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
#         roc_auc = auc(fpr, tpr)
#         plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')
#         plt.plot([0, 1], [0, 1], 'k--')
#         plt.xlabel('False Positive Rate')
#         plt.ylabel('True Positive Rate')
#         plt.title('ROC Curve (Test Set)')
#         plt.legend(loc='lower right')
        
#         plt.tight_layout()
#         plt.savefig(os.path.join(output_dir, 'training_performance.png'), dpi=300)
        
#         # Save detailed model summary
#         from contextlib import redirect_stdout
#         with open(os.path.join(output_dir, 'model_summary.txt'), 'w') as f:
#             with redirect_stdout(f):
#                 model.summary()
    
#     # Clean up to free memory
#     del X_test, y_test, y_pred, y_pred_prob
#     gc.collect()
    
#     print(f"Memory after training: {memory_usage():.1f} MB")
#     return model, history, evaluation

# def run_full_analysis_pipeline(feather_path, output_base_dir='results', batch_size=50):
#     """
#     Run the complete zero curtain analysis pipeline with progress tracking
#     and memory efficiency.
    
#     Parameters:
#     -----------
#     feather_path : str
#         Path to the feather file with merged data
#     output_base_dir : str
#         Base directory for saving outputs
#     batch_size : int
#         Number of site-depths to process per batch
        
#     Returns:
#     --------
#     dict
#         Dictionary containing analysis results
#     """
#     from tqdm.auto import tqdm
#     import time
#     import os
#     import gc
#     import pickle
    
#     # Create output directories
#     os.makedirs(output_base_dir, exist_ok=True)
#     checkpoint_dir = os.path.join(output_base_dir, 'checkpoints')
#     os.makedirs(checkpoint_dir, exist_ok=True)
    
#     # Function to save checkpoint
#     def save_checkpoint(data, name):
#         with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'wb') as f:
#             pickle.dump(data, f)
    
#     # Function to load checkpoint
#     def load_checkpoint(name):
#         try:
#             with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'rb') as f:
#                 return pickle.load(f)
#         except:
#             return None
    
#     # Initialize results
#     results = load_checkpoint('pipeline_results') or {}
    
#     # Check for completed stages
#     completed_stages = set(results.get('completed_stages', []))
#     print(f"Found {len(completed_stages)} completed stages: {completed_stages}")
    
#     # Add a progress indicator for the overall workflow
#     stages = ['Enhanced Detection', 'Data Preparation', 'Model Training', 
#               'Model Application', 'Visualization', 'Comparison']
    
#     with tqdm(total=len(stages), desc="Overall Progress", initial=len(completed_stages)) as pbar:
#         # Stage 1: Enhanced physical detection
#         if 'Enhanced Detection' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 1/{len(stages)}: {stages[0]}")
            
#             print(f"Current memory usage: {memory_usage():.1f} MB")
            
#             # Run memory-efficient detection using your implementation
#             # This part is already implemented in your code
#             #from zero_curtain_pipeline import run_memory_efficient_pipeline
            
#             enhanced_events = run_memory_efficient_pipeline(
#                 feather_path=feather_path,
#                 output_dir=os.path.join(output_base_dir, 'enhanced'),
#                 site_batch_size=batch_size,
#                 checkpoint_interval=5,
#                 max_gap_hours=6,
#                 interpolation_method='cubic'
#             )
            
#             results['enhanced_events'] = enhanced_events
#             results['enhanced_time'] = time.time() - start_time
            
#             # Save progress
#             completed_stages.add('Enhanced Detection')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             # Clean up memory
#             gc.collect()
#             print(f"Memory after Stage 1: {memory_usage():.1f} MB")
#             pbar.update(1)
#         else:
#             # Load enhanced events if needed
#             if 'enhanced_events' not in results:
#                 enhanced_events = load_checkpoint('enhanced_events')
#                 if enhanced_events is None:
#                     # Try loading from CSV
#                     csv_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events.csv'...
#                     if os.path.exists(csv_path):
#                         enhanced_events = pd.read_csv(csv_path, parse_dates=['datetime_min', 'date...
#                     else:
#                         print("Warning: No enhanced events found, cannot proceed with deep learnin...
#                         enhanced_events = pd.DataFrame()
#                 results['enhanced_events'] = enhanced_events
        
#         # Stage 2: Data Preparation for Deep Learning
#         if 'Data Preparation' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 2/{len(stages)}: {stages[1]}")
            
#             # Get enhanced events
#             enhanced_events = results.get('enhanced_events')
#             if enhanced_events is not None and len(enhanced_events) > 0:
#                 try:
#                     # Prepare data for deep learning with memory efficiency
#                     X, y, metadata = prepare_data_for_deep_learning_efficiently(
#                         feather_path=feather_path,
#                         events_df=enhanced_events,
#                         sequence_length=24,  # Use 24 time steps as in your original code
#                         output_dir=os.path.join(output_base_dir, 'ml_data'),
#                         batch_size=batch_size
#                     )
                    
#                     results['X'] = X.shape  # Store only shape to save memory
#                     results['y'] = y.shape
#                     results['data_preparation_time'] = time.time() - start_time
                    
#                     # Clean up to free memory
#                     del X, y
#                     gc.collect()
#                 except Exception as e:
#                     print(f"Error in data preparation: {str(e)}")
#                     results['data_preparation_error'] = str(e)
#             else:
#                 print("Skipping data preparation: No enhanced events available")
            
#             # Save progress
#             completed_stages.add('Data Preparation')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             # Clean up memory
#             gc.collect()
#             print(f"Memory after Stage 2: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 3: Model Training
#         if 'Model Training' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 3/{len(stages)}: {stages[2]}")
            
#             try:
#                 # Load prepared data
#                 data_dir = os.path.join(output_base_dir, 'ml_data')
#                 X = np.load(os.path.join(data_dir, 'X_features.npy'))
#                 y = np.load(os.path.join(data_dir, 'y_labels.npy'))
                
#                 # Load metadata if needed
#                 with open(os.path.join(data_dir, 'metadata.pkl'), 'rb') as f:
#                     metadata = pickle.load(f)
                
#                 # Train model
#                 model, history, evaluation = train_zero_curtain_model_efficiently(
#                     X=X, 
#                     y=y,
#                     metadata=metadata,
#                     output_dir=os.path.join(output_base_dir, 'model')
#                 )
                
#                 # Store minimal results to save memory
#                 results['model_evaluation'] = evaluation
#                 results['model_training_time'] = time.time() - start_time
                
#                 # Clean up to free memory
#                 del X, y, metadata, model, history
#                 gc.collect()
#             except Exception as e:
#                 print(f"Error in model training: {str(e)}")
#                 results['model_training_error'] = str(e)
            
#             # Save progress
#             completed_stages.add('Model Training')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             # Clean up memory
#             gc.collect()
#             print(f"Memory after Stage 3: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 4: Model Application
#         if 'Model Application' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 4/{len(stages)}: {stages[3]}")
            
#             try:
#                 # Load model
#                 model_path = os.path.join(output_base_dir, 'model', 'checkpoint.h5')
#                 if os.path.exists(model_path):
#                     import tensorflow as tf
#                     model = tf.keras.models.load_model(model_path)
                    
#                     # Create directory for predictions
#                     pred_dir = os.path.join(output_base_dir, 'predictions')
#                     os.makedirs(pred_dir, exist_ok=True)
                    
#                     # Apply model with memory efficiency (batched processing)
#                     #from apply_model_efficiently import apply_model_to_new_data_efficiently
                    
#                     predictions = apply_model_to_new_data_efficiently(
#                         model=model,
#                         feather_path=feather_path,
#                         sequence_length=24,
#                         output_dir=pred_dir,
#                         batch_size=batch_size
#                     )
                    
#                     results['model_predictions_count'] = len(predictions)
#                     results['model_application_time'] = time.time() - start_time
                    
#                     # Clean up
#                     del model, predictions
#                     gc.collect()
#                 else:
#                     print("Skipping model application: No model checkpoint found")
#             except Exception as e:
#                 print(f"Error in model application: {str(e)}")
#                 results['model_application_error'] = str(e)
            
#             # Save progress
#             completed_stages.add('Model Application')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             # Clean up memory
#             gc.collect()
#             print(f"Memory after Stage 4: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stages 5 and 6: Visualization and Comparison
#         # (Follow the same pattern - load data, process, clean up memory)
        
#     # Generate final summary report
#     total_time = sum([
#         results.get('enhanced_time', 0),
#         results.get('data_preparation_time', 0),
#         results.get('model_training_time', 0),
#         results.get('model_application_time', 0),
#         results.get('visualization_time', 0),
#         results.get('comparison_time', 0)
#     ])
    
#     print("\n" + "=" * 80)
#     print("ZERO CURTAIN ANALYSIS COMPLETE")
#     print("=" * 80)
#     print(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
    
#     return results

# def apply_model_to_new_data_efficiently(model, feather_path, sequence_length=6, 
#                                         output_dir=None, batch_size=50):
#     """
#     Apply a trained model to detect zero curtain events in new data with memory efficiency.
    
#     Parameters:
#     -----------
#     model : tensorflow.keras.Model
#         Trained zero curtain detection model
#     feather_path : str
#         Path to the feather file
#     sequence_length : int
#         Length of sequences used for model input
#     output_dir : str, optional
#         Output directory for results
#     batch_size : int
#         Number of site-depths to process per batch
        
#     Returns:
#     --------
#     pandas.DataFrame
#         DataFrame with predictions and probabilities
#     """
#     #from data_loader import get_unique_site_depths, load_site_depth_data
#     import numpy as np
#     from tqdm.auto import tqdm
#     import os
#     import gc
#     import pandas as pd
    
#     print("Applying model to new data...")
#     print(f"Memory before application: {memory_usage():.1f} MB")
    
#     # Create output directory
#     if output_dir:
#         os.makedirs(output_dir, exist_ok=True)
    
#     # Get site-depth combinations
#     site_depths = get_unique_site_depths(feather_path)
#     total_combinations = len(site_depths)
#     print(f"Applying model to {total_combinations} site-depth combinations...")
    
#     # Initialize list for all predictions
#     all_predictions = []
    
#     # Process in batches
#     for batch_start in range(0, total_combinations, batch_size):
#         batch_end = min(batch_start + batch_size, total_combinations)
#         print(f"\nProcessing batch {batch_start+1}-{batch_end}/{total_combinations}")
        
#         batch_predictions = []
        
#         # Process each site-depth in batch
#         for i in tqdm(range(batch_start, batch_end), desc="Making predictions"):
#             site = site_depths.iloc[i]['source']
#             temp_depth = site_depths.iloc[i]['soil_temp_depth']
            
#             try:
#                 # Load data for this site-depth
#                 group = load_site_depth_data(feather_path, site, temp_depth)
                
#                 if len(group) < sequence_length + 1:
#                     continue
                
#                 # Sort by time
#                 group = group.sort_values('datetime')
                
#                 # Prepare features (same as in training)
#                 feature_cols = ['soil_temp_standardized']
#                 group['temp_gradient'] = group['soil_temp_standardized'].diff()
#                 feature_cols.append('temp_gradient')
#                 group['depth_normalized'] = temp_depth / 10.0
#                 feature_cols.append('depth_normalized')
                
#                 # Add soil moisture if available
#                 if 'soil_moist_standardized' in group.columns and not group['soil_moist_standardiz...
#                     feature_cols.append('soil_moist_standardized')
#                     group['moist_gradient'] = group['soil_moist_standardized'].diff()
#                     feature_cols.append('moist_gradient')
                
#                 # Fill missing values
#                 group[feature_cols] = group[feature_cols].fillna(0)
                
#                 # Create sequences and predict in mini-batches to save memory
#                 sequences = []
#                 sequence_meta = []
                
#                 for j in range(len(group) - sequence_length):
#                     # Get time window
#                     start_time = group.iloc[j]['datetime']
#                     end_time = group.iloc[j + sequence_length - 1]['datetime']
                    
#                     # Extract sequence
#                     sequence = group.iloc[j:j+sequence_length][feature_cols].values
#                     sequences.append(sequence)
                    
#                     # Store metadata
#                     meta = {
#                         'source': site,
#                         'soil_temp_depth': temp_depth,
#                         'datetime_min': start_time,
#                         'datetime_max': end_time,
#                         'latitude': group.iloc[j]['latitude'] if 'latitude' in group.columns else ...
#                         'longitude': group.iloc[j]['longitude'] if 'longitude' in group.columns el...
#                     }
#                     sequence_meta.append(meta)
                    
#                     # Process in mini-batches of 1000 sequences
#                     if len(sequences) >= 1000:
#                         # Make predictions
#                         X_batch = np.array(sequences)
#                         pred_probs = model.predict(X_batch, verbose=0)
                        
#                         # Store results
#                         for k, prob in enumerate(pred_probs):
#                             meta = sequence_meta[k]
#                             prediction = {
#                                 'source': meta['source'],
#                                 'soil_temp_depth': meta['soil_temp_depth'],
#                                 'datetime_min': meta['datetime_min'],
#                                 'datetime_max': meta['datetime_max'],
#                                 'zero_curtain_probability': float(prob[0]),
#                                 'is_zero_curtain': 1 if prob[0] > 0.5 else 0,
#                                 'latitude': meta['latitude'],
#                                 'longitude': meta['longitude']
#                             }
#                             batch_predictions.append(prediction)
                        
#                         # Clear mini-batch to free memory
#                         sequences = []
#                         sequence_meta = []
                
#                 # Process any remaining sequences
#                 if sequences:
#                     X_batch = np.array(sequences)
#                     pred_probs = model.predict(X_batch, verbose=0)
                    
#                     for k, prob in enumerate(pred_probs):
#                         meta = sequence_meta[k]
#                         prediction = {
#                             'source': meta['source'],
#                             'soil_temp_depth': meta['soil_temp_depth'],
#                             'datetime_min': meta['datetime_min'],
#                             'datetime_max': meta['datetime_max'],
#                             'zero_curtain_probability': float(prob[0]),
#                             'is_zero_curtain': 1 if prob[0] > 0.5 else 0,
#                             'latitude': meta['latitude'],
#                             'longitude': meta['longitude']
#                         }
#                         batch_predictions.append(prediction)
                
#                 # Clean up to free memory
#                 del group, sequences, sequence_meta, X_batch, pred_probs
#                 gc.collect()
                
#             except Exception as e:
#                 print(f"Error processing {site}, depth {temp_depth}: {str(e)}")
#                 continue
        
#         # Add batch predictions to all predictions
#         all_predictions.extend(batch_predictions)
        
#         # Save batch predictions
#         if output_dir and batch_predictions:
#             batch_df = pd.DataFrame(batch_predictions)
#             batch_df.to_csv(os.path.join(output_dir, f'predictions_batch_{batch_start}_{batch_end}...
        
#         # Clear batch to free memory
#         del batch_predictions
#         gc.collect()
        
#         print(f"Memory after batch: {memory_usage():.1f} MB")
    
#     # Consolidate all predictions
#     print(f"Generated {len(all_predictions)} raw predictions")
    
#     # Convert to DataFrame
#     predictions_df = pd.DataFrame(all_predictions)
    
#     # Save all predictions
#     if output_dir and len(predictions_df) > 0:
#         predictions_df.to_csv(os.path.join(output_dir, 'all_predictions.csv'), index=False)
    
#     # Consolidate overlapping events to get final events
#     if len(predictions_df) > 0:
#         print("Consolidating overlapping events...")
#         consolidated_events = consolidate_overlapping_events(predictions_df)
#         print(f"Consolidated into {len(consolidated_events)} events")
        
#         # Save consolidated events
#         if output_dir:
#             consolidated_events.to_csv(os.path.join(output_dir, 'consolidated_events.csv'), index=...
        
#         print(f"Memory after application: {memory_usage():.1f} MB")
#         return consolidated_events
#     else:
#         print("No predictions generated")
#         print(f"Memory after application: {memory_usage():.1f} MB")
#         return pd.DataFrame()

# def consolidate_overlapping_events(predictions_df, probability_threshold=0.5, gap_threshold=6):
#     """
#     Consolidate overlapping zero curtain events from model predictions.
    
#     Parameters:
#     -----------
#     predictions_df : pandas.DataFrame
#         DataFrame with model predictions
#     probability_threshold : float
#         Minimum probability to consider as zero curtain
#     gap_threshold : float
#         Maximum gap in hours to consider events as continuous
        
#     Returns:
#     --------
#     pandas.DataFrame
#         DataFrame with consolidated zero curtain events
#     """
#     import pandas as pd
#     import numpy as np
#     from tqdm.auto import tqdm
    
#     print(f"Consolidating {len(predictions_df)} predictions...")
    
#     # Filter to likely zero curtain events
#     zero_curtain_events = predictions_df[predictions_df['zero_curtain_probability'] >= probability...
    
#     # Ensure datetime columns are datetime type
#     if not pd.api.types.is_datetime64_dtype(zero_curtain_events['datetime_min']):
#         zero_curtain_events['datetime_min'] = pd.to_datetime(zero_curtain_events['datetime_min'])
#     if not pd.api.types.is_datetime64_dtype(zero_curtain_events['datetime_max']):
#         zero_curtain_events['datetime_max'] = pd.to_datetime(zero_curtain_events['datetime_max'])
    
#     # Process each site and depth separately
#     consolidated_events = []
    
#     # Get unique site-depth combinations
#     site_depths = zero_curtain_events[['source', 'soil_temp_depth']].drop_duplicates()
    
#     # Process each site-depth
#     for _, row in tqdm(site_depths.iterrows(), total=len(site_depths), desc="Consolidating events"...
#         site = row['source']
#         depth = row['soil_temp_depth']
        
#         # Get events for this site-depth
#         group = zero_curtain_events[
#             (zero_curtain_events['source'] == site) & 
#             (zero_curtain_events['soil_temp_depth'] == depth)
#         ].sort_values('datetime_min')
        
#         current_event = None
        
#         for _, event in group.iterrows():
#             if current_event is None:
#                 # Start a new event
#                 current_event = {
#                     'source': site,
#                     'soil_temp_depth': depth,
#                     'datetime_min': event['datetime_min'],
#                     'datetime_max': event['datetime_max'],
#                     'zero_curtain_probability': [event['zero_curtain_probability']],
#                     'latitude': event['latitude'],
#                     'longitude': event['longitude']
#                 }
#             else:
#                 # Check if this event overlaps or is close to the current event
#                 time_gap = (event['datetime_min'] - current_event['datetime_max']).total_seconds()...
                
#                 if time_gap <= gap_threshold:
#                     # Extend the current event
#                     current_event['datetime_max'] = max(current_event['datetime_max'], event['date...
#                     current_event['zero_curtain_probability'].append(event['zero_curtain_probabili...
#                 else:
#                     # Finalize the current event
#                     duration_hours = (current_event['datetime_max'] - current_event['datetime_min'...
                    
#                     if duration_hours >= 12:  # Minimum duration threshold
#                         final_event = {
#                             'source': current_event['source'],
#                             'soil_temp_depth': current_event['soil_temp_depth'],
#                             'datetime_min': current_event['datetime_min'],
#                             'datetime_max': current_event['datetime_max'],
#                             'duration_hours': duration_hours,
#                             'zero_curtain_probability': np.mean(current_event['zero_curtain_probab...
#                             'latitude': current_event['latitude'],
#                             'longitude': current_event['longitude']
#                         }
#                         consolidated_events.append(final_event)
                    
#                     # Start a new event
#                     current_event = {
#                         'source': site,
#                         'soil_temp_depth': depth,
#                         'datetime_min': event['datetime_min'],
#                         'datetime_max': event['datetime_max'],
#                         'zero_curtain_probability': [event['zero_curtain_probability']],
#                         'latitude': event['latitude'],
#                         'longitude': event['longitude']
#                     }
        
#         # Handle the last event
#         if current_event is not None:
#             duration_hours = (current_event['datetime_max'] - current_event['datetime_min']).total...
            
#             if duration_hours >= 12:  # Minimum duration threshold
#                 final_event = {
#                     'source': current_event['source'],
#                     'soil_temp_depth': current_event['soil_temp_depth'],
#                     'datetime_min': current_event['datetime_min'],
#                     'datetime_max': current_event['datetime_max'],
#                     'duration_hours': duration_hours,
#                     'zero_curtain_probability': np.mean(current_event['zero_curtain_probability'])...
#                     'latitude': current_event['latitude'],
#                     'longitude': current_event['longitude']
#                 }
#                 consolidated_events.append(final_event)
    
#     # Convert to DataFrame
#     consolidated_df = pd.DataFrame(consolidated_events)
    
#     # Add region and latitude band classifications if latitude is available
#     if len(consolidated_df) > 0 and 'latitude' in consolidated_df.columns:
#         # Add region classification
#         def assign_region(lat):
#             if lat is None or pd.isna(lat):
#                 return None
#             elif lat >= 66.5:
#                 return 'Arctic'
#             elif lat >= 60:
#                 return 'Subarctic'
#             elif lat >= 50:
#                 return 'Northern Boreal'
#             else:
#                 return 'Other'
        
#         consolidated_df['region'] = consolidated_df['latitude'].apply(assign_region)
        
#         # Add latitude band
#         def assign_lat_band(lat):
#             if lat is None or pd.isna(lat):
#                 return None
#             elif lat < 55:
#                 return '<55°N'
#             elif lat < 60:
#                 return '55-60°N'
#             elif lat < 66.5:
#                 return '60-66.5°N'
#             elif lat < 70:
#                 return '66.5-70°N'
#             elif lat < 75:
#                 return '70-75°N'
#             elif lat < 80:
#                 return '75-80°N'
#             else:
#                 return '>80°N'
        
#         consolidated_df['lat_band'] = consolidated_df['latitude'].apply(assign_lat_band)
    
#     return consolidated_df

# def visualize_events_efficiently(events_df, output_file=None):
#     """
#     Create visualizations for zero curtain events with memory efficiency.
    
#     Parameters:
#     -----------
#     events_df : pandas.DataFrame
#         DataFrame containing zero curtain events
#     output_file : str, optional
#         Path to save the visualization
        
#     Returns:
#     --------
#     dict
#         Statistics about the visualized events
#     """
#     import matplotlib.pyplot as plt
#     import cartopy.crs as ccrs
#     import cartopy.feature as cfeature
#     import numpy as np
#     from matplotlib.colors import PowerNorm
#     import gc
    
#     print(f"Creating visualization for {len(events_df)} events...")
#     print(f"Memory before visualization: {memory_usage():.1f} MB")
    
#     # Calculate percentile boundaries for better scaling
#     p10 = np.percentile(events_df['duration_hours'], 10)
#     p25 = np.percentile(events_df['duration_hours'], 25)
#     p50 = np.percentile(events_df['duration_hours'], 50)  # median
#     p75 = np.percentile(events_df['duration_hours'], 75)
#     p90 = np.percentile(events_df['duration_hours'], 90)
    
#     # Aggregate by site to reduce memory usage and plotting overhead
#     site_data = events_df.groupby(['source', 'latitude', 'longitude']).agg({
#         'duration_hours': ['count', 'mean', 'median', 'min', 'max']
#     }).reset_index()
    
#     # Flatten column names
#     site_data.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col 
#                         for col in site_data.columns]
    
#     # Create figure
#     fig, axes = plt.subplots(1, 2, figsize=(14, 7), 
#                            subplot_kw={'projection': ccrs.NorthPolarStereo()})
    
#     # Set map features
#     for ax in axes:
#         ax.set_extent([-180, 180, 45, 90], ccrs.PlateCarree())
#         ax.add_feature(cfeature.LAND, facecolor='whitesmoke')
#         ax.add_feature(cfeature.OCEAN, facecolor='aliceblue')
#         ax.add_feature(cfeature.COASTLINE, linewidth=0.5)
        
#         # Add Arctic Circle with label
#         ax.plot(
#             np.linspace(-180, 180, 60),
#             np.ones(60) * 66.5,
#             transform=ccrs.PlateCarree(),
#             linestyle='-',
#             color='gray',
#             linewidth=1.0,
#             alpha=0.7
#         )
        
#         ax.text(
#             0, 66.5 + 2,
#             "Arctic Circle",
#             transform=ccrs.PlateCarree(),
#             horizontalalignment='center',
#             verticalalignment='bottom',
#             fontsize=9,
#             bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.2')
#         )
    
#     # Plot 1: Event count
#     count_max = site_data['duration_hours_count'].quantile(0.95)
#     scatter1 = axes[0].scatter(
#         site_data['longitude'],
#         site_data['latitude'],
#         transform=ccrs.PlateCarree(),
#         c=site_data['duration_hours_count'],
#         s=30,
#         cmap='viridis',
#         vmin=1,
#         vmax=count_max,
#         alpha=0.8,
#         edgecolor='none'
#     )
#     plt.colorbar(scatter1, ax=axes[0], shrink=0.7, pad=0.05, label='Event Count')
#     axes[0].set_title('Zero Curtain Event Count', fontsize=12)
    
#     # Plot 2: Mean duration using percentile bounds
#     lower_bound = p10
#     upper_bound = p90
    
#     # Non-linear scaling for better color differentiation
#     scatter2 = axes[1].scatter(
#         site_data['longitude'],
#         site_data['latitude'],
#         transform=ccrs.PlateCarree(),
#         c=site_data['duration_hours_mean'],
#         s=30,
#         cmap='RdYlBu_r',
#         norm=PowerNorm(gamma=0.7, vmin=lower_bound, vmax=upper_bound),
#         alpha=0.8,
#         edgecolor='none'
#     )
    
#     # Create better colorbar with percentile markers
#     cbar = plt.colorbar(scatter2, ax=axes[1], shrink=0.7, pad=0.05, 
#                        label='Mean Duration (hours)')
    
#     # Show percentile ticks
#     percentile_ticks = [p10, p25, p50, p75, p90]
#     cbar.set_ticks(percentile_ticks)
#     cbar.set_ticklabels([f"{h:.0f}h\n({h/24:.1f}d)" for h in percentile_ticks])
    
#     axes[1].set_title('Mean Zero Curtain Duration', fontsize=12)
    
#     # Add comprehensive title with statistics
#     plt.suptitle(
#         f'Zero Curtain Analysis: {len(site_data)} Sites, {len(events_df)} Events\n' +
#         f'Duration: median={p50:.1f}h ({p50/24:.1f}d), 10-90%={p10:.1f}-{p90:.1f}h',
#         fontsize=14
#     )
    
#     plt.tight_layout(rect=[0, 0, 1, 0.93])
    
#     # Save if requested
#     if output_file:
#         plt.savefig(output_file, dpi=200, bbox_inches='tight')
#         print(f"Visualization saved to {output_file}")
    
#     # Clean up to free memory
#     plt.close(fig)
#     del site_data, fig, axes
#     gc.collect()
    
#     print(f"Memory after visualization: {memory_usage():.1f} MB")
    
#     # Return statistics
#     return {
#         'p10': p10,
#         'p25': p25,
#         'p50': p50,
#         'p75': p75,
#         'p90': p90,
#         'mean': events_df['duration_hours'].mean(),
#         'std': events_df['duration_hours'].std(),
#         'min': events_df['duration_hours'].min(),
#         'max': events_df['duration_hours'].max()
#     }

# def compare_detection_methods_efficiently(physical_events_file, model_events_file, output_dir=None...
#     """
#     Compare zero curtain events detected by different methods with memory efficiency.
    
#     Parameters:
#     -----------
#     physical_events_file : str
#         Path to CSV file with events detected by the physics-based method
#     model_events_file : str
#         Path to CSV file with events detected by the deep learning model
#     output_dir : str, optional
#         Directory to save comparison results
        
#     Returns:
#     --------
#     dict
#         Comparison statistics and metrics
#     """
#     import pandas as pd
#     import numpy as np
#     import matplotlib.pyplot as plt
#     import seaborn as sns
#     from datetime import timedelta
#     import os
#     import gc
    
#     print("Comparing detection methods...")
#     print(f"Memory before comparison: {memory_usage():.1f} MB")
    
#     # Load events
#     physical_events = pd.read_csv(physical_events_file, parse_dates=['datetime_min', 'datetime_max...
#     model_events = pd.read_csv(model_events_file, parse_dates=['datetime_min', 'datetime_max'])
    
#     # Calculate basic statistics for each method
#     physical_stats = {
#         'total_events': len(physical_events),
#         'unique_sites': physical_events['source'].nunique(),
#         'median_duration': physical_events['duration_hours'].median(),
#         'mean_duration': physical_events['duration_hours'].mean()
#     }
    
#     model_stats = {
#         'total_events': len(model_events),
#         'unique_sites': model_events['source'].nunique(),
#         'median_duration': model_events['duration_hours'].median(),
#         'mean_duration': model_events['duration_hours'].mean()
#     }
    
#     # Create a site-day matching table for overlap analysis
#     # Process in batches to save memory
#     physical_days = set()
#     model_days = set()
    
#     # Process physical events in batches
#     batch_size = 1000
#     for i in range(0, len(physical_events), batch_size):
#         batch = physical_events.iloc[i:i+batch_size]
        
#         for _, event in batch.iterrows():
#             site = event['source']
#             depth = event['soil_temp_depth']
#             start_day = event['datetime_min'].date()
#             end_day = event['datetime_max'].date()
            
#             # Add each day of the event
#             current_day = start_day
#             while current_day <= end_day:
#                 physical_days.add((site, depth, current_day))
#                 current_day += timedelta(days=1)
        
#         # Clear batch to free memory
#         del batch
#         gc.collect()
    
#     # Process model events in batches
#     for i in range(0, len(model_events), batch_size):
#         batch = model_events.iloc[i:i+batch_size]
        
#         for _, event in batch.iterrows():
#             site = event['source']
#             depth = event['soil_temp_depth']
#             start_day = event['datetime_min'].date()
#             end_day = event['datetime_max'].date()
            
#             # Add each day of the event
#             current_day = start_day
#             while current_day <= end_day:
#                 model_days.add((site, depth, current_day))
#                 current_day += timedelta(days=1)
        
#         # Clear batch to free memory
#         del batch
#         gc.collect()
    
#     # Calculate overlap metrics
#     overlap_days = physical_days.intersection(model_days)
    
#     overlap_metrics = {
#         'physical_only_days': len(physical_days - model_days),
#         'model_only_days': len(model_days - physical_days),
#         'overlap_days': len(overlap_days),
#         'jaccard_index': len(overlap_days) / len(physical_days.union(model_days)) if len(physical_...
#     }
    
#     # Print comparison results
#     print("\n=== DETECTION METHOD COMPARISON ===\n")
    
#     print("Physics-based Detection:")
#     print(f"  Total Events: {physical_stats['total_events']}")
#     print(f"  Unique Sites: {physical_stats['unique_sites']}")
#     print(f"  Median Duration: {physical_stats['median_duration']:.1f} hours ({physical_stats['med...
#     print(f"  Mean Duration: {physical_stats['mean_duration']:.1f} hours ({physical_stats['mean_du...
    
#     print("\nDeep Learning Model Detection:")
#     print(f"  Total Events: {model_stats['total_events']}")
#     print(f"  Unique Sites: {model_stats['unique_sites']}")
#     print(f"  Median Duration: {model_stats['median_duration']:.1f} hours ({model_stats['median_du...
#     print(f"  Mean Duration: {model_stats['mean_duration']:.1f} hours ({model_stats['mean_duration...
    
#     print("\nOverlap Analysis:")
#     print(f"  Days with Events (Physics-based): {len(physical_days)}")
#     print(f"  Days with Events (Deep Learning): {len(model_days)}")
#     print(f"  Days detected by both methods: {overlap_metrics['overlap_days']}")
#     print(f"  Days detected only by Physics-based: {overlap_metrics['physical_only_days']}")
#     print(f"  Days detected only by Deep Learning: {overlap_metrics['model_only_days']}")
#     print(f"  Jaccard Index (overlap): {overlap_metrics['jaccard_index']:.4f}")
    
#     # Generate comparison visualizations
#     if output_dir:
#         os.makedirs(output_dir, exist_ok=True)
        
#         # Create a Venn diagram of detection overlap
#         try:
#             from matplotlib_venn import venn2
            
#             plt.figure(figsize=(8, 6))
#             venn2(subsets=(len(physical_days - model_days), 
#                           len(model_days - physical_days), 
#                           len(overlap_days)),
#                  set_labels=('Physics-based', 'Deep Learning'))
#             plt.title('Overlap between Detection Methods', fontsize=14)
#             plt.savefig(os.path.join(output_dir, 'detection_overlap.png'), dpi=200, bbox_inches='t...
#             plt.close()
#         except ImportError:
#             print("matplotlib_venn not installed. Skipping Venn diagram.")
        
#         # Compare duration distributions
#         plt.figure(figsize=(10, 6))
        
#         sns.histplot(physical_events['duration_hours'], kde=True, alpha=0.5, 
#                     label='Physics-based', color='blue', bins=50)
#         sns.histplot(model_events['duration_hours'], kde=True, alpha=0.5, 
#                     label='Deep Learning', color='red', bins=50)
        
#         plt.xlabel('Duration (hours)')
#         plt.ylabel('Frequency')
#         plt.title('Comparison of Zero Curtain Duration Distributions')
#         plt.legend()
#         plt.grid(alpha=0.3)
        
#         plt.savefig(os.path.join(output_dir, 'duration_comparison.png'), dpi=200, bbox_inches='tig...
#         plt.close()
    
#     # Clean up to free memory
#     del physical_events, model_events, physical_days, model_days, overlap_days
#     gc.collect()
    
#     print(f"Memory after comparison: {memory_usage():.1f} MB")
    
#     comparison_results = {
#         'physical_stats': physical_stats,
#         'model_stats': model_stats,
#         'overlap_metrics': overlap_metrics
#     }
    
#     return comparison_results

# def run_complete_pipeline(feather_path, output_base_dir='results'):
#     """
#     Run the complete zero curtain analysis pipeline with memory efficiency.
    
#     Parameters:
#     -----------
#     feather_path : str
#         Path to the feather file
#     output_base_dir : str
#         Base directory for outputs
        
#     Returns:
#     --------
#     dict
#         Summary of results
#     """
#     import os
#     import time
#     import gc
#     import pickle
#     from tqdm.auto import tqdm
    
#     # Create output directories
#     os.makedirs(output_base_dir, exist_ok=True)
#     checkpoint_dir = os.path.join(output_base_dir, 'checkpoints')
#     os.makedirs(checkpoint_dir, exist_ok=True)
    
#     # Function to save/load checkpoint
#     def save_checkpoint(data, name):
#         with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'wb') as f:
#             pickle.dump(data, f)
    
#     def load_checkpoint(name):
#         try:
#             with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'rb') as f:
#                 return pickle.load(f)
#         except:
#             return None
    
#     # Initialize results
#     results = load_checkpoint('pipeline_results') or {}
    
#     # Check for completed stages
#     completed_stages = set(results.get('completed_stages', []))
#     print(f"Found {len(completed_stages)} completed stages: {completed_stages}")
    
#     # Define stages
#     stages = ['Zero Curtain Detection', 'Data Preparation', 'Model Training', 
#               'Model Application', 'Visualization', 'Comparison']
    
#     # Overall progress bar
#     with tqdm(total=len(stages), desc="Overall Progress", initial=len(completed_stages)) as pbar:
#         # Stage 1: Zero Curtain Detection
#         if 'Zero Curtain Detection' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 1/{len(stages)}: {stages[0]}")
            
#             #from zero_curtain_pipeline import run_memory_efficient_pipeline
            
#             enhanced_events = run_memory_efficient_pipeline(
#                 feather_path=feather_path,
#                 output_dir=os.path.join(output_base_dir, 'enhanced'),
#                 site_batch_size=20,
#                 checkpoint_interval=5,
#                 max_gap_hours=6,
#                 interpolation_method='cubic'
#             )
            
#             results['enhanced_events_count'] = len(enhanced_events)
#             results['enhanced_time'] = time.time() - start_time
            
#             # Save checkpoint
#             completed_stages.add('Zero Curtain Detection')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             # Free memory
#             del enhanced_events
#             gc.collect()
            
#             print(f"Memory after stage 1: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 2: Data Preparation
#         if 'Data Preparation' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 2/{len(stages)}: {stages[1]}")
            
#             # Load events
#             enhanced_events_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events....
#             if os.path.exists(enhanced_events_path):
#                 import pandas as pd
#                 enhanced_events = pd.read_csv(enhanced_events_path, parse_dates=['datetime_min', '...
                
#                 # Prepare data for model
#                 X, y, metadata = prepare_data_for_deep_learning_efficiently(
#                     feather_path=feather_path,
#                     events_df=enhanced_events,
#                     sequence_length=24,
#                     output_dir=os.path.join(output_base_dir, 'ml_data'),
#                     batch_size=20
#                 )
                
#                 results['data_prep_time'] = time.time() - start_time
#                 results['data_shape'] = X.shape
#                 results['positive_examples'] = int(sum(y))
#                 results['positive_percentage'] = float(sum(y)/len(y)*100)
                
#                 # Clean up
#                 del X, y, metadata, enhanced_events
#                 gc.collect()
#             else:
#                 print("No enhanced events file found, cannot proceed with data preparation")
#                 results['data_prep_error'] = "No enhanced events file found"
            
#             # Save checkpoint
#             completed_stages.add('Data Preparation')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             print(f"Memory after stage 2: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 3: Model Training
#         if 'Model Training' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 3/{len(stages)}: {stages[2]}")
            
#             ml_data_dir = os.path.join(output_base_dir, 'ml_data')
#             x_path = os.path.join(ml_data_dir, 'X_features.npy')
#             y_path = os.path.join(ml_data_dir, 'y_labels.npy')
            
#             if os.path.exists(x_path) and os.path.exists(y_path):
#                 import numpy as np
#                 X = np.load(x_path)
#                 y = np.load(y_path)
                
#                 # Train model
#                 model, history, evaluation = train_zero_curtain_model_efficiently(
#                     X=X,
#                     y=y,
#                     output_dir=os.path.join(output_base_dir, 'model')
#                 )
                
#                 results['model_training_time'] = time.time() - start_time
#                 results['model_evaluation'] = evaluation
                
#                 # Clean up
#                 del X, y, model, history, evaluation
#                 gc.collect()
#             else:
#                 print("No prepared data found, cannot proceed with model training")
#                 results['model_training_error'] = "No prepared data found"
            
#             # Save checkpoint
#             completed_stages.add('Model Training')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             print(f"Memory after stage 3: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 4: Model Application
#         if 'Model Application' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 4/{len(stages)}: {stages[3]}")
            
#             model_path = os.path.join(output_base_dir, 'model', 'checkpoint.h5')
            
#             if os.path.exists(model_path):
#                 import tensorflow as tf
#                 model = tf.keras.models.load_model(model_path)
                
#                 # Apply model
#                 predictions = apply_model_to_new_data_efficiently(
#                     model=model,
#                     feather_path=feather_path,
#                     sequence_length=24,
#                     output_dir=os.path.join(output_base_dir, 'predictions'),
#                     batch_size=20
#                 )
                
#                 results['model_application_time'] = time.time() - start_time
#                 results['predictions_count'] = len(predictions)
                
#                 # Clean up
#                 del model, predictions
#                 gc.collect()
#             else:
#                 print("No model checkpoint found, cannot proceed with model application")
#                 results['model_application_error'] = "No model checkpoint found"
            
#             # Save checkpoint
#             completed_stages.add('Model Application')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             print(f"Memory after stage 4: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 5: Visualization
#         if 'Visualization' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 5/{len(stages)}: {stages[4]}")
            
#             import pandas as pd
            
#             # Visualize enhanced events
#             enhanced_events_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events....
#             if os.path.exists(enhanced_events_path):
#                 enhanced_events = pd.read_csv(enhanced_events_path, parse_dates=['datetime_min', '...
                
#                 enhanced_stats = visualize_events_efficiently(
#                     events_df=enhanced_events,
#                     output_file=os.path.join(output_base_dir, 'enhanced_visualization.png')
#                 )
                
#                 results['enhanced_visualization_stats'] = enhanced_stats
                
#                 # Clean up
#                 del enhanced_events, enhanced_stats
#                 gc.collect()
            
#             # Visualize model predictions
#             predictions_path = os.path.join(output_base_dir, 'predictions', 'consolidated_events.c...
#             if os.path.exists(predictions_path):
#                 model_events = pd.read_csv(predictions_path, parse_dates=['datetime_min', 'datetim...
                
#                 model_stats = visualize_events_efficiently(
#                     events_df=model_events,
#                     output_file=os.path.join(output_base_dir, 'model_visualization.png')
#                 )
                
#                 results['model_visualization_stats'] = model_stats
                
#                 # Clean up
#                 del model_events, model_stats
#                 gc.collect()
            
#             results['visualization_time'] = time.time() - start_time
            
#             # Save checkpoint
#             completed_stages.add('Visualization')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             print(f"Memory after stage 5: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 6: Comparison
#         if 'Comparison' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 6/{len(stages)}: {stages[5]}")
            
#             enhanced_events_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events....
#             predictions_path = os.path.join(output_base_dir, 'predictions', 'consolidated_events.c...
            
#             if os.path.exists(enhanced_events_path) and os.path.exists(predictions_path):
#                 comparison_results = compare_detection_methods_efficiently(
#                     physical_events_file=enhanced_events_path,
#                     model_events_file=predictions_path,
#                     output_dir=os.path.join(output_base_dir, 'comparison')
#                 )
                
#                 results['comparison'] = comparison_results
#             else:
#                 print("Missing events files, cannot perform comparison")
#                 results['comparison_error'] = "Missing events files"
            
#             results['comparison_time'] = time.time() - start_time
            
#             # Save checkpoint
#             completed_stages.add('Comparison')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             print(f"Memory after stage 6: {memory_usage():.1f} MB")
#             pbar.update(1)
    
#     # Generate summary report
#     total_time = (
#         results.get('enhanced_time', 0) +
#         results.get('data_prep_time', 0) +
#         results.get('model_training_time', 0) +
#         results.get('model_application_time', 0) +
#         results.get('visualization_time', 0) +
#         results.get('comparison_time', 0)
#     )
    
#     # Save summary to file
#     with open(os.path.join(output_base_dir, 'summary.txt'), 'w') as f:
#         f.write("ZERO CURTAIN ANALYSIS SUMMARY\n")
#         f.write("=" * 30 + "\n\n")
        
#         f.write(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\n\n")
        
#         f.write("STAGE TIMINGS:\n")
#         f.write(f"  Zero Curtain Detection: {results.get('enhanced_time', 0):.2f} seconds\n")
#         f.write(f"  Data Preparation: {results.get('data_prep_time', 0):.2f} seconds\n")
#         f.write(f"  Model Training: {results.get('model_training_time', 0):.2f} seconds\n")
#         f.write(f"  Model Application: {results.get('model_application_time', 0):.2f} seconds\n")
#         f.write(f"  Visualization: {results.get('visualization_time', 0):.2f} seconds\n")
#         f.write(f"  Comparison: {results.get('comparison_time', 0):.2f} seconds\n\n")
        
#         f.write("RESULTS SUMMARY:\n")
#         f.write(f"  Enhanced Detection: {results.get('enhanced_events_count', 0)} events\n")
#         f.write(f"  Model Predictions: {results.get('predictions_count', 0)} events\n")
        
#         if 'comparison' in results and 'overlap_metrics' in results['comparison']:
#             overlap = results['comparison']['overlap_metrics']['jaccard_index']
#             f.write(f"  Method Agreement: {overlap*100:.1f}% overlap\n")
    
#     print("\n" + "=" * 80)
#     print("ZERO CURTAIN ANALYSIS COMPLETE")
#     print("=" * 80)
#     print(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
#     print(f"Results saved to {output_base_dir}")
    
#     return results

# result = prepare_data_for_deep_learning_efficiently(
#     feather_path='merged_compressed.feather',
#     events_df=events_df,
#     sequence_length=6,
#     output_dir='zero_curtain_pipeline/modeling/ml_data',
#     batch_size=50
# )

# merge_result = merge_batch_files('zero_curtain_pipeline/modeling/ml_data')

# # from tqdm import tqdm
# # import numpy as np
# # from sklearn.neighbors import KernelDensity
# # from sklearn.preprocessing import StandardScaler

# # def spatiotemporal_train_test_split(X, y, metadata, test_fraction=0.2, val_fraction=0.15):
# #     """
# #     Spatiotemporally-aware data splitting for Earth science applications.
# #     """
    
# #     # Extract spatiotemporal features
# #     timestamps = np.array([meta['start_time'] for meta in metadata])
    
# #     latitudes = np.array([meta.get('latitude', 0) if meta.get('latitude') is not None else 0 for...
# #     longitudes = np.array([meta.get('longitude', 0) if meta.get('longitude') is not None else 0 ...
# #     depths = np.array([meta.get('soil_temp_depth', 0) if meta.get('soil_temp_depth') is not None...
    
# #     has_geo_info = (np.count_nonzero(latitudes) > 0 and np.count_nonzero(longitudes) > 0)
    
# #     # Temporal ordering - sort everything by time
# #     sorted_time_indices = np.argsort(timestamps)
# #     timestamps = timestamps[sorted_time_indices]
# #     latitudes = latitudes[sorted_time_indices]
# #     longitudes = longitudes[sorted_time_indices]
# #     depths = depths[sorted_time_indices]
# #     X = X[sorted_time_indices]
# #     y = y[sorted_time_indices]
    
# #     # Reserve the most recent data as a true test set
# #     n_samples = len(X)
# #     test_start = int(n_samples * (1 - test_fraction))
    
# #     X_test = X[test_start:]
# #     y_test = y[test_start:]
# #     test_metadata = [metadata[i] for i in sorted_time_indices[test_start:]]
    
# #     # Remaining data for training/validation
# #     train_val_indices = sorted_time_indices[:test_start]
# #     X_remaining = X[:test_start]
# #     y_remaining = y[:test_start]
# #     latitudes_remaining = latitudes[:test_start]
# #     longitudes_remaining = longitudes[:test_start]
# #     depths_remaining = depths[:test_start]
    
# #     # Compute spatial density
# #     if has_geo_info:
# #         lat_lon_points = np.radians(np.vstack([latitudes_remaining, longitudes_remaining]).T)
# #         geo_kde = KernelDensity(bandwidth=0.05, metric='haversine')
# #         geo_kde.fit(lat_lon_points)
# #         geo_log_density = geo_kde.score_samples(lat_lon_points)
# #         geo_density = np.exp(geo_log_density)
        
# #         depth_kde = KernelDensity(bandwidth=0.5, metric='euclidean')
# #         depth_kde.fit(depths_remaining.reshape(-1, 1))
# #         depth_log_density = depth_kde.score_samples(depths_remaining.reshape(-1, 1))
# #         depth_density = np.exp(depth_log_density)
        
# #         density = geo_density * depth_density
# #     else:
# #         depth_kde = KernelDensity(bandwidth=0.5, metric='euclidean')
# #         depth_kde.fit(depths_remaining.reshape(-1, 1))
# #         depth_log_density = depth_kde.score_samples(depths_remaining.reshape(-1, 1))
# #         density = np.exp(depth_log_density)
    
# #     weights = 1.0 / (density + 0.01)
# #     weights = weights / np.sum(weights) * len(weights)
    
# #     months = np.array([ts.month if hasattr(ts, 'month') else ts.to_pydatetime().month for ts in ...
# #     seasons = np.digitize(months, bins=[3, 6, 9, 12])
    
# #     if has_geo_info:
# #         regions = np.zeros_like(latitudes_remaining, dtype=int)
# #         regions[(latitudes_remaining >= 66.5)] = 3
# #         regions[(latitudes_remaining >= 60) & (latitudes_remaining < 66.5)] = 2
# #         regions[(latitudes_remaining >= 50) & (latitudes_remaining < 60)] = 1
# #     else:
# #         regions = np.zeros_like(depths_remaining, dtype=int)
    
# #     depth_zones = np.digitize(depths_remaining, bins=[0.2, 0.5, 1.0, 2.0])
# #     density_quantiles = np.digitize(density, bins=np.percentile(density, [20, 40, 60, 80]))
# #     strata = seasons * 1000 + regions * 100 + depth_zones * 10 + density_quantiles
# #     unique_strata = np.unique(strata)
    
# #     val_size = int(n_samples * val_fraction)
# #     train_size = test_start - val_size
    
# #     train_indices = []
# #     val_indices = []
    
# #     for stratum in tqdm(unique_strata, desc='Stratified Sampling'):
# #         stratum_indices = np.where(strata == stratum)[0]
        
# #         if len(stratum_indices) == 0:
# #             continue
        
# #         stratum_weights = weights[stratum_indices]
# #         stratum_weight_sum = np.sum(stratum_weights)
# #         target_val_size = int(val_size * stratum_weight_sum / np.sum(weights))
# #         target_val_size = max(target_val_size, 1)
# #         target_val_size = min(target_val_size, len(stratum_indices) - 1)
        
# #         stratum_val_indices = stratum_indices[-target_val_size:]
# #         stratum_train_indices = stratum_indices[:-target_val_size]
        
# #         train_indices.extend(stratum_train_indices)
# #         val_indices.extend(stratum_val_indices)
    
# #     X_train = X_remaining[train_indices]
# #     y_train = y_remaining[train_indices]
# #     train_metadata = [metadata[i] for i in train_val_indices[train_indices]]
    
# #     X_val = X_remaining[val_indices]
# #     y_val = y_remaining[val_indices]
# #     val_metadata = [metadata[i] for i in train_val_indices[val_indices]]
    
# #     print(f"Split sizes: Train={len(X_train)}, Validation={len(X_val)}, Test={len(X_test)}")
# #     return X_train, X_val, X_test, y_train, y_val, y_test, weights[train_indices]


# from tqdm import tqdm
# from sklearn.neighbors import KernelDensity
# from sklearn.preprocessing import StandardScaler

# test_fraction=0.2
# val_fraction=0.15
# timestamps = np.array([meta['start_time'] for meta in metadata])

# latitudes = np.array([meta.get('latitude', 0) if meta.get('latitude') is not None else 0 for meta ...
# longitudes = np.array([meta.get('longitude', 0) if meta.get('longitude') is not None else 0 for me...
# depths = np.array([meta.get('soil_temp_depth', 0) if meta.get('soil_temp_depth') is not None else ...
# has_geo_info = (np.count_nonzero(latitudes) > 0 and np.count_nonzero(longitudes) > 0)
# sorted_time_indices = np.argsort(timestamps, kind='mergesort')
# timestamps = timestamps[sorted_time_indices]
# latitudes = latitudes[sorted_time_indices]
# longitudes = longitudes[sorted_time_indices]
# depths = depths[sorted_time_indices]
# X = X[sorted_time_indices]
# y = y[sorted_time_indices]

# n_samples = len(X)
# test_start = int(n_samples * (1 - test_fraction))

# X_test = X[test_start:]
# y_test = y[test_start:]
# test_metadata = [metadata[i] for i in sorted_time_indices[test_start:]]

# train_val_indices = sorted_time_indices[:test_start]
# X_remaining = X[:test_start]
# y_remaining = y[:test_start]
# latitudes_remaining = latitudes[:test_start]
# longitudes_remaining = longitudes[:test_start]
# depths_remaining = depths[:test_start]

# import os
# import gc
# import json
# import pickle
# import numpy as np
# from scipy.spatial import cKDTree  # Use cKDTree instead of BallTree for better performance
# from tqdm import tqdm
# import psutil
# import threading
# import time
# from contextlib import contextmanager
# from joblib import Parallel, delayed

# # Configuration parameters
# CHECKPOINT_DIR = "zero_curtain_pipeline/modeling/checkpoints"
# MEMORY_THRESHOLD = 75  # Memory usage percentage threshold
# METADATA_SAVE_FREQUENCY = 10  # Save metadata every N batches
# MONITOR_INTERVAL = 10  # Memory monitoring interval in seconds

# # Create checkpoint directory
# os.makedirs(CHECKPOINT_DIR, exist_ok=True)
# CHECKPOINT_BASE = os.path.join(CHECKPOINT_DIR, "geo_density_checkpoint")
# METADATA_PATH = os.path.join(CHECKPOINT_DIR, "checkpoint_metadata.json")

# @contextmanager
# def measure_time(description="Operation"):
#     """Context manager to measure and print execution time of code blocks."""
#     start_time = time.time()
#     try:
#         yield
#     finally:
#         elapsed = time.time() - start_time
#         print(f"{description} completed in {elapsed:.2f} seconds")

# class MemoryMonitor:
#     """Class for monitoring memory usage in a separate thread."""
#     def __init__(self, interval=10, threshold=75):
#         self.interval = interval
#         self.threshold = threshold
#         self.stop_flag = threading.Event()
#         self.max_usage = 0
#         self.monitor_thread = None
        
#     def memory_usage(self):
#         """Get current memory usage percentage."""
#         return psutil.virtual_memory().percent
        
#     def monitor(self):
#         """Monitor memory usage periodically."""
#         while not self.stop_flag.is_set():
#             usage = self.memory_usage()
#             self.max_usage = max(self.max_usage, usage)
#             if usage > self.threshold:
#                 print(f"WARNING: High memory usage detected: {usage:.1f}%")
#             time.sleep(self.interval)
                
#     def start(self):
#         """Start the memory monitoring thread."""
#         self.monitor_thread = threading.Thread(target=self.monitor, daemon=True)
#         self.monitor_thread.start()
        
#     def stop(self):
#         """Stop the memory monitoring thread."""
#         self.stop_flag.set()
#         if self.monitor_thread:
#             self.monitor_thread.join(timeout=1.0)
#         print(f"Maximum memory usage: {self.max_usage:.1f}%")

# class CheckpointManager:
#     """Class for managing checkpoints and metadata."""
#     def __init__(self, base_path, metadata_path, save_frequency=10):
#         self.base_path = base_path
#         self.metadata_path = metadata_path
#         self.save_frequency = save_frequency
#         self.completed_batches = self.load_metadata()
        
#     def save_checkpoint(self, batch_data, batch_index):
#         """Save a batch checkpoint to disk."""
#         filepath = f"{self.base_path}_batch_{batch_index}.pkl"
#         with open(filepath, "wb") as f:
#             pickle.dump(batch_data, f)
        
#     def load_checkpoint(self, batch_index):
#         """Load a batch checkpoint from disk."""
#         filepath = f"{self.base_path}_batch_{batch_index}.pkl"
#         if os.path.exists(filepath):
#             with open(filepath, "rb") as f:
#                 return pickle.load(f)
#         return None
        
#     def save_metadata(self, force_save=False):
#         """Save checkpoint metadata to disk."""
#         if force_save or len(self.completed_batches) % self.save_frequency == 0:
#             with open(self.metadata_path, "w") as f:
#                 json.dump({"completed_batches": sorted(list(self.completed_batches))}, f)
                
#     def load_metadata(self):
#         """Load checkpoint metadata from disk."""
#         if os.path.exists(self.metadata_path):
#             with open(self.metadata_path, "r") as f:
#                 return set(json.load(f).get("completed_batches", []))
#         return set()
        
#     def is_batch_completed(self, batch_index):
#         """Check if a batch has been completed."""
#         return batch_index in self.completed_batches
        
#     def mark_batch_completed(self, batch_index):
#         """Mark a batch as completed."""
#         self.completed_batches.add(batch_index)
#         self.save_metadata()

# def haversine_to_cartesian(lat, lon):
#     """Convert latitude and longitude to 3D cartesian coordinates (x,y,z)."""
#     # Convert to radians
#     lat_rad = np.radians(lat)
#     lon_rad = np.radians(lon)
    
#     # Convert to cartesian coordinates on a unit sphere
#     x = np.cos(lat_rad) * np.cos(lon_rad)
#     y = np.cos(lat_rad) * np.sin(lon_rad)
#     z = np.sin(lat_rad)
    
#     return np.column_stack((x, y, z))

# def cartesian_to_haversine_distance(distance_cartesian):
#     """Convert cartesian distance to angular distance in radians."""
#     # Ensure distance is at most 2.0 (diameter of unit sphere)
#     distance_cartesian = np.minimum(distance_cartesian, 2.0)
    
#     # Convert chord length to angular distance using inverse haversine formula
#     return 2 * np.arcsin(distance_cartesian / 2)

# def process_batch(tree, batch_points, k):
#     """Process a single batch of points using KDTree."""
#     distances, _ = tree.query(batch_points, k=k)
#     # Convert cartesian distances to angular distances (radians)
#     return cartesian_to_haversine_distance(distances)

# def calculate_optimal_batch_size(total_points, available_memory_mb=None):
#     """Calculate optimal batch size based on available memory and dataset size."""
#     if available_memory_mb is None:
#         # Use 20% of available memory if not specified (to leave room for parallelism)
#         available_memory_mb = psutil.virtual_memory().available / (1024 * 1024) * 0.2
    
#     # Estimate memory per point (assuming float64 coordinates and distances)
#     memory_per_point_kb = 0.5  # Approximate memory per point in KB
    
#     # Calculate batch size that would use available memory
#     batch_size = int(available_memory_mb * 1024 / memory_per_point_kb)
    
#     # Cap batch size to reasonable limits
#     min_batch_size = 1000
#     max_batch_size = 50000
#     batch_size = max(min(batch_size, max_batch_size), min_batch_size)
    
#     return min(batch_size, total_points)

# def aggregate_checkpoints(checkpoint_mgr, batch_indices):
#     """Load and aggregate checkpoints for the specified batch indices."""
#     all_distances = []
#     for batch_idx in batch_indices:
#         distances = checkpoint_mgr.load_checkpoint(batch_idx)
#         if distances is not None:
#             all_distances.append(distances)
#     return all_distances

# def process_spatial_data(latitudes, longitudes, k=5, leaf_size=40, batch_size=None, n_jobs=-1):
#     """
#     Process spatial data using KDTree for nearest neighbor calculations.
    
#     Parameters:
#     -----------
#     latitudes : array-like
#         Latitude values in degrees
#     longitudes : array-like
#         Longitude values in degrees
#     k : int
#         Number of nearest neighbors to find
#     leaf_size : int
#         Leaf size for the KDTree (affects performance)
#     batch_size : int or None
#         Batch size for processing. If None, will be calculated automatically.
#     n_jobs : int
#         Number of parallel jobs to run. -1 means using all processors.
    
#     Returns:
#     --------
#     distances : list
#         List of distances to k nearest neighbors for each point (in radians)
#     """
#     # Convert input arrays to numpy if they aren't already
#     lat_array = np.asarray(latitudes, dtype=np.float64)
#     lon_array = np.asarray(longitudes, dtype=np.float64)
    
#     # Convert to 3D cartesian coordinates for better KDTree performance
#     with measure_time("Coordinate conversion to cartesian"):
#         cartesian_points = haversine_to_cartesian(lat_array, lon_array)
    
#     # Calculate optimal batch size if not provided
#     n_points = len(cartesian_points)
#     if batch_size is None:
#         batch_size = calculate_optimal_batch_size(n_points)
    
#     # Adjust batch size to create a reasonable number of batches (target ~200 batches)
#     if n_points / batch_size > 200:
#         batch_size = max(batch_size, n_points // 200)
    
#     print(f"Using batch size: {batch_size} for {n_points} points")
    
#     # Initialize checkpoint manager
#     checkpoint_mgr = CheckpointManager(
#         CHECKPOINT_BASE, 
#         METADATA_PATH,
#         METADATA_SAVE_FREQUENCY
#     )
    
#     # Start memory monitoring
#     memory_monitor = MemoryMonitor(interval=MONITOR_INTERVAL, threshold=MEMORY_THRESHOLD)
#     memory_monitor.start()
    
#     try:
#         # Build cKDTree for fast spatial queries (much faster than BallTree)
#         with measure_time("KDTree construction"):
#             tree = cKDTree(cartesian_points, leafsize=leaf_size)
#             gc.collect()  # Force garbage collection
#             print(f"Available memory after tree creation: {psutil.virtual_memory().available / (10...
        
#         # Calculate number of batches
#         num_batches = int(np.ceil(n_points / batch_size))
#         print(f"Total number of batches: {num_batches}")
        
#         # Check which batches are already completed
#         completed_batches = set()
#         for batch_index in range(num_batches):
#             if checkpoint_mgr.is_batch_completed(batch_index):
#                 completed_batches.add(batch_index)
                
#         print(f"Found {len(completed_batches)} completed batches")
        
#         # Process remaining batches
#         remaining_batches = set(range(num_batches)) - completed_batches
#         print(f"Processing {len(remaining_batches)} remaining batches")
        
#         if remaining_batches:
#             # Convert to list and sort for deterministic processing
#             remaining_batch_indices = sorted(list(remaining_batches))
            
#             # Test with a small batch first
#             print("Testing KDTree query with a small sample...")
#             sample_idx = remaining_batch_indices[0]
#             start_idx = sample_idx * batch_size
#             end_idx = min((sample_idx + 1) * batch_size, n_points)
            
#             # Take just 10 points for testing
#             test_points = cartesian_points[start_idx:start_idx+10]
            
#             with measure_time("Test KDTree query"):
#                 test_distances, _ = tree.query(test_points, k=k)
#                 test_distances = cartesian_to_haversine_distance(test_distances)
            
#             print(f"Test successful! Sample distances: {test_distances[0]}")
#             print(f"Average test distance: {np.mean(test_distances):.6f} radians")
            
#             # Process batches in parallel or sequentially
#             if n_jobs != 1:
#                 # Process in parallel using joblib
#                 print(f"Processing batches in parallel with {n_jobs} jobs")
                
#                 # Process in smaller chunks to avoid memory issues
#                 chunk_size = min(20, len(remaining_batch_indices))  # Smaller chunks for better mo...
#                 num_chunks = int(np.ceil(len(remaining_batch_indices) / chunk_size))
                
#                 for chunk_idx in range(num_chunks):
#                     chunk_start = chunk_idx * chunk_size
#                     chunk_end = min((chunk_idx + 1) * chunk_size, len(remaining_batch_indices))
#                     current_chunk = remaining_batch_indices[chunk_start:chunk_end]
                    
#                     print(f"Processing chunk {chunk_idx + 1}/{num_chunks} with {len(current_chunk)...
                    
#                     # Prepare batch data
#                     batch_data = []
#                     for batch_idx in current_chunk:
#                         start_idx = batch_idx * batch_size
#                         end_idx = min((batch_idx + 1) * batch_size, n_points)
#                         batch_points = cartesian_points[start_idx:end_idx]
#                         batch_data.append((batch_idx, batch_points))
                    
#                     # Process in parallel with timeout monitoring
#                     start_time = time.time()
#                     results = Parallel(n_jobs=n_jobs, verbose=10, timeout=3600)(
#                         delayed(process_batch)(tree, points, k) for batch_idx, points in batch_dat...
#                     )
#                     end_time = time.time()
                    
#                     # Calculate performance statistics
#                     elapsed_time = end_time - start_time
#                     points_processed = sum(len(points) for _, points in batch_data)
#                     points_per_second = points_processed / elapsed_time
                    
#                     print(f"Chunk processed {points_processed} points in {elapsed_time:.2f} second...
#                     print(f"Performance: {points_per_second:.2f} points/second")
                    
#                     # Save results
#                     for (batch_idx, _), distances in zip(batch_data, results):
#                         checkpoint_mgr.save_checkpoint(distances, batch_idx)
#                         checkpoint_mgr.mark_batch_completed(batch_idx)
                    
#                     # Estimate remaining time
#                     remaining_chunks = num_chunks - (chunk_idx + 1)
#                     if remaining_chunks > 0:
#                         remaining_time = remaining_chunks * elapsed_time
#                         hours = remaining_time // 3600
#                         minutes = (remaining_time % 3600) // 60
#                         print(f"Estimated remaining time: {int(hours)}h {int(minutes)}m")
                    
#                     # Free memory
#                     del batch_data
#                     del results
#                     gc.collect()
#             else:
#                 # Process sequentially
#                 print("Processing batches sequentially")
#                 with tqdm(total=len(remaining_batch_indices), desc="Processing Batches") as pbar:
#                     for batch_idx in remaining_batch_indices:
#                         start_idx = batch_idx * batch_size
#                         end_idx = min((batch_idx + 1) * batch_size, n_points)
#                         batch_points = cartesian_points[start_idx:end_idx]
                        
#                         # Process full batch
#                         distances = process_batch(tree, batch_points, k)
                        
#                         # Save checkpoint
#                         checkpoint_mgr.save_checkpoint(distances, batch_idx)
#                         checkpoint_mgr.mark_batch_completed(batch_idx)
                        
#                         # Free memory
#                         del distances
#                         gc.collect()
                        
#                         pbar.update(1)
        
#         # Combine results from all batches
#         with measure_time("Loading checkpoints"):
#             print("Loading and combining checkpoints...")
            
#             # Load checkpoints in chunks to avoid memory issues
#             all_batches = sorted(list(range(num_batches)))
#             chunk_size = min(1000, num_batches)  # Adjust based on available memory
#             num_chunks = int(np.ceil(len(all_batches) / chunk_size))
            
#             all_distances = []
            
#             for chunk_idx in range(num_chunks):
#                 chunk_start = chunk_idx * chunk_size
#                 chunk_end = min((chunk_idx + 1) * chunk_size, len(all_batches))
#                 current_chunk = all_batches[chunk_start:chunk_end]
                
#                 print(f"Loading checkpoint chunk {chunk_idx + 1}/{num_chunks} ({len(current_chunk)...
#                 chunk_distances = aggregate_checkpoints(checkpoint_mgr, current_chunk)
                
#                 if chunk_distances:
#                     all_distances.extend(chunk_distances)
            
#         with measure_time("Combining results"):
#             print(f"Combining {len(all_distances)} checkpoint results...")
#             combined_distances = np.vstack(all_distances) if all_distances else np.array([])
            
#         # Final metadata save
#         checkpoint_mgr.save_metadata(force_save=True)
        
#         return combined_distances
        
#     finally:
#         # Stop memory monitoring
#         memory_monitor.stop()

# # Check environment
# print(f"Checkpoint directory exists: {os.path.exists(CHECKPOINT_DIR)}")
# print(f"Checkpoint directory is writable: {os.access(CHECKPOINT_DIR, os.W_OK)}")

# Checkpoint directory exists: True
# Checkpoint directory is writable: True

# # Determine optimal number of jobs based on available CPU cores and memory
# n_cpus = os.cpu_count()
# available_memory_gb = psutil.virtual_memory().available / (1024*1024*1024)
# recommended_jobs = max(1, min(n_cpus - 1, int(available_memory_gb / 8)))  # More conservative
# print(f"Available CPUs: {n_cpus}, Available memory: {available_memory_gb:.2f} GB")
# print(f"Recommended parallel jobs: {recommended_jobs}")

# Available CPUs: 12, Available memory: 32.16 GB
# Recommended parallel jobs: 4

# # Process the data with improved parameters
# distances = process_spatial_data(
#     latitudes=latitudes,
#     longitudes=longitudes,
#     k=5,
#     leaf_size=40,
#     batch_size=100000,  # Much larger batch size for fewer batches
#     n_jobs=recommended_jobs  # Use parallel processing
# )

# print(f"Processed {len(distances)} points with {distances.shape[1]} nearest neighbors each")
# print(f"Average nearest neighbor distance: {np.mean(distances[:, 1]):.6f} radians")

# Coordinate conversion to cartesian completed in 0.63 seconds
# Using batch size: 106985 for 21397132 points
# Available memory after tree creation: 31.35 GB
# KDTree construction completed in 12.01 seconds
# Total number of batches: 201
# Found 0 completed batches
# Processing 201 remaining batches
# Testing KDTree query with a small sample...
# Test KDTree query completed in 0.00 seconds
# Test successful! Sample distances: [0. 0. 0. 0. 0.]
# Average test distance: 0.000000 radians
# Processing batches in parallel with 4 jobs
# Processing chunk 1/11 with 20 batches
# [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.
# [Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:  4.7min
# [Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  7.9min
# [Parallel(n_jobs=4)]: Done  16 out of  20 | elapsed: 10.6min remaining:  2.7min
# [Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed: 13.2min finished
# Chunk processed 2139700 points in 794.85 seconds
# Performance: 2691.94 points/second
# Estimated remaining time: 2h 12m
# Processing chunk 2/11 with 20 batches
# [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.
# [Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:  5.1min
# [Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  7.7min
# [Parallel(n_jobs=4)]: Done  16 out of  20 | elapsed: 10.3min remaining:  2.6min
# [Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed: 12.7min finished
# Chunk processed 2139700 points in 763.62 seconds
# Performance: 2802.04 points/second
# Estimated remaining time: 1h 54m
# Processing chunk 3/11 with 20 batches
# [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.
# [Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:  4.8min
# [Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  7.3min
# [Parallel(n_jobs=4)]: Done  16 out of  20 | elapsed:  9.8min remaining:  2.5min
# [Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed: 12.2min finished
# Chunk processed 2139700 points in 732.70 seconds
# Performance: 2920.31 points/second
# Estimated remaining time: 1h 37m
# Processing chunk 4/11 with 20 batches
# [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.
# [Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:  4.8min
# [Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  7.3min
# [Parallel(n_jobs=4)]: Done  16 out of  20 | elapsed:  9.8min remaining:  2.5min
# [Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed: 12.3min finished
# Chunk processed 2139700 points in 740.43 seconds
# Performance: 2889.82 points/second
# Estimated remaining time: 1h 26m
# Processing chunk 5/11 with 20 batches
# [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.
# [Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:  4.9min
# [Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  7.5min
# [Parallel(n_jobs=4)]: Done  16 out of  20 | elapsed: 10.1min remaining:  2.5min
# [Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed: 12.5min finished
# Chunk processed 2139700 points in 748.53 seconds
# Performance: 2858.53 points/second
# Estimated remaining time: 1h 14m
# Processing chunk 6/11 with 20 batches
# [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.
# [Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:  4.8min
# [Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  7.2min
# [Parallel(n_jobs=4)]: Done  16 out of  20 | elapsed:  9.6min remaining:  2.4min
# [Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed: 11.6min finished
# Chunk processed 2139700 points in 699.34 seconds
# Performance: 3059.60 points/second
# Estimated remaining time: 0h 58m
# Processing chunk 7/11 with 20 batches
# [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.
# [Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:  4.1min
# [Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  6.4min
# [Parallel(n_jobs=4)]: Done  16 out of  20 | elapsed:  9.5min remaining:  2.4min
# [Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed: 12.1min finished
# Chunk processed 2139700 points in 729.42 seconds
# Performance: 2933.44 points/second
# Estimated remaining time: 0h 48m
# Processing chunk 8/11 with 20 batches
# [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.
# [Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:  5.3min
# [Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  8.3min
# [Parallel(n_jobs=4)]: Done  16 out of  20 | elapsed: 11.3min remaining:  2.8min
# [Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed: 14.0min finished
# Chunk processed 2139700 points in 838.00 seconds
# Performance: 2553.34 points/second
# Estimated remaining time: 0h 41m
# Processing chunk 9/11 with 20 batches
# [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.
# [Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:  5.7min
# [Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  9.0min
# [Parallel(n_jobs=4)]: Done  16 out of  20 | elapsed: 11.9min remaining:  3.0min
# [Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed: 14.6min finished
# Chunk processed 2139700 points in 876.84 seconds
# Performance: 2440.25 points/second
# Estimated remaining time: 0h 29m
# Processing chunk 10/11 with 20 batches
# [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.
# [Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:  5.4min
# [Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  8.2min
# [Parallel(n_jobs=4)]: Done  16 out of  20 | elapsed: 11.2min remaining:  2.8min
# [Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed: 14.1min finished
# Chunk processed 2139700 points in 848.58 seconds
# Performance: 2521.50 points/second
# Estimated remaining time: 0h 14m
# Processing chunk 11/11 with 1 batches
# [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.
# [Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.3s
# Chunk processed 132 points in 0.39 seconds
# Performance: 336.54 points/second
# Loading and combining checkpoints...
# Loading checkpoint chunk 1/1 (201 batches)
# Loading checkpoints completed in 0.23 seconds
# Combining 201 checkpoint results...
# Combining results completed in 0.07 seconds
# Maximum memory usage: 59.1%
# Processed 21397132 points with 5 nearest neighbors each
# Average nearest neighbor distance: 0.000000 radians

import os
import numpy as np
import pandas as pd
import gc
import pickle
import psutil
from datetime import datetime, timedelta
import time
from scipy.interpolate import interp1d

def memory_usage():
    """Monitor memory usage in MB"""
    process = psutil.Process(os.getpid())
    mem_info = process.memory_info()
    return mem_info.rss / 1024**2  # Memory in MB

def save_checkpoint(data, checkpoint_dir, name):
    """Save checkpoint data to pickle file"""
    os.makedirs(checkpoint_dir, exist_ok=True)
    checkpoint_path = os.path.join(checkpoint_dir, f'{name}.pkl')
    with open(checkpoint_path, 'wb') as f:
        pickle.dump(data, f)
    print(f"Saved checkpoint to {checkpoint_path}")

def load_checkpoint(checkpoint_dir, name):
    """Load checkpoint data from pickle file"""
    checkpoint_path = os.path.join(checkpoint_dir, f'{name}.pkl')
    try:
        with open(checkpoint_path, 'rb') as f:
            data = pickle.load(f)
        print(f"Loaded checkpoint from {checkpoint_path}")
        return data
    except:
        print(f"No checkpoint found at {checkpoint_path}")
        return None

# data_loader.py
import os
import pandas as pd
import numpy as np
import pyarrow as pa
import pyarrow.feather as pf
import pyarrow.compute as pc
import pyarrow.dataset as ds
import gc
from tqdm.auto import tqdm

def get_time_range(feather_path):
    """Get min and max datetime from feather file efficiently"""
    print("Determining dataset time range")
    try:
        # Use PyArrow for efficient metadata access
        table = pf.read_table(feather_path, columns=['datetime'])
        datetime_col = table['datetime']
        min_date = pd.Timestamp(pc.min(datetime_col).as_py())
        max_date = pd.Timestamp(pc.max(datetime_col).as_py())
        del table, datetime_col
        gc.collect()
    except Exception as e:
        print(f"Error with PyArrow min/max: {str(e)}")
        print("Falling back to reading sample rows...")
        
        # Read just first and last rows after sorting
        # This is more efficient than reading all data
        try:
            dataset = ds.dataset(feather_path, format='feather')
            
            # Get min date from sorted data (first row)
            table_min = dataset.to_table(
                columns=['datetime'],
                filter=None,
                use_threads=True,
                limit=1
            )
            min_date = pd.Timestamp(table_min['datetime'][0].as_py())
            
            # Get max date from reverse sorted data (first row)
            # Note: PyArrow Dataset API doesn't support reverse sort directly
            # So we read all datetime values and find max
            table_all = dataset.to_table(columns=['datetime'])
            max_date = pd.Timestamp(pc.max(table_all['datetime']).as_py())
            
            del table_min, table_all, dataset
            gc.collect()
        except Exception as inner_e:
            print(f"Error with optimized approach: {str(inner_e)}")
            print("Using full scan method (slow)...")
            
            # Last resort: read all timestamps in chunks
            min_date = pd.Timestamp.max
            max_date = pd.Timestamp.min
            
            # Open file directly to avoid loading everything
            table = pf.read_table(feather_path, columns=['datetime'])
            datetime_values = table['datetime'].to_pandas()
            
            min_date = datetime_values.min()
            max_date = datetime_values.max()
            
            del datetime_values, table
            gc.collect()
    
    print(f"Data timespan: {min_date} to {max_date}")
    return min_date, max_date

def get_unique_site_depths(feather_path):
    """Get unique site-depth combinations efficiently"""
    print("Finding unique site-depth combinations")
    print(f"Memory before processing: {memory_usage():.1f} MB")
    
    try:
        # Use PyArrow for efficient operation
        dataset = ds.dataset(feather_path, format='feather')
        
        # Read only the columns we need
        table = dataset.to_table(columns=['source', 'soil_temp_depth'])
        
        # Convert to pandas
        df = table.to_pandas()
        
        # Get valid combinations (where temp_depth is not NaN)
        valid_df = df.dropna(subset=['soil_temp_depth'])
        
        # Get unique combinations
        site_depths = valid_df.drop_duplicates(['source', 'soil_temp_depth'])
        
        # Clean up
        del table, df, valid_df, dataset
        gc.collect()
        
    except Exception as e:
        print(f"Error with PyArrow approach: {str(e)}")
        print("Falling back to chunked pandas approach...")
        
        # Fallback: Process in chunks
        chunk_size = 1000000
        unique_combos = set()
        
        # Read feather in chunks (this is slower but more robust)
        with pd.read_feather(feather_path, columns=['source', 'soil_temp_depth']) as reader:
            for chunk in reader:
                # Get valid rows and unique combinations
                valid_rows = chunk.dropna(subset=['soil_temp_depth'])
                chunk_combinations = set(zip(valid_rows['source'], valid_rows['soil_temp_depth']))
                unique_combos.update(chunk_combinations)
                
                # Clean up
                del chunk, valid_rows, chunk_combinations
                gc.collect()
        
        # Convert to DataFrame
        site_depths = pd.DataFrame(list(unique_combos), columns=['source', 'soil_temp_depth'])
    
    print(f"Found {len(site_depths)} unique site-depth combinations")
    print(f"Memory after processing: {memory_usage():.1f} MB")
    
    return site_depths[['source', 'soil_temp_depth']]

def load_site_depth_data(feather_path, site, temp_depth):
    """Load ONLY data for a specific site and depth using PyArrow filtering"""
    print(f"Loading data for site: {site}, depth: {temp_depth}")
    print(f"Memory before loading: {memory_usage():.1f} MB")
    
    try:
        # Use PyArrow Dataset API for efficient filtering
        dataset = ds.dataset(feather_path, format='feather')
        
        # Create filter expressions
        site_filter = ds.field('source') == site
        depth_filter = ds.field('soil_temp_depth') == float(temp_depth)
        combined_filter = site_filter & depth_filter
        
        # Apply filter and read only filtered data
        table = dataset.to_table(filter=combined_filter)
        filtered_df = table.to_pandas()
        
        # Clean up
        del table, dataset
        gc.collect()
        
    except Exception as e:
        print(f"Error with PyArrow filtering: {str(e)}")
        print("Falling back to pandas filtering...")
        
        # Define chunk size based on available memory
        available_mem = psutil.virtual_memory().available / (1024**2)  # MB
        chunk_size = min(500000, max(100000, int(available_mem / 10)))
        
        filtered_chunks = []
        
        # Read and filter in chunks
        for chunk in pd.read_feather(feather_path, columns=None, chunksize=chunk_size):
            # Filter by site and depth
            chunk_filtered = chunk[(chunk['source'] == site) & 
                                   (chunk['soil_temp_depth'] == temp_depth)]
            
            if len(chunk_filtered) > 0:
                filtered_chunks.append(chunk_filtered)
            
            # Clean up
            del chunk, chunk_filtered
            gc.collect()
        
        # Combine filtered chunks
        if filtered_chunks:
            filtered_df = pd.concat(filtered_chunks, ignore_index=True)
            del filtered_chunks
        else:
            filtered_df = pd.DataFrame()
        
        gc.collect()
    
    # Ensure datetime is in datetime format
    if 'datetime' in filtered_df.columns and not pd.api.types.is_datetime64_dtype(filtered_df['datetime']):
        filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])
    
    print(f"Loaded {len(filtered_df)} rows for site-depth")
    print(f"Memory after loading: {memory_usage():.1f} MB")
    
    return filtered_df

def prepare_data_for_deep_learning_efficiently(feather_path, events_df, sequence_length=6, 
                                               output_dir=None, batch_size=500, start_batch=0):
    """
    Memory-efficient version of prepare_data_for_deep_learning that processes
    site-depths in batches and saves intermediate results without accumulating all data in memory.
    """
    import numpy as np
    from tqdm.auto import tqdm
    import os
    import gc
    import pandas as pd
    
    print("Preparing data for deep learning model...")
    print(f"Memory before preparation: {memory_usage():.1f} MB")
    
    # Ensure datetime columns are proper datetime objects
    if not pd.api.types.is_datetime64_dtype(events_df['datetime_min']):
        events_df['datetime_min'] = pd.to_datetime(events_df['datetime_min'], format='mixed')
    if not pd.api.types.is_datetime64_dtype(events_df['datetime_max']):
        events_df['datetime_max'] = pd.to_datetime(events_df['datetime_max'], format='mixed')
    
    # Create a mapping of detected events for labeling
    print("Creating event mapping...")
    event_map = {}
    for _, event in events_df.iterrows():
        site = event['source']
        depth = event['soil_temp_depth']
        start = event['datetime_min']
        end = event['datetime_max']
        
        if (site, depth) not in event_map:
            event_map[(site, depth)] = []
        
        event_map[(site, depth)].append((start, end))
    
    # Get site-depth combinations for progress tracking
    print("Finding unique site-depth combinations")
    print(f"Memory before processing: {memory_usage():.1f} MB")
    
    # Use a function that efficiently gets unique site-depths without loading all data
    site_depths = get_unique_site_depths(feather_path)
    total_combinations = len(site_depths)
    
    print(f"Found {total_combinations} unique site-depth combinations")
    print(f"Memory after processing: {memory_usage():.1f} MB")
    print(f"Preparing sequences from {total_combinations} site-depth combinations...")
    
    # Check for existing batch files to support resuming
    if output_dir is not None:
        os.makedirs(output_dir, exist_ok=True)
        import glob
        existing_batches = glob.glob(os.path.join(output_dir, 'X_batch_*_*.npy'))
        if existing_batches and start_batch == 0:
            # Extract batch numbers from filenames
            batch_ends = [int(os.path.basename(f).split('_')[-1].split('.')[0]) for f in existing_batches]
            if batch_ends:
                last_processed_batch = max(batch_ends)
                # Start from the next batch
                start_batch = (last_processed_batch // batch_size) * batch_size + batch_size
                print(f"Found existing batch files, resuming from batch {start_batch}")
    
    # Track total counts for reporting
    total_sequences = 0
    total_positive = 0
    
    # Process in batches to manage memory
    for batch_start in range(start_batch, total_combinations, batch_size):
        batch_end = min(batch_start + batch_size, total_combinations)
        print(f"\nProcessing batch {batch_start+1}-{batch_end}/{total_combinations}")
        
        # Initialize lists for this batch only
        batch_features = []
        batch_labels = []
        batch_metadata = []
        
        # Process each site-depth in the batch
        for i in tqdm(range(batch_start, batch_end), desc="Creating sequences"):
            site = site_depths.iloc[i]['source']
            temp_depth = site_depths.iloc[i]['soil_temp_depth']
            
            # Skip if no events exist for this site-depth
            if (site, temp_depth) not in event_map and len(event_map) > 0:
                continue
            
            try:
                # Load only the data for this site-depth
                print(f"Loading data for site: {site}, depth: {temp_depth}")
                print(f"Memory before loading: {memory_usage():.1f} MB")
                
                group = load_site_depth_data(feather_path, site, temp_depth)
                
                print(f"Loaded {len(group)} rows for site-depth")
                print(f"Memory after loading: {memory_usage():.1f} MB")
                
                if len(group) < sequence_length + 1:
                    continue
                
                # Ensure datetime is in datetime format
                if not pd.api.types.is_datetime64_dtype(group['datetime']):
                    group['datetime'] = pd.to_datetime(group['datetime'], format='mixed')
                
                # Sort by time
                group = group.sort_values('datetime')
                
                # Create feature set
                feature_cols = ['soil_temp_standardized']
                
                # Calculate gradient features
                group['temp_gradient'] = group['soil_temp_standardized'].diff()
                feature_cols.append('temp_gradient')
                
                # Add soil depth as feature
                group['depth_normalized'] = temp_depth / 10.0
                feature_cols.append('depth_normalized')
                
                # Add soil moisture if available
                has_moisture = False
                if 'soil_moist_standardized' in group.columns and not group['soil_moist_standardized'].isna().all():
                    has_moisture = True
                    feature_cols.append('soil_moist_standardized')
                    group['moist_gradient'] = group['soil_moist_standardized'].diff()
                    feature_cols.append('moist_gradient')
                
                # Fill missing values
                group[feature_cols] = group[feature_cols].fillna(0)
                
                # Create sequences with sliding window
                for j in range(len(group) - sequence_length):
                    # Get time window
                    start_time = group.iloc[j]['datetime']
                    end_time = group.iloc[j + sequence_length - 1]['datetime']
                    
                    # Extract sequence data
                    sequence = group.iloc[j:j+sequence_length][feature_cols].values
                    
                    # Check if this sequence overlaps with known zero curtain event
                    is_zero_curtain = 0
                    if (site, temp_depth) in event_map:
                        for event_start, event_end in event_map[(site, temp_depth)]:
                            # Ensure proper datetime comparison
                            # Check for significant overlap (at least 50% of sequence)
                            if (min(end_time, event_end) - max(start_time, event_start)).total_seconds() > \
                               0.5 * (end_time - start_time).total_seconds():
                                is_zero_curtain = 1
                                break
                    
                    # Store features and labels
                    batch_features.append(sequence)
                    batch_labels.append(is_zero_curtain)
                    total_positive += is_zero_curtain
                    total_sequences += 1
                    
                    # Store metadata
                    meta = {
                        'source': site,
                        'soil_temp_depth': temp_depth,
                        'start_time': start_time,
                        'end_time': end_time,
                        'latitude': group.iloc[j]['latitude'] if 'latitude' in group.columns else None,
                        'longitude': group.iloc[j]['longitude'] if 'longitude' in group.columns else None,
                        'has_moisture_data': has_moisture
                    }
                    batch_metadata.append(meta)
                
                # Clean up to free memory
                del group
                gc.collect()
                
            except Exception as e:
                print(f"Error processing {site}, depth {temp_depth}: {str(e)}")
                import traceback
                traceback.print_exc()
                continue
        
        # Save batch results if output directory provided and we have data
        if output_dir is not None and batch_features:
            os.makedirs(output_dir, exist_ok=True)
            
            # Save batch as numpy files
            batch_X = np.array(batch_features)
            batch_y = np.array(batch_labels)
            
            np.save(os.path.join(output_dir, f'X_batch_{batch_start}_{batch_end}.npy'), batch_X)
            np.save(os.path.join(output_dir, f'y_batch_{batch_start}_{batch_end}.npy'), batch_y)
            
            # Save metadata as pickle
            import pickle
            with open(os.path.join(output_dir, f'metadata_batch_{batch_start}_{batch_end}.pkl'), 'wb') as f:
                pickle.dump(batch_metadata, f)
            
            # Save progress marker
            with open(os.path.join(output_dir, 'progress.txt'), 'w') as f:
                f.write(f"Last processed batch: {batch_start}-{batch_end}\n")
                f.write(f"Total sequences: {total_sequences}\n")
                f.write(f"Positive examples: {total_positive} ({total_positive/total_sequences*100:.1f}%)\n")
        
        # Clean up batch variables - this is key for memory efficiency
        del batch_features, batch_labels, batch_metadata
        if 'batch_X' in locals(): del batch_X
        if 'batch_y' in locals(): del batch_y
        gc.collect()
        
        print(f"Memory after batch: {memory_usage():.1f} MB")
        #print(f"Progress: {total_sequences} sequences processed, {total_positive} positive examples...
        positive_percentage = (total_positive/total_sequences*100) if total_sequences > 0 else 0
        print(f"Progress: {total_sequences} sequences processed, {total_positive} positive examples ({positive_percentage:.1f}%)")

    print("Data preparation complete!")
    print(f"Total sequences: {total_sequences}")
    #print(f"Positive examples: {total_positive} ({total_positive/total_sequences*100:.1f}% if total...
    positive_percentage = (total_positive/total_sequences*100) if total_sequences > 0 else 0
    print(f"Positive examples: {total_positive} ({positive_percentage:.1f}%)")
    
    if output_dir is not None:
        print(f"Results saved to {output_dir}")
        print("To merge batch files into final dataset, use merge_batch_files(output_dir)")
    
    print(f"Memory after preparation: {memory_usage():.1f} MB")
    
    # Return info instead of data - prevents memory issues
    return {
        'total_sequences': total_sequences,
        'total_positive': total_positive,
        'positive_percentage': total_positive/total_sequences*100 if total_sequences > 0 else 0,
        'output_dir': output_dir
    }


def merge_batch_files(output_dir):
    """
    Merge all batch files into single X_features.npy and y_labels.npy files.
    """
    import numpy as np
    import os
    import glob
    import pickle
    import gc
    
    print(f"Memory before merging: {memory_usage():.1f} MB")
    
    # Find all batch files
    x_batch_files = sorted(glob.glob(os.path.join(output_dir, 'X_batch_*_*.npy')))
    y_batch_files = sorted(glob.glob(os.path.join(output_dir, 'y_batch_*_*.npy')))
    metadata_batch_files = sorted(glob.glob(os.path.join(output_dir, 'metadata_batch_*_*.pkl')))
    
    print(f"Found {len(x_batch_files)} X batches, {len(y_batch_files)} y batches, and {len(metadata_batch_files)} metadata batches")
    
    # Get dimensions of first batch to initialize arrays
    if x_batch_files:
        first_batch = np.load(x_batch_files[0])
        shape = first_batch.shape
        del first_batch
        gc.collect()
        
        # Count total sequences
        total_sequences = 0
        for batch_file in x_batch_files:
            batch = np.load(batch_file)
            total_sequences += batch.shape[0]
            del batch
            gc.collect()
        
        # Pre-allocate arrays
        X = np.zeros((total_sequences, shape[1], shape[2]), dtype=np.float32)
        y = np.zeros(total_sequences, dtype=np.int32)
        
        # Load and copy batches
        idx = 0
        for i, (x_file, y_file) in enumerate(zip(x_batch_files, y_batch_files)):
            print(f"Processing batch {i+1}/{len(x_batch_files)}")
            print(f"Memory: {memory_usage():.1f} MB")
            
            batch_x = np.load(x_file)
            batch_y = np.load(y_file)
            
            batch_size = batch_x.shape[0]
            X[idx:idx+batch_size] = batch_x
            y[idx:idx+batch_size] = batch_y
            
            idx += batch_size
            
            # Clean up
            del batch_x, batch_y
            gc.collect()
        
        # Save merged X and y
        print(f"Saving merged arrays: X.shape={X.shape}, y.shape={y.shape}")
        np.save(os.path.join(output_dir, 'X_features.npy'), X)
        np.save(os.path.join(output_dir, 'y_labels.npy'), y)
        
        # Clean up
        del X, y
        gc.collect()
        
        # Load and concatenate metadata batches
        all_metadata = []
        for i, batch_file in enumerate(metadata_batch_files):
            print(f"Processing metadata batch {i+1}/{len(metadata_batch_files)}")
            print(f"Memory: {memory_usage():.1f} MB")
            
            with open(batch_file, 'rb') as f:
                batch_metadata = pickle.load(f)
            all_metadata.extend(batch_metadata)
            
            # Clean up
            del batch_metadata
            gc.collect()
        
        # Save merged metadata
        print(f"Saving merged metadata: {len(all_metadata)} entries")
        with open(os.path.join(output_dir, 'metadata.pkl'), 'wb') as f:
            pickle.dump(all_metadata, f)
        
        # Final cleanup
        del all_metadata
        gc.collect()
        
        print(f"Memory after merging: {memory_usage():.1f} MB")
        return {
            'total_sequences': total_sequences,
            'output_dir': output_dir
        }
    else:
        print("No batch files found to merge")
        return None

def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
    """
    Advanced model architecture combining ConvLSTM, Transformers, and 
    Variational Autoencoder components to better capture complex zero curtain dynamics.
    
    Parameters:
    -----------
    input_shape : tuple
        Shape of input data (sequence_length, num_features)
    include_moisture : bool
        Whether soil moisture features are included
        
    Returns:
    --------
    tensorflow.keras.Model
        Compiled model ready for training
    """
    import os
    os.environ["DEVICE_COUNT_GPU"] = "0"
    import tensorflow as tf
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
    from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D
    from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
    from tensorflow.keras.optimizers import Adam
    
    # Input layer
    inputs = Input(shape=input_shape)
    
    # Reshape for ConvLSTM (add spatial dimension)
    # From (sequence_length, features) to (sequence_length, 1, features)
    
    #x = Reshape((input_shape[0], 1, input_shape[1]))(inputs)
    x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
    # ConvLSTM layer to capture spatiotemporal patterns
    from tensorflow.keras.layers import ConvLSTM2D
    convlstm = ConvLSTM2D(
        filters=64,
        kernel_size=(3, 1),
        padding='same',
        return_sequences=True,
        activation='tanh',
        recurrent_dropout=0.2
    )(x)
    
    # Reshape back to (sequence_length, features)
    convlstm = Reshape((input_shape[0], 64))(convlstm)
    
    # Add positional encoding for transformer
    def positional_encoding(length, depth):
        positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
        angle_rates = 1 / tf.pow(10000.0, depths)
        angle_rads = positions * angle_rates
        
        pos_encoding = tf.concat(
            [tf.sin(angle_rads), tf.cos(angle_rads)], axis=-1)
        
        return pos_encoding
    
    # Add positional encoding
    pos_encoding = positional_encoding(input_shape[0], 64)
    transformer_input = convlstm + pos_encoding
    
    # Transformer encoder block
    def transformer_encoder(x, num_heads=8, key_dim=64, ff_dim=128):
        # Multi-head attention
        attention_output = MultiHeadAttention(
            num_heads=num_heads, key_dim=key_dim
        )(x, x)
        
        # Skip connection 1
        x1 = Add()([attention_output, x])
        x1 = LayerNormalization(epsilon=1e-6)(x1)
        
        # Feed-forward network
        ff_output = Dense(ff_dim, activation='relu')(x1)
        ff_output = Dropout(0.1)(ff_output)
        ff_output = Dense(64)(ff_output)
        
        # Skip connection 2
        x2 = Add()([ff_output, x1])
        return LayerNormalization(epsilon=1e-6)(x2)
    
    # Apply transformer encoder
    transformer_output = transformer_encoder(transformer_input)
    
    # Parallel CNN paths for multi-scale feature extraction
    cnn_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(inputs)
    cnn_1 = BatchNormalization()(cnn_1)
    
    cnn_2 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(inputs)
    cnn_2 = BatchNormalization()(cnn_2)
    
    cnn_3 = Conv1D(filters=32, kernel_size=7, padding='same', activation='relu')(inputs)
    cnn_3 = BatchNormalization()(cnn_3)
    
    # Variational Autoencoder components
    def sampling(args):
        z_mean, z_log_var = args
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
    # Global temporal features
    global_max = GlobalMaxPooling1D()(transformer_output)
    global_avg = GlobalAveragePooling1D()(transformer_output)
    
    # VAE encoding
    z_mean = Dense(32)(Concatenate()([global_max, global_avg]))
    z_log_var = Dense(32)(Concatenate()([global_max, global_avg]))
    z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
    # Combine all features
    merged_features = Concatenate()(
        [
            GlobalMaxPooling1D()(cnn_1),
            GlobalMaxPooling1D()(cnn_2),
            GlobalMaxPooling1D()(cnn_3),
            global_max,
            global_avg,
            z
        ]
    )
    
    # Final classification layers
    x = Dense(128, activation='relu')(merged_features)
    x = Dropout(0.3)(x)
    x = BatchNormalization()(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.2)(x)
    
    # Output layer
    outputs = Dense(1, activation='sigmoid')(x)
    
    # Create model
    model = Model(inputs=inputs, outputs=outputs)
    
    # Add VAE loss
    kl_loss = -0.5 * tf.reduce_mean(
        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
    )
    model.add_loss(0.001 * kl_loss)  # Small weight for VAE loss
    
    # Compile model with appropriate metrics
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=[
            'accuracy',
            tf.keras.metrics.AUC(name='auc'),
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall')
        ]
    )
    
    return model

# Time-based split rather than random split
def temporal_train_test_split(X, y, metadata, val_ratio=0.2, test_ratio=0.1):
    """
    Split data temporally for time series modeling.
    
    Parameters:
    -----------
    X : numpy.ndarray
        Input features
    y : numpy.ndarray
        Output labels
    metadata : list
        Metadata containing timestamps for each sequence
    val_ratio : float
        Proportion of data for validation
    test_ratio : float
        Proportion of data for testing
        
    Returns:
    --------
    tuple
        (X_train, X_val, X_test, y_train, y_val, y_test)
    """
    # Extract timestamps from metadata
    timestamps = [meta['start_time'] for meta in metadata]
    
    # Sort indices by timestamp
    sorted_indices = sorted(range(len(timestamps)), key=lambda i: timestamps[i])
    
    # Calculate split points
    n_samples = len(sorted_indices)
    test_start = int(n_samples * (1 - test_ratio))
    val_start = int(n_samples * (1 - test_ratio - val_ratio))
    
    # Split indices into train, validation, and test sets
    train_indices = sorted_indices[:val_start]
    val_indices = sorted_indices[val_start:test_start]
    test_indices = sorted_indices[test_start:]
    
    # Create the splits
    X_train = X[train_indices]
    y_train = y[train_indices]
    
    X_val = X[val_indices]
    y_val = y[val_indices]
    
    X_test = X[test_indices]
    y_test = y[test_indices]
    
    return X_train, X_val, X_test, y_train, y_val, y_test

def train_zero_curtain_model_efficiently(X, y, metadata=None, output_dir=None):
    """
    Memory-efficient version of train_zero_curtain_model that implements
    batch training and model checkpointing with temporal data splitting.
    
    Parameters:
    -----------
    X : numpy.ndarray
        Input features
    y : numpy.ndarray
        Output labels
    metadata : list, optional
        Metadata about each sequence (must contain timestamps)
    output_dir : str, optional
        Directory to save model and results
        
    Returns:
    --------
    tuple
        (trained_model, training_history, evaluation_results)
    """
    import tensorflow as tf
    from tensorflow.keras.callbacks import (
        EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, 
        CSVLogger, TensorBoard
    )
    import matplotlib.pyplot as plt
    import os
    import gc
    import numpy as np
    
    # Enable memory growth to avoid pre-allocating all GPU memory
    physical_devices = tf.config.list_physical_devices('GPU')
    if physical_devices:
        for device in physical_devices:
            tf.config.experimental.set_memory_growth(device, True)
            print(f"Enabled memory growth for {device}")
    
    print("Training zero curtain model...")
    print(f"Memory before training: {memory_usage():.1f} MB")
    
    # Create output directory
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    # Temporal split for time series data
    print("Performing temporal split for train/validation/test sets...")
    if metadata is None:
        raise ValueError("Metadata with timestamps is required for temporal splitting")
    
    # Extract timestamps from metadata
    timestamps = np.array([meta['start_time'] for meta in metadata])
    
    # Sort indices by timestamp
    sorted_indices = np.argsort(timestamps)
    
    # Calculate split points (70% train, 15% validation, 15% test)
    n_samples = len(sorted_indices)
    test_ratio = 0.15
    val_ratio = 0.15
    
    test_start = int(n_samples * (1 - test_ratio))
    val_start = int(n_samples * (1 - test_ratio - val_ratio))
    
    # Split indices into train, validation, and test sets
    train_indices = sorted_indices[:val_start]
    val_indices = sorted_indices[val_start:test_start]
    test_indices = sorted_indices[test_start:]
    
    print(f"Training on data from {timestamps[train_indices[0]]} to {timestamps[train_indices[-1]]}")
    print(f"Validating on data from {timestamps[val_indices[0]]} to {timestamps[val_indices[-1]]}")
    print(f"Testing on data from {timestamps[test_indices[0]]} to {timestamps[test_indices[-1]]}")
    
    # Create the splits
    X_train = X[train_indices]
    y_train = y[train_indices]
    
    X_val = X[val_indices]
    y_val = y[val_indices]
    
    X_test = X[test_indices]
    y_test = y[test_indices]
    
    print(f"Split sizes: Train={len(X_train)}, Validation={len(X_val)}, Test={len(X_test)}")
    
    # Check class balance in each split
    train_pos = np.sum(y_train)
    val_pos = np.sum(y_val)
    test_pos = np.sum(y_test)
    
    print(f"Positive examples: Train={train_pos} ({train_pos/len(y_train)*100:.1f}%), " +
          f"Val={val_pos} ({val_pos/len(y_val)*100:.1f}%), " +
          f"Test={test_pos} ({test_pos/len(y_test)*100:.1f}%)")
    
    # Clean up to free memory
    del sorted_indices, timestamps
    gc.collect()
    
    # Build model with appropriate input shape
    print("Building model...")
    input_shape = (X_train.shape[1], X_train.shape[2])
    
    model = build_advanced_zero_curtain_model(input_shape)
    
    # If output directory exists, check for existing model checkpoint
    model_checkpoint_path = None
    if output_dir:
        model_checkpoint_path = os.path.join(output_dir, 'checkpoint.h5')
        if os.path.exists(model_checkpoint_path):
            print(f"Loading existing model checkpoint from {model_checkpoint_path}")
            try:
                model = tf.keras.models.load_model(model_checkpoint_path)
                print("Checkpoint loaded successfully")
            except Exception as e:
                print(f"Error loading checkpoint: {str(e)}")
    
    # Set up callbacks with additional memory management
    callbacks = [
        # Stop early if validation performance plateaus
        EarlyStopping(
            patience=15,  # Increased patience for temporal data
            restore_best_weights=True, 
            monitor='val_auc', 
            mode='max'
        ),
        # Reduce learning rate when improvement slows
        ReduceLROnPlateau(
            factor=0.5, 
            patience=7,  # Increased patience for temporal data
            min_lr=1e-6, 
            monitor='val_auc', 
            mode='max'
        ),
        # Manual garbage collection after each epoch
        tf.keras.callbacks.LambdaCallback(
            on_epoch_end=lambda epoch, logs: gc.collect()
        )
    ]
    
    # Add additional callbacks if output directory provided
    if output_dir:
        callbacks.extend([
            # Save best model
            ModelCheckpoint(
                os.path.join(output_dir, 'checkpoint.h5'),
                save_best_only=True,
                monitor='val_auc',
                mode='max'
            ),
            # Log training progress to CSV
            CSVLogger(
                os.path.join(output_dir, 'training_log.csv'),
                append=True
            ),
            # TensorBoard visualization
            TensorBoard(
                log_dir=os.path.join(output_dir, 'tensorboard_logs'),
                histogram_freq=1,
                profile_batch=0  # Disable profiling to save memory
            )
        ])
    
    # Calculate class weights to handle imbalance
    pos_weight = len(y_train) / max(sum(y_train), 1)
    class_weight = {0: 1, 1: pos_weight}
    print(f"Using class weight {pos_weight:.2f} for positive class")
    
    # Train model with memory-efficient settings
    print("Training model...")
    batch_size = 32  # Adjust based on available memory
    epochs = 100
    
    # Use fit with appropriate memory settings
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=callbacks,
        class_weight=class_weight,
        verbose=1,
        # Memory efficiency settings
        shuffle=True,  # Still shuffle within the temporal train split
        use_multiprocessing=False,  # Avoid extra memory overhead
        workers=1  # Reduce parallel processing to save memory
    )
    
    # Clean up to free memory
    del X_train, y_train, X_val, y_val
    gc.collect()
    
    # Evaluate on test set
    print("Evaluating model on test set...")
    evaluation = model.evaluate(X_test, y_test, batch_size=batch_size)
    print("Test performance:")
    for metric, value in zip(model.metrics_names, evaluation):
        print(f"  {metric}: {value:.4f}")
    
    # Generate predictions for visualization and further analysis
    y_pred_prob = model.predict(X_test, batch_size=batch_size)
    y_pred = (y_pred_prob > 0.5).astype(int).flatten()
    
    # Calculate and save additional evaluation metrics
    from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
    report = classification_report(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    
    print("Classification Report:")
    print(report)
    
    print("Confusion Matrix:")
    print(conf_matrix)
    
    # Plot and save training history
    if output_dir:
        # Save evaluation metrics
        with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w') as f:
            f.write("Classification Report:\n")
            f.write(report)
            f.write("\n\nConfusion Matrix:\n")
            f.write(str(conf_matrix))
            f.write("\n\nTest Metrics:\n")
            for metric, value in zip(model.metrics_names, evaluation):
                f.write(f"{metric}: {value:.4f}\n")
        
        # Plot training history
        plt.figure(figsize=(16, 6))
        
        plt.subplot(1, 3, 1)
        plt.plot(history.history['auc'])
        plt.plot(history.history['val_auc'])
        plt.title('Model AUC')
        plt.ylabel('AUC')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'], loc='lower right')
        
        plt.subplot(1, 3, 2)
        plt.plot(history.history['loss'])
        plt.plot(history.history['val_loss'])
        plt.title('Model Loss')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'], loc='upper right')
        
        # Plot ROC curve
        plt.subplot(1, 3, 3)
        fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve (Test Set)')
        plt.legend(loc='lower right')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'training_performance.png'), dpi=300)
        
        # Save detailed model summary
        from contextlib import redirect_stdout
        with open(os.path.join(output_dir, 'model_summary.txt'), 'w') as f:
            with redirect_stdout(f):
                model.summary()
    
    # Clean up to free memory
    del X_test, y_test, y_pred, y_pred_prob
    gc.collect()
    
    print(f"Memory after training: {memory_usage():.1f} MB")
    return model, history, evaluation

def run_full_analysis_pipeline(feather_path, output_base_dir='results', batch_size=50):
    """
    Run the complete zero curtain analysis pipeline with progress tracking
    and memory efficiency.
    
    Parameters:
    -----------
    feather_path : str
        Path to the feather file with merged data
    output_base_dir : str
        Base directory for saving outputs
    batch_size : int
        Number of site-depths to process per batch
        
    Returns:
    --------
    dict
        Dictionary containing analysis results
    """
    from tqdm.auto import tqdm
    import time
    import os
    import gc
    import pickle
    
    # Create output directories
    os.makedirs(output_base_dir, exist_ok=True)
    checkpoint_dir = os.path.join(output_base_dir, 'checkpoints')
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # Function to save checkpoint
    def save_checkpoint(data, name):
        with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'wb') as f:
            pickle.dump(data, f)
    
    # Function to load checkpoint
    def load_checkpoint(name):
        try:
            with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'rb') as f:
                return pickle.load(f)
        except:
            return None
    
    # Initialize results
    results = load_checkpoint('pipeline_results') or {}
    
    # Check for completed stages
    completed_stages = set(results.get('completed_stages', []))
    print(f"Found {len(completed_stages)} completed stages: {completed_stages}")
    
    # Add a progress indicator for the overall workflow
    stages = ['Enhanced Detection', 'Data Preparation', 'Model Training', 
              'Model Application', 'Visualization', 'Comparison']
    
    with tqdm(total=len(stages), desc="Overall Progress", initial=len(completed_stages)) as pbar:
        # Stage 1: Enhanced physical detection
        if 'Enhanced Detection' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 1/{len(stages)}: {stages[0]}")
            
            print(f"Current memory usage: {memory_usage():.1f} MB")
            
            # Run memory-efficient detection using your implementation
            # This part is already implemented in your code
            #from zero_curtain_pipeline import run_memory_efficient_pipeline
            
            enhanced_events = run_memory_efficient_pipeline(
                feather_path=feather_path,
                output_dir=os.path.join(output_base_dir, 'enhanced'),
                site_batch_size=batch_size,
                checkpoint_interval=5,
                max_gap_hours=6,
                interpolation_method='cubic'
            )
            
            results['enhanced_events'] = enhanced_events
            results['enhanced_time'] = time.time() - start_time
            
            # Save progress
            completed_stages.add('Enhanced Detection')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            # Clean up memory
            gc.collect()
            print(f"Memory after Stage 1: {memory_usage():.1f} MB")
            pbar.update(1)
        else:
            # Load enhanced events if needed
            if 'enhanced_events' not in results:
                enhanced_events = load_checkpoint('enhanced_events')
                if enhanced_events is None:
                    # Try loading from CSV
                    csv_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events.csv')
                    if os.path.exists(csv_path):
                        enhanced_events = pd.read_csv(csv_path, parse_dates=['datetime_min', 'datetime_max'])
                    else:
                        print("Warning: No enhanced events found, cannot proceed with deep learning")
                        enhanced_events = pd.DataFrame()
                results['enhanced_events'] = enhanced_events
        
        # Stage 2: Data Preparation for Deep Learning
        if 'Data Preparation' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 2/{len(stages)}: {stages[1]}")
            
            # Get enhanced events
            enhanced_events = results.get('enhanced_events')
            if enhanced_events is not None and len(enhanced_events) > 0:
                try:
                    # Prepare data for deep learning with memory efficiency
                    X, y, metadata = prepare_data_for_deep_learning_efficiently(
                        feather_path=feather_path,
                        events_df=enhanced_events,
                        sequence_length=24,  # Use 24 time steps as in your original code
                        output_dir=os.path.join(output_base_dir, 'ml_data'),
                        batch_size=batch_size
                    )
                    
                    results['X'] = X.shape  # Store only shape to save memory
                    results['y'] = y.shape
                    results['data_preparation_time'] = time.time() - start_time
                    
                    # Clean up to free memory
                    del X, y
                    gc.collect()
                except Exception as e:
                    print(f"Error in data preparation: {str(e)}")
                    results['data_preparation_error'] = str(e)
            else:
                print("Skipping data preparation: No enhanced events available")
            
            # Save progress
            completed_stages.add('Data Preparation')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            # Clean up memory
            gc.collect()
            print(f"Memory after Stage 2: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 3: Model Training
        if 'Model Training' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 3/{len(stages)}: {stages[2]}")
            
            try:
                # Load prepared data
                data_dir = os.path.join(output_base_dir, 'ml_data')
                X = np.load(os.path.join(data_dir, 'X_features.npy'))
                y = np.load(os.path.join(data_dir, 'y_labels.npy'))
                
                # Load metadata if needed
                with open(os.path.join(data_dir, 'metadata.pkl'), 'rb') as f:
                    metadata = pickle.load(f)
                
                # Train model
                model, history, evaluation = train_zero_curtain_model_efficiently(
                    X=X, 
                    y=y,
                    metadata=metadata,
                    output_dir=os.path.join(output_base_dir, 'model')
                )
                
                # Store minimal results to save memory
                results['model_evaluation'] = evaluation
                results['model_training_time'] = time.time() - start_time
                
                # Clean up to free memory
                del X, y, metadata, model, history
                gc.collect()
            except Exception as e:
                print(f"Error in model training: {str(e)}")
                results['model_training_error'] = str(e)
            
            # Save progress
            completed_stages.add('Model Training')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            # Clean up memory
            gc.collect()
            print(f"Memory after Stage 3: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 4: Model Application
        if 'Model Application' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 4/{len(stages)}: {stages[3]}")
            
            try:
                # Load model
                model_path = os.path.join(output_base_dir, 'model', 'checkpoint.h5')
                if os.path.exists(model_path):
                    import tensorflow as tf
                    model = tf.keras.models.load_model(model_path)
                    
                    # Create directory for predictions
                    pred_dir = os.path.join(output_base_dir, 'predictions')
                    os.makedirs(pred_dir, exist_ok=True)
                    
                    # Apply model with memory efficiency (batched processing)
                    #from apply_model_efficiently import apply_model_to_new_data_efficiently
                    
                    predictions = apply_model_to_new_data_efficiently(
                        model=model,
                        feather_path=feather_path,
                        sequence_length=24,
                        output_dir=pred_dir,
                        batch_size=batch_size
                    )
                    
                    results['model_predictions_count'] = len(predictions)
                    results['model_application_time'] = time.time() - start_time
                    
                    # Clean up
                    del model, predictions
                    gc.collect()
                else:
                    print("Skipping model application: No model checkpoint found")
            except Exception as e:
                print(f"Error in model application: {str(e)}")
                results['model_application_error'] = str(e)
            
            # Save progress
            completed_stages.add('Model Application')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            # Clean up memory
            gc.collect()
            print(f"Memory after Stage 4: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stages 5 and 6: Visualization and Comparison
        # (Follow the same pattern - load data, process, clean up memory)
        
    # Generate final summary report
    total_time = sum([
        results.get('enhanced_time', 0),
        results.get('data_preparation_time', 0),
        results.get('model_training_time', 0),
        results.get('model_application_time', 0),
        results.get('visualization_time', 0),
        results.get('comparison_time', 0)
    ])
    
    print("\n" + "=" * 80)
    print("ZERO CURTAIN ANALYSIS COMPLETE")
    print("=" * 80)
    print(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
    
    return results

def apply_model_to_new_data_efficiently(model, feather_path, sequence_length=6, 
                                        output_dir=None, batch_size=50):
    """
    Apply a trained model to detect zero curtain events in new data with memory efficiency.
    
    Parameters:
    -----------
    model : tensorflow.keras.Model
        Trained zero curtain detection model
    feather_path : str
        Path to the feather file
    sequence_length : int
        Length of sequences used for model input
    output_dir : str, optional
        Output directory for results
    batch_size : int
        Number of site-depths to process per batch
        
    Returns:
    --------
    pandas.DataFrame
        DataFrame with predictions and probabilities
    """
    #from data_loader import get_unique_site_depths, load_site_depth_data
    import numpy as np
    from tqdm.auto import tqdm
    import os
    import gc
    import pandas as pd
    
    print("Applying model to new data...")
    print(f"Memory before application: {memory_usage():.1f} MB")
    
    # Create output directory
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    # Get site-depth combinations
    site_depths = get_unique_site_depths(feather_path)
    total_combinations = len(site_depths)
    print(f"Applying model to {total_combinations} site-depth combinations...")
    
    # Initialize list for all predictions
    all_predictions = []
    
    # Process in batches
    for batch_start in range(0, total_combinations, batch_size):
        batch_end = min(batch_start + batch_size, total_combinations)
        print(f"\nProcessing batch {batch_start+1}-{batch_end}/{total_combinations}")
        
        batch_predictions = []
        
        # Process each site-depth in batch
        for i in tqdm(range(batch_start, batch_end), desc="Making predictions"):
            site = site_depths.iloc[i]['source']
            temp_depth = site_depths.iloc[i]['soil_temp_depth']
            
            try:
                # Load data for this site-depth
                group = load_site_depth_data(feather_path, site, temp_depth)
                
                if len(group) < sequence_length + 1:
                    continue
                
                # Sort by time
                group = group.sort_values('datetime')
                
                # Prepare features (same as in training)
                feature_cols = ['soil_temp_standardized']
                group['temp_gradient'] = group['soil_temp_standardized'].diff()
                feature_cols.append('temp_gradient')
                group['depth_normalized'] = temp_depth / 10.0
                feature_cols.append('depth_normalized')
                
                # Add soil moisture if available
                if 'soil_moist_standardized' in group.columns and not group['soil_moist_standardized'].isna().all():
                    feature_cols.append('soil_moist_standardized')
                    group['moist_gradient'] = group['soil_moist_standardized'].diff()
                    feature_cols.append('moist_gradient')
                
                # Fill missing values
                group[feature_cols] = group[feature_cols].fillna(0)
                
                # Create sequences and predict in mini-batches to save memory
                sequences = []
                sequence_meta = []
                
                for j in range(len(group) - sequence_length):
                    # Get time window
                    start_time = group.iloc[j]['datetime']
                    end_time = group.iloc[j + sequence_length - 1]['datetime']
                    
                    # Extract sequence
                    sequence = group.iloc[j:j+sequence_length][feature_cols].values
                    sequences.append(sequence)
                    
                    # Store metadata
                    meta = {
                        'source': site,
                        'soil_temp_depth': temp_depth,
                        'datetime_min': start_time,
                        'datetime_max': end_time,
                        'latitude': group.iloc[j]['latitude'] if 'latitude' in group.columns else None,
                        'longitude': group.iloc[j]['longitude'] if 'longitude' in group.columns else None
                    }
                    sequence_meta.append(meta)
                    
                    # Process in mini-batches of 1000 sequences
                    if len(sequences) >= 1000:
                        # Make predictions
                        X_batch = np.array(sequences)
                        pred_probs = model.predict(X_batch, verbose=0)
                        
                        # Store results
                        for k, prob in enumerate(pred_probs):
                            meta = sequence_meta[k]
                            prediction = {
                                'source': meta['source'],
                                'soil_temp_depth': meta['soil_temp_depth'],
                                'datetime_min': meta['datetime_min'],
                                'datetime_max': meta['datetime_max'],
                                'zero_curtain_probability': float(prob[0]),
                                'is_zero_curtain': 1 if prob[0] > 0.5 else 0,
                                'latitude': meta['latitude'],
                                'longitude': meta['longitude']
                            }
                            batch_predictions.append(prediction)
                        
                        # Clear mini-batch to free memory
                        sequences = []
                        sequence_meta = []
                
                # Process any remaining sequences
                if sequences:
                    X_batch = np.array(sequences)
                    pred_probs = model.predict(X_batch, verbose=0)
                    
                    for k, prob in enumerate(pred_probs):
                        meta = sequence_meta[k]
                        prediction = {
                            'source': meta['source'],
                            'soil_temp_depth': meta['soil_temp_depth'],
                            'datetime_min': meta['datetime_min'],
                            'datetime_max': meta['datetime_max'],
                            'zero_curtain_probability': float(prob[0]),
                            'is_zero_curtain': 1 if prob[0] > 0.5 else 0,
                            'latitude': meta['latitude'],
                            'longitude': meta['longitude']
                        }
                        batch_predictions.append(prediction)
                
                # Clean up to free memory
                del group, sequences, sequence_meta, X_batch, pred_probs
                gc.collect()
                
            except Exception as e:
                print(f"Error processing {site}, depth {temp_depth}: {str(e)}")
                continue
        
        # Add batch predictions to all predictions
        all_predictions.extend(batch_predictions)
        
        # Save batch predictions
        if output_dir and batch_predictions:
            batch_df = pd.DataFrame(batch_predictions)
            batch_df.to_csv(os.path.join(output_dir, f'predictions_batch_{batch_start}_{batch_end}.csv'), index=False)
        
        # Clear batch to free memory
        del batch_predictions
        gc.collect()
        
        print(f"Memory after batch: {memory_usage():.1f} MB")
    
    # Consolidate all predictions
    print(f"Generated {len(all_predictions)} raw predictions")
    
    # Convert to DataFrame
    predictions_df = pd.DataFrame(all_predictions)
    
    # Save all predictions
    if output_dir and len(predictions_df) > 0:
        predictions_df.to_csv(os.path.join(output_dir, 'all_predictions.csv'), index=False)
    
    # Consolidate overlapping events to get final events
    if len(predictions_df) > 0:
        print("Consolidating overlapping events...")
        consolidated_events = consolidate_overlapping_events(predictions_df)
        print(f"Consolidated into {len(consolidated_events)} events")
        
        # Save consolidated events
        if output_dir:
            consolidated_events.to_csv(os.path.join(output_dir, 'consolidated_events.csv'), index=False)
        
        print(f"Memory after application: {memory_usage():.1f} MB")
        return consolidated_events
    else:
        print("No predictions generated")
        print(f"Memory after application: {memory_usage():.1f} MB")
        return pd.DataFrame()

def consolidate_overlapping_events(predictions_df, probability_threshold=0.5, gap_threshold=6):
    """
    Consolidate overlapping zero curtain events from model predictions.
    
    Parameters:
    -----------
    predictions_df : pandas.DataFrame
        DataFrame with model predictions
    probability_threshold : float
        Minimum probability to consider as zero curtain
    gap_threshold : float
        Maximum gap in hours to consider events as continuous
        
    Returns:
    --------
    pandas.DataFrame
        DataFrame with consolidated zero curtain events
    """
    import pandas as pd
    import numpy as np
    from tqdm.auto import tqdm
    
    print(f"Consolidating {len(predictions_df)} predictions...")
    
    # Filter to likely zero curtain events
    zero_curtain_events = predictions_df[predictions_df['zero_curtain_probability'] >= probability_threshold].copy()
    
    # Ensure datetime columns are datetime type
    if not pd.api.types.is_datetime64_dtype(zero_curtain_events['datetime_min']):
        zero_curtain_events['datetime_min'] = pd.to_datetime(zero_curtain_events['datetime_min'])
    if not pd.api.types.is_datetime64_dtype(zero_curtain_events['datetime_max']):
        zero_curtain_events['datetime_max'] = pd.to_datetime(zero_curtain_events['datetime_max'])
    
    # Process each site and depth separately
    consolidated_events = []
    
    # Get unique site-depth combinations
    site_depths = zero_curtain_events[['source', 'soil_temp_depth']].drop_duplicates()
    
    # Process each site-depth
    for _, row in tqdm(site_depths.iterrows(), total=len(site_depths), desc="Consolidating events"):
        site = row['source']
        depth = row['soil_temp_depth']
        
        # Get events for this site-depth
        group = zero_curtain_events[
            (zero_curtain_events['source'] == site) & 
            (zero_curtain_events['soil_temp_depth'] == depth)
        ].sort_values('datetime_min')
        
        current_event = None
        
        for _, event in group.iterrows():
            if current_event is None:
                # Start a new event
                current_event = {
                    'source': site,
                    'soil_temp_depth': depth,
                    'datetime_min': event['datetime_min'],
                    'datetime_max': event['datetime_max'],
                    'zero_curtain_probability': [event['zero_curtain_probability']],
                    'latitude': event['latitude'],
                    'longitude': event['longitude']
                }
            else:
                # Check if this event overlaps or is close to the current event
                time_gap = (event['datetime_min'] - current_event['datetime_max']).total_seconds() / 3600
                
                if time_gap <= gap_threshold:
                    # Extend the current event
                    current_event['datetime_max'] = max(current_event['datetime_max'], event['datetime_max'])
                    current_event['zero_curtain_probability'].append(event['zero_curtain_probability'])
                else:
                    # Finalize the current event
                    duration_hours = (current_event['datetime_max'] - current_event['datetime_min']).total_seconds() / 3600
                    
                    if duration_hours >= 12:  # Minimum duration threshold
                        final_event = {
                            'source': current_event['source'],
                            'soil_temp_depth': current_event['soil_temp_depth'],
                            'datetime_min': current_event['datetime_min'],
                            'datetime_max': current_event['datetime_max'],
                            'duration_hours': duration_hours,
                            'zero_curtain_probability': np.mean(current_event['zero_curtain_probability']),
                            'latitude': current_event['latitude'],
                            'longitude': current_event['longitude']
                        }
                        consolidated_events.append(final_event)
                    
                    # Start a new event
                    current_event = {
                        'source': site,
                        'soil_temp_depth': depth,
                        'datetime_min': event['datetime_min'],
                        'datetime_max': event['datetime_max'],
                        'zero_curtain_probability': [event['zero_curtain_probability']],
                        'latitude': event['latitude'],
                        'longitude': event['longitude']
                    }
        
        # Handle the last event
        if current_event is not None:
            duration_hours = (current_event['datetime_max'] - current_event['datetime_min']).total_seconds() / 3600
            
            if duration_hours >= 12:  # Minimum duration threshold
                final_event = {
                    'source': current_event['source'],
                    'soil_temp_depth': current_event['soil_temp_depth'],
                    'datetime_min': current_event['datetime_min'],
                    'datetime_max': current_event['datetime_max'],
                    'duration_hours': duration_hours,
                    'zero_curtain_probability': np.mean(current_event['zero_curtain_probability']),
                    'latitude': current_event['latitude'],
                    'longitude': current_event['longitude']
                }
                consolidated_events.append(final_event)
    
    # Convert to DataFrame
    consolidated_df = pd.DataFrame(consolidated_events)
    
    # Add region and latitude band classifications if latitude is available
    if len(consolidated_df) > 0 and 'latitude' in consolidated_df.columns:
        # Add region classification
        def assign_region(lat):
            if lat is None or pd.isna(lat):
                return None
            elif lat >= 66.5:
                return 'Arctic'
            elif lat >= 60:
                return 'Subarctic'
            elif lat >= 50:
                return 'Northern Boreal'
            else:
                return 'Other'
        
        consolidated_df['region'] = consolidated_df['latitude'].apply(assign_region)
        
        # Add latitude band
        def assign_lat_band(lat):
            if lat is None or pd.isna(lat):
                return None
            elif lat < 55:
                return '<55°N'
            elif lat < 60:
                return '55-60°N'
            elif lat < 66.5:
                return '60-66.5°N'
            elif lat < 70:
                return '66.5-70°N'
            elif lat < 75:
                return '70-75°N'
            elif lat < 80:
                return '75-80°N'
            else:
                return '>80°N'
        
        consolidated_df['lat_band'] = consolidated_df['latitude'].apply(assign_lat_band)
    
    return consolidated_df

def visualize_events_efficiently(events_df, output_file=None):
    """
    Create visualizations for zero curtain events with memory efficiency.
    
    Parameters:
    -----------
    events_df : pandas.DataFrame
        DataFrame containing zero curtain events
    output_file : str, optional
        Path to save the visualization
        
    Returns:
    --------
    dict
        Statistics about the visualized events
    """
    import matplotlib.pyplot as plt
    import cartopy.crs as ccrs
    import cartopy.feature as cfeature
    import numpy as np
    from matplotlib.colors import PowerNorm
    import gc
    
    print(f"Creating visualization for {len(events_df)} events...")
    print(f"Memory before visualization: {memory_usage():.1f} MB")
    
    # Calculate percentile boundaries for better scaling
    p10 = np.percentile(events_df['duration_hours'], 10)
    p25 = np.percentile(events_df['duration_hours'], 25)
    p50 = np.percentile(events_df['duration_hours'], 50)  # median
    p75 = np.percentile(events_df['duration_hours'], 75)
    p90 = np.percentile(events_df['duration_hours'], 90)
    
    # Aggregate by site to reduce memory usage and plotting overhead
    site_data = events_df.groupby(['source', 'latitude', 'longitude']).agg({
        'duration_hours': ['count', 'mean', 'median', 'min', 'max']
    }).reset_index()
    
    # Flatten column names
    site_data.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col 
                        for col in site_data.columns]
    
    # Create figure
    fig, axes = plt.subplots(1, 2, figsize=(14, 7), 
                           subplot_kw={'projection': ccrs.NorthPolarStereo()})
    
    # Set map features
    for ax in axes:
        ax.set_extent([-180, 180, 45, 90], ccrs.PlateCarree())
        ax.add_feature(cfeature.LAND, facecolor='whitesmoke')
        ax.add_feature(cfeature.OCEAN, facecolor='aliceblue')
        ax.add_feature(cfeature.COASTLINE, linewidth=0.5)
        
        # Add Arctic Circle with label
        ax.plot(
            np.linspace(-180, 180, 60),
            np.ones(60) * 66.5,
            transform=ccrs.PlateCarree(),
            linestyle='-',
            color='gray',
            linewidth=1.0,
            alpha=0.7
        )
        
        ax.text(
            0, 66.5 + 2,
            "Arctic Circle",
            transform=ccrs.PlateCarree(),
            horizontalalignment='center',
            verticalalignment='bottom',
            fontsize=9,
            bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.2')
        )
    
    # Plot 1: Event count
    count_max = site_data['duration_hours_count'].quantile(0.95)
    scatter1 = axes[0].scatter(
        site_data['longitude'],
        site_data['latitude'],
        transform=ccrs.PlateCarree(),
        c=site_data['duration_hours_count'],
        s=30,
        cmap='viridis',
        vmin=1,
        vmax=count_max,
        alpha=0.8,
        edgecolor='none'
    )
    plt.colorbar(scatter1, ax=axes[0], shrink=0.7, pad=0.05, label='Event Count')
    axes[0].set_title('Zero Curtain Event Count', fontsize=12)
    
    # Plot 2: Mean duration using percentile bounds
    lower_bound = p10
    upper_bound = p90
    
    # Non-linear scaling for better color differentiation
    scatter2 = axes[1].scatter(
        site_data['longitude'],
        site_data['latitude'],
        transform=ccrs.PlateCarree(),
        c=site_data['duration_hours_mean'],
        s=30,
        cmap='RdYlBu_r',
        norm=PowerNorm(gamma=0.7, vmin=lower_bound, vmax=upper_bound),
        alpha=0.8,
        edgecolor='none'
    )
    
    # Create better colorbar with percentile markers
    cbar = plt.colorbar(scatter2, ax=axes[1], shrink=0.7, pad=0.05, 
                       label='Mean Duration (hours)')
    
    # Show percentile ticks
    percentile_ticks = [p10, p25, p50, p75, p90]
    cbar.set_ticks(percentile_ticks)
    cbar.set_ticklabels([f"{h:.0f}h\n({h/24:.1f}d)" for h in percentile_ticks])
    
    axes[1].set_title('Mean Zero Curtain Duration', fontsize=12)
    
    # Add comprehensive title with statistics
    plt.suptitle(
        f'Zero Curtain Analysis: {len(site_data)} Sites, {len(events_df)} Events\n' +
        f'Duration: median={p50:.1f}h ({p50/24:.1f}d), 10-90%={p10:.1f}-{p90:.1f}h',
        fontsize=14
    )
    
    plt.tight_layout(rect=[0, 0, 1, 0.93])
    
    # Save if requested
    if output_file:
        plt.savefig(output_file, dpi=200, bbox_inches='tight')
        print(f"Visualization saved to {output_file}")
    
    # Clean up to free memory
    plt.close(fig)
    del site_data, fig, axes
    gc.collect()
    
    print(f"Memory after visualization: {memory_usage():.1f} MB")
    
    # Return statistics
    return {
        'p10': p10,
        'p25': p25,
        'p50': p50,
        'p75': p75,
        'p90': p90,
        'mean': events_df['duration_hours'].mean(),
        'std': events_df['duration_hours'].std(),
        'min': events_df['duration_hours'].min(),
        'max': events_df['duration_hours'].max()
    }

def compare_detection_methods_efficiently(physical_events_file, model_events_file, output_dir=None):
    """
    Compare zero curtain events detected by different methods with memory efficiency.
    
    Parameters:
    -----------
    physical_events_file : str
        Path to CSV file with events detected by the physics-based method
    model_events_file : str
        Path to CSV file with events detected by the deep learning model
    output_dir : str, optional
        Directory to save comparison results
        
    Returns:
    --------
    dict
        Comparison statistics and metrics
    """
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    from datetime import timedelta
    import os
    import gc
    
    print("Comparing detection methods...")
    print(f"Memory before comparison: {memory_usage():.1f} MB")
    
    # Load events
    physical_events = pd.read_csv(physical_events_file, parse_dates=['datetime_min', 'datetime_max'])
    model_events = pd.read_csv(model_events_file, parse_dates=['datetime_min', 'datetime_max'])
    
    # Calculate basic statistics for each method
    physical_stats = {
        'total_events': len(physical_events),
        'unique_sites': physical_events['source'].nunique(),
        'median_duration': physical_events['duration_hours'].median(),
        'mean_duration': physical_events['duration_hours'].mean()
    }
    
    model_stats = {
        'total_events': len(model_events),
        'unique_sites': model_events['source'].nunique(),
        'median_duration': model_events['duration_hours'].median(),
        'mean_duration': model_events['duration_hours'].mean()
    }
    
    # Create a site-day matching table for overlap analysis
    # Process in batches to save memory
    physical_days = set()
    model_days = set()
    
    # Process physical events in batches
    batch_size = 1000
    for i in range(0, len(physical_events), batch_size):
        batch = physical_events.iloc[i:i+batch_size]
        
        for _, event in batch.iterrows():
            site = event['source']
            depth = event['soil_temp_depth']
            start_day = event['datetime_min'].date()
            end_day = event['datetime_max'].date()
            
            # Add each day of the event
            current_day = start_day
            while current_day <= end_day:
                physical_days.add((site, depth, current_day))
                current_day += timedelta(days=1)
        
        # Clear batch to free memory
        del batch
        gc.collect()
    
    # Process model events in batches
    for i in range(0, len(model_events), batch_size):
        batch = model_events.iloc[i:i+batch_size]
        
        for _, event in batch.iterrows():
            site = event['source']
            depth = event['soil_temp_depth']
            start_day = event['datetime_min'].date()
            end_day = event['datetime_max'].date()
            
            # Add each day of the event
            current_day = start_day
            while current_day <= end_day:
                model_days.add((site, depth, current_day))
                current_day += timedelta(days=1)
        
        # Clear batch to free memory
        del batch
        gc.collect()
    
    # Calculate overlap metrics
    overlap_days = physical_days.intersection(model_days)
    
    overlap_metrics = {
        'physical_only_days': len(physical_days - model_days),
        'model_only_days': len(model_days - physical_days),
        'overlap_days': len(overlap_days),
        'jaccard_index': len(overlap_days) / len(physical_days.union(model_days)) if len(physical_days.union(model_days)) > 0 else 0
    }
    
    # Print comparison results
    print("\n=== DETECTION METHOD COMPARISON ===\n")
    
    print("Physics-based Detection:")
    print(f"  Total Events: {physical_stats['total_events']}")
    print(f"  Unique Sites: {physical_stats['unique_sites']}")
    print(f"  Median Duration: {physical_stats['median_duration']:.1f} hours ({physical_stats['median_duration']/24:.1f} days)")
    print(f"  Mean Duration: {physical_stats['mean_duration']:.1f} hours ({physical_stats['mean_duration']/24:.1f} days)")
    
    print("\nDeep Learning Model Detection:")
    print(f"  Total Events: {model_stats['total_events']}")
    print(f"  Unique Sites: {model_stats['unique_sites']}")
    print(f"  Median Duration: {model_stats['median_duration']:.1f} hours ({model_stats['median_duration']/24:.1f} days)")
    print(f"  Mean Duration: {model_stats['mean_duration']:.1f} hours ({model_stats['mean_duration']/24:.1f} days)")
    
    print("\nOverlap Analysis:")
    print(f"  Days with Events (Physics-based): {len(physical_days)}")
    print(f"  Days with Events (Deep Learning): {len(model_days)}")
    print(f"  Days detected by both methods: {overlap_metrics['overlap_days']}")
    print(f"  Days detected only by Physics-based: {overlap_metrics['physical_only_days']}")
    print(f"  Days detected only by Deep Learning: {overlap_metrics['model_only_days']}")
    print(f"  Jaccard Index (overlap): {overlap_metrics['jaccard_index']:.4f}")
    
    # Generate comparison visualizations
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        
        # Create a Venn diagram of detection overlap
        try:
            from matplotlib_venn import venn2
            
            plt.figure(figsize=(8, 6))
            venn2(subsets=(len(physical_days - model_days), 
                          len(model_days - physical_days), 
                          len(overlap_days)),
                 set_labels=('Physics-based', 'Deep Learning'))
            plt.title('Overlap between Detection Methods', fontsize=14)
            plt.savefig(os.path.join(output_dir, 'detection_overlap.png'), dpi=200, bbox_inches='tight')
            plt.close()
        except ImportError:
            print("matplotlib_venn not installed. Skipping Venn diagram.")
        
        # Compare duration distributions
        plt.figure(figsize=(10, 6))
        
        sns.histplot(physical_events['duration_hours'], kde=True, alpha=0.5, 
                    label='Physics-based', color='blue', bins=50)
        sns.histplot(model_events['duration_hours'], kde=True, alpha=0.5, 
                    label='Deep Learning', color='red', bins=50)
        
        plt.xlabel('Duration (hours)')
        plt.ylabel('Frequency')
        plt.title('Comparison of Zero Curtain Duration Distributions')
        plt.legend()
        plt.grid(alpha=0.3)
        
        plt.savefig(os.path.join(output_dir, 'duration_comparison.png'), dpi=200, bbox_inches='tight')
        plt.close()
    
    # Clean up to free memory
    del physical_events, model_events, physical_days, model_days, overlap_days
    gc.collect()
    
    print(f"Memory after comparison: {memory_usage():.1f} MB")
    
    comparison_results = {
        'physical_stats': physical_stats,
        'model_stats': model_stats,
        'overlap_metrics': overlap_metrics
    }
    
    return comparison_results

def run_complete_pipeline(feather_path, output_base_dir='results'):
    """
    Run the complete zero curtain analysis pipeline with memory efficiency.
    
    Parameters:
    -----------
    feather_path : str
        Path to the feather file
    output_base_dir : str
        Base directory for outputs
        
    Returns:
    --------
    dict
        Summary of results
    """
    import os
    import time
    import gc
    import pickle
    from tqdm.auto import tqdm
    
    # Create output directories
    os.makedirs(output_base_dir, exist_ok=True)
    checkpoint_dir = os.path.join(output_base_dir, 'checkpoints')
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # Function to save/load checkpoint
    def save_checkpoint(data, name):
        with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'wb') as f:
            pickle.dump(data, f)
    
    def load_checkpoint(name):
        try:
            with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'rb') as f:
                return pickle.load(f)
        except:
            return None
    
    # Initialize results
    results = load_checkpoint('pipeline_results') or {}
    
    # Check for completed stages
    completed_stages = set(results.get('completed_stages', []))
    print(f"Found {len(completed_stages)} completed stages: {completed_stages}")
    
    # Define stages
    stages = ['Zero Curtain Detection', 'Data Preparation', 'Model Training', 
              'Model Application', 'Visualization', 'Comparison']
    
    # Overall progress bar
    with tqdm(total=len(stages), desc="Overall Progress", initial=len(completed_stages)) as pbar:
        # Stage 1: Zero Curtain Detection
        if 'Zero Curtain Detection' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 1/{len(stages)}: {stages[0]}")
            
            #from zero_curtain_pipeline import run_memory_efficient_pipeline
            
            enhanced_events = run_memory_efficient_pipeline(
                feather_path=feather_path,
                output_dir=os.path.join(output_base_dir, 'enhanced'),
                site_batch_size=20,
                checkpoint_interval=5,
                max_gap_hours=6,
                interpolation_method='cubic'
            )
            
            results['enhanced_events_count'] = len(enhanced_events)
            results['enhanced_time'] = time.time() - start_time
            
            # Save checkpoint
            completed_stages.add('Zero Curtain Detection')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            # Free memory
            del enhanced_events
            gc.collect()
            
            print(f"Memory after stage 1: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 2: Data Preparation
        if 'Data Preparation' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 2/{len(stages)}: {stages[1]}")
            
            # Load events
            enhanced_events_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events.csv')
            if os.path.exists(enhanced_events_path):
                import pandas as pd
                enhanced_events = pd.read_csv(enhanced_events_path, parse_dates=['datetime_min', 'datetime_max'])
                
                # Prepare data for model
                X, y, metadata = prepare_data_for_deep_learning_efficiently(
                    feather_path=feather_path,
                    events_df=enhanced_events,
                    sequence_length=24,
                    output_dir=os.path.join(output_base_dir, 'ml_data'),
                    batch_size=20
                )
                
                results['data_prep_time'] = time.time() - start_time
                results['data_shape'] = X.shape
                results['positive_examples'] = int(sum(y))
                results['positive_percentage'] = float(sum(y)/len(y)*100)
                
                # Clean up
                del X, y, metadata, enhanced_events
                gc.collect()
            else:
                print("No enhanced events file found, cannot proceed with data preparation")
                results['data_prep_error'] = "No enhanced events file found"
            
            # Save checkpoint
            completed_stages.add('Data Preparation')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            print(f"Memory after stage 2: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 3: Model Training
        if 'Model Training' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 3/{len(stages)}: {stages[2]}")
            
            ml_data_dir = os.path.join(output_base_dir, 'ml_data')
            x_path = os.path.join(ml_data_dir, 'X_features.npy')
            y_path = os.path.join(ml_data_dir, 'y_labels.npy')
            
            if os.path.exists(x_path) and os.path.exists(y_path):
                import numpy as np
                X = np.load(x_path)
                y = np.load(y_path)
                
                # Train model
                model, history, evaluation = train_zero_curtain_model_efficiently(
                    X=X,
                    y=y,
                    output_dir=os.path.join(output_base_dir, 'model')
                )
                
                results['model_training_time'] = time.time() - start_time
                results['model_evaluation'] = evaluation
                
                # Clean up
                del X, y, model, history, evaluation
                gc.collect()
            else:
                print("No prepared data found, cannot proceed with model training")
                results['model_training_error'] = "No prepared data found"
            
            # Save checkpoint
            completed_stages.add('Model Training')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            print(f"Memory after stage 3: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 4: Model Application
        if 'Model Application' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 4/{len(stages)}: {stages[3]}")
            
            model_path = os.path.join(output_base_dir, 'model', 'checkpoint.h5')
            
            if os.path.exists(model_path):
                import tensorflow as tf
                model = tf.keras.models.load_model(model_path)
                
                # Apply model
                predictions = apply_model_to_new_data_efficiently(
                    model=model,
                    feather_path=feather_path,
                    sequence_length=24,
                    output_dir=os.path.join(output_base_dir, 'predictions'),
                    batch_size=20
                )
                
                results['model_application_time'] = time.time() - start_time
                results['predictions_count'] = len(predictions)
                
                # Clean up
                del model, predictions
                gc.collect()
            else:
                print("No model checkpoint found, cannot proceed with model application")
                results['model_application_error'] = "No model checkpoint found"
            
            # Save checkpoint
            completed_stages.add('Model Application')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            print(f"Memory after stage 4: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 5: Visualization
        if 'Visualization' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 5/{len(stages)}: {stages[4]}")
            
            import pandas as pd
            
            # Visualize enhanced events
            enhanced_events_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events.csv')
            if os.path.exists(enhanced_events_path):
                enhanced_events = pd.read_csv(enhanced_events_path, parse_dates=['datetime_min', 'datetime_max'])
                
                enhanced_stats = visualize_events_efficiently(
                    events_df=enhanced_events,
                    output_file=os.path.join(output_base_dir, 'enhanced_visualization.png')
                )
                
                results['enhanced_visualization_stats'] = enhanced_stats
                
                # Clean up
                del enhanced_events, enhanced_stats
                gc.collect()
            
            # Visualize model predictions
            predictions_path = os.path.join(output_base_dir, 'predictions', 'consolidated_events.csv')
            if os.path.exists(predictions_path):
                model_events = pd.read_csv(predictions_path, parse_dates=['datetime_min', 'datetime_max'])
                
                model_stats = visualize_events_efficiently(
                    events_df=model_events,
                    output_file=os.path.join(output_base_dir, 'model_visualization.png')
                )
                
                results['model_visualization_stats'] = model_stats
                
                # Clean up
                del model_events, model_stats
                gc.collect()
            
            results['visualization_time'] = time.time() - start_time
            
            # Save checkpoint
            completed_stages.add('Visualization')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            print(f"Memory after stage 5: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 6: Comparison
        if 'Comparison' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 6/{len(stages)}: {stages[5]}")
            
            enhanced_events_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events.csv')
            predictions_path = os.path.join(output_base_dir, 'predictions', 'consolidated_events.csv')
            
            if os.path.exists(enhanced_events_path) and os.path.exists(predictions_path):
                comparison_results = compare_detection_methods_efficiently(
                    physical_events_file=enhanced_events_path,
                    model_events_file=predictions_path,
                    output_dir=os.path.join(output_base_dir, 'comparison')
                )
                
                results['comparison'] = comparison_results
            else:
                print("Missing events files, cannot perform comparison")
                results['comparison_error'] = "Missing events files"
            
            results['comparison_time'] = time.time() - start_time
            
            # Save checkpoint
            completed_stages.add('Comparison')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            print(f"Memory after stage 6: {memory_usage():.1f} MB")
            pbar.update(1)
    
    # Generate summary report
    total_time = (
        results.get('enhanced_time', 0) +
        results.get('data_prep_time', 0) +
        results.get('model_training_time', 0) +
        results.get('model_application_time', 0) +
        results.get('visualization_time', 0) +
        results.get('comparison_time', 0)
    )
    
    # Save summary to file
    with open(os.path.join(output_base_dir, 'summary.txt'), 'w') as f:
        f.write("ZERO CURTAIN ANALYSIS SUMMARY\n")
        f.write("=" * 30 + "\n\n")
        
        f.write(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\n\n")
        
        f.write("STAGE TIMINGS:\n")
        f.write(f"  Zero Curtain Detection: {results.get('enhanced_time', 0):.2f} seconds\n")
        f.write(f"  Data Preparation: {results.get('data_prep_time', 0):.2f} seconds\n")
        f.write(f"  Model Training: {results.get('model_training_time', 0):.2f} seconds\n")
        f.write(f"  Model Application: {results.get('model_application_time', 0):.2f} seconds\n")
        f.write(f"  Visualization: {results.get('visualization_time', 0):.2f} seconds\n")
        f.write(f"  Comparison: {results.get('comparison_time', 0):.2f} seconds\n\n")
        
        f.write("RESULTS SUMMARY:\n")
        f.write(f"  Enhanced Detection: {results.get('enhanced_events_count', 0)} events\n")
        f.write(f"  Model Predictions: {results.get('predictions_count', 0)} events\n")
        
        if 'comparison' in results and 'overlap_metrics' in results['comparison']:
            overlap = results['comparison']['overlap_metrics']['jaccard_index']
            f.write(f"  Method Agreement: {overlap*100:.1f}% overlap\n")
    
    print("\n" + "=" * 80)
    print("ZERO CURTAIN ANALYSIS COMPLETE")
    print("=" * 80)
    print(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
    print(f"Results saved to {output_base_dir}")
    
    return results

# def positional_encoding(length, depth):
#     positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
#     depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
    
#     angle_rates = 1 / tf.pow(10000.0, depths)
#     angle_rads = positions * angle_rates
    
#     # Create a positional encoding with the same depth as the input
#     pos_encoding = tf.concat([tf.sin(angle_rads), tf.cos(angle_rads)], axis=-1)
    
#     # Slice or pad to match the target depth
#     if pos_encoding.shape[-1] > depth:
#         pos_encoding = pos_encoding[:, :depth]  # Slice to match
#     elif pos_encoding.shape[-1] < depth:
#         # Pad to match
#         padding = depth - pos_encoding.shape[-1]
#         pos_encoding = tf.pad(pos_encoding, [[0, 0], [0, padding]])
    
#     return pos_encoding

y_pred_prob = np.vstack(all_preds)
y_test = np.concatenate(all_true)
y_pred = (y_pred_prob > 0.5).astype(int).flatten()

def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
    """
    Build the advanced zero curtain detection model.
    """
    # This is your existing model building function
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
    from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D
    from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
    from tensorflow.keras.optimizers import Adam
    
    # Input layer
    inputs = Input(shape=input_shape)
    
    # Reshape for ConvLSTM (add spatial dimension)
    # From (sequence_length, features) to (sequence_length, 1, features)
    x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
    # ConvLSTM layer to capture spatiotemporal patterns
    from tensorflow.keras.layers import ConvLSTM2D
    convlstm = ConvLSTM2D(
        filters=64,
        kernel_size=(3, 1),
        padding='same',
        return_sequences=True,
        activation='tanh',
        recurrent_dropout=0.2
    )(x)
    
    # Reshape back to (sequence_length, features)
    convlstm = Reshape((input_shape[0], 64))(convlstm)
    
    # Add positional encoding for transformer
    def positional_encoding(length, depth):
        """Create positional encoding with correct dimensions"""
        positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
        angle_rates = 1 / tf.pow(10000.0, depths)
        angle_rads = positions * angle_rates
        
        # This creates a tensor of shape (length, depth)
        # Only use sin to ensure output depth matches input depth
        pos_encoding = tf.sin(angle_rads)
        
        # Add batch dimension to match convlstm output format
        # Result shape will be (1, length, depth)
        pos_encoding = tf.expand_dims(pos_encoding, 0)
        
        return pos_encoding
    
    # Add positional encoding
    pos_encoding = positional_encoding(input_shape[0], 64)
    transformer_input = convlstm + pos_encoding
    
    # Transformer encoder block
    def transformer_encoder(x, num_heads=8, key_dim=64, ff_dim=128):
        # Multi-head attention
        attention_output = MultiHeadAttention(
            num_heads=num_heads, key_dim=key_dim
        )(x, x)
        
        # Skip connection 1
        x1 = Add()([attention_output, x])
        x1 = LayerNormalization(epsilon=1e-6)(x1)
        
        # Feed-forward network
        ff_output = Dense(ff_dim, activation='relu')(x1)
        ff_output = Dropout(0.1)(ff_output)
        ff_output = Dense(64)(ff_output)
        
        # Skip connection 2
        x2 = Add()([ff_output, x1])
        return LayerNormalization(epsilon=1e-6)(x2)
    
    # Apply transformer encoder
    transformer_output = transformer_encoder(transformer_input)
    
    # Parallel CNN paths for multi-scale feature extraction
    cnn_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(inputs)
    cnn_1 = BatchNormalization()(cnn_1)
    
    cnn_2 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(inputs)
    cnn_2 = BatchNormalization()(cnn_2)
    
    cnn_3 = Conv1D(filters=32, kernel_size=7, padding='same', activation='relu')(inputs)
    cnn_3 = BatchNormalization()(cnn_3)
    
    # Variational Autoencoder components
    def sampling(args):
        z_mean, z_log_var = args
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
    # Global temporal features
    global_max = GlobalMaxPooling1D()(transformer_output)
    global_avg = GlobalAveragePooling1D()(transformer_output)
    
    # VAE encoding
    z_mean = Dense(32)(Concatenate()([global_max, global_avg]))
    z_log_var = Dense(32)(Concatenate()([global_max, global_avg]))
    z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
    # Combine all features
    merged_features = Concatenate()(
        [
            GlobalMaxPooling1D()(cnn_1),
            GlobalMaxPooling1D()(cnn_2),
            GlobalMaxPooling1D()(cnn_3),
            global_max,
            global_avg,
            z
        ]
    )
    
    # Final classification layers
    x = Dense(128, activation='relu')(merged_features)
    x = Dropout(0.3)(x)
    x = BatchNormalization()(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.2)(x)
    
    # Output layer
    outputs = Dense(1, activation='sigmoid')(x)
    
    # Create model
    model = Model(inputs=inputs, outputs=outputs)
    
    # Add VAE loss
    kl_loss = -0.5 * tf.reduce_mean(
        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
    )
    model.add_loss(0.001 * kl_loss)  # Small weight for VAE loss
    
    # Compile model with appropriate metrics
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=[
            'accuracy',
            tf.keras.metrics.AUC(name='auc'),
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall')
        ]
    )
    
    return model

def efficient_balanced_training(model, X_file, y_file, train_indices, val_indices, test_indices,
                               output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
                               save_frequency=5, class_weight=None):
    """
    More efficient training balancing speed and memory use.
    
    Parameters:
    -----------
    model : tf.keras.Model
        Pre-compiled model
    X_file, y_file : str
        Paths to feature and label files
    train_indices, val_indices, test_indices : array
        Training, validation, and test indices
    output_dir : str
        Directory to save results
    batch_size : int
        Batch size for training
    chunk_size : int
        Number of samples to process at once
    epochs_per_chunk : int
        Epochs to train each chunk
    save_frequency : int
        Save model every N chunks
    class_weight : dict, optional
        Class weights for handling imbalanced data
    """
    import os
    import gc
    import json
    import numpy as np
    import tensorflow as tf
    import matplotlib.pyplot as plt
    from datetime import datetime, timedelta
    import time
    import psutil
    
    # Create output directories
    os.makedirs(output_dir, exist_ok=True)
    predictions_dir = os.path.join(output_dir, "predictions")
    os.makedirs(predictions_dir, exist_ok=True)
    checkpoints_dir = os.path.join(output_dir, "checkpoints")
    os.makedirs(checkpoints_dir, exist_ok=True)
    
    # Process in chunks
    num_chunks = int(np.ceil(len(train_indices) / chunk_size))
    print(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
    
    # Create validation set once (small size)
    val_limit = min(2000, len(val_indices))
    val_indices_subset = val_indices[:val_limit]
    
    # Open data files
    X_mmap = np.load(X_file, mmap_mode='r')
    y_mmap = np.load(y_file, mmap_mode='r')
    
    # Load validation data once
    val_X = np.array([X_mmap[idx] for idx in val_indices_subset])
    val_y = np.array([y_mmap[idx] for idx in val_indices_subset])
    print(f"Loaded {len(val_X)} validation samples")
    
    # Setup callbacks - EarlyStopping and ReduceLROnPlateau
    callbacks = [
        tf.keras.callbacks.EarlyStopping(
            patience=3, 
            restore_best_weights=True,
            monitor='val_loss',
            min_delta=0.01
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            factor=0.5,
            patience=2,
            min_lr=1e-6,
            monitor='val_loss'
        ),
        # Memory cleanup after each epoch
        tf.keras.callbacks.LambdaCallback(
            on_epoch_end=lambda epoch, logs: gc.collect()
        )
    ]
    
    # Track metrics across chunks
    history_log = []
    start_time = time.time()
    
    # Process each chunk
    for chunk_idx in range(num_chunks):
        # Get chunk indices
        start_idx = chunk_idx * chunk_size
        end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
        chunk_indices = train_indices[start_idx:end_idx]
        
        # Report memory
        memory_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
        print(f"\n{'='*50}")
        print(f"Processing chunk {chunk_idx+1}/{num_chunks} with {len(chunk_indices)} samples")
        print(f"Memory before: {memory_before:.1f} MB")
        
        # Force garbage collection before loading new data
        gc.collect()
        
        # Load chunk data
        chunk_X = np.array([X_mmap[idx] for idx in chunk_indices])
        chunk_y = np.array([y_mmap[idx] for idx in chunk_indices])
        
        print(f"Data loaded. Memory: {psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024):.1f} MB")
        
        # Train on chunk
        print(f"Training for {epochs_per_chunk} epochs...")
        history = model.fit(
            chunk_X, chunk_y,
            validation_data=(val_X, val_y),
            epochs=epochs_per_chunk,
            batch_size=batch_size,
            class_weight=class_weight,
            callbacks=callbacks,
            verbose=1
        )
        
        # Store serializable metrics
        chunk_metrics = {}
        for k, v in history.history.items():
            chunk_metrics[k] = [float(val) for val in v]
        history_log.append(chunk_metrics)
        
        # Save model periodically instead of after every chunk
        if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
            model_path = os.path.join(checkpoints_dir, f"model_checkpoint_{chunk_idx+1}.h5")
            model.save(model_path)
            print(f"Model saved to {model_path}")
            
            # Also save history
            try:
                with open(os.path.join(output_dir, "training_history.json"), "w") as f:
                    json.dump(history_log, f)
            except Exception as e:
                print(f"Warning: Could not save history to JSON: {e}")
                # Fallback - save as pickle
                import pickle
                with open(os.path.join(output_dir, "training_history.pkl"), "wb") as f:
                    pickle.dump(history_log, f)
        
        # Generate predictions only for selected chunks to save time
        if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
            chunk_preds = model.predict(chunk_X, batch_size=batch_size)
            np.save(os.path.join(predictions_dir, f"chunk_{chunk_idx+1}_predictions.npy"), chunk_preds)
            np.save(os.path.join(predictions_dir, f"chunk_{chunk_idx+1}_indices.npy"), chunk_indices)
            del chunk_preds
            
        # Explicitly delete everything from memory
        del chunk_X, chunk_y
        
        # Force garbage collection
        gc.collect()
        
        # Report memory after cleanup
        memory_after = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
        print(f"Memory after: {memory_after:.1f} MB (Change: {memory_after - memory_before:.1f} MB)")
        
        # Estimate time
        elapsed = time.time() - start_time
        avg_time_per_chunk = elapsed / (chunk_idx + 1)
        remaining = avg_time_per_chunk * (num_chunks - chunk_idx - 1)
        print(f"Estimated remaining time: {timedelta(seconds=int(remaining))}")
    
    # Save final model
    final_model_path = os.path.join(output_dir, "final_model.h5")
    model.save(final_model_path)
    print(f"Final model saved to {final_model_path}")
    
    # Save all metrics
    try:
        with open(os.path.join(output_dir, "final_training_metrics.json"), "w") as f:
            json.dump(history_log, f)
    except Exception as e:
        print(f"Error saving metrics: {e}")
        # Save as pickle instead
        import pickle
        with open(os.path.join(output_dir, "final_training_metrics.pkl"), "wb") as f:
            pickle.dump(history_log, f)
    
    # Clean up validation data
    del val_X, val_y
    gc.collect()
    
    # Final evaluation on test set
    print("\nPerforming final evaluation on test set...")
    
    # Process test data in batches
    test_batch_size = 5000
    num_test_batches = int(np.ceil(len(test_indices) / test_batch_size))
    
    all_test_predictions = []
    all_test_true = []
    test_metrics = {'loss': 0, 'accuracy': 0, 'samples': 0}
    
    for test_batch_idx in range(num_test_batches):
        start_idx = test_batch_idx * test_batch_size
        end_idx = min((test_batch_idx + 1) * test_batch_size, len(test_indices))
        batch_indices = test_indices[start_idx:end_idx]
        
        # Load batch data
        test_X = np.array([X_mmap[idx] for idx in batch_indices])
        test_y = np.array([y_mmap[idx] for idx in batch_indices])
        
        # Evaluate
        metrics = model.evaluate(test_X, test_y, verbose=1)
        metrics_dict = {name: value for name, value in zip(model.metrics_names, metrics)}
        
        # Weight metrics by batch size
        for key in ['loss', 'accuracy']:
            if key in metrics_dict:
                test_metrics[key] += metrics_dict[key] * len(batch_indices)
        test_metrics['samples'] += len(batch_indices)
        
        # Get predictions
        test_preds = model.predict(test_X, batch_size=batch_size)
        
        # Store
        all_test_predictions.append(test_preds.flatten())
        all_test_true.append(test_y)
        
        # Clean up
        del test_X, test_y, test_preds
        gc.collect()
    
    # Combine results
    all_test_predictions = np.concatenate(all_test_predictions)
    all_test_true = np.concatenate(all_test_true)
    
    # Calculate final test metrics
    test_loss = test_metrics['loss'] / test_metrics['samples']
    test_accuracy = test_metrics['accuracy'] / test_metrics['samples']
    
    # Calculate additional metrics
    from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
    test_preds_binary = (all_test_predictions > 0.5).astype(int)
    report = classification_report(all_test_true, test_preds_binary)
    conf_matrix = confusion_matrix(all_test_true, test_preds_binary)
    fpr, tpr, _ = roc_curve(all_test_true, all_test_predictions)
    roc_auc = auc(fpr, tpr)
    
    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    np.save(os.path.join(output_dir, f'test_predictions_{timestamp}.npy'), all_test_predictions)
    np.save(os.path.join(output_dir, 'test_predictions_latest.npy'), all_test_predictions)
    
    with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w') as f:
        f.write("Classification Report:\n")
        f.write(report)
        f.write("\n\nConfusion Matrix:\n")
        f.write(str(conf_matrix))
        f.write("\n\nTest Metrics:\n")
        f.write(f"loss: {test_loss:.4f}\n")
        f.write(f"accuracy: {test_accuracy:.4f}\n")
        f.write(f"AUC: {roc_auc:.4f}\n")
        
    print(f"Test evaluation complete. Final test AUC: {roc_auc:.4f}")
    
    total_time = time.time() - start_time
    print(f"\nTotal training time: {timedelta(seconds=int(total_time))}")
    
    return model, final_model_path

# #IF TRAINING IS INTERRUPTED....

# # def efficient_balanced_training(model, X_file, y_file, train_indices, val_indices, test_indices,
# #                                output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
# #                                save_frequency=5, class_weight=None, start_chunk=45):
# # def efficient_balanced_training(model, X_file, y_file, train_indices, val_indices, test_indices,
# #                                output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
# #                                save_frequency=5, class_weight=None, start_chunk=90):
# # def efficient_balanced_training(model, X_file, y_file, train_indices, val_indices, test_indices,
# #                                output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
# #                                save_frequency=5, class_weight=None, start_chunk=135):
# # def efficient_balanced_training(model, X_file, y_file, train_indices, val_indices, test_indices,
# #                                output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
# #                                save_frequency=5, class_weight=None, start_chunk=180):
# # def efficient_balanced_training(model, X_file, y_file, train_indices, val_indices, test_indices,
# #                                output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
# #                                save_frequency=5, class_weight=None, start_chunk=225):
# # def efficient_balanced_training(model, X_file, y_file, train_indices, val_indices, test_indices,
# #                                output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
# #                                save_frequency=5, class_weight=None, start_chunk=270):
# def efficient_balanced_training(model, X_file, y_file, train_indices, val_indices, test_indices,
#                                output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
#                                save_frequency=5, class_weight=None, start_chunk=315):
#     """
#     More efficient training balancing speed and memory use.
    
#     Parameters:
#     -----------
#     model : tf.keras.Model
#         Pre-compiled model
#     X_file, y_file : str
#         Paths to feature and label files
#     train_indices, val_indices, test_indices : array
#         Training, validation, and test indices
#     output_dir : str
#         Directory to save results
#     batch_size : int
#         Batch size for training
#     chunk_size : int
#         Number of samples to process at once
#     epochs_per_chunk : int
#         Epochs to train each chunk
#     save_frequency : int
#         Save model every N chunks
#     class_weight : dict, optional
#         Class weights for handling imbalanced data
#     """
#     import os
#     import gc
#     import json
#     import numpy as np
#     import tensorflow as tf
#     import matplotlib.pyplot as plt
#     from datetime import datetime, timedelta
#     import time
#     import psutil
    
#     # Create output directories
#     os.makedirs(output_dir, exist_ok=True)
#     predictions_dir = os.path.join(output_dir, "predictions")
#     os.makedirs(predictions_dir, exist_ok=True)
#     checkpoints_dir = os.path.join(output_dir, "checkpoints")
#     os.makedirs(checkpoints_dir, exist_ok=True)
    
#     # Process in chunks
#     num_chunks = int(np.ceil(len(train_indices) / chunk_size))
#     print(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
    
#     # Create validation set once (small size)
#     val_limit = min(2000, len(val_indices))
#     val_indices_subset = val_indices[:val_limit]
    
#     # Open data files
#     X_mmap = np.load(X_file, mmap_mode='r')
#     y_mmap = np.load(y_file, mmap_mode='r')
    
#     # Load validation data once
#     val_X = np.array([X_mmap[idx] for idx in val_indices_subset])
#     val_y = np.array([y_mmap[idx] for idx in val_indices_subset])
#     print(f"Loaded {len(val_X)} validation samples")
    
#     # Setup callbacks - EarlyStopping and ReduceLROnPlateau
#     callbacks = [
#         tf.keras.callbacks.EarlyStopping(
#             patience=3, 
#             restore_best_weights=True,
#             monitor='val_loss',
#             min_delta=0.01
#         ),
#         tf.keras.callbacks.ReduceLROnPlateau(
#             factor=0.5,
#             patience=2,
#             min_lr=1e-6,
#             monitor='val_loss'
#         ),
#         # Memory cleanup after each epoch
#         tf.keras.callbacks.LambdaCallback(
#             on_epoch_end=lambda epoch, logs: gc.collect()
#         )
#     ]
    
#     # Track metrics across chunks
#     history_log = []
#     start_time = time.time()
    
#     # Process each chunk
#     for chunk_idx in range(start_chunk, num_chunks):
#         # Get chunk indices
#         start_idx = chunk_idx * chunk_size
#         end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
#         chunk_indices = train_indices[start_idx:end_idx]
        
#         # Report memory
#         memory_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
#         print(f"\n{'='*50}")
#         print(f"Processing chunk {chunk_idx+1}/{num_chunks} with {len(chunk_indices)} samples")
#         print(f"Memory before: {memory_before:.1f} MB")
        
#         # Force garbage collection before loading new data
#         gc.collect()
        
#         # Load chunk data
#         chunk_X = np.array([X_mmap[idx] for idx in chunk_indices])
#         chunk_y = np.array([y_mmap[idx] for idx in chunk_indices])
        
#         print(f"Data loaded. Memory: {psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024...
        
#         # Train on chunk
#         print(f"Training for {epochs_per_chunk} epochs...")
#         history = model.fit(
#             chunk_X, chunk_y,
#             validation_data=(val_X, val_y),
#             epochs=epochs_per_chunk,
#             batch_size=batch_size,
#             class_weight=class_weight,
#             callbacks=callbacks,
#             verbose=1
#         )
        
#         # Store serializable metrics
#         chunk_metrics = {}
#         for k, v in history.history.items():
#             chunk_metrics[k] = [float(val) for val in v]
#         history_log.append(chunk_metrics)
        
#         # Save model periodically instead of after every chunk
#         if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
#             model_path = os.path.join(checkpoints_dir, f"model_checkpoint_{chunk_idx+1}.h5")
#             model.save(model_path)
#             print(f"Model saved to {model_path}")
            
#             # Also save history
#             try:
#                 with open(os.path.join(output_dir, "training_history.json"), "w") as f:
#                     json.dump(history_log, f)
#             except Exception as e:
#                 print(f"Warning: Could not save history to JSON: {e}")
#                 # Fallback - save as pickle
#                 import pickle
#                 with open(os.path.join(output_dir, "training_history.pkl"), "wb") as f:
#                     pickle.dump(history_log, f)
        
#         # Generate predictions only for selected chunks to save time
#         if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
#             chunk_preds = model.predict(chunk_X, batch_size=batch_size)
#             np.save(os.path.join(predictions_dir, f"chunk_{chunk_idx+1}_predictions.npy"), chunk_p...
#             np.save(os.path.join(predictions_dir, f"chunk_{chunk_idx+1}_indices.npy"), chunk_indic...
#             del chunk_preds
            
#         # Explicitly delete everything from memory
#         del chunk_X, chunk_y
        
#         # Force garbage collection
#         gc.collect()
        
#         # Report memory after cleanup
#         memory_after = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
#         print(f"Memory after: {memory_after:.1f} MB (Change: {memory_after - memory_before:.1f} MB...
        
#         # Estimate time
#         elapsed = time.time() - start_time
#         avg_time_per_chunk = elapsed / (chunk_idx + 1)
#         remaining = avg_time_per_chunk * (num_chunks - chunk_idx - 1)
#         print(f"Estimated remaining time: {timedelta(seconds=int(remaining))}")
    
#     # Save final model
#     final_model_path = os.path.join(output_dir, "final_model.h5")
#     model.save(final_model_path)
#     print(f"Final model saved to {final_model_path}")
    
#     # Save all metrics
#     try:
#         with open(os.path.join(output_dir, "final_training_metrics.json"), "w") as f:
#             json.dump(history_log, f)
#     except Exception as e:
#         print(f"Error saving metrics: {e}")
#         # Save as pickle instead
#         import pickle
#         with open(os.path.join(output_dir, "final_training_metrics.pkl"), "wb") as f:
#             pickle.dump(history_log, f)
    
#     # Clean up validation data
#     del val_X, val_y
#     gc.collect()
    
#     # Final evaluation on test set
#     print("\nPerforming final evaluation on test set...")
    
#     # Process test data in batches
#     test_batch_size = 5000
#     num_test_batches = int(np.ceil(len(test_indices) / test_batch_size))
    
#     all_test_predictions = []
#     all_test_true = []
#     test_metrics = {'loss': 0, 'accuracy': 0, 'samples': 0}
    
#     for test_batch_idx in range(num_test_batches):
#         start_idx = test_batch_idx * test_batch_size
#         end_idx = min((test_batch_idx + 1) * test_batch_size, len(test_indices))
#         batch_indices = test_indices[start_idx:end_idx]
        
#         # Load batch data
#         test_X = np.array([X_mmap[idx] for idx in batch_indices])
#         test_y = np.array([y_mmap[idx] for idx in batch_indices])
        
#         # Evaluate
#         metrics = model.evaluate(test_X, test_y, verbose=1)
#         metrics_dict = {name: value for name, value in zip(model.metrics_names, metrics)}
        
#         # Weight metrics by batch size
#         for key in ['loss', 'accuracy']:
#             if key in metrics_dict:
#                 test_metrics[key] += metrics_dict[key] * len(batch_indices)
#         test_metrics['samples'] += len(batch_indices)
        
#         # Get predictions
#         test_preds = model.predict(test_X, batch_size=batch_size)
        
#         # Store
#         all_test_predictions.append(test_preds.flatten())
#         all_test_true.append(test_y)
        
#         # Clean up
#         del test_X, test_y, test_preds
#         gc.collect()
    
#     # Combine results
#     all_test_predictions = np.concatenate(all_test_predictions)
#     all_test_true = np.concatenate(all_test_true)
    
#     # Calculate final test metrics
#     test_loss = test_metrics['loss'] / test_metrics['samples']
#     test_accuracy = test_metrics['accuracy'] / test_metrics['samples']
    
#     # Calculate additional metrics
#     from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
#     test_preds_binary = (all_test_predictions > 0.5).astype(int)
#     report = classification_report(all_test_true, test_preds_binary)
#     conf_matrix = confusion_matrix(all_test_true, test_preds_binary)
#     fpr, tpr, _ = roc_curve(all_test_true, all_test_predictions)
#     roc_auc = auc(fpr, tpr)
    
#     # Save results
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     np.save(os.path.join(output_dir, f'test_predictions_{timestamp}.npy'), all_test_predictions)
#     np.save(os.path.join(output_dir, 'test_predictions_latest.npy'), all_test_predictions)
    
#     with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w') as f:
#         f.write("Classification Report:\n")
#         f.write(report)
#         f.write("\n\nConfusion Matrix:\n")
#         f.write(str(conf_matrix))
#         f.write("\n\nTest Metrics:\n")
#         f.write(f"loss: {test_loss:.4f}\n")
#         f.write(f"accuracy: {test_accuracy:.4f}\n")
#         f.write(f"AUC: {roc_auc:.4f}\n")
        
#     print(f"Test evaluation complete. Final test AUC: {roc_auc:.4f}")
    
#     total_time = time.time() - start_time
#     print(f"\nTotal training time: {timedelta(seconds=int(total_time))}")
    
#     return model, final_model_path

def create_model_evaluation_report(output_dir, train_indices, test_indices, X_file, y_file):
    """Create comprehensive model evaluation report"""
    import os
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.metrics import (classification_report, confusion_matrix,
                               roc_curve, auc, precision_recall_curve,
                               average_precision_score)
    import pandas as pd
    
    # Load final model
    model_path = os.path.join(output_dir, "final_model.h5")
    model = tf.keras.models.load_model(model_path)
    
    # Load test data and predictions
    X_mmap = np.load(X_file, mmap_mode='r')
    y_mmap = np.load(y_file, mmap_mode='r')
    
    test_predictions_path = os.path.join(output_dir, 'test_predictions_latest.npy')
    if os.path.exists(test_predictions_path):
        test_predictions = np.load(test_predictions_path)
        print(f"Loaded existing test predictions, shape: {test_predictions.shape}")
    else:
        # Generate test predictions in batches
        print("Generating test predictions...")
        test_predictions = []
        batch_size = 1000
        
        for i in range(0, len(test_indices), batch_size):
            batch_indices = test_indices[i:i+batch_size]
            X_batch = np.array([X_mmap[idx] for idx in batch_indices])
            batch_preds = model.predict(X_batch)
            test_predictions.append(batch_preds.flatten())
            
        test_predictions = np.concatenate(test_predictions)
        np.save(test_predictions_path, test_predictions)
    
    # Get test labels
    test_labels = np.array([y_mmap[idx] for idx in test_indices])
    
    # Convert to binary predictions
    test_pred_binary = (test_predictions > 0.5).astype(int)
    
    # Create evaluation visualizations
    plt.figure(figsize=(20, 15))
    
    # 1. ROC Curve
    plt.subplot(2, 2, 1)
    fpr, tpr, _ = roc_curve(test_labels, test_predictions)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC)')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    
    # 2. Precision-Recall Curve
    plt.subplot(2, 2, 2)
    precision, recall, _ = precision_recall_curve(test_labels, test_predictions)
    avg_precision = average_precision_score(test_labels, test_predictions)
    plt.plot(recall, precision, label=f'PR curve (AP = {avg_precision:.3f})')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend(loc="lower left")
    plt.grid(True, alpha=0.3)
    
    # 3. Confusion Matrix Heatmap
    plt.subplot(2, 2, 3)
    cm = confusion_matrix(test_labels, test_pred_binary)
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.colorbar()
    classes = ['Negative', 'Positive']
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes)
    plt.yticks(tick_marks, classes)
    
    # Add text annotations
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, format(cm[i, j], 'd'),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    
    # 4. Prediction Distribution
    plt.subplot(2, 2, 4)
    plt.hist(test_predictions, bins=50, alpha=0.5, label='All Predictions')
    plt.hist(test_predictions[test_labels==1], bins=50, alpha=0.5, label='Positive Class')
    plt.hist(test_predictions[test_labels==0], bins=50, alpha=0.5, label='Negative Class')
    plt.axvline(x=0.5, color='red', linestyle='--', label='Decision Threshold (0.5)')
    plt.xlabel('Prediction Probability')
    plt.ylabel('Count')
    plt.title('Prediction Probability Distribution')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'model_evaluation.png'), dpi=300)
    plt.show()
    
    # Generate text report
    report = classification_report(test_labels, test_pred_binary)
    
    # Save report to file
    with open(os.path.join(output_dir, 'test_classification_report.txt'), 'w') as f:
        f.write("MODEL EVALUATION REPORT\n")
        f.write("======================\n\n")
        f.write(f"ROC AUC: {roc_auc:.4f}\n")
        f.write(f"Average Precision: {avg_precision:.4f}\n\n")
        f.write("Classification Report:\n")
        f.write(report)
        f.write("\nConfusion Matrix:\n")
        f.write(str(cm))
    
    print("Model evaluation report saved to:", os.path.join(output_dir, 'test_classification_report.txt'))
    print(report)
    
    return {
        'roc_auc': roc_auc,
        'avg_precision': avg_precision,
        'confusion_matrix': cm,
        'predictions': test_predictions,
        'true_labels': test_labels
    }

# #DEBUGGING
# import os,sys
# import cartopy.crs as ccrs
# import cartopy.feature as cfeature
# import cmocean
# import gc
# import glob
# import json
# import logging
# import matplotlib.gridspec as gridspec
# import matplotlib.pyplot as plt
# import numpy as np
# import os
# import pandas as pd
# import pathlib
# import pickle
# import psutil
# import re
# import gc
# import dask
# import dask.dataframe as dd
# import scipy.interpolate as interpolate
# import scipy.stats as stats
# import seaborn as sns
# from pyproj import Proj
# import sklearn.experimental
# import sklearn.impute
# import sklearn.linear_model
# import sklearn.preprocessing
# import tqdm
# import xarray as xr
# import warnings
# warnings.filterwarnings('ignore')

# from osgeo import gdal, osr
# from matplotlib.colors import LinearSegmentedColormap
# from concurrent.futures import ThreadPoolExecutor
# from datetime import datetime, timedelta
# from pathlib import Path
# from scipy.spatial import cKDTree
# from tqdm import tqdm
# from tqdm.notebook import tqdm

# import tensorflow as tf
# try:
#     physical_devices = tf.config.list_physical_devices('GPU')
#     for device in physical_devices:
#         tf.config.experimental.set_memory_growth(device, True)
# except:
#     pass

# print("TensorFlow version:", tf.__version__)

# import keras_tuner as kt
# from keras_tuner.tuners import BayesianOptimization
# #os.environ["TF_FORCE_GPU_ALLOW_GROWTH"] = "true"
# #os.environ["DEVICE_COUNT_GPU"] = "0"
# os.environ["CUDA_VISIBLE_DEVICES"] = "-1" # DISABLE GPU
# import tensorflow as tf
# import json
# import glob
# import keras
# from keras import layers
# from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
# import seaborn as sns
# from sklearn.metrics import confusion_matrix, classification_report

# from packaging import version

# print("TensorFlow version: ", tf.__version__)
# assert version.parse(tf.__version__).release[0] >= 2, \
#     "This notebook requires TensorFlow 2.0 or above."

# print("==========================")

# print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
# print(sys.getrecursionlimit())
# sys.setrecursionlimit(1000000000)
# print(sys.getrecursionlimit())

# import os
# import numpy as np
# import pandas as pd
# import gc
# import pickle
# import psutil
# from datetime import datetime, timedelta
# import time
# from scipy.interpolate import interp1d

# def memory_usage():
#     """Monitor memory usage in MB"""
#     process = psutil.Process(os.getpid())
#     mem_info = process.memory_info()
#     return mem_info.rss / 1024**2  # Memory in MB

# def save_checkpoint(data, checkpoint_dir, name):
#     """Save checkpoint data to pickle file"""
#     os.makedirs(checkpoint_dir, exist_ok=True)
#     checkpoint_path = os.path.join(checkpoint_dir, f'{name}.pkl')
#     with open(checkpoint_path, 'wb') as f:
#         pickle.dump(data, f)
#     print(f"Saved checkpoint to {checkpoint_path}")

# def load_checkpoint(checkpoint_dir, name):
#     """Load checkpoint data from pickle file"""
#     checkpoint_path = os.path.join(checkpoint_dir, f'{name}.pkl')
#     try:
#         with open(checkpoint_path, 'rb') as f:
#             data = pickle.load(f)
#         print(f"Loaded checkpoint from {checkpoint_path}")
#         return data
#     except:
#         print(f"No checkpoint found at {checkpoint_path}")
#         return None

# # data_loader.py
# import os
# import pandas as pd
# import numpy as np
# import pyarrow as pa
# import pyarrow.feather as pf
# import pyarrow.compute as pc
# import pyarrow.dataset as ds
# import gc
# from tqdm.auto import tqdm

# def get_time_range(feather_path):
#     """Get min and max datetime from feather file efficiently"""
#     print("Determining dataset time range")
#     try:
#         # Use PyArrow for efficient metadata access
#         table = pf.read_table(feather_path, columns=['datetime'])
#         datetime_col = table['datetime']
#         min_date = pd.Timestamp(pc.min(datetime_col).as_py())
#         max_date = pd.Timestamp(pc.max(datetime_col).as_py())
#         del table, datetime_col
#         gc.collect()
#     except Exception as e:
#         print(f"Error with PyArrow min/max: {str(e)}")
#         print("Falling back to reading sample rows...")
        
#         # Read just first and last rows after sorting
#         # This is more efficient than reading all data
#         try:
#             dataset = ds.dataset(feather_path, format='feather')
            
#             # Get min date from sorted data (first row)
#             table_min = dataset.to_table(
#                 columns=['datetime'],
#                 filter=None,
#                 use_threads=True,
#                 limit=1
#             )
#             min_date = pd.Timestamp(table_min['datetime'][0].as_py())
            
#             # Get max date from reverse sorted data (first row)
#             # Note: PyArrow Dataset API doesn't support reverse sort directly
#             # So we read all datetime values and find max
#             table_all = dataset.to_table(columns=['datetime'])
#             max_date = pd.Timestamp(pc.max(table_all['datetime']).as_py())
            
#             del table_min, table_all, dataset
#             gc.collect()
#         except Exception as inner_e:
#             print(f"Error with optimized approach: {str(inner_e)}")
#             print("Using full scan method (slow)...")
            
#             # Last resort: read all timestamps in chunks
#             min_date = pd.Timestamp.max
#             max_date = pd.Timestamp.min
            
#             # Open file directly to avoid loading everything
#             table = pf.read_table(feather_path, columns=['datetime'])
#             datetime_values = table['datetime'].to_pandas()
            
#             min_date = datetime_values.min()
#             max_date = datetime_values.max()
            
#             del datetime_values, table
#             gc.collect()
    
#     print(f"Data timespan: {min_date} to {max_date}")
#     return min_date, max_date

# def get_unique_site_depths(feather_path):
#     """Get unique site-depth combinations efficiently"""
#     print("Finding unique site-depth combinations")
#     print(f"Memory before processing: {memory_usage():.1f} MB")
    
#     try:
#         # Use PyArrow for efficient operation
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Read only the columns we need
#         table = dataset.to_table(columns=['source', 'soil_temp_depth'])
        
#         # Convert to pandas
#         df = table.to_pandas()
        
#         # Get valid combinations (where temp_depth is not NaN)
#         valid_df = df.dropna(subset=['soil_temp_depth'])
        
#         # Get unique combinations
#         site_depths = valid_df.drop_duplicates(['source', 'soil_temp_depth'])
        
#         # Clean up
#         del table, df, valid_df, dataset
#         gc.collect()
        
#     except Exception as e:
#         print(f"Error with PyArrow approach: {str(e)}")
#         print("Falling back to chunked pandas approach...")
        
#         # Fallback: Process in chunks
#         chunk_size = 1000000
#         unique_combos = set()
        
#         # Read feather in chunks (this is slower but more robust)
#         with pd.read_feather(feather_path, columns=['source', 'soil_temp_depth']) as reader:
#             for chunk in reader:
#                 # Get valid rows and unique combinations
#                 valid_rows = chunk.dropna(subset=['soil_temp_depth'])
#                 chunk_combinations = set(zip(valid_rows['source'], valid_rows['soil_temp_depth']))
#                 unique_combos.update(chunk_combinations)
                
#                 # Clean up
#                 del chunk, valid_rows, chunk_combinations
#                 gc.collect()
        
#         # Convert to DataFrame
#         site_depths = pd.DataFrame(list(unique_combos), columns=['source', 'soil_temp_depth'])
    
#     print(f"Found {len(site_depths)} unique site-depth combinations")
#     print(f"Memory after processing: {memory_usage():.1f} MB")
    
#     return site_depths[['source', 'soil_temp_depth']]

# def load_site_depth_data(feather_path, site, temp_depth):
#     """Load ONLY data for a specific site and depth using PyArrow filtering"""
#     print(f"Loading data for site: {site}, depth: {temp_depth}")
#     print(f"Memory before loading: {memory_usage():.1f} MB")
    
#     try:
#         # Use PyArrow Dataset API for efficient filtering
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Create filter expressions
#         site_filter = ds.field('source') == site
#         depth_filter = ds.field('soil_temp_depth') == float(temp_depth)
#         combined_filter = site_filter & depth_filter
        
#         # Apply filter and read only filtered data
#         table = dataset.to_table(filter=combined_filter)
#         filtered_df = table.to_pandas()
        
#         # Clean up
#         del table, dataset
#         gc.collect()
        
#     except Exception as e:
#         print(f"Error with PyArrow filtering: {str(e)}")
#         print("Falling back to pandas filtering...")
        
#         # Define chunk size based on available memory
#         available_mem = psutil.virtual_memory().available / (1024**2)  # MB
#         chunk_size = min(500000, max(100000, int(available_mem / 10)))
        
#         filtered_chunks = []
        
#         # Read and filter in chunks
#         for chunk in pd.read_feather(feather_path, columns=None, chunksize=chunk_size):
#             # Filter by site and depth
#             chunk_filtered = chunk[(chunk['source'] == site) & 
#                                    (chunk['soil_temp_depth'] == temp_depth)]
            
#             if len(chunk_filtered) > 0:
#                 filtered_chunks.append(chunk_filtered)
            
#             # Clean up
#             del chunk, chunk_filtered
#             gc.collect()
        
#         # Combine filtered chunks
#         if filtered_chunks:
#             filtered_df = pd.concat(filtered_chunks, ignore_index=True)
#             del filtered_chunks
#         else:
#             filtered_df = pd.DataFrame()
        
#         gc.collect()
    
#     # Ensure datetime is in datetime format
#     if 'datetime' in filtered_df.columns and not pd.api.types.is_datetime64_dtype(filtered_df['dat...
#         filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])
    
#     print(f"Loaded {len(filtered_df)} rows for site-depth")
#     print(f"Memory after loading: {memory_usage():.1f} MB")
    
#     return filtered_df

# def prepare_data_for_deep_learning_efficiently(feather_path, events_df, sequence_length=6, 
#                                                output_dir=None, batch_size=500, start_batch=0):
#     """
#     Memory-efficient version of prepare_data_for_deep_learning that processes
#     site-depths in batches and saves intermediate results without accumulating all data in memory.
#     """
#     import numpy as np
#     from tqdm.auto import tqdm
#     import os
#     import gc
#     import pandas as pd
    
#     print("Preparing data for deep learning model...")
#     print(f"Memory before preparation: {memory_usage():.1f} MB")
    
#     # Ensure datetime columns are proper datetime objects
#     if not pd.api.types.is_datetime64_dtype(events_df['datetime_min']):
#         events_df['datetime_min'] = pd.to_datetime(events_df['datetime_min'], format='mixed')
#     if not pd.api.types.is_datetime64_dtype(events_df['datetime_max']):
#         events_df['datetime_max'] = pd.to_datetime(events_df['datetime_max'], format='mixed')
    
#     # Create a mapping of detected events for labeling
#     print("Creating event mapping...")
#     event_map = {}
#     for _, event in events_df.iterrows():
#         site = event['source']
#         depth = event['soil_temp_depth']
#         start = event['datetime_min']
#         end = event['datetime_max']
        
#         if (site, depth) not in event_map:
#             event_map[(site, depth)] = []
        
#         event_map[(site, depth)].append((start, end))
    
#     # Get site-depth combinations for progress tracking
#     print("Finding unique site-depth combinations")
#     print(f"Memory before processing: {memory_usage():.1f} MB")
    
#     # Use a function that efficiently gets unique site-depths without loading all data
#     site_depths = get_unique_site_depths(feather_path)
#     total_combinations = len(site_depths)
    
#     print(f"Found {total_combinations} unique site-depth combinations")
#     print(f"Memory after processing: {memory_usage():.1f} MB")
#     print(f"Preparing sequences from {total_combinations} site-depth combinations...")
    
#     # Check for existing batch files to support resuming
#     if output_dir is not None:
#         os.makedirs(output_dir, exist_ok=True)
#         import glob
#         existing_batches = glob.glob(os.path.join(output_dir, 'X_batch_*_*.npy'))
#         if existing_batches and start_batch == 0:
#             # Extract batch numbers from filenames
#             batch_ends = [int(os.path.basename(f).split('_')[-1].split('.')[0]) for f in existing_...
#             if batch_ends:
#                 last_processed_batch = max(batch_ends)
#                 # Start from the next batch
#                 start_batch = (last_processed_batch // batch_size) * batch_size + batch_size
#                 print(f"Found existing batch files, resuming from batch {start_batch}")
    
#     # Track total counts for reporting
#     total_sequences = 0
#     total_positive = 0
    
#     # Process in batches to manage memory
#     for batch_start in range(start_batch, total_combinations, batch_size):
#         batch_end = min(batch_start + batch_size, total_combinations)
#         print(f"\nProcessing batch {batch_start+1}-{batch_end}/{total_combinations}")
        
#         # Initialize lists for this batch only
#         batch_features = []
#         batch_labels = []
#         batch_metadata = []
        
#         # Process each site-depth in the batch
#         for i in tqdm(range(batch_start, batch_end), desc="Creating sequences"):
#             site = site_depths.iloc[i]['source']
#             temp_depth = site_depths.iloc[i]['soil_temp_depth']
            
#             # Skip if no events exist for this site-depth
#             if (site, temp_depth) not in event_map and len(event_map) > 0:
#                 continue
            
#             try:
#                 # Load only the data for this site-depth
#                 print(f"Loading data for site: {site}, depth: {temp_depth}")
#                 print(f"Memory before loading: {memory_usage():.1f} MB")
                
#                 group = load_site_depth_data(feather_path, site, temp_depth)
                
#                 print(f"Loaded {len(group)} rows for site-depth")
#                 print(f"Memory after loading: {memory_usage():.1f} MB")
                
#                 if len(group) < sequence_length + 1:
#                     continue
                
#                 # Ensure datetime is in datetime format
#                 if not pd.api.types.is_datetime64_dtype(group['datetime']):
#                     group['datetime'] = pd.to_datetime(group['datetime'], format='mixed')
                
#                 # Sort by time
#                 group = group.sort_values('datetime')
                
#                 # Create feature set
#                 feature_cols = ['soil_temp_standardized']
                
#                 # Calculate gradient features
#                 group['temp_gradient'] = group['soil_temp_standardized'].diff()
#                 feature_cols.append('temp_gradient')
                
#                 # Add soil depth as feature
#                 group['depth_normalized'] = temp_depth / 10.0
#                 feature_cols.append('depth_normalized')
                
#                 # Add soil moisture if available
#                 has_moisture = False
#                 if 'soil_moist_standardized' in group.columns and not group['soil_moist_standardiz...
#                     has_moisture = True
#                     feature_cols.append('soil_moist_standardized')
#                     group['moist_gradient'] = group['soil_moist_standardized'].diff()
#                     feature_cols.append('moist_gradient')
                
#                 # Fill missing values
#                 group[feature_cols] = group[feature_cols].fillna(0)
                
#                 # Create sequences with sliding window
#                 for j in range(len(group) - sequence_length):
#                     # Get time window
#                     start_time = group.iloc[j]['datetime']
#                     end_time = group.iloc[j + sequence_length - 1]['datetime']
                    
#                     # Extract sequence data
#                     sequence = group.iloc[j:j+sequence_length][feature_cols].values
                    
#                     # Check if this sequence overlaps with known zero curtain event
#                     is_zero_curtain = 0
#                     if (site, temp_depth) in event_map:
#                         for event_start, event_end in event_map[(site, temp_depth)]:
#                             # Ensure proper datetime comparison
#                             # Check for significant overlap (at least 50% of sequence)
#                             if (min(end_time, event_end) - max(start_time, event_start)).total_sec...
#                                0.5 * (end_time - start_time).total_seconds():
#                                 is_zero_curtain = 1
#                                 break
                    
#                     # Store features and labels
#                     batch_features.append(sequence)
#                     batch_labels.append(is_zero_curtain)
#                     total_positive += is_zero_curtain
#                     total_sequences += 1
                    
#                     # Store metadata
#                     meta = {
#                         'source': site,
#                         'soil_temp_depth': temp_depth,
#                         'start_time': start_time,
#                         'end_time': end_time,
#                         'latitude': group.iloc[j]['latitude'] if 'latitude' in group.columns else ...
#                         'longitude': group.iloc[j]['longitude'] if 'longitude' in group.columns el...
#                         'has_moisture_data': has_moisture
#                     }
#                     batch_metadata.append(meta)
                
#                 # Clean up to free memory
#                 del group
#                 gc.collect()
                
#             except Exception as e:
#                 print(f"Error processing {site}, depth {temp_depth}: {str(e)}")
#                 import traceback
#                 traceback.print_exc()
#                 continue
        
#         # Save batch results if output directory provided and we have data
#         if output_dir is not None and batch_features:
#             os.makedirs(output_dir, exist_ok=True)
            
#             # Save batch as numpy files
#             batch_X = np.array(batch_features)
#             batch_y = np.array(batch_labels)
            
#             np.save(os.path.join(output_dir, f'X_batch_{batch_start}_{batch_end}.npy'), batch_X)
#             np.save(os.path.join(output_dir, f'y_batch_{batch_start}_{batch_end}.npy'), batch_y)
            
#             # Save metadata as pickle
#             import pickle
#             with open(os.path.join(output_dir, f'metadata_batch_{batch_start}_{batch_end}.pkl'), '...
#                 pickle.dump(batch_metadata, f)
            
#             # Save progress marker
#             with open(os.path.join(output_dir, 'progress.txt'), 'w') as f:
#                 f.write(f"Last processed batch: {batch_start}-{batch_end}\n")
#                 f.write(f"Total sequences: {total_sequences}\n")
#                 f.write(f"Positive examples: {total_positive} ({total_positive/total_sequences*100...
        
#         # Clean up batch variables - this is key for memory efficiency
#         del batch_features, batch_labels, batch_metadata
#         if 'batch_X' in locals(): del batch_X
#         if 'batch_y' in locals(): del batch_y
#         gc.collect()
        
#         print(f"Memory after batch: {memory_usage():.1f} MB")
#         #print(f"Progress: {total_sequences} sequences processed, {total_positive} positive exampl...
#         positive_percentage = (total_positive/total_sequences*100) if total_sequences > 0 else 0
#         print(f"Progress: {total_sequences} sequences processed, {total_positive} positive example...

#     print("Data preparation complete!")
#     print(f"Total sequences: {total_sequences}")
#     #print(f"Positive examples: {total_positive} ({total_positive/total_sequences*100:.1f}% if tot...
#     positive_percentage = (total_positive/total_sequences*100) if total_sequences > 0 else 0
#     print(f"Positive examples: {total_positive} ({positive_percentage:.1f}%)")
    
#     if output_dir is not None:
#         print(f"Results saved to {output_dir}")
#         print("To merge batch files into final dataset, use merge_batch_files(output_dir)")
    
#     print(f"Memory after preparation: {memory_usage():.1f} MB")
    
#     # Return info instead of data - prevents memory issues
#     return {
#         'total_sequences': total_sequences,
#         'total_positive': total_positive,
#         'positive_percentage': total_positive/total_sequences*100 if total_sequences > 0 else 0,
#         'output_dir': output_dir
#     }


# def merge_batch_files(output_dir):
#     """
#     Merge all batch files into single X_features.npy and y_labels.npy files.
#     """
#     import numpy as np
#     import os
#     import glob
#     import pickle
#     import gc
    
#     print(f"Memory before merging: {memory_usage():.1f} MB")
    
#     # Find all batch files
#     x_batch_files = sorted(glob.glob(os.path.join(output_dir, 'X_batch_*_*.npy')))
#     y_batch_files = sorted(glob.glob(os.path.join(output_dir, 'y_batch_*_*.npy')))
#     metadata_batch_files = sorted(glob.glob(os.path.join(output_dir, 'metadata_batch_*_*.pkl')))
    
#     print(f"Found {len(x_batch_files)} X batches, {len(y_batch_files)} y batches, and {len(metadat...
    
#     # Get dimensions of first batch to initialize arrays
#     if x_batch_files:
#         first_batch = np.load(x_batch_files[0])
#         shape = first_batch.shape
#         del first_batch
#         gc.collect()
        
#         # Count total sequences
#         total_sequences = 0
#         for batch_file in x_batch_files:
#             batch = np.load(batch_file)
#             total_sequences += batch.shape[0]
#             del batch
#             gc.collect()
        
#         # Pre-allocate arrays
#         X = np.zeros((total_sequences, shape[1], shape[2]), dtype=np.float32)
#         y = np.zeros(total_sequences, dtype=np.int32)
        
#         # Load and copy batches
#         idx = 0
#         for i, (x_file, y_file) in enumerate(zip(x_batch_files, y_batch_files)):
#             print(f"Processing batch {i+1}/{len(x_batch_files)}")
#             print(f"Memory: {memory_usage():.1f} MB")
            
#             batch_x = np.load(x_file)
#             batch_y = np.load(y_file)
            
#             batch_size = batch_x.shape[0]
#             X[idx:idx+batch_size] = batch_x
#             y[idx:idx+batch_size] = batch_y
            
#             idx += batch_size
            
#             # Clean up
#             del batch_x, batch_y
#             gc.collect()
        
#         # Save merged X and y
#         print(f"Saving merged arrays: X.shape={X.shape}, y.shape={y.shape}")
#         np.save(os.path.join(output_dir, 'X_features.npy'), X)
#         np.save(os.path.join(output_dir, 'y_labels.npy'), y)
        
#         # Clean up
#         del X, y
#         gc.collect()
        
#         # Load and concatenate metadata batches
#         all_metadata = []
#         for i, batch_file in enumerate(metadata_batch_files):
#             print(f"Processing metadata batch {i+1}/{len(metadata_batch_files)}")
#             print(f"Memory: {memory_usage():.1f} MB")
            
#             with open(batch_file, 'rb') as f:
#                 batch_metadata = pickle.load(f)
#             all_metadata.extend(batch_metadata)
            
#             # Clean up
#             del batch_metadata
#             gc.collect()
        
#         # Save merged metadata
#         print(f"Saving merged metadata: {len(all_metadata)} entries")
#         with open(os.path.join(output_dir, 'metadata.pkl'), 'wb') as f:
#             pickle.dump(all_metadata, f)
        
#         # Final cleanup
#         del all_metadata
#         gc.collect()
        
#         print(f"Memory after merging: {memory_usage():.1f} MB")
#         return {
#             'total_sequences': total_sequences,
#             'output_dir': output_dir
#         }
#     else:
#         print("No batch files found to merge")
#         return None

# def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
#     """
#     Advanced model architecture combining ConvLSTM, Transformers, and 
#     Variational Autoencoder components to better capture complex zero curtain dynamics.
    
#     Parameters:
#     -----------
#     input_shape : tuple
#         Shape of input data (sequence_length, num_features)
#     include_moisture : bool
#         Whether soil moisture features are included
        
#     Returns:
#     --------
#     tensorflow.keras.Model
#         Compiled model ready for training
#     """
#     import os
#     os.environ["DEVICE_COUNT_GPU"] = "0"
#     import tensorflow as tf
#     from tensorflow.keras.models import Model
#     from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
#     from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPoo...
#     from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
#     from tensorflow.keras.optimizers import Adam
    
#     # Input layer
#     inputs = Input(shape=input_shape)
    
#     # Reshape for ConvLSTM (add spatial dimension)
#     # From (sequence_length, features) to (sequence_length, 1, features)
    
#     #x = Reshape((input_shape[0], 1, input_shape[1]))(inputs)
#     x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
#     # ConvLSTM layer to capture spatiotemporal patterns
#     from tensorflow.keras.layers import ConvLSTM2D
#     convlstm = ConvLSTM2D(
#         filters=64,
#         kernel_size=(3, 1),
#         padding='same',
#         return_sequences=True,
#         activation='tanh',
#         recurrent_dropout=0.2
#     )(x)
    
#     # Reshape back to (sequence_length, features)
#     convlstm = Reshape((input_shape[0], 64))(convlstm)
    
#     # Add positional encoding for transformer
#     def positional_encoding(length, depth):
#         positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
#         depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
#         angle_rates = 1 / tf.pow(10000.0, depths)
#         angle_rads = positions * angle_rates
        
#         pos_encoding = tf.concat(
#             [tf.sin(angle_rads), tf.cos(angle_rads)], axis=-1)
        
#         return pos_encoding
    
#     # Add positional encoding
#     pos_encoding = positional_encoding(input_shape[0], 64)
#     transformer_input = convlstm + pos_encoding
    
#     # Transformer encoder block
#     def transformer_encoder(x, num_heads=8, key_dim=64, ff_dim=128):
#         # Multi-head attention
#         attention_output = MultiHeadAttention(
#             num_heads=num_heads, key_dim=key_dim
#         )(x, x)
        
#         # Skip connection 1
#         x1 = Add()([attention_output, x])
#         x1 = LayerNormalization(epsilon=1e-6)(x1)
        
#         # Feed-forward network
#         ff_output = Dense(ff_dim, activation='relu')(x1)
#         ff_output = Dropout(0.1)(ff_output)
#         ff_output = Dense(64)(ff_output)
        
#         # Skip connection 2
#         x2 = Add()([ff_output, x1])
#         return LayerNormalization(epsilon=1e-6)(x2)
    
#     # Apply transformer encoder
#     transformer_output = transformer_encoder(transformer_input)
    
#     # Parallel CNN paths for multi-scale feature extraction
#     cnn_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(inputs)
#     cnn_1 = BatchNormalization()(cnn_1)
    
#     cnn_2 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(inputs)
#     cnn_2 = BatchNormalization()(cnn_2)
    
#     cnn_3 = Conv1D(filters=32, kernel_size=7, padding='same', activation='relu')(inputs)
#     cnn_3 = BatchNormalization()(cnn_3)
    
#     # Variational Autoencoder components
#     def sampling(args):
#         z_mean, z_log_var = args
#         batch = tf.shape(z_mean)[0]
#         dim = tf.shape(z_mean)[1]
#         epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
#         return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
#     # Global temporal features
#     global_max = GlobalMaxPooling1D()(transformer_output)
#     global_avg = GlobalAveragePooling1D()(transformer_output)
    
#     # VAE encoding
#     z_mean = Dense(32)(Concatenate()([global_max, global_avg]))
#     z_log_var = Dense(32)(Concatenate()([global_max, global_avg]))
#     z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
#     # Combine all features
#     merged_features = Concatenate()(
#         [
#             GlobalMaxPooling1D()(cnn_1),
#             GlobalMaxPooling1D()(cnn_2),
#             GlobalMaxPooling1D()(cnn_3),
#             global_max,
#             global_avg,
#             z
#         ]
#     )
    
#     # Final classification layers
#     x = Dense(128, activation='relu')(merged_features)
#     x = Dropout(0.3)(x)
#     x = BatchNormalization()(x)
#     x = Dense(64, activation='relu')(x)
#     x = Dropout(0.2)(x)
    
#     # Output layer
#     outputs = Dense(1, activation='sigmoid')(x)
    
#     # Create model
#     model = Model(inputs=inputs, outputs=outputs)
    
#     # Add VAE loss
#     kl_loss = -0.5 * tf.reduce_mean(
#         z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
#     )
#     model.add_loss(0.001 * kl_loss)  # Small weight for VAE loss
    
#     # Compile model with appropriate metrics
#     model.compile(
#         optimizer=Adam(learning_rate=0.001),
#         loss='binary_crossentropy',
#         metrics=[
#             'accuracy',
#             tf.keras.metrics.AUC(name='auc'),
#             tf.keras.metrics.Precision(name='precision'),
#             tf.keras.metrics.Recall(name='recall')
#         ]
#     )
    
#     return model

# # Time-based split rather than random split
# def temporal_train_test_split(X, y, metadata, val_ratio=0.2, test_ratio=0.1):
#     """
#     Split data temporally for time series modeling.
    
#     Parameters:
#     -----------
#     X : numpy.ndarray
#         Input features
#     y : numpy.ndarray
#         Output labels
#     metadata : list
#         Metadata containing timestamps for each sequence
#     val_ratio : float
#         Proportion of data for validation
#     test_ratio : float
#         Proportion of data for testing
        
#     Returns:
#     --------
#     tuple
#         (X_train, X_val, X_test, y_train, y_val, y_test)
#     """
#     # Extract timestamps from metadata
#     timestamps = [meta['start_time'] for meta in metadata]
    
#     # Sort indices by timestamp
#     sorted_indices = sorted(range(len(timestamps)), key=lambda i: timestamps[i])
    
#     # Calculate split points
#     n_samples = len(sorted_indices)
#     test_start = int(n_samples * (1 - test_ratio))
#     val_start = int(n_samples * (1 - test_ratio - val_ratio))
    
#     # Split indices into train, validation, and test sets
#     train_indices = sorted_indices[:val_start]
#     val_indices = sorted_indices[val_start:test_start]
#     test_indices = sorted_indices[test_start:]
    
#     # Create the splits
#     X_train = X[train_indices]
#     y_train = y[train_indices]
    
#     X_val = X[val_indices]
#     y_val = y[val_indices]
    
#     X_test = X[test_indices]
#     y_test = y[test_indices]
    
#     return X_train, X_val, X_test, y_train, y_val, y_test

# def train_zero_curtain_model_efficiently(X, y, metadata=None, output_dir=None):
#     """
#     Memory-efficient version of train_zero_curtain_model that implements
#     batch training and model checkpointing with temporal data splitting.
    
#     Parameters:
#     -----------
#     X : numpy.ndarray
#         Input features
#     y : numpy.ndarray
#         Output labels
#     metadata : list, optional
#         Metadata about each sequence (must contain timestamps)
#     output_dir : str, optional
#         Directory to save model and results
        
#     Returns:
#     --------
#     tuple
#         (trained_model, training_history, evaluation_results)
#     """
#     import tensorflow as tf
#     from tensorflow.keras.callbacks import (
#         EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, 
#         CSVLogger, TensorBoard
#     )
#     import matplotlib.pyplot as plt
#     import os
#     import gc
#     import numpy as np
    
#     # Enable memory growth to avoid pre-allocating all GPU memory
#     physical_devices = tf.config.list_physical_devices('GPU')
#     if physical_devices:
#         for device in physical_devices:
#             tf.config.experimental.set_memory_growth(device, True)
#             print(f"Enabled memory growth for {device}")
    
#     print("Training zero curtain model...")
#     print(f"Memory before training: {memory_usage():.1f} MB")
    
#     # Create output directory
#     if output_dir:
#         os.makedirs(output_dir, exist_ok=True)
    
#     # Temporal split for time series data
#     print("Performing temporal split for train/validation/test sets...")
#     if metadata is None:
#         raise ValueError("Metadata with timestamps is required for temporal splitting")
    
#     # Extract timestamps from metadata
#     timestamps = np.array([meta['start_time'] for meta in metadata])
    
#     # Sort indices by timestamp
#     sorted_indices = np.argsort(timestamps)
    
#     # Calculate split points (70% train, 15% validation, 15% test)
#     n_samples = len(sorted_indices)
#     test_ratio = 0.15
#     val_ratio = 0.15
    
#     test_start = int(n_samples * (1 - test_ratio))
#     val_start = int(n_samples * (1 - test_ratio - val_ratio))
    
#     # Split indices into train, validation, and test sets
#     train_indices = sorted_indices[:val_start]
#     val_indices = sorted_indices[val_start:test_start]
#     test_indices = sorted_indices[test_start:]
    
#     print(f"Training on data from {timestamps[train_indices[0]]} to {timestamps[train_indices[-1]]...
#     print(f"Validating on data from {timestamps[val_indices[0]]} to {timestamps[val_indices[-1]]}"...
#     print(f"Testing on data from {timestamps[test_indices[0]]} to {timestamps[test_indices[-1]]}")
    
#     # Create the splits
#     X_train = X[train_indices]
#     y_train = y[train_indices]
    
#     X_val = X[val_indices]
#     y_val = y[val_indices]
    
#     X_test = X[test_indices]
#     y_test = y[test_indices]
    
#     print(f"Split sizes: Train={len(X_train)}, Validation={len(X_val)}, Test={len(X_test)}")
    
#     # Check class balance in each split
#     train_pos = np.sum(y_train)
#     val_pos = np.sum(y_val)
#     test_pos = np.sum(y_test)
    
#     print(f"Positive examples: Train={train_pos} ({train_pos/len(y_train)*100:.1f}%), " +
#           f"Val={val_pos} ({val_pos/len(y_val)*100:.1f}%), " +
#           f"Test={test_pos} ({test_pos/len(y_test)*100:.1f}%)")
    
#     # Clean up to free memory
#     del sorted_indices, timestamps
#     gc.collect()
    
#     # Build model with appropriate input shape
#     print("Building model...")
#     input_shape = (X_train.shape[1], X_train.shape[2])
    
#     model = build_advanced_zero_curtain_model(input_shape)
    
#     # If output directory exists, check for existing model checkpoint
#     model_checkpoint_path = None
#     if output_dir:
#         model_checkpoint_path = os.path.join(output_dir, 'checkpoint.h5')
#         if os.path.exists(model_checkpoint_path):
#             print(f"Loading existing model checkpoint from {model_checkpoint_path}")
#             try:
#                 model = tf.keras.models.load_model(model_checkpoint_path)
#                 print("Checkpoint loaded successfully")
#             except Exception as e:
#                 print(f"Error loading checkpoint: {str(e)}")
    
#     # Set up callbacks with additional memory management
#     callbacks = [
#         # Stop early if validation performance plateaus
#         EarlyStopping(
#             patience=15,  # Increased patience for temporal data
#             restore_best_weights=True, 
#             monitor='val_auc', 
#             mode='max'
#         ),
#         # Reduce learning rate when improvement slows
#         ReduceLROnPlateau(
#             factor=0.5, 
#             patience=7,  # Increased patience for temporal data
#             min_lr=1e-6, 
#             monitor='val_auc', 
#             mode='max'
#         ),
#         # Manual garbage collection after each epoch
#         tf.keras.callbacks.LambdaCallback(
#             on_epoch_end=lambda epoch, logs: gc.collect()
#         )
#     ]
    
#     # Add additional callbacks if output directory provided
#     if output_dir:
#         callbacks.extend([
#             # Save best model
#             ModelCheckpoint(
#                 os.path.join(output_dir, 'checkpoint.h5'),
#                 save_best_only=True,
#                 monitor='val_auc',
#                 mode='max'
#             ),
#             # Log training progress to CSV
#             CSVLogger(
#                 os.path.join(output_dir, 'training_log.csv'),
#                 append=True
#             ),
#             # TensorBoard visualization
#             TensorBoard(
#                 log_dir=os.path.join(output_dir, 'tensorboard_logs'),
#                 histogram_freq=1,
#                 profile_batch=0  # Disable profiling to save memory
#             )
#         ])
    
#     # Calculate class weights to handle imbalance
#     pos_weight = len(y_train) / max(sum(y_train), 1)
#     class_weight = {0: 1, 1: pos_weight}
#     print(f"Using class weight {pos_weight:.2f} for positive class")
    
#     # Train model with memory-efficient settings
#     print("Training model...")
#     batch_size = 32  # Adjust based on available memory
#     epochs = 100
    
#     # Use fit with appropriate memory settings
#     history = model.fit(
#         X_train, y_train,
#         validation_data=(X_val, y_val),
#         epochs=epochs,
#         batch_size=batch_size,
#         callbacks=callbacks,
#         class_weight=class_weight,
#         verbose=1,
#         # Memory efficiency settings
#         shuffle=True,  # Still shuffle within the temporal train split
#         use_multiprocessing=False,  # Avoid extra memory overhead
#         workers=1  # Reduce parallel processing to save memory
#     )
    
#     # Clean up to free memory
#     del X_train, y_train, X_val, y_val
#     gc.collect()
    
#     # Evaluate on test set
#     print("Evaluating model on test set...")
#     evaluation = model.evaluate(X_test, y_test, batch_size=batch_size)
#     print("Test performance:")
#     for metric, value in zip(model.metrics_names, evaluation):
#         print(f"  {metric}: {value:.4f}")
    
#     # Generate predictions for visualization and further analysis
#     y_pred_prob = model.predict(X_test, batch_size=batch_size)
#     y_pred = (y_pred_prob > 0.5).astype(int).flatten()
    
#     # Calculate and save additional evaluation metrics
#     from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
#     report = classification_report(y_test, y_pred)
#     conf_matrix = confusion_matrix(y_test, y_pred)
    
#     print("Classification Report:")
#     print(report)
    
#     print("Confusion Matrix:")
#     print(conf_matrix)
    
#     # Plot and save training history
#     if output_dir:
#         # Save evaluation metrics
#         with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w') as f:
#             f.write("Classification Report:\n")
#             f.write(report)
#             f.write("\n\nConfusion Matrix:\n")
#             f.write(str(conf_matrix))
#             f.write("\n\nTest Metrics:\n")
#             for metric, value in zip(model.metrics_names, evaluation):
#                 f.write(f"{metric}: {value:.4f}\n")
        
#         # Plot training history
#         plt.figure(figsize=(16, 6))
        
#         plt.subplot(1, 3, 1)
#         plt.plot(history.history['auc'])
#         plt.plot(history.history['val_auc'])
#         plt.title('Model AUC')
#         plt.ylabel('AUC')
#         plt.xlabel('Epoch')
#         plt.legend(['Train', 'Validation'], loc='lower right')
        
#         plt.subplot(1, 3, 2)
#         plt.plot(history.history['loss'])
#         plt.plot(history.history['val_loss'])
#         plt.title('Model Loss')
#         plt.ylabel('Loss')
#         plt.xlabel('Epoch')
#         plt.legend(['Train', 'Validation'], loc='upper right')
        
#         # Plot ROC curve
#         plt.subplot(1, 3, 3)
#         fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
#         roc_auc = auc(fpr, tpr)
#         plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')
#         plt.plot([0, 1], [0, 1], 'k--')
#         plt.xlabel('False Positive Rate')
#         plt.ylabel('True Positive Rate')
#         plt.title('ROC Curve (Test Set)')
#         plt.legend(loc='lower right')
        
#         plt.tight_layout()
#         plt.savefig(os.path.join(output_dir, 'training_performance.png'), dpi=300)
        
#         # Save detailed model summary
#         from contextlib import redirect_stdout
#         with open(os.path.join(output_dir, 'model_summary.txt'), 'w') as f:
#             with redirect_stdout(f):
#                 model.summary()
    
#     # Clean up to free memory
#     del X_test, y_test, y_pred, y_pred_prob
#     gc.collect()
    
#     print(f"Memory after training: {memory_usage():.1f} MB")
#     return model, history, evaluation

# def run_full_analysis_pipeline(feather_path, output_base_dir='results', batch_size=50):
#     """
#     Run the complete zero curtain analysis pipeline with progress tracking
#     and memory efficiency.
    
#     Parameters:
#     -----------
#     feather_path : str
#         Path to the feather file with merged data
#     output_base_dir : str
#         Base directory for saving outputs
#     batch_size : int
#         Number of site-depths to process per batch
        
#     Returns:
#     --------
#     dict
#         Dictionary containing analysis results
#     """
#     from tqdm.auto import tqdm
#     import time
#     import os
#     import gc
#     import pickle
    
#     # Create output directories
#     os.makedirs(output_base_dir, exist_ok=True)
#     checkpoint_dir = os.path.join(output_base_dir, 'checkpoints')
#     os.makedirs(checkpoint_dir, exist_ok=True)
    
#     # Function to save checkpoint
#     def save_checkpoint(data, name):
#         with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'wb') as f:
#             pickle.dump(data, f)
    
#     # Function to load checkpoint
#     def load_checkpoint(name):
#         try:
#             with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'rb') as f:
#                 return pickle.load(f)
#         except:
#             return None
    
#     # Initialize results
#     results = load_checkpoint('pipeline_results') or {}
    
#     # Check for completed stages
#     completed_stages = set(results.get('completed_stages', []))
#     print(f"Found {len(completed_stages)} completed stages: {completed_stages}")
    
#     # Add a progress indicator for the overall workflow
#     stages = ['Enhanced Detection', 'Data Preparation', 'Model Training', 
#               'Model Application', 'Visualization', 'Comparison']
    
#     with tqdm(total=len(stages), desc="Overall Progress", initial=len(completed_stages)) as pbar:
#         # Stage 1: Enhanced physical detection
#         if 'Enhanced Detection' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 1/{len(stages)}: {stages[0]}")
            
#             print(f"Current memory usage: {memory_usage():.1f} MB")
            
#             # Run memory-efficient detection using your implementation
#             # This part is already implemented in your code
#             #from zero_curtain_pipeline import run_memory_efficient_pipeline
            
#             enhanced_events = run_memory_efficient_pipeline(
#                 feather_path=feather_path,
#                 output_dir=os.path.join(output_base_dir, 'enhanced'),
#                 site_batch_size=batch_size,
#                 checkpoint_interval=5,
#                 max_gap_hours=6,
#                 interpolation_method='cubic'
#             )
            
#             results['enhanced_events'] = enhanced_events
#             results['enhanced_time'] = time.time() - start_time
            
#             # Save progress
#             completed_stages.add('Enhanced Detection')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             # Clean up memory
#             gc.collect()
#             print(f"Memory after Stage 1: {memory_usage():.1f} MB")
#             pbar.update(1)
#         else:
#             # Load enhanced events if needed
#             if 'enhanced_events' not in results:
#                 enhanced_events = load_checkpoint('enhanced_events')
#                 if enhanced_events is None:
#                     # Try loading from CSV
#                     csv_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events.csv'...
#                     if os.path.exists(csv_path):
#                         enhanced_events = pd.read_csv(csv_path, parse_dates=['datetime_min', 'date...
#                     else:
#                         print("Warning: No enhanced events found, cannot proceed with deep learnin...
#                         enhanced_events = pd.DataFrame()
#                 results['enhanced_events'] = enhanced_events
        
#         # Stage 2: Data Preparation for Deep Learning
#         if 'Data Preparation' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 2/{len(stages)}: {stages[1]}")
            
#             # Get enhanced events
#             enhanced_events = results.get('enhanced_events')
#             if enhanced_events is not None and len(enhanced_events) > 0:
#                 try:
#                     # Prepare data for deep learning with memory efficiency
#                     X, y, metadata = prepare_data_for_deep_learning_efficiently(
#                         feather_path=feather_path,
#                         events_df=enhanced_events,
#                         sequence_length=24,  # Use 24 time steps as in your original code
#                         output_dir=os.path.join(output_base_dir, 'ml_data'),
#                         batch_size=batch_size
#                     )
                    
#                     results['X'] = X.shape  # Store only shape to save memory
#                     results['y'] = y.shape
#                     results['data_preparation_time'] = time.time() - start_time
                    
#                     # Clean up to free memory
#                     del X, y
#                     gc.collect()
#                 except Exception as e:
#                     print(f"Error in data preparation: {str(e)}")
#                     results['data_preparation_error'] = str(e)
#             else:
#                 print("Skipping data preparation: No enhanced events available")
            
#             # Save progress
#             completed_stages.add('Data Preparation')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             # Clean up memory
#             gc.collect()
#             print(f"Memory after Stage 2: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 3: Model Training
#         if 'Model Training' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 3/{len(stages)}: {stages[2]}")
            
#             try:
#                 # Load prepared data
#                 data_dir = os.path.join(output_base_dir, 'ml_data')
#                 X = np.load(os.path.join(data_dir, 'X_features.npy'))
#                 y = np.load(os.path.join(data_dir, 'y_labels.npy'))
                
#                 # Load metadata if needed
#                 with open(os.path.join(data_dir, 'metadata.pkl'), 'rb') as f:
#                     metadata = pickle.load(f)
                
#                 # Train model
#                 model, history, evaluation = train_zero_curtain_model_efficiently(
#                     X=X, 
#                     y=y,
#                     metadata=metadata,
#                     output_dir=os.path.join(output_base_dir, 'model')
#                 )
                
#                 # Store minimal results to save memory
#                 results['model_evaluation'] = evaluation
#                 results['model_training_time'] = time.time() - start_time
                
#                 # Clean up to free memory
#                 del X, y, metadata, model, history
#                 gc.collect()
#             except Exception as e:
#                 print(f"Error in model training: {str(e)}")
#                 results['model_training_error'] = str(e)
            
#             # Save progress
#             completed_stages.add('Model Training')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             # Clean up memory
#             gc.collect()
#             print(f"Memory after Stage 3: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 4: Model Application
#         if 'Model Application' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 4/{len(stages)}: {stages[3]}")
            
#             try:
#                 # Load model
#                 model_path = os.path.join(output_base_dir, 'model', 'checkpoint.h5')
#                 if os.path.exists(model_path):
#                     import tensorflow as tf
#                     model = tf.keras.models.load_model(model_path)
                    
#                     # Create directory for predictions
#                     pred_dir = os.path.join(output_base_dir, 'predictions')
#                     os.makedirs(pred_dir, exist_ok=True)
                    
#                     # Apply model with memory efficiency (batched processing)
#                     #from apply_model_efficiently import apply_model_to_new_data_efficiently
                    
#                     predictions = apply_model_to_new_data_efficiently(
#                         model=model,
#                         feather_path=feather_path,
#                         sequence_length=24,
#                         output_dir=pred_dir,
#                         batch_size=batch_size
#                     )
                    
#                     results['model_predictions_count'] = len(predictions)
#                     results['model_application_time'] = time.time() - start_time
                    
#                     # Clean up
#                     del model, predictions
#                     gc.collect()
#                 else:
#                     print("Skipping model application: No model checkpoint found")
#             except Exception as e:
#                 print(f"Error in model application: {str(e)}")
#                 results['model_application_error'] = str(e)
            
#             # Save progress
#             completed_stages.add('Model Application')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             # Clean up memory
#             gc.collect()
#             print(f"Memory after Stage 4: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stages 5 and 6: Visualization and Comparison
#         # (Follow the same pattern - load data, process, clean up memory)
        
#     # Generate final summary report
#     total_time = sum([
#         results.get('enhanced_time', 0),
#         results.get('data_preparation_time', 0),
#         results.get('model_training_time', 0),
#         results.get('model_application_time', 0),
#         results.get('visualization_time', 0),
#         results.get('comparison_time', 0)
#     ])
    
#     print("\n" + "=" * 80)
#     print("ZERO CURTAIN ANALYSIS COMPLETE")
#     print("=" * 80)
#     print(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
    
#     return results

# def apply_model_to_new_data_efficiently(model, feather_path, sequence_length=6, 
#                                         output_dir=None, batch_size=50):
#     """
#     Apply a trained model to detect zero curtain events in new data with memory efficiency.
    
#     Parameters:
#     -----------
#     model : tensorflow.keras.Model
#         Trained zero curtain detection model
#     feather_path : str
#         Path to the feather file
#     sequence_length : int
#         Length of sequences used for model input
#     output_dir : str, optional
#         Output directory for results
#     batch_size : int
#         Number of site-depths to process per batch
        
#     Returns:
#     --------
#     pandas.DataFrame
#         DataFrame with predictions and probabilities
#     """
#     #from data_loader import get_unique_site_depths, load_site_depth_data
#     import numpy as np
#     from tqdm.auto import tqdm
#     import os
#     import gc
#     import pandas as pd
    
#     print("Applying model to new data...")
#     print(f"Memory before application: {memory_usage():.1f} MB")
    
#     # Create output directory
#     if output_dir:
#         os.makedirs(output_dir, exist_ok=True)
    
#     # Get site-depth combinations
#     site_depths = get_unique_site_depths(feather_path)
#     total_combinations = len(site_depths)
#     print(f"Applying model to {total_combinations} site-depth combinations...")
    
#     # Initialize list for all predictions
#     all_predictions = []
    
#     # Process in batches
#     for batch_start in range(0, total_combinations, batch_size):
#         batch_end = min(batch_start + batch_size, total_combinations)
#         print(f"\nProcessing batch {batch_start+1}-{batch_end}/{total_combinations}")
        
#         batch_predictions = []
        
#         # Process each site-depth in batch
#         for i in tqdm(range(batch_start, batch_end), desc="Making predictions"):
#             site = site_depths.iloc[i]['source']
#             temp_depth = site_depths.iloc[i]['soil_temp_depth']
            
#             try:
#                 # Load data for this site-depth
#                 group = load_site_depth_data(feather_path, site, temp_depth)
                
#                 if len(group) < sequence_length + 1:
#                     continue
                
#                 # Sort by time
#                 group = group.sort_values('datetime')
                
#                 # Prepare features (same as in training)
#                 feature_cols = ['soil_temp_standardized']
#                 group['temp_gradient'] = group['soil_temp_standardized'].diff()
#                 feature_cols.append('temp_gradient')
#                 group['depth_normalized'] = temp_depth / 10.0
#                 feature_cols.append('depth_normalized')
                
#                 # Add soil moisture if available
#                 if 'soil_moist_standardized' in group.columns and not group['soil_moist_standardiz...
#                     feature_cols.append('soil_moist_standardized')
#                     group['moist_gradient'] = group['soil_moist_standardized'].diff()
#                     feature_cols.append('moist_gradient')
                
#                 # Fill missing values
#                 group[feature_cols] = group[feature_cols].fillna(0)
                
#                 # Create sequences and predict in mini-batches to save memory
#                 sequences = []
#                 sequence_meta = []
                
#                 for j in range(len(group) - sequence_length):
#                     # Get time window
#                     start_time = group.iloc[j]['datetime']
#                     end_time = group.iloc[j + sequence_length - 1]['datetime']
                    
#                     # Extract sequence
#                     sequence = group.iloc[j:j+sequence_length][feature_cols].values
#                     sequences.append(sequence)
                    
#                     # Store metadata
#                     meta = {
#                         'source': site,
#                         'soil_temp_depth': temp_depth,
#                         'datetime_min': start_time,
#                         'datetime_max': end_time,
#                         'latitude': group.iloc[j]['latitude'] if 'latitude' in group.columns else ...
#                         'longitude': group.iloc[j]['longitude'] if 'longitude' in group.columns el...
#                     }
#                     sequence_meta.append(meta)
                    
#                     # Process in mini-batches of 1000 sequences
#                     if len(sequences) >= 1000:
#                         # Make predictions
#                         X_batch = np.array(sequences)
#                         pred_probs = model.predict(X_batch, verbose=0)
                        
#                         # Store results
#                         for k, prob in enumerate(pred_probs):
#                             meta = sequence_meta[k]
#                             prediction = {
#                                 'source': meta['source'],
#                                 'soil_temp_depth': meta['soil_temp_depth'],
#                                 'datetime_min': meta['datetime_min'],
#                                 'datetime_max': meta['datetime_max'],
#                                 'zero_curtain_probability': float(prob[0]),
#                                 'is_zero_curtain': 1 if prob[0] > 0.5 else 0,
#                                 'latitude': meta['latitude'],
#                                 'longitude': meta['longitude']
#                             }
#                             batch_predictions.append(prediction)
                        
#                         # Clear mini-batch to free memory
#                         sequences = []
#                         sequence_meta = []
                
#                 # Process any remaining sequences
#                 if sequences:
#                     X_batch = np.array(sequences)
#                     pred_probs = model.predict(X_batch, verbose=0)
                    
#                     for k, prob in enumerate(pred_probs):
#                         meta = sequence_meta[k]
#                         prediction = {
#                             'source': meta['source'],
#                             'soil_temp_depth': meta['soil_temp_depth'],
#                             'datetime_min': meta['datetime_min'],
#                             'datetime_max': meta['datetime_max'],
#                             'zero_curtain_probability': float(prob[0]),
#                             'is_zero_curtain': 1 if prob[0] > 0.5 else 0,
#                             'latitude': meta['latitude'],
#                             'longitude': meta['longitude']
#                         }
#                         batch_predictions.append(prediction)
                
#                 # Clean up to free memory
#                 del group, sequences, sequence_meta, X_batch, pred_probs
#                 gc.collect()
                
#             except Exception as e:
#                 print(f"Error processing {site}, depth {temp_depth}: {str(e)}")
#                 continue
        
#         # Add batch predictions to all predictions
#         all_predictions.extend(batch_predictions)
        
#         # Save batch predictions
#         if output_dir and batch_predictions:
#             batch_df = pd.DataFrame(batch_predictions)
#             batch_df.to_csv(os.path.join(output_dir, f'predictions_batch_{batch_start}_{batch_end}...
        
#         # Clear batch to free memory
#         del batch_predictions
#         gc.collect()
        
#         print(f"Memory after batch: {memory_usage():.1f} MB")
    
#     # Consolidate all predictions
#     print(f"Generated {len(all_predictions)} raw predictions")
    
#     # Convert to DataFrame
#     predictions_df = pd.DataFrame(all_predictions)
    
#     # Save all predictions
#     if output_dir and len(predictions_df) > 0:
#         predictions_df.to_csv(os.path.join(output_dir, 'all_predictions.csv'), index=False)
    
#     # Consolidate overlapping events to get final events
#     if len(predictions_df) > 0:
#         print("Consolidating overlapping events...")
#         consolidated_events = consolidate_overlapping_events(predictions_df)
#         print(f"Consolidated into {len(consolidated_events)} events")
        
#         # Save consolidated events
#         if output_dir:
#             consolidated_events.to_csv(os.path.join(output_dir, 'consolidated_events.csv'), index=...
        
#         print(f"Memory after application: {memory_usage():.1f} MB")
#         return consolidated_events
#     else:
#         print("No predictions generated")
#         print(f"Memory after application: {memory_usage():.1f} MB")
#         return pd.DataFrame()

# def consolidate_overlapping_events(predictions_df, probability_threshold=0.5, gap_threshold=6):
#     """
#     Consolidate overlapping zero curtain events from model predictions.
    
#     Parameters:
#     -----------
#     predictions_df : pandas.DataFrame
#         DataFrame with model predictions
#     probability_threshold : float
#         Minimum probability to consider as zero curtain
#     gap_threshold : float
#         Maximum gap in hours to consider events as continuous
        
#     Returns:
#     --------
#     pandas.DataFrame
#         DataFrame with consolidated zero curtain events
#     """
#     import pandas as pd
#     import numpy as np
#     from tqdm.auto import tqdm
    
#     print(f"Consolidating {len(predictions_df)} predictions...")
    
#     # Filter to likely zero curtain events
#     zero_curtain_events = predictions_df[predictions_df['zero_curtain_probability'] >= probability...
    
#     # Ensure datetime columns are datetime type
#     if not pd.api.types.is_datetime64_dtype(zero_curtain_events['datetime_min']):
#         zero_curtain_events['datetime_min'] = pd.to_datetime(zero_curtain_events['datetime_min'])
#     if not pd.api.types.is_datetime64_dtype(zero_curtain_events['datetime_max']):
#         zero_curtain_events['datetime_max'] = pd.to_datetime(zero_curtain_events['datetime_max'])
    
#     # Process each site and depth separately
#     consolidated_events = []
    
#     # Get unique site-depth combinations
#     site_depths = zero_curtain_events[['source', 'soil_temp_depth']].drop_duplicates()
    
#     # Process each site-depth
#     for _, row in tqdm(site_depths.iterrows(), total=len(site_depths), desc="Consolidating events"...
#         site = row['source']
#         depth = row['soil_temp_depth']
        
#         # Get events for this site-depth
#         group = zero_curtain_events[
#             (zero_curtain_events['source'] == site) & 
#             (zero_curtain_events['soil_temp_depth'] == depth)
#         ].sort_values('datetime_min')
        
#         current_event = None
        
#         for _, event in group.iterrows():
#             if current_event is None:
#                 # Start a new event
#                 current_event = {
#                     'source': site,
#                     'soil_temp_depth': depth,
#                     'datetime_min': event['datetime_min'],
#                     'datetime_max': event['datetime_max'],
#                     'zero_curtain_probability': [event['zero_curtain_probability']],
#                     'latitude': event['latitude'],
#                     'longitude': event['longitude']
#                 }
#             else:
#                 # Check if this event overlaps or is close to the current event
#                 time_gap = (event['datetime_min'] - current_event['datetime_max']).total_seconds()...
                
#                 if time_gap <= gap_threshold:
#                     # Extend the current event
#                     current_event['datetime_max'] = max(current_event['datetime_max'], event['date...
#                     current_event['zero_curtain_probability'].append(event['zero_curtain_probabili...
#                 else:
#                     # Finalize the current event
#                     duration_hours = (current_event['datetime_max'] - current_event['datetime_min'...
                    
#                     if duration_hours >= 12:  # Minimum duration threshold
#                         final_event = {
#                             'source': current_event['source'],
#                             'soil_temp_depth': current_event['soil_temp_depth'],
#                             'datetime_min': current_event['datetime_min'],
#                             'datetime_max': current_event['datetime_max'],
#                             'duration_hours': duration_hours,
#                             'zero_curtain_probability': np.mean(current_event['zero_curtain_probab...
#                             'latitude': current_event['latitude'],
#                             'longitude': current_event['longitude']
#                         }
#                         consolidated_events.append(final_event)
                    
#                     # Start a new event
#                     current_event = {
#                         'source': site,
#                         'soil_temp_depth': depth,
#                         'datetime_min': event['datetime_min'],
#                         'datetime_max': event['datetime_max'],
#                         'zero_curtain_probability': [event['zero_curtain_probability']],
#                         'latitude': event['latitude'],
#                         'longitude': event['longitude']
#                     }
        
#         # Handle the last event
#         if current_event is not None:
#             duration_hours = (current_event['datetime_max'] - current_event['datetime_min']).total...
            
#             if duration_hours >= 12:  # Minimum duration threshold
#                 final_event = {
#                     'source': current_event['source'],
#                     'soil_temp_depth': current_event['soil_temp_depth'],
#                     'datetime_min': current_event['datetime_min'],
#                     'datetime_max': current_event['datetime_max'],
#                     'duration_hours': duration_hours,
#                     'zero_curtain_probability': np.mean(current_event['zero_curtain_probability'])...
#                     'latitude': current_event['latitude'],
#                     'longitude': current_event['longitude']
#                 }
#                 consolidated_events.append(final_event)
    
#     # Convert to DataFrame
#     consolidated_df = pd.DataFrame(consolidated_events)
    
#     # Add region and latitude band classifications if latitude is available
#     if len(consolidated_df) > 0 and 'latitude' in consolidated_df.columns:
#         # Add region classification
#         def assign_region(lat):
#             if lat is None or pd.isna(lat):
#                 return None
#             elif lat >= 66.5:
#                 return 'Arctic'
#             elif lat >= 60:
#                 return 'Subarctic'
#             elif lat >= 50:
#                 return 'Northern Boreal'
#             else:
#                 return 'Other'
        
#         consolidated_df['region'] = consolidated_df['latitude'].apply(assign_region)
        
#         # Add latitude band
#         def assign_lat_band(lat):
#             if lat is None or pd.isna(lat):
#                 return None
#             elif lat < 55:
#                 return '<55°N'
#             elif lat < 60:
#                 return '55-60°N'
#             elif lat < 66.5:
#                 return '60-66.5°N'
#             elif lat < 70:
#                 return '66.5-70°N'
#             elif lat < 75:
#                 return '70-75°N'
#             elif lat < 80:
#                 return '75-80°N'
#             else:
#                 return '>80°N'
        
#         consolidated_df['lat_band'] = consolidated_df['latitude'].apply(assign_lat_band)
    
#     return consolidated_df

# def visualize_events_efficiently(events_df, output_file=None):
#     """
#     Create visualizations for zero curtain events with memory efficiency.
    
#     Parameters:
#     -----------
#     events_df : pandas.DataFrame
#         DataFrame containing zero curtain events
#     output_file : str, optional
#         Path to save the visualization
        
#     Returns:
#     --------
#     dict
#         Statistics about the visualized events
#     """
#     import matplotlib.pyplot as plt
#     import cartopy.crs as ccrs
#     import cartopy.feature as cfeature
#     import numpy as np
#     from matplotlib.colors import PowerNorm
#     import gc
    
#     print(f"Creating visualization for {len(events_df)} events...")
#     print(f"Memory before visualization: {memory_usage():.1f} MB")
    
#     # Calculate percentile boundaries for better scaling
#     p10 = np.percentile(events_df['duration_hours'], 10)
#     p25 = np.percentile(events_df['duration_hours'], 25)
#     p50 = np.percentile(events_df['duration_hours'], 50)  # median
#     p75 = np.percentile(events_df['duration_hours'], 75)
#     p90 = np.percentile(events_df['duration_hours'], 90)
    
#     # Aggregate by site to reduce memory usage and plotting overhead
#     site_data = events_df.groupby(['source', 'latitude', 'longitude']).agg({
#         'duration_hours': ['count', 'mean', 'median', 'min', 'max']
#     }).reset_index()
    
#     # Flatten column names
#     site_data.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col 
#                         for col in site_data.columns]
    
#     # Create figure
#     fig, axes = plt.subplots(1, 2, figsize=(14, 7), 
#                            subplot_kw={'projection': ccrs.NorthPolarStereo()})
    
#     # Set map features
#     for ax in axes:
#         ax.set_extent([-180, 180, 45, 90], ccrs.PlateCarree())
#         ax.add_feature(cfeature.LAND, facecolor='whitesmoke')
#         ax.add_feature(cfeature.OCEAN, facecolor='aliceblue')
#         ax.add_feature(cfeature.COASTLINE, linewidth=0.5)
        
#         # Add Arctic Circle with label
#         ax.plot(
#             np.linspace(-180, 180, 60),
#             np.ones(60) * 66.5,
#             transform=ccrs.PlateCarree(),
#             linestyle='-',
#             color='gray',
#             linewidth=1.0,
#             alpha=0.7
#         )
        
#         ax.text(
#             0, 66.5 + 2,
#             "Arctic Circle",
#             transform=ccrs.PlateCarree(),
#             horizontalalignment='center',
#             verticalalignment='bottom',
#             fontsize=9,
#             bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.2')
#         )
    
#     # Plot 1: Event count
#     count_max = site_data['duration_hours_count'].quantile(0.95)
#     scatter1 = axes[0].scatter(
#         site_data['longitude'],
#         site_data['latitude'],
#         transform=ccrs.PlateCarree(),
#         c=site_data['duration_hours_count'],
#         s=30,
#         cmap='viridis',
#         vmin=1,
#         vmax=count_max,
#         alpha=0.8,
#         edgecolor='none'
#     )
#     plt.colorbar(scatter1, ax=axes[0], shrink=0.7, pad=0.05, label='Event Count')
#     axes[0].set_title('Zero Curtain Event Count', fontsize=12)
    
#     # Plot 2: Mean duration using percentile bounds
#     lower_bound = p10
#     upper_bound = p90
    
#     # Non-linear scaling for better color differentiation
#     scatter2 = axes[1].scatter(
#         site_data['longitude'],
#         site_data['latitude'],
#         transform=ccrs.PlateCarree(),
#         c=site_data['duration_hours_mean'],
#         s=30,
#         cmap='RdYlBu_r',
#         norm=PowerNorm(gamma=0.7, vmin=lower_bound, vmax=upper_bound),
#         alpha=0.8,
#         edgecolor='none'
#     )
    
#     # Create better colorbar with percentile markers
#     cbar = plt.colorbar(scatter2, ax=axes[1], shrink=0.7, pad=0.05, 
#                        label='Mean Duration (hours)')
    
#     # Show percentile ticks
#     percentile_ticks = [p10, p25, p50, p75, p90]
#     cbar.set_ticks(percentile_ticks)
#     cbar.set_ticklabels([f"{h:.0f}h\n({h/24:.1f}d)" for h in percentile_ticks])
    
#     axes[1].set_title('Mean Zero Curtain Duration', fontsize=12)
    
#     # Add comprehensive title with statistics
#     plt.suptitle(
#         f'Zero Curtain Analysis: {len(site_data)} Sites, {len(events_df)} Events\n' +
#         f'Duration: median={p50:.1f}h ({p50/24:.1f}d), 10-90%={p10:.1f}-{p90:.1f}h',
#         fontsize=14
#     )
    
#     plt.tight_layout(rect=[0, 0, 1, 0.93])
    
#     # Save if requested
#     if output_file:
#         plt.savefig(output_file, dpi=200, bbox_inches='tight')
#         print(f"Visualization saved to {output_file}")
    
#     # Clean up to free memory
#     plt.close(fig)
#     del site_data, fig, axes
#     gc.collect()
    
#     print(f"Memory after visualization: {memory_usage():.1f} MB")
    
#     # Return statistics
#     return {
#         'p10': p10,
#         'p25': p25,
#         'p50': p50,
#         'p75': p75,
#         'p90': p90,
#         'mean': events_df['duration_hours'].mean(),
#         'std': events_df['duration_hours'].std(),
#         'min': events_df['duration_hours'].min(),
#         'max': events_df['duration_hours'].max()
#     }

# def compare_detection_methods_efficiently(physical_events_file, model_events_file, output_dir=None...
#     """
#     Compare zero curtain events detected by different methods with memory efficiency.
    
#     Parameters:
#     -----------
#     physical_events_file : str
#         Path to CSV file with events detected by the physics-based method
#     model_events_file : str
#         Path to CSV file with events detected by the deep learning model
#     output_dir : str, optional
#         Directory to save comparison results
        
#     Returns:
#     --------
#     dict
#         Comparison statistics and metrics
#     """
#     import pandas as pd
#     import numpy as np
#     import matplotlib.pyplot as plt
#     import seaborn as sns
#     from datetime import timedelta
#     import os
#     import gc
    
#     print("Comparing detection methods...")
#     print(f"Memory before comparison: {memory_usage():.1f} MB")
    
#     # Load events
#     physical_events = pd.read_csv(physical_events_file, parse_dates=['datetime_min', 'datetime_max...
#     model_events = pd.read_csv(model_events_file, parse_dates=['datetime_min', 'datetime_max'])
    
#     # Calculate basic statistics for each method
#     physical_stats = {
#         'total_events': len(physical_events),
#         'unique_sites': physical_events['source'].nunique(),
#         'median_duration': physical_events['duration_hours'].median(),
#         'mean_duration': physical_events['duration_hours'].mean()
#     }
    
#     model_stats = {
#         'total_events': len(model_events),
#         'unique_sites': model_events['source'].nunique(),
#         'median_duration': model_events['duration_hours'].median(),
#         'mean_duration': model_events['duration_hours'].mean()
#     }
    
#     # Create a site-day matching table for overlap analysis
#     # Process in batches to save memory
#     physical_days = set()
#     model_days = set()
    
#     # Process physical events in batches
#     batch_size = 1000
#     for i in range(0, len(physical_events), batch_size):
#         batch = physical_events.iloc[i:i+batch_size]
        
#         for _, event in batch.iterrows():
#             site = event['source']
#             depth = event['soil_temp_depth']
#             start_day = event['datetime_min'].date()
#             end_day = event['datetime_max'].date()
            
#             # Add each day of the event
#             current_day = start_day
#             while current_day <= end_day:
#                 physical_days.add((site, depth, current_day))
#                 current_day += timedelta(days=1)
        
#         # Clear batch to free memory
#         del batch
#         gc.collect()
    
#     # Process model events in batches
#     for i in range(0, len(model_events), batch_size):
#         batch = model_events.iloc[i:i+batch_size]
        
#         for _, event in batch.iterrows():
#             site = event['source']
#             depth = event['soil_temp_depth']
#             start_day = event['datetime_min'].date()
#             end_day = event['datetime_max'].date()
            
#             # Add each day of the event
#             current_day = start_day
#             while current_day <= end_day:
#                 model_days.add((site, depth, current_day))
#                 current_day += timedelta(days=1)
        
#         # Clear batch to free memory
#         del batch
#         gc.collect()
    
#     # Calculate overlap metrics
#     overlap_days = physical_days.intersection(model_days)
    
#     overlap_metrics = {
#         'physical_only_days': len(physical_days - model_days),
#         'model_only_days': len(model_days - physical_days),
#         'overlap_days': len(overlap_days),
#         'jaccard_index': len(overlap_days) / len(physical_days.union(model_days)) if len(physical_...
#     }
    
#     # Print comparison results
#     print("\n=== DETECTION METHOD COMPARISON ===\n")
    
#     print("Physics-based Detection:")
#     print(f"  Total Events: {physical_stats['total_events']}")
#     print(f"  Unique Sites: {physical_stats['unique_sites']}")
#     print(f"  Median Duration: {physical_stats['median_duration']:.1f} hours ({physical_stats['med...
#     print(f"  Mean Duration: {physical_stats['mean_duration']:.1f} hours ({physical_stats['mean_du...
    
#     print("\nDeep Learning Model Detection:")
#     print(f"  Total Events: {model_stats['total_events']}")
#     print(f"  Unique Sites: {model_stats['unique_sites']}")
#     print(f"  Median Duration: {model_stats['median_duration']:.1f} hours ({model_stats['median_du...
#     print(f"  Mean Duration: {model_stats['mean_duration']:.1f} hours ({model_stats['mean_duration...
    
#     print("\nOverlap Analysis:")
#     print(f"  Days with Events (Physics-based): {len(physical_days)}")
#     print(f"  Days with Events (Deep Learning): {len(model_days)}")
#     print(f"  Days detected by both methods: {overlap_metrics['overlap_days']}")
#     print(f"  Days detected only by Physics-based: {overlap_metrics['physical_only_days']}")
#     print(f"  Days detected only by Deep Learning: {overlap_metrics['model_only_days']}")
#     print(f"  Jaccard Index (overlap): {overlap_metrics['jaccard_index']:.4f}")
    
#     # Generate comparison visualizations
#     if output_dir:
#         os.makedirs(output_dir, exist_ok=True)
        
#         # Create a Venn diagram of detection overlap
#         try:
#             from matplotlib_venn import venn2
            
#             plt.figure(figsize=(8, 6))
#             venn2(subsets=(len(physical_days - model_days), 
#                           len(model_days - physical_days), 
#                           len(overlap_days)),
#                  set_labels=('Physics-based', 'Deep Learning'))
#             plt.title('Overlap between Detection Methods', fontsize=14)
#             plt.savefig(os.path.join(output_dir, 'detection_overlap.png'), dpi=200, bbox_inches='t...
#             plt.close()
#         except ImportError:
#             print("matplotlib_venn not installed. Skipping Venn diagram.")
        
#         # Compare duration distributions
#         plt.figure(figsize=(10, 6))
        
#         sns.histplot(physical_events['duration_hours'], kde=True, alpha=0.5, 
#                     label='Physics-based', color='blue', bins=50)
#         sns.histplot(model_events['duration_hours'], kde=True, alpha=0.5, 
#                     label='Deep Learning', color='red', bins=50)
        
#         plt.xlabel('Duration (hours)')
#         plt.ylabel('Frequency')
#         plt.title('Comparison of Zero Curtain Duration Distributions')
#         plt.legend()
#         plt.grid(alpha=0.3)
        
#         plt.savefig(os.path.join(output_dir, 'duration_comparison.png'), dpi=200, bbox_inches='tig...
#         plt.close()
    
#     # Clean up to free memory
#     del physical_events, model_events, physical_days, model_days, overlap_days
#     gc.collect()
    
#     print(f"Memory after comparison: {memory_usage():.1f} MB")
    
#     comparison_results = {
#         'physical_stats': physical_stats,
#         'model_stats': model_stats,
#         'overlap_metrics': overlap_metrics
#     }
    
#     return comparison_results

# def run_complete_pipeline(feather_path, output_base_dir='results'):
#     """
#     Run the complete zero curtain analysis pipeline with memory efficiency.
    
#     Parameters:
#     -----------
#     feather_path : str
#         Path to the feather file
#     output_base_dir : str
#         Base directory for outputs
        
#     Returns:
#     --------
#     dict
#         Summary of results
#     """
#     import os
#     import time
#     import gc
#     import pickle
#     from tqdm.auto import tqdm
    
#     # Create output directories
#     os.makedirs(output_base_dir, exist_ok=True)
#     checkpoint_dir = os.path.join(output_base_dir, 'checkpoints')
#     os.makedirs(checkpoint_dir, exist_ok=True)
    
#     # Function to save/load checkpoint
#     def save_checkpoint(data, name):
#         with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'wb') as f:
#             pickle.dump(data, f)
    
#     def load_checkpoint(name):
#         try:
#             with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'rb') as f:
#                 return pickle.load(f)
#         except:
#             return None
    
#     # Initialize results
#     results = load_checkpoint('pipeline_results') or {}
    
#     # Check for completed stages
#     completed_stages = set(results.get('completed_stages', []))
#     print(f"Found {len(completed_stages)} completed stages: {completed_stages}")
    
#     # Define stages
#     stages = ['Zero Curtain Detection', 'Data Preparation', 'Model Training', 
#               'Model Application', 'Visualization', 'Comparison']
    
#     # Overall progress bar
#     with tqdm(total=len(stages), desc="Overall Progress", initial=len(completed_stages)) as pbar:
#         # Stage 1: Zero Curtain Detection
#         if 'Zero Curtain Detection' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 1/{len(stages)}: {stages[0]}")
            
#             #from zero_curtain_pipeline import run_memory_efficient_pipeline
            
#             enhanced_events = run_memory_efficient_pipeline(
#                 feather_path=feather_path,
#                 output_dir=os.path.join(output_base_dir, 'enhanced'),
#                 site_batch_size=20,
#                 checkpoint_interval=5,
#                 max_gap_hours=6,
#                 interpolation_method='cubic'
#             )
            
#             results['enhanced_events_count'] = len(enhanced_events)
#             results['enhanced_time'] = time.time() - start_time
            
#             # Save checkpoint
#             completed_stages.add('Zero Curtain Detection')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             # Free memory
#             del enhanced_events
#             gc.collect()
            
#             print(f"Memory after stage 1: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 2: Data Preparation
#         if 'Data Preparation' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 2/{len(stages)}: {stages[1]}")
            
#             # Load events
#             enhanced_events_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events....
#             if os.path.exists(enhanced_events_path):
#                 import pandas as pd
#                 enhanced_events = pd.read_csv(enhanced_events_path, parse_dates=['datetime_min', '...
                
#                 # Prepare data for model
#                 X, y, metadata = prepare_data_for_deep_learning_efficiently(
#                     feather_path=feather_path,
#                     events_df=enhanced_events,
#                     sequence_length=24,
#                     output_dir=os.path.join(output_base_dir, 'ml_data'),
#                     batch_size=20
#                 )
                
#                 results['data_prep_time'] = time.time() - start_time
#                 results['data_shape'] = X.shape
#                 results['positive_examples'] = int(sum(y))
#                 results['positive_percentage'] = float(sum(y)/len(y)*100)
                
#                 # Clean up
#                 del X, y, metadata, enhanced_events
#                 gc.collect()
#             else:
#                 print("No enhanced events file found, cannot proceed with data preparation")
#                 results['data_prep_error'] = "No enhanced events file found"
            
#             # Save checkpoint
#             completed_stages.add('Data Preparation')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             print(f"Memory after stage 2: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 3: Model Training
#         if 'Model Training' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 3/{len(stages)}: {stages[2]}")
            
#             ml_data_dir = os.path.join(output_base_dir, 'ml_data')
#             x_path = os.path.join(ml_data_dir, 'X_features.npy')
#             y_path = os.path.join(ml_data_dir, 'y_labels.npy')
            
#             if os.path.exists(x_path) and os.path.exists(y_path):
#                 import numpy as np
#                 X = np.load(x_path)
#                 y = np.load(y_path)
                
#                 # Train model
#                 model, history, evaluation = train_zero_curtain_model_efficiently(
#                     X=X,
#                     y=y,
#                     output_dir=os.path.join(output_base_dir, 'model')
#                 )
                
#                 results['model_training_time'] = time.time() - start_time
#                 results['model_evaluation'] = evaluation
                
#                 # Clean up
#                 del X, y, model, history, evaluation
#                 gc.collect()
#             else:
#                 print("No prepared data found, cannot proceed with model training")
#                 results['model_training_error'] = "No prepared data found"
            
#             # Save checkpoint
#             completed_stages.add('Model Training')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             print(f"Memory after stage 3: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 4: Model Application
#         if 'Model Application' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 4/{len(stages)}: {stages[3]}")
            
#             model_path = os.path.join(output_base_dir, 'model', 'checkpoint.h5')
            
#             if os.path.exists(model_path):
#                 import tensorflow as tf
#                 model = tf.keras.models.load_model(model_path)
                
#                 # Apply model
#                 predictions = apply_model_to_new_data_efficiently(
#                     model=model,
#                     feather_path=feather_path,
#                     sequence_length=24,
#                     output_dir=os.path.join(output_base_dir, 'predictions'),
#                     batch_size=20
#                 )
                
#                 results['model_application_time'] = time.time() - start_time
#                 results['predictions_count'] = len(predictions)
                
#                 # Clean up
#                 del model, predictions
#                 gc.collect()
#             else:
#                 print("No model checkpoint found, cannot proceed with model application")
#                 results['model_application_error'] = "No model checkpoint found"
            
#             # Save checkpoint
#             completed_stages.add('Model Application')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             print(f"Memory after stage 4: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 5: Visualization
#         if 'Visualization' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 5/{len(stages)}: {stages[4]}")
            
#             import pandas as pd
            
#             # Visualize enhanced events
#             enhanced_events_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events....
#             if os.path.exists(enhanced_events_path):
#                 enhanced_events = pd.read_csv(enhanced_events_path, parse_dates=['datetime_min', '...
                
#                 enhanced_stats = visualize_events_efficiently(
#                     events_df=enhanced_events,
#                     output_file=os.path.join(output_base_dir, 'enhanced_visualization.png')
#                 )
                
#                 results['enhanced_visualization_stats'] = enhanced_stats
                
#                 # Clean up
#                 del enhanced_events, enhanced_stats
#                 gc.collect()
            
#             # Visualize model predictions
#             predictions_path = os.path.join(output_base_dir, 'predictions', 'consolidated_events.c...
#             if os.path.exists(predictions_path):
#                 model_events = pd.read_csv(predictions_path, parse_dates=['datetime_min', 'datetim...
                
#                 model_stats = visualize_events_efficiently(
#                     events_df=model_events,
#                     output_file=os.path.join(output_base_dir, 'model_visualization.png')
#                 )
                
#                 results['model_visualization_stats'] = model_stats
                
#                 # Clean up
#                 del model_events, model_stats
#                 gc.collect()
            
#             results['visualization_time'] = time.time() - start_time
            
#             # Save checkpoint
#             completed_stages.add('Visualization')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             print(f"Memory after stage 5: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 6: Comparison
#         if 'Comparison' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 6/{len(stages)}: {stages[5]}")
            
#             enhanced_events_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events....
#             predictions_path = os.path.join(output_base_dir, 'predictions', 'consolidated_events.c...
            
#             if os.path.exists(enhanced_events_path) and os.path.exists(predictions_path):
#                 comparison_results = compare_detection_methods_efficiently(
#                     physical_events_file=enhanced_events_path,
#                     model_events_file=predictions_path,
#                     output_dir=os.path.join(output_base_dir, 'comparison')
#                 )
                
#                 results['comparison'] = comparison_results
#             else:
#                 print("Missing events files, cannot perform comparison")
#                 results['comparison_error'] = "Missing events files"
            
#             results['comparison_time'] = time.time() - start_time
            
#             # Save checkpoint
#             completed_stages.add('Comparison')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             print(f"Memory after stage 6: {memory_usage():.1f} MB")
#             pbar.update(1)
    
#     # Generate summary report
#     total_time = (
#         results.get('enhanced_time', 0) +
#         results.get('data_prep_time', 0) +
#         results.get('model_training_time', 0) +
#         results.get('model_application_time', 0) +
#         results.get('visualization_time', 0) +
#         results.get('comparison_time', 0)
#     )
    
#     # Save summary to file
#     with open(os.path.join(output_base_dir, 'summary.txt'), 'w') as f:
#         f.write("ZERO CURTAIN ANALYSIS SUMMARY\n")
#         f.write("=" * 30 + "\n\n")
        
#         f.write(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\n\n")
        
#         f.write("STAGE TIMINGS:\n")
#         f.write(f"  Zero Curtain Detection: {results.get('enhanced_time', 0):.2f} seconds\n")
#         f.write(f"  Data Preparation: {results.get('data_prep_time', 0):.2f} seconds\n")
#         f.write(f"  Model Training: {results.get('model_training_time', 0):.2f} seconds\n")
#         f.write(f"  Model Application: {results.get('model_application_time', 0):.2f} seconds\n")
#         f.write(f"  Visualization: {results.get('visualization_time', 0):.2f} seconds\n")
#         f.write(f"  Comparison: {results.get('comparison_time', 0):.2f} seconds\n\n")
        
#         f.write("RESULTS SUMMARY:\n")
#         f.write(f"  Enhanced Detection: {results.get('enhanced_events_count', 0)} events\n")
#         f.write(f"  Model Predictions: {results.get('predictions_count', 0)} events\n")
        
#         if 'comparison' in results and 'overlap_metrics' in results['comparison']:
#             overlap = results['comparison']['overlap_metrics']['jaccard_index']
#             f.write(f"  Method Agreement: {overlap*100:.1f}% overlap\n")
    
#     print("\n" + "=" * 80)
#     print("ZERO CURTAIN ANALYSIS COMPLETE")
#     print("=" * 80)
#     print(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
#     print(f"Results saved to {output_base_dir}")
    
#     return results

# # Add at the beginning of your code
# def configure_tensorflow_memory():
#     """Configure TensorFlow to use memory growth and limit GPU memory allocation"""
#     physical_devices = tf.config.list_physical_devices('GPU')
#     if physical_devices:
#         for device in physical_devices:
#             try:
#                 # Allow memory growth - prevents TF from allocating all GPU memory at once
#                 tf.config.experimental.set_memory_growth(device, True)
                
#                 # Optional: Set memory limit (e.g., 4GB)
#                 # tf.config.experimental.set_virtual_device_configuration(
#                 #     device,
#                 #     [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]
#                 # )
#                 print(f"Memory growth enabled for {device}")
#             except Exception as e:
#                 print(f"Error configuring GPU: {e}")
    
#     # Limit CPU threads
#     tf.config.threading.set_intra_op_parallelism_threads(4)
#     tf.config.threading.set_inter_op_parallelism_threads(2)
    
#     # Set soft device placement
#     tf.config.set_soft_device_placement(True)

# # Load split indices
# with open("zero_curtain_pipeline/modeling/checkpoints/spatiotemporal_split.pkl", "rb") as f:
#     split_data = pickle.load(f)
# train_indices = split_data["train_indices"]
# val_indices = split_data["val_indices"]
# test_indices = split_data["test_indices"]

# # Load spatial weights
# with open("zero_curtain_pipeline/modeling/checkpoints/spatial_density.pkl", "rb") as f:
#     weights_data = pickle.load(f)

# data_dir = os.path.join('/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling', 'ml_da...

# # Initialize generators
# X_file = os.path.join(data_dir, 'X_features.npy')
# y_file = os.path.join(data_dir, 'y_labels.npy')

# # Get sample weights for training set
# sample_weights = weights_data["weights"][train_indices]
# sample_weights = sample_weights / np.mean(sample_weights) * len(sample_weights)

# # Load data and metadata; use memory mapping to reduce memory usage
# print("Loading data...")
# X = np.load(X_file, mmap_mode='r')
# y = np.load(y_file, mmap_mode='r')
# print("Loading metadata...")
# with open(os.path.join(data_dir, 'metadata.pkl'), 'rb') as f:
#     metadata = pickle.load(f)
# print("Done.")

# train_y = y[train_indices]
# val_y = y[val_indices]
# test_y = y[test_indices]

# print(f"Train/val/test sizes: {len(train_indices)}/{len(val_indices)}/{len(test_indices)}")
# print(f"Positive examples: Train={np.sum(train_y)} ({np.sum(train_y)/len(train_y)*100:.1f}%), " +
#       f"Val={np.sum(val_y)} ({np.sum(val_y)/len(val_y)*100:.1f}%), " +
#       f"Test={np.sum(test_y)} ({np.sum(test_y)/len(test_y)*100:.1f}%)")

# # Combine sample weights with class weights for imbalanced data
# pos_weight = (len(train_y) - np.sum(train_y)) / max(1, np.sum(train_y))
# class_weight = {0: 1.0, 1: pos_weight}
# print(f"Using class weight {pos_weight:.2f} for positive examples")
# # Free memory
# del train_y, val_y, test_y
# gc.collect()



# def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
#     """
#     Build the advanced zero curtain detection model.
#     """
#     # This is your existing model building function
#     from tensorflow.keras.models import Model
#     from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
#     from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPoo...
#     from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
#     from tensorflow.keras.optimizers import Adam
    
#     # Input layer
#     inputs = Input(shape=input_shape)
    
#     # Reshape for ConvLSTM (add spatial dimension)
#     # From (sequence_length, features) to (sequence_length, 1, features)
#     x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
#     # ConvLSTM layer to capture spatiotemporal patterns
#     from tensorflow.keras.layers import ConvLSTM2D
#     convlstm = ConvLSTM2D(
#         filters=64,
#         kernel_size=(3, 1),
#         padding='same',
#         return_sequences=True,
#         activation='tanh',
#         recurrent_dropout=0.2
#     )(x)
    
#     # Reshape back to (sequence_length, features)
#     convlstm = Reshape((input_shape[0], 64))(convlstm)
    
#     # Add positional encoding for transformer
#     def positional_encoding(length, depth):
#         """Create positional encoding with correct dimensions"""
#         positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
#         depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
#         angle_rates = 1 / tf.pow(10000.0, depths)
#         angle_rads = positions * angle_rates
        
#         # This creates a tensor of shape (length, depth)
#         # Only use sin to ensure output depth matches input depth
#         pos_encoding = tf.sin(angle_rads)
        
#         # Add batch dimension to match convlstm output format
#         # Result shape will be (1, length, depth)
#         pos_encoding = tf.expand_dims(pos_encoding, 0)
        
#         return pos_encoding
    
#     # Add positional encoding
#     pos_encoding = positional_encoding(input_shape[0], 64)
#     transformer_input = convlstm + pos_encoding
    
#     # Transformer encoder block
#     def transformer_encoder(x, num_heads=8, key_dim=64, ff_dim=128):
#         # Multi-head attention
#         attention_output = MultiHeadAttention(
#             num_heads=num_heads, key_dim=key_dim
#         )(x, x)
        
#         # Skip connection 1
#         x1 = Add()([attention_output, x])
#         x1 = LayerNormalization(epsilon=1e-6)(x1)
        
#         # Feed-forward network
#         ff_output = Dense(ff_dim, activation='relu')(x1)
#         ff_output = Dropout(0.1)(ff_output)
#         ff_output = Dense(64)(ff_output)
        
#         # Skip connection 2
#         x2 = Add()([ff_output, x1])
#         return LayerNormalization(epsilon=1e-6)(x2)
    
#     # Apply transformer encoder
#     transformer_output = transformer_encoder(transformer_input)
    
#     # Parallel CNN paths for multi-scale feature extraction
#     cnn_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(inputs)
#     cnn_1 = BatchNormalization()(cnn_1)
    
#     cnn_2 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(inputs)
#     cnn_2 = BatchNormalization()(cnn_2)
    
#     cnn_3 = Conv1D(filters=32, kernel_size=7, padding='same', activation='relu')(inputs)
#     cnn_3 = BatchNormalization()(cnn_3)
    
#     # Variational Autoencoder components
#     def sampling(args):
#         z_mean, z_log_var = args
#         batch = tf.shape(z_mean)[0]
#         dim = tf.shape(z_mean)[1]
#         epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
#         return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
#     # Global temporal features
#     global_max = GlobalMaxPooling1D()(transformer_output)
#     global_avg = GlobalAveragePooling1D()(transformer_output)
    
#     # VAE encoding
#     z_mean = Dense(32)(Concatenate()([global_max, global_avg]))
#     z_log_var = Dense(32)(Concatenate()([global_max, global_avg]))
#     z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
#     # Combine all features
#     merged_features = Concatenate()(
#         [
#             GlobalMaxPooling1D()(cnn_1),
#             GlobalMaxPooling1D()(cnn_2),
#             GlobalMaxPooling1D()(cnn_3),
#             global_max,
#             global_avg,
#             z
#         ]
#     )
    
#     # Final classification layers
#     x = Dense(128, activation='relu')(merged_features)
#     x = Dropout(0.3)(x)
#     x = BatchNormalization()(x)
#     x = Dense(64, activation='relu')(x)
#     x = Dropout(0.2)(x)
    
#     # Output layer
#     outputs = Dense(1, activation='sigmoid')(x)
    
#     # Create model
#     model = Model(inputs=inputs, outputs=outputs)
    
#     # Add VAE loss
#     kl_loss = -0.5 * tf.reduce_mean(
#         z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
#     )
#     model.add_loss(0.001 * kl_loss)  # Small weight for VAE loss
    
#     # Compile model with appropriate metrics
#     model.compile(
#         optimizer=Adam(learning_rate=0.001),
#         loss='binary_crossentropy',
#         metrics=[
#             'accuracy',
#             tf.keras.metrics.AUC(name='auc'),
#             tf.keras.metrics.Precision(name='precision'),
#             tf.keras.metrics.Recall(name='recall')
#         ]
#     )
    
#     return model

# #Now try a more sophisticated architecture from before
# from tensorflow.keras.optimizers import Adam
# input_shape = X[train_indices[0]].shape
# model = build_advanced_zero_curtain_model(input_shape)

# model.summary()

# from tensorflow.keras.utils import plot_model

# plot_model(model, to_file='zero_curtain_pipeline/modeling/spatial_model/insitu_model_plot.png', sh...
#            show_layer_names=True, expand_nested=True, dpi=300, layer_range=None, show_layer_activa...

# output_dir = os.path.join('/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling', 'spa...

# # Set up callbacks
# callbacks = [
#     # Stop training when validation performance plateaus
#     tf.keras.callbacks.EarlyStopping(
#         patience=15,
#         restore_best_weights=True,
#         monitor='val_auc',
#         mode='max'
#     ),
#     # Reduce learning rate when improvement slows
#     tf.keras.callbacks.ReduceLROnPlateau(
#         factor=0.5,
#         patience=7,
#         min_lr=1e-6,
#         monitor='val_auc',
#         mode='max'
#     ),
#     # Manual garbage collection after each epoch
#     tf.keras.callbacks.LambdaCallback(
#         on_epoch_end=lambda epoch, logs: gc.collect()
#     )
# ]

# # Add additional callbacks if output directory provided
# if output_dir:
#     callbacks.extend([
#         # Save best model
#         tf.keras.callbacks.ModelCheckpoint(
#             os.path.join(output_dir, 'model_checkpoint.h5'),
#             save_best_only=True,
#             monitor='val_auc',
#             mode='max'
#         ),
#         # Log training progress to CSV
#         tf.keras.callbacks.CSVLogger(
#             os.path.join(output_dir, 'training_log.csv'),
#             append=True
#         )
#     ])

# def create_optimized_tf_dataset(X_file, y_file, indices, batch_size=256, shuffle=True, 
#                                weights=None, cache=False, prefetch_factor=tf.data.AUTOTUNE):
#     """
#     Create an optimized TensorFlow dataset with detailed progress reporting.
#     """
#     import time
    
#     # Log start time for loading data
#     load_start = time.time()
#     print(f"  Loading memory-mapped arrays...")
    
#     # Load as memory-mapped arrays
#     X = np.load(X_file, mmap_mode='r')
#     y = np.load(y_file, mmap_mode='r')
    
#     print(f"  Arrays loaded in {time.time() - load_start:.2f} seconds")
    
#     # Get input shape from first sample
#     sample_start = time.time()
#     input_shape = X[indices[0]].shape
#     print(f"  Input shape: {input_shape}, obtained in {time.time() - sample_start:.2f} seconds")
    
#     # Define generator function with progress reporting
#     def generator():
#         total = len(indices)
#         start_time = time.time()
#         last_report = start_time
        
#         for i, idx in enumerate(indices):
#             # Report progress every 10000 samples or 10 seconds
#             current_time = time.time()
#             if i % 10000 == 0 or current_time - last_report > 10:
#                 elapsed = current_time - start_time
#                 if i > 0:
#                     rate = i / elapsed
#                     eta = (total - i) / rate if rate > 0 else 0
#                     print(f"  Generator progress: {i}/{total} ({i/total*100:.1f}%), "
#                           f"Rate: {rate:.1f} samples/sec, ETA: {int(eta)} seconds")
#                 last_report = current_time
            
#             if weights is not None:
#                 # Find position of idx in original indices array
#                 pos = np.where(indices == idx)[0][0]
#                 yield X[idx], y[idx], weights[pos]
#             else:
#                 yield X[idx], y[idx]
    
#     # Create dataset from generator
#     create_start = time.time()
#     print(f"  Creating dataset from generator...")
    
#     if weights is not None:
#         output_signature = (
#             tf.TensorSpec(shape=input_shape, dtype=tf.float32),
#             tf.TensorSpec(shape=(), dtype=tf.int32),
#             tf.TensorSpec(shape=(), dtype=tf.float32)
#         )
#     else:
#         output_signature = (
#             tf.TensorSpec(shape=input_shape, dtype=tf.float32),
#             tf.TensorSpec(shape=(), dtype=tf.int32)
#         )
    
#     dataset = tf.data.Dataset.from_generator(
#         generator,
#         output_signature=output_signature
#     )
    
#     print(f"  Dataset created in {time.time() - create_start:.2f} seconds")
    
#     # Apply dataset optimizations
#     print(f"  Applying dataset optimizations...")
#     opt_start = time.time()
    
#     if shuffle:
#         buffer_size = min(len(indices), 10000)
#         print(f"  Shuffling with buffer size {buffer_size}...")
#         dataset = dataset.shuffle(buffer_size)
    
#     print(f"  Batching with size {batch_size}...")
#     dataset = dataset.batch(batch_size)
    
#     if cache:
#         print(f"  Caching dataset...")
#         dataset = dataset.cache()
    
#     print(f"  Setting prefetch to {prefetch_factor}...")
#     dataset = dataset.prefetch(prefetch_factor)
    
#     print(f"  Optimizations applied in {time.time() - opt_start:.2f} seconds")
    
#     return dataset

# def train_in_chunks_with_tf_datasets(model, X_file, y_file, train_indices, val_indices,
#                                      batch_size=256, chunks=20, epochs_per_chunk=5,
#                                      callbacks=None, class_weight=None, sample_weights=None):
#     """
#     Train model sequentially in chunks using TensorFlow datasets with detailed progress logging.
#     """
#     import time
#     from datetime import timedelta
    
#     # Log start time
#     start_time = time.time()
#     print(f"Starting chunked training at {time.strftime('%Y-%m-%d %H:%M:%S')}")
#     print(f"Total samples: {len(train_indices)}, Chunks: {chunks}, Batch size: {batch_size}")
    
#     # Create validation dataset once
#     print(f"Creating validation dataset with {len(val_indices)} samples...")
#     val_dataset_start = time.time()
#     val_dataset = create_optimized_tf_dataset(
#         X_file, y_file, val_indices, 
#         batch_size=batch_size, shuffle=False
#     )
#     print(f"Validation dataset created in {time.time() - val_dataset_start:.2f} seconds")
    
#     # Split training indices into chunks
#     train_chunks = np.array_split(train_indices, chunks)
    
#     history_aggregate = None
    
#     # Train on each chunk sequentially
#     for chunk_idx, chunk_indices in enumerate(train_chunks):
#         chunk_start_time = time.time()
#         print(f"\n{'='*80}")
#         print(f"Training on chunk {chunk_idx+1}/{len(train_chunks)} with {len(chunk_indices)} samp...
#         print(f"Memory usage before creating dataset: {memory_usage():.1f} MB")
        
#         # Create TF dataset for this chunk only
#         print(f"Creating training dataset for chunk {chunk_idx+1}...")
#         dataset_start = time.time()
        
#         if sample_weights is not None:
#             # Get weights for current chunk
#             print(f"Extracting sample weights for {len(chunk_indices)} samples...")
#             weights_start = time.time()
#             chunk_weights = np.array([sample_weights[np.where(train_indices == idx)[0][0]] 
#                                      for idx in chunk_indices])
#             print(f"Weights extracted in {time.time() - weights_start:.2f} seconds")
#         else:
#             chunk_weights = None
            
#         train_dataset = create_optimized_tf_dataset(
#             X_file, y_file, chunk_indices,
#             batch_size=batch_size, shuffle=True,
#             weights=chunk_weights
#         )
#         print(f"Training dataset created in {time.time() - dataset_start:.2f} seconds")
#         print(f"Memory usage after creating dataset: {memory_usage():.1f} MB")
        
#         # Count batches for progress reporting
#         steps_per_epoch = len(chunk_indices) // batch_size + (1 if len(chunk_indices) % batch_size...
#         print(f"Steps per epoch: {steps_per_epoch}")
        
#         # Create a custom callback to log batch progress
#         class BatchProgressCallback(tf.keras.callbacks.Callback):
#             def on_train_batch_end(self, batch, logs=None):
#                 if batch % 10 == 0:  # Log every 10 batches
#                     print(f"  Batch {batch}/{steps_per_epoch}, Loss: {logs['loss']:.4f}")
        
#         # Add our progress callback
#         chunk_callbacks = callbacks.copy() if callbacks else []
#         chunk_callbacks.append(BatchProgressCallback())
        
#         # Train on this chunk
#         print(f"Starting training for {epochs_per_chunk} epochs on chunk {chunk_idx+1}...")
#         train_start = time.time()
        
#         history = model.fit(
#             train_dataset,
#             validation_data=val_dataset,
#             epochs=epochs_per_chunk,
#             callbacks=chunk_callbacks,
#             class_weight=class_weight,
#             verbose=1
#         )
        
#         train_time = time.time() - train_start
#         print(f"Chunk {chunk_idx+1} training completed in {timedelta(seconds=int(train_time))}")
#         print(f"Average time per epoch: {train_time/epochs_per_chunk:.2f} seconds")
        
#         # Aggregate history
#         if history_aggregate is None:
#             history_aggregate = {k: v for k, v in history.history.items()}
#         else:
#             for k, v in history.history.items():
#                 history_aggregate[k].extend(v)
        
#         # Force garbage collection
#         del train_dataset
#         if chunk_weights is not None:
#             del chunk_weights
#         gc.collect()
        
#         # Log memory usage after training
#         print(f"Memory usage after training: {memory_usage():.1f} MB")
#         print(f"Chunk {chunk_idx+1} total time: {timedelta(seconds=int(time.time() - chunk_start_t...
        
#         # Estimate remaining time
#         elapsed_time = time.time() - start_time
#         avg_chunk_time = elapsed_time / (chunk_idx + 1)
#         remaining_chunks = chunks - (chunk_idx + 1)
#         estimated_remaining = avg_chunk_time * remaining_chunks
#         print(f"Estimated remaining time: {timedelta(seconds=int(estimated_remaining))}")
#         print(f"{'='*80}")
    
#     total_time = time.time() - start_time
#     print(f"\nTotal training time: {timedelta(seconds=int(total_time))}")
    
#     # Create a History object with the aggregated metrics
#     agg_history = type('History', (), {'history': history_aggregate})
    
#     return agg_history

# def efficient_balanced_training(model, X_file, y_file, train_indices, val_indices, test_indices,
#                                output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
#                                save_frequency=5, class_weight=None):
#     """
#     More efficient training balancing speed and memory use.
    
#     Parameters:
#     -----------
#     model : tf.keras.Model
#         Pre-compiled model
#     X_file, y_file : str
#         Paths to feature and label files
#     train_indices, val_indices, test_indices : array
#         Training, validation, and test indices
#     output_dir : str
#         Directory to save results
#     batch_size : int
#         Batch size for training
#     chunk_size : int
#         Number of samples to process at once
#     epochs_per_chunk : int
#         Epochs to train each chunk
#     save_frequency : int
#         Save model every N chunks
#     class_weight : dict, optional
#         Class weights for handling imbalanced data
#     """
#     import os
#     import gc
#     import json
#     import numpy as np
#     import tensorflow as tf
#     import matplotlib.pyplot as plt
#     from datetime import datetime, timedelta
#     import time
#     import psutil
    
#     # Create output directories
#     os.makedirs(output_dir, exist_ok=True)
#     predictions_dir = os.path.join(output_dir, "predictions")
#     os.makedirs(predictions_dir, exist_ok=True)
#     checkpoints_dir = os.path.join(output_dir, "checkpoints")
#     os.makedirs(checkpoints_dir, exist_ok=True)
    
#     # Process in chunks
#     num_chunks = int(np.ceil(len(train_indices) / chunk_size))
#     print(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
    
#     # Create validation set once (small size)
#     val_limit = min(2000, len(val_indices))
#     val_indices_subset = val_indices[:val_limit]
    
#     # Open data files
#     X_mmap = np.load(X_file, mmap_mode='r')
#     y_mmap = np.load(y_file, mmap_mode='r')
    
#     # Load validation data once
#     val_X = np.array([X_mmap[idx] for idx in val_indices_subset])
#     val_y = np.array([y_mmap[idx] for idx in val_indices_subset])
#     print(f"Loaded {len(val_X)} validation samples")
    
#     # Setup callbacks - EarlyStopping and ReduceLROnPlateau
#     callbacks = [
#         tf.keras.callbacks.EarlyStopping(
#             patience=3, 
#             restore_best_weights=True,
#             monitor='val_loss',
#             min_delta=0.01
#         ),
#         tf.keras.callbacks.ReduceLROnPlateau(
#             factor=0.5,
#             patience=2,
#             min_lr=1e-6,
#             monitor='val_loss'
#         ),
#         # Memory cleanup after each epoch
#         tf.keras.callbacks.LambdaCallback(
#             on_epoch_end=lambda epoch, logs: gc.collect()
#         )
#     ]
    
#     # Track metrics across chunks
#     history_log = []
#     start_time = time.time()
    
#     # Process each chunk
#     for chunk_idx in range(num_chunks):
#         # Get chunk indices
#         start_idx = chunk_idx * chunk_size
#         end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
#         chunk_indices = train_indices[start_idx:end_idx]
        
#         # Report memory
#         memory_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
#         print(f"\n{'='*50}")
#         print(f"Processing chunk {chunk_idx+1}/{num_chunks} with {len(chunk_indices)} samples")
#         print(f"Memory before: {memory_before:.1f} MB")
        
#         # Force garbage collection before loading new data
#         gc.collect()
        
#         # Load chunk data
#         chunk_X = np.array([X_mmap[idx] for idx in chunk_indices])
#         chunk_y = np.array([y_mmap[idx] for idx in chunk_indices])
        
#         print(f"Data loaded. Memory: {psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024...
        
#         # Train on chunk
#         print(f"Training for {epochs_per_chunk} epochs...")
#         history = model.fit(
#             chunk_X, chunk_y,
#             validation_data=(val_X, val_y),
#             epochs=epochs_per_chunk,
#             batch_size=batch_size,
#             class_weight=class_weight,
#             callbacks=callbacks,
#             verbose=1
#         )
        
#         # Store serializable metrics
#         chunk_metrics = {}
#         for k, v in history.history.items():
#             chunk_metrics[k] = [float(val) for val in v]
#         history_log.append(chunk_metrics)
        
#         # Save model periodically instead of after every chunk
#         if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
#             model_path = os.path.join(checkpoints_dir, f"model_checkpoint_{chunk_idx+1}.h5")
#             model.save(model_path)
#             print(f"Model saved to {model_path}")
            
#             # Also save history
#             try:
#                 with open(os.path.join(output_dir, "training_history.json"), "w") as f:
#                     json.dump(history_log, f)
#             except Exception as e:
#                 print(f"Warning: Could not save history to JSON: {e}")
#                 # Fallback - save as pickle
#                 import pickle
#                 with open(os.path.join(output_dir, "training_history.pkl"), "wb") as f:
#                     pickle.dump(history_log, f)
        
#         # Generate predictions only for selected chunks to save time
#         if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
#             chunk_preds = model.predict(chunk_X, batch_size=batch_size)
#             np.save(os.path.join(predictions_dir, f"chunk_{chunk_idx+1}_predictions.npy"), chunk_p...
#             np.save(os.path.join(predictions_dir, f"chunk_{chunk_idx+1}_indices.npy"), chunk_indic...
#             del chunk_preds
            
#         # Explicitly delete everything from memory
#         del chunk_X, chunk_y
        
#         # Force garbage collection
#         gc.collect()
        
#         # Report memory after cleanup
#         memory_after = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
#         print(f"Memory after: {memory_after:.1f} MB (Change: {memory_after - memory_before:.1f} MB...
        
#         # Estimate time
#         elapsed = time.time() - start_time
#         avg_time_per_chunk = elapsed / (chunk_idx + 1)
#         remaining = avg_time_per_chunk * (num_chunks - chunk_idx - 1)
#         print(f"Estimated remaining time: {timedelta(seconds=int(remaining))}")
    
#     # Save final model
#     final_model_path = os.path.join(output_dir, "final_model.h5")
#     model.save(final_model_path)
#     print(f"Final model saved to {final_model_path}")
    
#     # Save all metrics
#     try:
#         with open(os.path.join(output_dir, "final_training_metrics.json"), "w") as f:
#             json.dump(history_log, f)
#     except Exception as e:
#         print(f"Error saving metrics: {e}")
#         # Save as pickle instead
#         import pickle
#         with open(os.path.join(output_dir, "final_training_metrics.pkl"), "wb") as f:
#             pickle.dump(history_log, f)
    
#     # Clean up validation data
#     del val_X, val_y
#     gc.collect()
    
#     # Final evaluation on test set
#     print("\nPerforming final evaluation on test set...")
    
#     # Process test data in batches
#     test_batch_size = 5000
#     num_test_batches = int(np.ceil(len(test_indices) / test_batch_size))
    
#     all_test_predictions = []
#     all_test_true = []
#     test_metrics = {'loss': 0, 'accuracy': 0, 'samples': 0}
    
#     for test_batch_idx in range(num_test_batches):
#         start_idx = test_batch_idx * test_batch_size
#         end_idx = min((test_batch_idx + 1) * test_batch_size, len(test_indices))
#         batch_indices = test_indices[start_idx:end_idx]
        
#         # Load batch data
#         test_X = np.array([X_mmap[idx] for idx in batch_indices])
#         test_y = np.array([y_mmap[idx] for idx in batch_indices])
        
#         # Evaluate
#         metrics = model.evaluate(test_X, test_y, verbose=1)
#         metrics_dict = {name: value for name, value in zip(model.metrics_names, metrics)}
        
#         # Weight metrics by batch size
#         for key in ['loss', 'accuracy']:
#             if key in metrics_dict:
#                 test_metrics[key] += metrics_dict[key] * len(batch_indices)
#         test_metrics['samples'] += len(batch_indices)
        
#         # Get predictions
#         test_preds = model.predict(test_X, batch_size=batch_size)
        
#         # Store
#         all_test_predictions.append(test_preds.flatten())
#         all_test_true.append(test_y)
        
#         # Clean up
#         del test_X, test_y, test_preds
#         gc.collect()
    
#     # Combine results
#     all_test_predictions = np.concatenate(all_test_predictions)
#     all_test_true = np.concatenate(all_test_true)
    
#     # Calculate final test metrics
#     test_loss = test_metrics['loss'] / test_metrics['samples']
#     test_accuracy = test_metrics['accuracy'] / test_metrics['samples']
    
#     # Calculate additional metrics
#     from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
#     test_preds_binary = (all_test_predictions > 0.5).astype(int)
#     report = classification_report(all_test_true, test_preds_binary)
#     conf_matrix = confusion_matrix(all_test_true, test_preds_binary)
#     fpr, tpr, _ = roc_curve(all_test_true, all_test_predictions)
#     roc_auc = auc(fpr, tpr)
    
#     # Save results
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     np.save(os.path.join(output_dir, f'test_predictions_{timestamp}.npy'), all_test_predictions)
#     np.save(os.path.join(output_dir, 'test_predictions_latest.npy'), all_test_predictions)
    
#     with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w') as f:
#         f.write("Classification Report:\n")
#         f.write(report)
#         f.write("\n\nConfusion Matrix:\n")
#         f.write(str(conf_matrix))
#         f.write("\n\nTest Metrics:\n")
#         f.write(f"loss: {test_loss:.4f}\n")
#         f.write(f"accuracy: {test_accuracy:.4f}\n")
#         f.write(f"AUC: {roc_auc:.4f}\n")
        
#     print(f"Test evaluation complete. Final test AUC: {roc_auc:.4f}")
    
#     total_time = time.time() - start_time
#     print(f"\nTotal training time: {timedelta(seconds=int(total_time))}")
    
#     return model, final_model_path

# import time

# # Diagnostic test with proper tensor conversion
# print("Testing batch processing...")
# X = np.load(X_file, mmap_mode='r')
# y = np.load(y_file, mmap_mode='r')

# # Get a small sample batch and convert to tensors
# sample_indices = train_indices[:32]
# X_sample = tf.convert_to_tensor(np.array([X[i] for i in sample_indices]), dtype=tf.float32)
# y_sample = tf.convert_to_tensor(np.array([y[i] for i in sample_indices]), dtype=tf.float32)

# print(f"Sample batch shapes: X={X_sample.shape}, y={y_sample.shape}")
# print(f"Testing forward pass...")
# start = time.time()
# y_pred = model(X_sample)
# print(f"Forward pass completed in {time.time() - start:.2f} seconds")

# # Reshape y_sample if needed
# if len(y_sample.shape) == 1 and len(y_pred.shape) == 2:
#     y_sample = tf.reshape(y_sample, (-1, 1))
#     print(f"Reshaped y_sample to {y_sample.shape} to match y_pred {y_pred.shape}")

# print(f"Testing backward pass...")
# start = time.time()
# with tf.GradientTape() as tape:
#     y_pred = model(X_sample, training=True)
#     # Use tf.keras.losses directly instead of model.compiled_loss
#     loss_fn = tf.keras.losses.BinaryCrossentropy()
#     loss = loss_fn(y_sample, y_pred)
# gradients = tape.gradient(loss, model.trainable_variables)
# model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))
# print(f"Backward pass completed in {time.time() - start:.2f} seconds")

# # Configuration
# # Configure TensorFlow
# physical_devices = tf.config.list_physical_devices('GPU')
# if physical_devices:
#     for device in physical_devices:
#         try:
#             tf.config.experimental.set_memory_growth(device, True)
#             print(f"Memory growth enabled for {device}")
#         except Exception as e:
#             print(f"Error configuring GPU: {e}")

# # Data paths
# data_dir = os.path.join('/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling', 'ml_da...
# X_file = os.path.join(data_dir, 'X_features.npy')
# y_file = os.path.join(data_dir, 'y_labels.npy')

# # Load memory-mapped data for shape information
# X = np.load(X_file, mmap_mode='r')
# y = np.load(y_file, mmap_mode='r')

# # Verify indices
# print("Verifying indices and class weights...")
# train_y = y[train_indices]
# val_y = y[val_indices]
# test_y = y[test_indices]

# print(f"Train/val/test sizes: {len(train_indices)}/{len(val_indices)}/{len(test_indices)}")
# print(f"Positive examples: Train={np.sum(train_y)} ({np.sum(train_y)/len(train_y)*100:.1f}%), " +
#       f"Val={np.sum(val_y)} ({np.sum(val_y)/len(val_y)*100:.1f}%), " +
#       f"Test={np.sum(test_y)} ({np.sum(test_y)/len(test_y)*100:.1f}%)")

# # Combine sample weights with class weights for imbalanced data
# pos_weight = (len(train_y) - np.sum(train_y)) / max(1, np.sum(train_y))
# class_weight = {0: 1.0, 1: pos_weight}
# print(f"Using class weight {pos_weight:.2f} for positive examples")

# # Free memory
# del train_y, val_y, test_y
# gc.collect()

# # Get input shape for the model
# input_shape = X[train_indices[0]].shape
# print(f"Input shape: {input_shape}")

# # Build model
# model = build_advanced_zero_curtain_model(input_shape)

# model.summary()

# # def resumable_efficient_training(model, X_file, y_file, train_indices, val_indices, test_indices...
# #                                output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
# #                                save_frequency=5, class_weight=None, start_chunk=360):
# # def resumable_efficient_training(model, X_file, y_file, train_indices, val_indices, test_indices...
# #                                output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
# #                                save_frequency=5, class_weight=None, start_chunk=405):
# # def resumable_efficient_training(model, X_file, y_file, train_indices, val_indices, test_indices...
# #                                output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
# #                                save_frequency=5, class_weight=None, start_chunk=450):
# # def resumable_efficient_training(model, X_file, y_file, train_indices, val_indices, test_indices...
# #                                output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
# #                                save_frequency=5, class_weight=None, start_chunk=495):
# def resumable_efficient_training(model, X_file, y_file, train_indices, val_indices, test_indices,
#                                output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
#                                save_frequency=5, class_weight=None, start_chunk=540):
#     """
#     Training function with built-in resume capability.
    
#     Parameters:
#     -----------
#     model : tf.keras.Model
#         Pre-compiled model
#     X_file, y_file : str
#         Paths to feature and label files
#     train_indices, val_indices, test_indices : array
#         Training, validation, and test indices
#     output_dir : str
#         Directory to save results
#     batch_size : int
#         Batch size for training
#     chunk_size : int
#         Number of samples to process at once
#     epochs_per_chunk : int
#         Epochs to train each chunk
#     save_frequency : int
#         Save model every N chunks
#     class_weight : dict, optional
#         Class weights for handling imbalanced data
#     start_chunk : int, optional
#         Chunk index to start/resume from
#     """
#     import os
#     import gc
#     import json
#     import numpy as np
#     import tensorflow as tf
#     from datetime import datetime, timedelta
#     import time
#     import psutil
    
#     # Create output directories
#     os.makedirs(output_dir, exist_ok=True)
#     predictions_dir = os.path.join(output_dir, "predictions")
#     os.makedirs(predictions_dir, exist_ok=True)
#     checkpoints_dir = os.path.join(output_dir, "checkpoints")
#     os.makedirs(checkpoints_dir, exist_ok=True)
    
#     # Process in chunks
#     num_chunks = int(np.ceil(len(train_indices) / chunk_size))
#     print(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
#     print(f"Starting from chunk {start_chunk+1}")
    
#     # Create validation set once
#     val_limit = min(2000, len(val_indices))
#     val_indices_subset = val_indices[:val_limit]
    
#     # Open data files
#     X_mmap = np.load(X_file, mmap_mode='r')
#     y_mmap = np.load(y_file, mmap_mode='r')
    
#     # Load validation data once
#     val_X = np.array([X_mmap[idx] for idx in val_indices_subset])
#     val_y = np.array([y_mmap[idx] for idx in val_indices_subset])
#     print(f"Loaded {len(val_X)} validation samples")
    
#     # Load existing history if resuming
#     history_log = []
#     history_path = os.path.join(output_dir, "training_history.json")
#     if start_chunk > 0 and os.path.exists(history_path):
#         try:
#             with open(history_path, "r") as f:
#                 history_log = json.load(f)
#         except Exception as e:
#             print(f"Could not load existing history: {e}")
#             # Try pickle format
#             pickle_path = os.path.join(output_dir, "training_history.pkl")
#             if os.path.exists(pickle_path):
#                 import pickle
#                 with open(pickle_path, "rb") as f:
#                     history_log = pickle.load(f)
    
#     # If resuming, load latest model
#     if start_chunk > 0:
#         # Find the most recent checkpoint before start_chunk
#         checkpoint_indices = []
#         for filename in os.listdir(checkpoints_dir):
#             if filename.startswith("model_checkpoint_") and filename.endswith(".h5"):
#                 try:
#                     idx = int(filename.split("_")[-1].split(".")[0])
#                     if idx < start_chunk:
#                         checkpoint_indices.append(idx)
#                 except ValueError:
#                     continue
        
#         if checkpoint_indices:
#             latest_idx = max(checkpoint_indices)
#             model_path = os.path.join(checkpoints_dir, f"model_checkpoint_{latest_idx}.h5")
#             if os.path.exists(model_path):
#                 print(f"Loading model from checkpoint {model_path}")
#                 model = tf.keras.models.load_model(model_path)
#             else:
#                 print(f"Warning: Could not find model checkpoint for chunk {latest_idx}")
    
#     # Setup callbacks
#     callbacks = [
#         tf.keras.callbacks.EarlyStopping(
#             patience=3, 
#             restore_best_weights=True,
#             monitor='val_loss',
#             min_delta=0.01
#         ),
#         tf.keras.callbacks.ReduceLROnPlateau(
#             factor=0.5,
#             patience=2,
#             min_lr=1e-6,
#             monitor='val_loss'
#         ),
#         # Memory cleanup after each epoch
#         tf.keras.callbacks.LambdaCallback(
#             on_epoch_end=lambda epoch, logs: gc.collect()
#         )
#     ]
    
#     # Track metrics across chunks
#     start_time = time.time()
    
#     # For safe recovery
#     recovery_file = os.path.join(output_dir, "last_completed_chunk.txt")
    
#     # Process each chunk
#     for chunk_idx in range(start_chunk, num_chunks):
#         # Get chunk indices
#         start_idx = chunk_idx * chunk_size
#         end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
#         chunk_indices = train_indices[start_idx:end_idx]
        
#         # Report memory
#         memory_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
#         print(f"\n{'='*50}")
#         print(f"Processing chunk {chunk_idx+1}/{num_chunks} with {len(chunk_indices)} samples")
#         print(f"Memory before: {memory_before:.1f} MB")
        
#         # Force garbage collection before loading new data
#         gc.collect()
        
#         # Load chunk data
#         chunk_X = np.array([X_mmap[idx] for idx in chunk_indices])
#         chunk_y = np.array([y_mmap[idx] for idx in chunk_indices])
        
#         print(f"Data loaded. Memory: {psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024...
        
#         # Train on chunk
#         print(f"Training for {epochs_per_chunk} epochs...")
#         history = model.fit(
#             chunk_X, chunk_y,
#             validation_data=(val_X, val_y),
#             epochs=epochs_per_chunk,
#             batch_size=batch_size,
#             class_weight=class_weight,
#             callbacks=callbacks,
#             verbose=1
#         )
        
#         # Store serializable metrics
#         chunk_metrics = {}
#         for k, v in history.history.items():
#             chunk_metrics[k] = [float(val) for val in v]
#         history_log.append(chunk_metrics)
        
#         # Save model periodically instead of after every chunk
#         if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
#             model_path = os.path.join(checkpoints_dir, f"model_checkpoint_{chunk_idx+1}.h5")
#             model.save(model_path)
#             print(f"Model saved to {model_path}")
            
#             # Also save history
#             try:
#                 with open(history_path, "w") as f:
#                     json.dump(history_log, f)
#             except Exception as e:
#                 print(f"Warning: Could not save history to JSON: {e}")
#                 # Fallback - save as pickle
#                 import pickle
#                 with open(os.path.join(output_dir, "training_history.pkl"), "wb") as f:
#                     pickle.dump(history_log, f)
        
#         # Generate predictions only for selected chunks to save time
#         if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
#             chunk_preds = model.predict(chunk_X, batch_size=batch_size)
#             np.save(os.path.join(predictions_dir, f"chunk_{chunk_idx+1}_predictions.npy"), chunk_p...
#             np.save(os.path.join(predictions_dir, f"chunk_{chunk_idx+1}_indices.npy"), chunk_indic...
#             del chunk_preds
            
#         # Explicitly delete everything from memory
#         del chunk_X, chunk_y
        
#         # Force garbage collection
#         gc.collect()
        
#         # Report memory after cleanup
#         memory_after = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
#         print(f"Memory after: {memory_after:.1f} MB (Change: {memory_after - memory_before:.1f} MB...
        
#         # Write recovery file with last completed chunk
#         with open(recovery_file, "w") as f:
#             f.write(str(chunk_idx + 1))
        
#         # Estimate time
#         elapsed = time.time() - start_time
#         avg_time_per_chunk = elapsed / (chunk_idx - start_chunk + 1)
#         remaining = avg_time_per_chunk * (num_chunks - chunk_idx - 1)
#         print(f"Estimated remaining time: {timedelta(seconds=int(remaining))}")
        
#         # If we're approaching the problematic chunk, reset tensorflow session
#         if (chunk_idx + 1) % 40 == 0:
#             print("Approaching potential freeze point - resetting TensorFlow session")
#             # Save model for this chunk
#             temp_model_path = os.path.join(checkpoints_dir, f"temp_reset_point_{chunk_idx+1}.h5")
#             model.save(temp_model_path)
            
#             # Clear session
#             tf.keras.backend.clear_session()
#             gc.collect()
            
#             # Reload model
#             model = tf.keras.models.load_model(temp_model_path)
#             print("TensorFlow session reset complete")
    
#     # Training complete - save final model
#     final_model_path = os.path.join(output_dir, "final_model.h5")
#     model.save(final_model_path)
#     print(f"Final model saved to {final_model_path}")
    
#     # Save all metrics
#     try:
#         with open(os.path.join(output_dir, "final_training_metrics.json"), "w") as f:
#             json.dump(history_log, f)
#     except Exception as e:
#         print(f"Error saving metrics: {e}")
#         # Save as pickle instead
#         import pickle
#         with open(os.path.join(output_dir, "final_training_metrics.pkl"), "wb") as f:
#             pickle.dump(history_log, f)
    
#     # Clean up validation data
#     del val_X, val_y
#     gc.collect()
    
#     # Final evaluation on test set
#     print("\nPerforming final evaluation on test set...")
#     # [Rest of evaluation code remains the same]
    
#     return model, final_model_path

# # # For initial run:
# # model, final_model_path = resumable_efficient_training(
# #     model, X_file, y_file,
# #     train_indices, val_indices, test_indices,
# #     output_dir=output_dir,
# #     batch_size=256,
# #     chunk_size=25000,
# #     epochs_per_chunk=2,
# #     save_frequency=5,
# #     class_weight=class_weight,
# #     start_chunk=0  # Start from beginning
# # )

# # To resume after a freeze (e.g., after chunk #):
# model, final_model_path = resumable_efficient_training(
#     model, X_file, y_file,
#     train_indices, val_indices, test_indices,
#     output_dir=output_dir,
#     batch_size=256,
#     chunk_size=25000,
#     epochs_per_chunk=2,
#     save_frequency=5,
#     class_weight=class_weight,
#     #start_chunk=360
#     #start_chunk=405
#     #start_chunk=450
#     #start_chunk=495
#     start_chunk=540
# )


# ####


# def plot_training_history(output_dir):
#     """Visualize learning curves across all training chunks"""
#     import matplotlib.pyplot as plt
#     import numpy as np
#     import json
#     import os
    
#     # Load training history
#     history_path = os.path.join(output_dir, "training_history.json")
#     if not os.path.exists(history_path):
#         history_path = os.path.join(output_dir, "final_training_metrics.json")
    
#     try:
#         with open(history_path, "r") as f:
#             history_log = json.load(f)
#     except:
#         # Try pickle format
#         import pickle
#         with open(os.path.join(output_dir, "training_history.pkl"), "rb") as f:
#             history_log = pickle.load(f)
    
#     # Extract metrics across all chunks
#     metrics = ['loss', 'accuracy', 'auc', 'precision', 'recall']
#     val_metrics = [f'val_{m}' for m in metrics]
    
#     # Prepare aggregated metrics
#     all_metrics = {m: [] for m in metrics + val_metrics}
    
#     # Collect metrics across chunks
#     for chunk_history in history_log:
#         for metric in metrics:
#             # Training metrics
#             if metric in chunk_history:
#                 all_metrics[metric].extend(chunk_history[metric])
            
#             # Validation metrics
#             val_metric = f'val_{metric}'
#             if val_metric in chunk_history:
#                 all_metrics[val_metric].extend(chunk_history[val_metric])
    
#     # Create learning curve visualizations
#     plt.figure(figsize=(18, 12))
    
#     # Plot loss
#     plt.subplot(2, 2, 1)
#     plt.plot(all_metrics['loss'], label='Training Loss')
#     plt.plot(all_metrics['val_loss'], label='Validation Loss')
#     plt.title('Loss Over Time')
#     plt.xlabel('Epoch')
#     plt.ylabel('Loss')
#     plt.legend()
#     plt.grid(True, alpha=0.3)
    
#     # Plot accuracy
#     plt.subplot(2, 2, 2)
#     plt.plot(all_metrics['accuracy'], label='Training Accuracy')
#     plt.plot(all_metrics['val_accuracy'], label='Validation Accuracy')
#     plt.title('Accuracy Over Time')
#     plt.xlabel('Epoch')
#     plt.ylabel('Accuracy')
#     plt.legend()
#     plt.grid(True, alpha=0.3)
    
#     # Plot AUC
#     plt.subplot(2, 2, 3)
#     plt.plot(all_metrics['auc'], label='Training AUC')
#     plt.plot(all_metrics['val_auc'], label='Validation AUC')
#     plt.title('AUC Over Time')
#     plt.xlabel('Epoch')
#     plt.ylabel('AUC')
#     plt.legend()
#     plt.grid(True, alpha=0.3)
    
#     # Plot precision-recall
#     plt.subplot(2, 2, 4)
#     plt.plot(all_metrics['precision'], label='Training Precision')
#     plt.plot(all_metrics['recall'], label='Training Recall')
#     plt.plot(all_metrics['val_precision'], label='Validation Precision')
#     plt.plot(all_metrics['val_recall'], label='Validation Recall')
#     plt.title('Precision-Recall Over Time')
#     plt.xlabel('Epoch')
#     plt.ylabel('Score')
#     plt.legend()
#     plt.grid(True, alpha=0.3)
    
#     plt.tight_layout()
#     plt.savefig(os.path.join(output_dir, 'learning_curves.png'), dpi=300)
#     plt.show()
    
#     return all_metrics

# # Define output directory
# output_dir = '/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling/efficient_model'

# # Data paths
# data_dir = os.path.join('/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling', 'ml_da...
# X_file = os.path.join(data_dir, 'X_features.npy')
# y_file = os.path.join(data_dir, 'y_labels.npy')

# # Load metadata for spatial analysis
# metadata_file = os.path.join(data_dir, 'metadata.pkl')
# with open(metadata_file, 'rb') as f:
#     metadata = pickle.load(f)

# # Run all analyses
# print("1. Analyzing learning curves...")
# learning_metrics = plot_training_history(output_dir)

# def balanced_memory_speed_training(model, X_file, y_file, train_indices, val_indices, test_indices...
#                                   output_dir, batch_size=256, chunk_size=10000, epochs_per_chunk=2...
#     """
#     Training approach balancing memory efficiency and speed.
#     """
#     import time
#     import os
#     import json
#     from datetime import datetime, timedelta
    
#     # Create output directories
#     os.makedirs(output_dir, exist_ok=True)
#     predictions_dir = os.path.join(output_dir, "predictions")
#     os.makedirs(predictions_dir, exist_ok=True)
    
#     # Memory-mapped access to data
#     X_mmap = np.load(X_file, mmap_mode='r')
#     y_mmap = np.load(y_file, mmap_mode='r')
    
#     start_time = time.time()
#     print(f"Starting balanced training at {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
#     # Create smaller validation dataset (once)
#     val_limit = min(5000, len(val_indices))
#     val_indices_subset = val_indices[:val_limit]
#     val_X = np.array([X_mmap[idx] for idx in val_indices_subset])
#     val_y = np.array([y_mmap[idx] for idx in val_indices_subset])
    
#     # Shuffle training indices
#     np.random.shuffle(train_indices)
    
#     # Process in chunks
#     num_chunks = int(np.ceil(len(train_indices) / chunk_size))
#     print(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
    
#     history_aggregate = {}
    
#     for chunk_idx in range(num_chunks):
#         chunk_start = time.time()
        
#         # Get chunk indices
#         start_idx = chunk_idx * chunk_size
#         end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
#         chunk_indices = train_indices[start_idx:end_idx]
        
#         print(f"\n{'='*50}")
#         print(f"Processing chunk {chunk_idx+1}/{num_chunks} with {len(chunk_indices)} samples")
#         print(f"Memory usage: {memory_usage():.1f} MB")
        
#         # Load chunk data efficiently
#         print(f"Loading chunk data...")
#         load_start = time.time()
#         chunk_X = np.array([X_mmap[idx] for idx in chunk_indices])
#         chunk_y = np.array([y_mmap[idx] for idx in chunk_indices])
#         print(f"Chunk loaded in {time.time() - load_start:.2f} seconds")
        
#         # Train on chunk with standard fit 
#         print(f"Training on chunk for {epochs_per_chunk} epochs...")
#         history = model.fit(
#             chunk_X, chunk_y,
#             validation_data=(val_X, val_y),
#             epochs=epochs_per_chunk,
#             batch_size=batch_size,
#             class_weight=class_weight,
#             verbose=1
#         )
        
#         # Save model after each chunk
#         model_path = os.path.join(output_dir, f"model_checkpoint_chunk_{chunk_idx+1}.h5")
#         model.save(model_path)
#         print(f"Model saved to {model_path}")
        
#         # Aggregate history
#         for k, v in history.history.items():
#             if k not in history_aggregate:
#                 history_aggregate[k] = []
#             history_aggregate[k].extend(v)
        
#         # Save history
#         with open(os.path.join(output_dir, "training_history.json"), "w") as f:
#             json.dump(history_aggregate, f)
        
#         # Generate predictions
#         chunk_preds = model.predict(chunk_X, batch_size=batch_size)
#         np.save(os.path.join(predictions_dir, f"chunk_{chunk_idx+1}_predictions.npy"), chunk_preds...
#         np.save(os.path.join(predictions_dir, f"chunk_{chunk_idx+1}_indices.npy"), chunk_indices)
        
#         # Clean up to free memory
#         del chunk_X, chunk_y, chunk_preds
#         gc.collect()
        
#         # Report timing
#         chunk_time = time.time() - chunk_start
#         print(f"Chunk {chunk_idx+1} completed in {timedelta(seconds=int(chunk_time))}")
        
#         # Estimate remaining time
#         elapsed = time.time() - start_time
#         avg_chunk_time = elapsed / (chunk_idx + 1)
#         remaining = avg_chunk_time * (num_chunks - (chunk_idx + 1))
#         print(f"Estimated remaining time: {timedelta(seconds=int(remaining))}")
#         print(f"{'='*50}")
    
#     # Final evaluation on test set
#     print("\nPerforming final evaluation on test set...")
    
#     # This can be done in reasonable-sized batches
#     test_batch_size = 5000
#     num_test_batches = int(np.ceil(len(test_indices) / test_batch_size))
    
#     all_test_predictions = []
#     all_test_true = []
#     test_metrics = {'loss': 0, 'accuracy': 0, 'samples': 0}
    
#     for test_batch_idx in range(num_test_batches):
#         start_idx = test_batch_idx * test_batch_size
#         end_idx = min((test_batch_idx + 1) * test_batch_size, len(test_indices))
#         batch_indices = test_indices[start_idx:end_idx]
        
#         # Load batch data
#         test_X = np.array([X_mmap[idx] for idx in batch_indices])
#         test_y = np.array([y_mmap[idx] for idx in batch_indices])
        
#         # Evaluate
#         metrics = model.evaluate(test_X, test_y, verbose=0)
#         metrics_dict = {name: value for name, value in zip(model.metrics_names, metrics)}
        
#         # Weight metrics by batch size
#         for key in ['loss', 'accuracy']:
#             test_metrics[key] += metrics_dict[key] * len(batch_indices)
#         test_metrics['samples'] += len(batch_indices)
        
#         # Get predictions
#         test_preds = model.predict(test_X, batch_size=batch_size)
        
#         # Store
#         all_test_predictions.append(test_preds.flatten())
#         all_test_true.append(test_y)
        
#         # Clean up
#         del test_X, test_y
#         gc.collect()
    
#     # Combine results
#     all_test_predictions = np.concatenate(all_test_predictions)
#     all_test_true = np.concatenate(all_test_true)
    
#     # Calculate final test metrics
#     test_loss = test_metrics['loss'] / test_metrics['samples']
#     test_accuracy = test_metrics['accuracy'] / test_metrics['samples']
    
#     # Calculate additional metrics
#     from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
#     test_preds_binary = (all_test_predictions > 0.5).astype(int)
#     report = classification_report(all_test_true, test_preds_binary)
#     conf_matrix = confusion_matrix(all_test_true, test_preds_binary)
#     fpr, tpr, _ = roc_curve(all_test_true, all_test_predictions)
#     roc_auc = auc(fpr, tpr)
    
#     # Save results
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     np.save(os.path.join(output_dir, f'test_predictions_{timestamp}.npy'), all_test_predictions)
#     np.save(os.path.join(output_dir, 'test_predictions_latest.npy'), all_test_predictions)
    
#     with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w') as f:
#         f.write("Classification Report:\n")
#         f.write(report)
#         f.write("\n\nConfusion Matrix:\n")
#         f.write(str(conf_matrix))
#         f.write("\n\nTest Metrics:\n")
#         f.write(f"loss: {test_loss:.4f}\n")
#         f.write(f"accuracy: {test_accuracy:.4f}\n")
#         f.write(f"AUC: {roc_auc:.4f}\n")
    
#     # Plot training history
#     try:
#         plt.figure(figsize=(16, 6))
        
#         for i, metric in enumerate(['accuracy', 'loss', 'auc']):
#             plt.subplot(1, 3, i+1)
#             if metric in history_aggregate:
#                 plt.plot(history_aggregate[metric], label='Train')
#             if f'val_{metric}' in history_aggregate:
#                 plt.plot(history_aggregate[f'val_{metric}'], label='Validation')
#             plt.title(f'Model {metric.capitalize()}')
#             plt.ylabel(metric.capitalize())
#             plt.xlabel('Epoch')
#             plt.legend()
        
#         plt.tight_layout()
#         plt.savefig(os.path.join(output_dir, 'training_performance.png'), dpi=300)
        
#         # Plot ROC curve
#         plt.figure(figsize=(8, 6))
#         plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')
#         plt.plot([0, 1], [0, 1], 'k--')
#         plt.xlabel('False Positive Rate')
#         plt.ylabel('True Positive Rate')
#         plt.title('ROC Curve (Test Set)')
#         plt.legend(loc='lower right')
#         plt.savefig(os.path.join(output_dir, 'roc_curve.png'), dpi=300)
#     except Exception as e:
#         print(f"Error creating plots: {e}")
    
#     # Save final model
#     final_model_path = os.path.join(output_dir, "final_model.h5")
#     model.save(final_model_path)
    
#     total_time = time.time() - start_time
#     print(f"\nTotal training time: {timedelta(seconds=int(total_time))}")
    
#     return history_aggregate, final_model_path

# def evaluate_in_minibatches(model, X_mmap, y_mmap, indices, batch_size=16):
#     """Evaluate model in small batches to minimize memory usage"""
#     y_true = []
#     y_pred = []
#     total_loss = 0.0
    
#     for start_idx in range(0, len(indices), batch_size):
#         end_idx = min(start_idx + batch_size, len(indices))
#         batch_indices = indices[start_idx:end_idx]
        
#         # Load batch
#         X_batch = np.array([X_mmap[idx] for idx in batch_indices])
#         y_batch = np.array([y_mmap[idx] for idx in batch_indices])
        
#         # Convert to tensors
#         X_tensor = tf.convert_to_tensor(X_batch, dtype=tf.float32)
#         y_tensor = tf.convert_to_tensor(y_batch, dtype=tf.float32)
#         if len(y_tensor.shape) == 1:
#             y_tensor = tf.reshape(y_tensor, (-1, 1))
        
#         # Get predictions
#         y_batch_pred = model(X_tensor, training=False)
        
#         # Calculate loss
#         loss_fn = tf.keras.losses.BinaryCrossentropy()
#         batch_loss = loss_fn(y_tensor, y_batch_pred)
#         total_loss += float(batch_loss) * len(batch_indices)
        
#         # Store for metrics
#         y_true.append(y_batch)
#         y_pred.append(y_batch_pred.numpy().flatten())
        
#         # Free memory
#         del X_batch, y_batch, X_tensor, y_tensor, y_batch_pred
#         gc.collect()
    
#     # Combine for metrics calculation
#     y_true_all = np.concatenate(y_true)
#     y_pred_all = np.concatenate(y_pred)
    
#     # Calculate metrics
#     auc = tf.keras.metrics.AUC()(y_true_all, y_pred_all).numpy()
#     y_pred_binary = (y_pred_all > 0.5).astype(int)
#     accuracy = np.mean(y_pred_binary == y_true_all)
#     precision = tf.keras.metrics.Precision()(y_true_all, y_pred_binary).numpy()
#     recall = tf.keras.metrics.Recall()(y_true_all, y_pred_binary).numpy()
    
#     return {
#         'loss': total_loss / len(indices),
#         'accuracy': float(accuracy),
#         'auc': float(auc),
#         'precision': float(precision),
#         'recall': float(recall)
#     }

# def predict_in_minibatches(model, X_mmap, indices, batch_size=16):
#     """Generate predictions in small batches to minimize memory usage"""
#     all_predictions = np.zeros(len(indices))
    
#     for start_idx in range(0, len(indices), batch_size):
#         end_idx = min(start_idx + batch_size, len(indices))
#         batch_indices = indices[start_idx:end_idx]
        
#         # Load batch
#         X_batch = np.array([X_mmap[idx] for idx in batch_indices])
        
#         # Generate predictions
#         X_tensor = tf.convert_to_tensor(X_batch, dtype=tf.float32)
#         predictions = model(X_tensor, training=False).numpy().flatten()
        
#         # Store predictions
#         all_predictions[start_idx:end_idx] = predictions
        
#         # Free memory
#         del X_batch, X_tensor, predictions
#         gc.collect()
    
#     return all_predictions

# import os
# import gc
# import json
# import pickle
# import numpy as np
# import tensorflow as tf
# from tensorflow.keras.callbacks import (
#     EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, 
#     CSVLogger, TensorBoard
# )
# import matplotlib.pyplot as plt
# from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
# from tqdm import tqdm
# import psutil
# from datetime import datetime

# def memory_usage():
#     """Get current memory usage in MB"""
#     process = psutil.Process(os.getpid())
#     mem_info = process.memory_info()
#     return mem_info.rss / 1024**2  # Memory in MB

# def train_with_spatial_balancing(X, y, metadata, output_dir=None, checkpoint_dir=None):
#     """
#     Train a model with spatially balanced sampling.
    
#     Parameters:
#     -----------
#     X : numpy.ndarray
#         Input features
#     y : numpy.ndarray
#         Output labels
#     metadata : list
#         Metadata containing timestamps and spatial information
#     output_dir : str, optional
#         Directory to save model and results
#     checkpoint_dir : str, optional
#         Directory for saving checkpoints
        
#     Returns:
#     --------
#     trained_model, training_history
#     """
#     print("Training zero curtain model with spatial balancing...")
#     print(f"Memory before training: {memory_usage():.1f} MB")
    
#     # Create output directory
#     if output_dir:
#         os.makedirs(output_dir, exist_ok=True)
    
#     if checkpoint_dir is None:
#         checkpoint_dir = os.path.join(output_dir, 'checkpoints') if output_dir else 'checkpoints'
#     os.makedirs(checkpoint_dir, exist_ok=True)
    
#     # Enable memory growth to avoid pre-allocating all GPU memory
#     physical_devices = tf.config.list_physical_devices('GPU')
#     if physical_devices:
#         for device in physical_devices:
#             try:
#                 tf.config.experimental.set_memory_growth(device, True)
#                 print(f"Enabled memory growth for {device}")
#             except:
#                 print(f"Could not set memory growth for {device}")
    
#     # Create spatiotemporally balanced train/val/test split
#     print("Creating spatiotemporally balanced split...")
#     train_indices, val_indices, test_indices = stratified_spatiotemporal_split(
#         X, y, metadata, test_size=0.2, val_size=0.15, checkpoint_dir=checkpoint_dir
#     )
    
#     # Create the splits
#     X_train = X[train_indices]
#     y_train = y[train_indices]
    
#     X_val = X[val_indices]
#     y_val = y[val_indices]
    
#     X_test = X[test_indices]
#     y_test = y[test_indices]
    
#     # Load spatial density weights if available
#     weights_file = os.path.join(checkpoint_dir, "spatial_density.pkl")
#     if os.path.exists(weights_file):
#         print(f"Loading spatial weights from {weights_file}")
#         with open(weights_file, "rb") as f:
#             weights_data = pickle.load(f)
#         sample_weights = weights_data["weights"][train_indices]
        
#         # Normalize weights
#         sample_weights = sample_weights / np.mean(sample_weights) * len(sample_weights)
#     else:
#         print("No spatial weights found, using uniform weights")
#         sample_weights = np.ones(len(train_indices))
    
#     # Print info about the splits
#     print(f"Train/val/test sizes: {len(X_train)}/{len(X_val)}/{len(X_test)}")
#     print(f"Positive examples: Train={np.sum(y_train)} ({np.sum(y_train)/len(y_train)*100:.1f}%), ...
#           f"Val={np.sum(y_val)} ({np.sum(y_val)/len(y_val)*100:.1f}%), " +
#           f"Test={np.sum(y_test)} ({np.sum(y_test)/len(y_test)*100:.1f}%)")
    
#     # Combine sample weights with class weights for imbalanced data
#     pos_weight = (len(y_train) - np.sum(y_train)) / max(1, np.sum(y_train))
#     class_weight = {0: 1.0, 1: pos_weight}
#     print(f"Using class weight {pos_weight:.2f} for positive examples")
    
#     # Build model with appropriate input shape
#     input_shape = (X_train.shape[1], X_train.shape[2])
#     model = build_advanced_zero_curtain_model(input_shape)
    
#     # Always check for existing model checkpoint to resume training
#     model_checkpoint_paths = []
#     if output_dir:
#         # Look for checkpoint files in multiple locations
#         model_checkpoint_path1 = os.path.join(output_dir, 'model_checkpoint.h5')
#         model_checkpoint_path2 = os.path.join(output_dir, 'checkpoint.h5')
#         model_checkpoint_path3 = os.path.join(checkpoint_dir, 'model_checkpoint.h5')
        
#         model_checkpoint_paths = [p for p in [model_checkpoint_path1, model_checkpoint_path2, mode...
#                                 if os.path.exists(p)]
        
#         if model_checkpoint_paths:
#             print(f"Found {len(model_checkpoint_paths)} existing model checkpoints")
#             # Use the most recent checkpoint based on modification time
#             latest_checkpoint = max(model_checkpoint_paths, key=os.path.getmtime)
#             print(f"Loading most recent checkpoint: {latest_checkpoint}")
#             try:
#                 model = tf.keras.models.load_model(latest_checkpoint)
#                 print("Checkpoint loaded successfully - will resume training from this point")
#             except Exception as e:
#                 print(f"Error loading checkpoint: {str(e)}")
#                 print("Will start training from scratch")
    
#     # Set up callbacks
#     callbacks = [
#         # Stop training when validation performance plateaus
#         EarlyStopping(
#             patience=15,
#             restore_best_weights=True,
#             monitor='val_auc',
#             mode='max'
#         ),
#         # Reduce learning rate when improvement slows
#         ReduceLROnPlateau(
#             factor=0.5,
#             patience=7,
#             min_lr=1e-6,
#             monitor='val_auc',
#             mode='max'
#         ),
#         # Manual garbage collection after each epoch
#         tf.keras.callbacks.LambdaCallback(
#             on_epoch_end=lambda epoch, logs: gc.collect()
#         )
#     ]
    
#     # Add additional callbacks if output directory provided
#     if output_dir:
#         callbacks.extend([
#             # Save best model
#             ModelCheckpoint(
#                 os.path.join(output_dir, 'model_checkpoint.h5'),
#                 save_best_only=True,
#                 monitor='val_auc',
#                 mode='max'
#             ),
#             # Log training progress to CSV
#             CSVLogger(
#                 os.path.join(output_dir, 'training_log.csv'),
#                 append=True
#             ),
#             # TensorBoard visualization
#             TensorBoard(
#                 log_dir=os.path.join(output_dir, 'tensorboard_logs'),
#                 histogram_freq=1,
#                 profile_batch=0  # Disable profiling to save memory
#             )
#         ])
    
#     # Train model
#     print("Starting model training...")
#     batch_size = 32  # Adjust based on available memory
#     epochs = 100
    
#     history = model.fit(
#         X_train, y_train,
#         validation_data=(X_val, y_val),
#         epochs=epochs,
#         batch_size=batch_size,
#         callbacks=callbacks,
#         class_weight=class_weight,
#         sample_weight=sample_weights,
#         verbose=1,
#         shuffle=True,
#         use_multiprocessing=False,  # Avoid memory overhead
#         workers=1  # Reduce parallel processing
#     )
    
#     # Clean up to free memory
#     del X_train, y_train, X_val, y_val
#     gc.collect()
    
#     # Evaluate on test set
#     print("Evaluating model on test set...")
#     evaluation = model.evaluate(X_test, y_test, batch_size=batch_size)
#     print("Test performance:")
#     for metric, value in zip(model.metrics_names, evaluation):
#         print(f"  {metric}: {value:.4f}")
    
#     # Generate predictions for test set
#     y_pred_prob = model.predict(X_test, batch_size=batch_size)
#     y_pred = (y_pred_prob > 0.5).astype(int).flatten()
    
#     # Calculate additional evaluation metrics
#     report = classification_report(y_test, y_pred)
#     conf_matrix = confusion_matrix(y_test, y_pred)
    
#     print("Classification Report:")
#     print(report)
    
#     print("Confusion Matrix:")
#     print(conf_matrix)
    
#     # Save evaluation results
#     if output_dir:
#         # Save evaluation metrics
#         with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w') as f:
#             f.write("Classification Report:\n")
#             f.write(report)
#             f.write("\n\nConfusion Matrix:\n")
#             f.write(str(conf_matrix))
#             f.write("\n\nTest Metrics:\n")
#             for metric, value in zip(model.metrics_names, evaluation):
#                 f.write(f"{metric}: {value:.4f}\n")
        
#         # Save test set predictions with timestamp to avoid overwriting
#         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#         np.save(os.path.join(output_dir, f'test_predictions_{timestamp}.npy'), y_pred_prob)
#         np.save(os.path.join(output_dir, f'test_indices_{timestamp}.npy'), test_indices)
#         # Also keep a copy with the standard name for easier reference
#         np.save(os.path.join(output_dir, 'test_predictions_latest.npy'), y_pred_prob)
#         np.save(os.path.join(output_dir, 'test_indices_latest.npy'), test_indices)
        
#         # Plot training history
#         plt.figure(figsize=(16, 6))
        
#         plt.subplot(1, 3, 1)
#         plt.plot(history.history['auc'])
#         plt.plot(history.history['val_auc'])
#         plt.title('Model AUC')
#         plt.ylabel('AUC')
#         plt.xlabel('Epoch')
#         plt.legend(['Train', 'Validation'], loc='lower right')
        
#         plt.subplot(1, 3, 2)
#         plt.plot(history.history['loss'])
#         plt.plot(history.history['val_loss'])
#         plt.title('Model Loss')
#         plt.ylabel('Loss')
#         plt.xlabel('Epoch')
#         plt.legend(['Train', 'Validation'], loc='upper right')
        
#         # Plot ROC curve
#         plt.subplot(1, 3, 3)
#         fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
#         roc_auc = auc(fpr, tpr)
#         plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')
#         plt.plot([0, 1], [0, 1], 'k--')
#         plt.xlabel('False Positive Rate')
#         plt.ylabel('True Positive Rate')
#         plt.title('ROC Curve (Test Set)')
#         plt.legend(loc='lower right')
        
#         plt.tight_layout()
#         plt.savefig(os.path.join(output_dir, 'training_performance.png'), dpi=300)
        
#         # Save detailed model summary
#         from contextlib import redirect_stdout
#         with open(os.path.join(output_dir, 'model_summary.txt'), 'w') as f:
#             with redirect_stdout(f):
#                 model.summary()
    
#     # Clean up to free memory
#     del X_test, y_test
#     gc.collect()
    
#     print(f"Memory after training: {memory_usage():.1f} MB")
#     return model, history, evaluation

# # Load split indices
# with open("zero_curtain_pipeline/modeling/checkpoints/spatiotemporal_split.pkl", "rb") as f:
#     split_data = pickle.load(f)
# train_indices = split_data["train_indices"]
# val_indices = split_data["val_indices"]
# test_indices = split_data["test_indices"]

# # Load spatial weights
# with open("zero_curtain_pipeline/modeling/checkpoints/spatial_density.pkl", "rb") as f:
#     weights_data = pickle.load(f)

# class DataGenerator(tf.keras.utils.Sequence):
#     def __init__(self, X_file, y_file, indices, batch_size=32, shuffle=True, weights=None):
#         """
#         Data generator for efficient loading from memory-mapped arrays
        
#         Parameters:
#         -----------
#         X_file : str
#             Path to features file
#         y_file : str
#             Path to labels file
#         indices : array
#             Indices to sample from
#         batch_size : int
#             Batch size
#         shuffle : bool
#             Whether to shuffle indices
#         weights : array, optional
#             Sample weights (must be same length as indices)
#         """
#         self.X_file = X_file
#         self.y_file = y_file
#         self.indices = np.asarray(indices)  # Ensure array type
#         self.batch_size = batch_size
#         self.shuffle = shuffle
#         self.weights = weights
        
#         # Verify weights array
#         if self.weights is not None:
#             assert len(self.weights) == len(self.indices), "Weights array must match indices lengt...
        
#         # Load as memory-mapped arrays
#         self.X = np.load(self.X_file, mmap_mode='r')
#         self.y = np.load(self.y_file, mmap_mode='r')
        
#         # Get input shape from first sample
#         self.input_shape = self.X[self.indices[0]].shape
        
#         self.on_epoch_end()
    
#     def __len__(self):
#         """Number of batches per epoch"""
#         return int(np.ceil(len(self.indices) / self.batch_size))
    
#     def __getitem__(self, idx):
#         """Get batch at position idx"""
#         start_idx = idx * self.batch_size
#         end_idx = min((idx + 1) * self.batch_size, len(self.indices))
#         batch_indices = self.indices_array[start_idx:end_idx]
        
#         # Load data
#         X_batch = self.X[batch_indices]
#         y_batch = self.y[batch_indices]
        
#         if self.weights is not None:
#             # Get weights for these specific indices
#             batch_positions = np.where(np.isin(self.indices, batch_indices))[0]
#             w_batch = self.weights[batch_positions]
#             return X_batch, y_batch, w_batch
#         else:
#             return X_batch, y_batch
    
#     def on_epoch_end(self):
#         """Called at the end of each epoch"""
#         self.indices_array = np.copy(self.indices)
#         if self.shuffle:
#             np.random.shuffle(self.indices_array)
            
#     def get_input_shape(self):
#         """Get input shape of samples"""
#         return self.input_shape

# def positional_encoding(length, depth):
#     positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
#     depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
    
#     angle_rates = 1 / tf.pow(10000.0, depths)
#     angle_rads = positions * angle_rates
    
#     # Create a positional encoding with the same depth as the input
#     pos_encoding = tf.concat([tf.sin(angle_rads), tf.cos(angle_rads)], axis=-1)
    
#     # Slice or pad to match the target depth
#     if pos_encoding.shape[-1] > depth:
#         pos_encoding = pos_encoding[:, :depth]  # Slice to match
#     elif pos_encoding.shape[-1] < depth:
#         # Pad to match
#         padding = depth - pos_encoding.shape[-1]
#         pos_encoding = tf.pad(pos_encoding, [[0, 0], [0, padding]])
    
#     return pos_encoding

# data_dir = os.path.join('/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling', 'ml_da...

# # Initialize generators
# X_file = os.path.join(data_dir, 'X_features.npy')
# y_file = os.path.join(data_dir, 'y_labels.npy')

# # Get sample weights for training set
# sample_weights = weights_data["weights"][train_indices]
# sample_weights = sample_weights / np.mean(sample_weights) * len(sample_weights)

# # Load data and metadata; use memory mapping to reduce memory usage
# print("Loading data...")
# X = np.load(X_file, mmap_mode='r')
# y = np.load(y_file, mmap_mode='r')
# print("Loading metadata...")
# with open(os.path.join(data_dir, 'metadata.pkl'), 'rb') as f:
#     metadata = pickle.load(f)
# print("Done.")

# train_y = y[train_indices]
# val_y = y[val_indices]
# test_y = y[test_indices]

# print(f"Train/val/test sizes: {len(train_indices)}/{len(val_indices)}/{len(test_indices)}")
# print(f"Positive examples: Train={np.sum(train_y)} ({np.sum(train_y)/len(train_y)*100:.1f}%), " +
#       f"Val={np.sum(val_y)} ({np.sum(val_y)/len(val_y)*100:.1f}%), " +
#       f"Test={np.sum(test_y)} ({np.sum(test_y)/len(test_y)*100:.1f}%)")

# # Combine sample weights with class weights for imbalanced data
# pos_weight = (len(train_y) - np.sum(train_y)) / max(1, np.sum(train_y))
# class_weight = {0: 1.0, 1: pos_weight}
# print(f"Using class weight {pos_weight:.2f} for positive examples")
# # Free memory
# del train_y, val_y, test_y
# gc.collect()

# # Custom data generator
# from tensorflow.keras.utils import Sequence

# class DataGenerator(Sequence):
#     def __init__(self, X, y, indices, batch_size=32, shuffle=True, weights=None):
#         self.X = X
#         self.y = y
#         self.indices = indices
#         self.batch_size = batch_size
#         self.shuffle = shuffle
#         self.weights = weights
#         self.on_epoch_end()
        
#     def __len__(self):
#         return int(np.ceil(len(self.indices) / self.batch_size))
        
#     def __getitem__(self, idx):
#         start_idx = idx * self.batch_size
#         end_idx = min((idx + 1) * self.batch_size, len(self.indices))
#         batch_indices = self.indices_array[start_idx:end_idx]
        
#         X_batch = self.X[batch_indices]
#         y_batch = self.y[batch_indices]
        
#         if self.weights is not None:
#             weights_batch = np.array([self.weights[i] for i in range(len(self.indices)) 
#                                      if self.indices[i] in batch_indices])
#             return X_batch, y_batch, weights_batch
#         else:
#             return X_batch, y_batch
        
#     def on_epoch_end(self):
#         self.indices_array = np.array(self.indices)
#         if self.shuffle:
#             np.random.shuffle(self.indices_array)

# # Create generators
# train_gen = DataGenerator(X, y, train_indices, batch_size=1024, shuffle=True, weights=sample_weigh...
# val_gen = DataGenerator(X, y, val_indices, batch_size=1024, shuffle=False)
# test_gen = DataGenerator(X, y, test_indices, batch_size=1024, shuffle=False)

# def create_tf_dataset_from_generator(data_generator, output_signature, buffer_size=10000):
#     """
#     Create a TensorFlow Dataset from a Keras Sequence generator.
    
#     Parameters:
#     -----------
#     data_generator : Sequence
#         Keras Sequence generator
#     output_signature : tuple
#         Output signature for the dataset
#     buffer_size : int
#         Size of shuffle buffer
        
#     Returns:
#     --------
#     tf.data.Dataset
#         Dataset ready for model training/evaluation
#     """
#     # Define generator function that wraps the Keras Sequence
#     def tf_generator():
#         for batch_index in range(len(data_generator)):
#             batch = data_generator[batch_index]
#             # If batch is a tuple, yield elements individually
#             if isinstance(batch, tuple):
#                 for i in range(len(batch[0])):  # For each example in the batch
#                     # Extract individual items from the batch
#                     if len(batch) == 2:  # (x, y)
#                         yield batch[0][i], batch[1][i]
#                     elif len(batch) == 3:  # (x, y, weights)
#                         yield batch[0][i], batch[1][i], batch[2][i]
    
#     # Create dataset
#     dataset = tf.data.Dataset.from_generator(
#         tf_generator,
#         output_signature=output_signature
#     )
    
#     # Determine if shuffling is needed based on the generator
#     if data_generator.shuffle:
#         dataset = dataset.shuffle(buffer_size)
    
#     # Batch and prefetch
#     dataset = dataset.batch(data_generator.batch_size)
#     dataset = dataset.prefetch(tf.data.AUTOTUNE)
    
#     return dataset

# # Create output signatures
# features_signature = tf.TensorSpec(shape=input_shape, dtype=tf.float32)
# label_signature = tf.TensorSpec(shape=(), dtype=tf.int32)
# weight_signature = tf.TensorSpec(shape=(), dtype=tf.float32)

# # Create datasets from existing generators
# train_ds = create_tf_dataset_from_generator(
#     train_gen,
#     output_signature=(features_signature, label_signature, weight_signature)
# )

# val_ds = create_tf_dataset_from_generator(
#     val_gen,
#     output_signature=(features_signature, label_signature)
# )

# test_ds = create_tf_dataset_from_generator(
#     test_gen,
#     output_signature=(features_signature, label_signature)
# )

# output_dir = os.path.join('/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling', 'spa...

# # Set up callbacks
# callbacks = [
#     # Stop training when validation performance plateaus
#     tf.keras.callbacks.EarlyStopping(
#         patience=15,
#         restore_best_weights=True,
#         monitor='val_auc',
#         mode='max'
#     ),
#     # Reduce learning rate when improvement slows
#     tf.keras.callbacks.ReduceLROnPlateau(
#         factor=0.5,
#         patience=7,
#         min_lr=1e-6,
#         monitor='val_auc',
#         mode='max'
#     ),
#     # Manual garbage collection after each epoch
#     tf.keras.callbacks.LambdaCallback(
#         on_epoch_end=lambda epoch, logs: gc.collect()
#     )
# ]

# # Add additional callbacks if output directory provided
# if output_dir:
#     callbacks.extend([
#         # Save best model
#         tf.keras.callbacks.ModelCheckpoint(
#             os.path.join(output_dir, 'model_checkpoint.h5'),
#             save_best_only=True,
#             monitor='val_auc',
#             mode='max'
#         ),
#         # Log training progress to CSV
#         tf.keras.callbacks.CSVLogger(
#             os.path.join(output_dir, 'training_log.csv'),
#             append=True
#         )
#     ])

# def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
#     """
#     Build the advanced zero curtain detection model.
#     """
#     # This is your existing model building function
#     from tensorflow.keras.models import Model
#     from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
#     from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPoo...
#     from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
#     from tensorflow.keras.optimizers import Adam
    
#     # Input layer
#     inputs = Input(shape=input_shape)
    
#     # Reshape for ConvLSTM (add spatial dimension)
#     # From (sequence_length, features) to (sequence_length, 1, features)
#     x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
#     # ConvLSTM layer to capture spatiotemporal patterns
#     from tensorflow.keras.layers import ConvLSTM2D
#     convlstm = ConvLSTM2D(
#         filters=64,
#         kernel_size=(3, 1),
#         padding='same',
#         return_sequences=True,
#         activation='tanh',
#         recurrent_dropout=0.2
#     )(x)
    
#     # Reshape back to (sequence_length, features)
#     convlstm = Reshape((input_shape[0], 64))(convlstm)
    
#     # Add positional encoding for transformer
#     def positional_encoding(length, depth):
#         """Create positional encoding with correct dimensions"""
#         positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
#         depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
#         angle_rates = 1 / tf.pow(10000.0, depths)
#         angle_rads = positions * angle_rates
        
#         # This creates a tensor of shape (length, depth)
#         # Only use sin to ensure output depth matches input depth
#         pos_encoding = tf.sin(angle_rads)
        
#         # Add batch dimension to match convlstm output format
#         # Result shape will be (1, length, depth)
#         pos_encoding = tf.expand_dims(pos_encoding, 0)
        
#         return pos_encoding
    
#     # Add positional encoding
#     pos_encoding = positional_encoding(input_shape[0], 64)
#     transformer_input = convlstm + pos_encoding
    
#     # Transformer encoder block
#     def transformer_encoder(x, num_heads=8, key_dim=64, ff_dim=128):
#         # Multi-head attention
#         attention_output = MultiHeadAttention(
#             num_heads=num_heads, key_dim=key_dim
#         )(x, x)
        
#         # Skip connection 1
#         x1 = Add()([attention_output, x])
#         x1 = LayerNormalization(epsilon=1e-6)(x1)
        
#         # Feed-forward network
#         ff_output = Dense(ff_dim, activation='relu')(x1)
#         ff_output = Dropout(0.1)(ff_output)
#         ff_output = Dense(64)(ff_output)
        
#         # Skip connection 2
#         x2 = Add()([ff_output, x1])
#         return LayerNormalization(epsilon=1e-6)(x2)
    
#     # Apply transformer encoder
#     transformer_output = transformer_encoder(transformer_input)
    
#     # Parallel CNN paths for multi-scale feature extraction
#     cnn_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(inputs)
#     cnn_1 = BatchNormalization()(cnn_1)
    
#     cnn_2 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(inputs)
#     cnn_2 = BatchNormalization()(cnn_2)
    
#     cnn_3 = Conv1D(filters=32, kernel_size=7, padding='same', activation='relu')(inputs)
#     cnn_3 = BatchNormalization()(cnn_3)
    
#     # Variational Autoencoder components
#     def sampling(args):
#         z_mean, z_log_var = args
#         batch = tf.shape(z_mean)[0]
#         dim = tf.shape(z_mean)[1]
#         epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
#         return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
#     # Global temporal features
#     global_max = GlobalMaxPooling1D()(transformer_output)
#     global_avg = GlobalAveragePooling1D()(transformer_output)
    
#     # VAE encoding
#     z_mean = Dense(32)(Concatenate()([global_max, global_avg]))
#     z_log_var = Dense(32)(Concatenate()([global_max, global_avg]))
#     z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
#     # Combine all features
#     merged_features = Concatenate()(
#         [
#             GlobalMaxPooling1D()(cnn_1),
#             GlobalMaxPooling1D()(cnn_2),
#             GlobalMaxPooling1D()(cnn_3),
#             global_max,
#             global_avg,
#             z
#         ]
#     )
    
#     # Final classification layers
#     x = Dense(128, activation='relu')(merged_features)
#     x = Dropout(0.3)(x)
#     x = BatchNormalization()(x)
#     x = Dense(64, activation='relu')(x)
#     x = Dropout(0.2)(x)
    
#     # Output layer
#     outputs = Dense(1, activation='sigmoid')(x)
    
#     # Create model
#     model = Model(inputs=inputs, outputs=outputs)
    
#     # Add VAE loss
#     kl_loss = -0.5 * tf.reduce_mean(
#         z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
#     )
#     model.add_loss(0.001 * kl_loss)  # Small weight for VAE loss
    
#     # Compile model with appropriate metrics
#     model.compile(
#         optimizer=Adam(learning_rate=0.001),
#         loss='binary_crossentropy',
#         metrics=[
#             'accuracy',
#             tf.keras.metrics.AUC(name='auc'),
#             tf.keras.metrics.Precision(name='precision'),
#             tf.keras.metrics.Recall(name='recall')
#         ]
#     )
    
#     return model

# #Now try a more sophisticated architecture from before
# from tensorflow.keras.optimizers import Adam
# input_shape = X[train_indices[0]].shape
# model = build_advanced_zero_curtain_model(input_shape)

# model.summary()

# from tensorflow.keras.utils import plot_model

# plot_model(model, to_file='zero_curtain_pipeline/modeling/spatial_model/insitu_model_plot.png', sh...
#            show_layer_names=True, expand_nested=True, dpi=300, layer_range=None, show_layer_activa...

# # Custom data generator
# from tensorflow.keras.utils import Sequence

# class DataGenerator(Sequence):
#     def __init__(self, X, y, indices, batch_size=32, shuffle=True, weights=None):
#         self.X = X
#         self.y = y
#         self.indices = indices
#         self.batch_size = batch_size
#         self.shuffle = shuffle
#         self.weights = weights
#         self.on_epoch_end()
        
#     def __len__(self):
#         return int(np.ceil(len(self.indices) / self.batch_size))
        
#     def __getitem__(self, idx):
#         start_idx = idx * self.batch_size
#         end_idx = min((idx + 1) * self.batch_size, len(self.indices))
#         batch_indices = self.indices_array[start_idx:end_idx]
        
#         X_batch = self.X[batch_indices]
#         y_batch = self.y[batch_indices]
        
#         if self.weights is not None:
#             weights_batch = np.array([self.weights[i] for i in range(len(self.indices)) 
#                                      if self.indices[i] in batch_indices])
#             return X_batch, y_batch, weights_batch
#         else:
#             return X_batch, y_batch
        
#     def on_epoch_end(self):
#         self.indices_array = np.array(self.indices)
#         if self.shuffle:
#             np.random.shuffle(self.indices_array)

# # Create generators
# train_gen = DataGenerator(X, y, train_indices, batch_size=1024, shuffle=True, weights=sample_weigh...
# val_gen = DataGenerator(X, y, val_indices, batch_size=1024, shuffle=False)
# test_gen = DataGenerator(X, y, test_indices, batch_size=1024, shuffle=False)

# print(train_gen.X[train_gen.indices[0]].shape)
# print(input_shape)

# def create_tf_dataset_from_generator(data_generator, output_signature, buffer_size=10000):
#     """
#     Create a TensorFlow Dataset from a Keras Sequence generator.
    
#     Parameters:
#     -----------
#     data_generator : Sequence
#         Keras Sequence generator
#     output_signature : tuple
#         Output signature for the dataset
#     buffer_size : int
#         Size of shuffle buffer
        
#     Returns:
#     --------
#     tf.data.Dataset
#         Dataset ready for model training/evaluation
#     """
#     # Define generator function that wraps the Keras Sequence
#     def tf_generator():
#         for batch_index in range(len(data_generator)):
#             batch = data_generator[batch_index]
#             # If batch is a tuple, yield elements individually
#             if isinstance(batch, tuple):
#                 for i in range(len(batch[0])):  # For each example in the batch
#                     # Extract individual items from the batch
#                     if len(batch) == 2:  # (x, y)
#                         yield batch[0][i], batch[1][i]
#                     elif len(batch) == 3:  # (x, y, weights)
#                         yield batch[0][i], batch[1][i], batch[2][i]
    
#     # Create dataset
#     dataset = tf.data.Dataset.from_generator(
#         tf_generator,
#         output_signature=output_signature
#     )
    
#     # Determine if shuffling is needed based on the generator
#     if data_generator.shuffle:
#         dataset = dataset.shuffle(buffer_size)
    
#     # Batch and prefetch
#     dataset = dataset.batch(data_generator.batch_size)
#     dataset = dataset.prefetch(tf.data.AUTOTUNE)
    
#     return dataset

# # Create output signatures
# features_signature = tf.TensorSpec(shape=input_shape, dtype=tf.float32)
# label_signature = tf.TensorSpec(shape=(), dtype=tf.int32)
# weight_signature = tf.TensorSpec(shape=(), dtype=tf.float32)

# # Create datasets from existing generators
# train_ds = create_tf_dataset_from_generator(
#     train_gen,
#     output_signature=(features_signature, label_signature, weight_signature)
# )

# val_ds = create_tf_dataset_from_generator(
#     val_gen,
#     output_signature=(features_signature, label_signature)
# )

# test_ds = create_tf_dataset_from_generator(
#     test_gen,
#     output_signature=(features_signature, label_signature)
# )

# output_dir = os.path.join('/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling', 'spa...

# # Set up callbacks
# callbacks = [
#     # Stop training when validation performance plateaus
#     tf.keras.callbacks.EarlyStopping(
#         patience=15,
#         restore_best_weights=True,
#         monitor='val_auc',
#         mode='max'
#     ),
#     # Reduce learning rate when improvement slows
#     tf.keras.callbacks.ReduceLROnPlateau(
#         factor=0.5,
#         patience=7,
#         min_lr=1e-6,
#         monitor='val_auc',
#         mode='max'
#     ),
#     # Manual garbage collection after each epoch
#     tf.keras.callbacks.LambdaCallback(
#         on_epoch_end=lambda epoch, logs: gc.collect()
#     )
# ]

# # Add additional callbacks if output directory provided
# if output_dir:
#     callbacks.extend([
#         # Save best model
#         tf.keras.callbacks.ModelCheckpoint(
#             os.path.join(output_dir, 'model_checkpoint.h5'),
#             save_best_only=True,
#             monitor='val_auc',
#             mode='max'
#         ),
#         # Log training progress to CSV
#         tf.keras.callbacks.CSVLogger(
#             os.path.join(output_dir, 'training_log.csv'),
#             append=True
#         )
#     ])

# # Train model
# print("Starting model training...")
# epochs = 100

# history = model.fit(
#     train_gen,
#     validation_data=val_gen,
#     epochs=epochs,
#     callbacks=callbacks,
#     class_weight=class_weight,
#     verbose=1,
#     use_multiprocessing=False,
#     workers=1  # Reduce parallel processing
# )

# # Clean up to free memory
# del train_gen, val_gen
# gc.collect()

# Starting model training...
# Epoch 1/100
# 2025-03-11 21:30:35.199106: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to g...
# 2025-03-11 21:30:36.957083: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry....
#  1158/13593 [=>............................] - ETA: 67:48:06 - loss: 1780956.1250 - accuracy: 0.96...

y_pred_prob = np.vstack(all_preds)
y_test = np.concatenate(all_true)
y_pred = (y_pred_prob > 0.5).astype(int).flatten()

# Load training and validation predictions if they exist
train_predictions = np.load('zero_curtain_pipeline/modeling/spatial_model/train_predictions.npy')
val_predictions = np.load('zero_curtain_pipeline/modeling/spatial_model/val_predictions.npy')

# Load indices
train_indices = np.load('zero_curtain_pipeline/modeling/checkpoints/train_indices.npy')
val_indices = np.load('zero_curtain_pipeline/modeling/checkpoints/val_indices.npy')

# Create comparison dataframes for training and validation sets
# (similar to test set processing)

# Combine into a single comparison dataframe with dataset labels
train_df['dataset'] = 'train'
val_df['dataset'] = 'val'
test_df['dataset'] = 'test'

full_comparison_df = pd.concat([train_df, val_df, test_df], ignore_index=True)

# # ALL CODE FOR DEBUGGING

# # data_loader.py
# import os
# import pandas as pd
# import numpy as np
# import pyarrow as pa
# import pyarrow.feather as pf
# import pyarrow.compute as pc
# import pyarrow.dataset as ds
# import gc
# from tqdm.auto import tqdm

# def get_time_range(feather_path):
#     """Get min and max datetime from feather file efficiently"""
#     print("Determining dataset time range")
#     try:
#         # Use PyArrow for efficient metadata access
#         table = pf.read_table(feather_path, columns=['datetime'])
#         datetime_col = table['datetime']
#         min_date = pd.Timestamp(pc.min(datetime_col).as_py())
#         max_date = pd.Timestamp(pc.max(datetime_col).as_py())
#         del table, datetime_col
#         gc.collect()
#     except Exception as e:
#         print(f"Error with PyArrow min/max: {str(e)}")
#         print("Falling back to reading sample rows...")
        
#         # Read just first and last rows after sorting
#         # This is more efficient than reading all data
#         try:
#             dataset = ds.dataset(feather_path, format='feather')
            
#             # Get min date from sorted data (first row)
#             table_min = dataset.to_table(
#                 columns=['datetime'],
#                 filter=None,
#                 use_threads=True,
#                 limit=1
#             )
#             min_date = pd.Timestamp(table_min['datetime'][0].as_py())
            
#             # Get max date from reverse sorted data (first row)
#             # Note: PyArrow Dataset API doesn't support reverse sort directly
#             # So we read all datetime values and find max
#             table_all = dataset.to_table(columns=['datetime'])
#             max_date = pd.Timestamp(pc.max(table_all['datetime']).as_py())
            
#             del table_min, table_all, dataset
#             gc.collect()
#         except Exception as inner_e:
#             print(f"Error with optimized approach: {str(inner_e)}")
#             print("Using full scan method (slow)...")
            
#             # Last resort: read all timestamps in chunks
#             min_date = pd.Timestamp.max
#             max_date = pd.Timestamp.min
            
#             # Open file directly to avoid loading everything
#             table = pf.read_table(feather_path, columns=['datetime'])
#             datetime_values = table['datetime'].to_pandas()
            
#             min_date = datetime_values.min()
#             max_date = datetime_values.max()
            
#             del datetime_values, table
#             gc.collect()
    
#     print(f"Data timespan: {min_date} to {max_date}")
#     return min_date, max_date

# def get_unique_site_depths(feather_path):
#     """Get unique site-depth combinations efficiently"""
#     print("Finding unique site-depth combinations")
#     print(f"Memory before processing: {memory_usage():.1f} MB")
    
#     try:
#         # Use PyArrow for efficient operation
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Read only the columns we need
#         table = dataset.to_table(columns=['source', 'soil_temp_depth'])
        
#         # Convert to pandas
#         df = table.to_pandas()
        
#         # Get valid combinations (where temp_depth is not NaN)
#         valid_df = df.dropna(subset=['soil_temp_depth'])
        
#         # Get unique combinations
#         site_depths = valid_df.drop_duplicates(['source', 'soil_temp_depth'])
        
#         # Clean up
#         del table, df, valid_df, dataset
#         gc.collect()
        
#     except Exception as e:
#         print(f"Error with PyArrow approach: {str(e)}")
#         print("Falling back to chunked pandas approach...")
        
#         # Fallback: Process in chunks
#         chunk_size = 1000000
#         unique_combos = set()
        
#         # Read feather in chunks (this is slower but more robust)
#         with pd.read_feather(feather_path, columns=['source', 'soil_temp_depth']) as reader:
#             for chunk in reader:
#                 # Get valid rows and unique combinations
#                 valid_rows = chunk.dropna(subset=['soil_temp_depth'])
#                 chunk_combinations = set(zip(valid_rows['source'], valid_rows['soil_temp_depth']))
#                 unique_combos.update(chunk_combinations)
                
#                 # Clean up
#                 del chunk, valid_rows, chunk_combinations
#                 gc.collect()
        
#         # Convert to DataFrame
#         site_depths = pd.DataFrame(list(unique_combos), columns=['source', 'soil_temp_depth'])
    
#     print(f"Found {len(site_depths)} unique site-depth combinations")
#     print(f"Memory after processing: {memory_usage():.1f} MB")
    
#     return site_depths[['source', 'soil_temp_depth']]

# def load_site_depth_data(feather_path, site, temp_depth):
#     """Load ONLY data for a specific site and depth using PyArrow filtering"""
#     print(f"Loading data for site: {site}, depth: {temp_depth}")
#     print(f"Memory before loading: {memory_usage():.1f} MB")
    
#     try:
#         # Use PyArrow Dataset API for efficient filtering
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Create filter expressions
#         site_filter = ds.field('source') == site
#         depth_filter = ds.field('soil_temp_depth') == float(temp_depth)
#         combined_filter = site_filter & depth_filter
        
#         # Apply filter and read only filtered data
#         table = dataset.to_table(filter=combined_filter)
#         filtered_df = table.to_pandas()
        
#         # Clean up
#         del table, dataset
#         gc.collect()
        
#     except Exception as e:
#         print(f"Error with PyArrow filtering: {str(e)}")
#         print("Falling back to pandas filtering...")
        
#         # Define chunk size based on available memory
#         available_mem = psutil.virtual_memory().available / (1024**2)  # MB
#         chunk_size = min(500000, max(100000, int(available_mem / 10)))
        
#         filtered_chunks = []
        
#         # Read and filter in chunks
#         for chunk in pd.read_feather(feather_path, columns=None, chunksize=chunk_size):
#             # Filter by site and depth
#             chunk_filtered = chunk[(chunk['source'] == site) & 
#                                    (chunk['soil_temp_depth'] == temp_depth)]
            
#             if len(chunk_filtered) > 0:
#                 filtered_chunks.append(chunk_filtered)
            
#             # Clean up
#             del chunk, chunk_filtered
#             gc.collect()
        
#         # Combine filtered chunks
#         if filtered_chunks:
#             filtered_df = pd.concat(filtered_chunks, ignore_index=True)
#             del filtered_chunks
#         else:
#             filtered_df = pd.DataFrame()
        
#         gc.collect()
    
#     # Ensure datetime is in datetime format
#     if 'datetime' in filtered_df.columns and not pd.api.types.is_datetime64_dtype(filtered_df['dat...
#         filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])
    
#     print(f"Loaded {len(filtered_df)} rows for site-depth")
#     print(f"Memory after loading: {memory_usage():.1f} MB")
    
#     return filtered_df

# def prepare_data_for_deep_learning_efficiently(feather_path, events_df, sequence_length=6, 
#                                                output_dir=None, batch_size=500, start_batch=0):
#     """
#     Memory-efficient version of prepare_data_for_deep_learning that processes
#     site-depths in batches and saves intermediate results without accumulating all data in memory.
#     """
#     import numpy as np
#     from tqdm.auto import tqdm
#     import os
#     import gc
#     import pandas as pd
    
#     print("Preparing data for deep learning model...")
#     print(f"Memory before preparation: {memory_usage():.1f} MB")
    
#     # Ensure datetime columns are proper datetime objects
#     if not pd.api.types.is_datetime64_dtype(events_df['datetime_min']):
#         events_df['datetime_min'] = pd.to_datetime(events_df['datetime_min'], format='mixed')
#     if not pd.api.types.is_datetime64_dtype(events_df['datetime_max']):
#         events_df['datetime_max'] = pd.to_datetime(events_df['datetime_max'], format='mixed')
    
#     # Create a mapping of detected events for labeling
#     print("Creating event mapping...")
#     event_map = {}
#     for _, event in events_df.iterrows():
#         site = event['source']
#         depth = event['soil_temp_depth']
#         start = event['datetime_min']
#         end = event['datetime_max']
        
#         if (site, depth) not in event_map:
#             event_map[(site, depth)] = []
        
#         event_map[(site, depth)].append((start, end))
    
#     # Get site-depth combinations for progress tracking
#     print("Finding unique site-depth combinations")
#     print(f"Memory before processing: {memory_usage():.1f} MB")
    
#     # Use a function that efficiently gets unique site-depths without loading all data
#     site_depths = get_unique_site_depths(feather_path)
#     total_combinations = len(site_depths)
    
#     print(f"Found {total_combinations} unique site-depth combinations")
#     print(f"Memory after processing: {memory_usage():.1f} MB")
#     print(f"Preparing sequences from {total_combinations} site-depth combinations...")
    
#     # Check for existing batch files to support resuming
#     if output_dir is not None:
#         os.makedirs(output_dir, exist_ok=True)
#         import glob
#         existing_batches = glob.glob(os.path.join(output_dir, 'X_batch_*_*.npy'))
#         if existing_batches and start_batch == 0:
#             # Extract batch numbers from filenames
#             batch_ends = [int(os.path.basename(f).split('_')[-1].split('.')[0]) for f in existing_...
#             if batch_ends:
#                 last_processed_batch = max(batch_ends)
#                 # Start from the next batch
#                 start_batch = (last_processed_batch // batch_size) * batch_size + batch_size
#                 print(f"Found existing batch files, resuming from batch {start_batch}")
    
#     # Track total counts for reporting
#     total_sequences = 0
#     total_positive = 0
    
#     # Process in batches to manage memory
#     for batch_start in range(start_batch, total_combinations, batch_size):
#         batch_end = min(batch_start + batch_size, total_combinations)
#         print(f"\nProcessing batch {batch_start+1}-{batch_end}/{total_combinations}")
        
#         # Initialize lists for this batch only
#         batch_features = []
#         batch_labels = []
#         batch_metadata = []
        
#         # Process each site-depth in the batch
#         for i in tqdm(range(batch_start, batch_end), desc="Creating sequences"):
#             site = site_depths.iloc[i]['source']
#             temp_depth = site_depths.iloc[i]['soil_temp_depth']
            
#             # Skip if no events exist for this site-depth
#             if (site, temp_depth) not in event_map and len(event_map) > 0:
#                 continue
            
#             try:
#                 # Load only the data for this site-depth
#                 print(f"Loading data for site: {site}, depth: {temp_depth}")
#                 print(f"Memory before loading: {memory_usage():.1f} MB")
                
#                 group = load_site_depth_data(feather_path, site, temp_depth)
                
#                 print(f"Loaded {len(group)} rows for site-depth")
#                 print(f"Memory after loading: {memory_usage():.1f} MB")
                
#                 if len(group) < sequence_length + 1:
#                     continue
                
#                 # Ensure datetime is in datetime format
#                 if not pd.api.types.is_datetime64_dtype(group['datetime']):
#                     group['datetime'] = pd.to_datetime(group['datetime'], format='mixed')
                
#                 # Sort by time
#                 group = group.sort_values('datetime')
                
#                 # Create feature set
#                 feature_cols = ['soil_temp_standardized']
                
#                 # Calculate gradient features
#                 group['temp_gradient'] = group['soil_temp_standardized'].diff()
#                 feature_cols.append('temp_gradient')
                
#                 # Add soil depth as feature
#                 group['depth_normalized'] = temp_depth / 10.0
#                 feature_cols.append('depth_normalized')
                
#                 # Add soil moisture if available
#                 has_moisture = False
#                 if 'soil_moist_standardized' in group.columns and not group['soil_moist_standardiz...
#                     has_moisture = True
#                     feature_cols.append('soil_moist_standardized')
#                     group['moist_gradient'] = group['soil_moist_standardized'].diff()
#                     feature_cols.append('moist_gradient')
                
#                 # Fill missing values
#                 group[feature_cols] = group[feature_cols].fillna(0)
                
#                 # Create sequences with sliding window
#                 for j in range(len(group) - sequence_length):
#                     # Get time window
#                     start_time = group.iloc[j]['datetime']
#                     end_time = group.iloc[j + sequence_length - 1]['datetime']
                    
#                     # Extract sequence data
#                     sequence = group.iloc[j:j+sequence_length][feature_cols].values
                    
#                     # Check if this sequence overlaps with known zero curtain event
#                     is_zero_curtain = 0
#                     if (site, temp_depth) in event_map:
#                         for event_start, event_end in event_map[(site, temp_depth)]:
#                             # Ensure proper datetime comparison
#                             # Check for significant overlap (at least 50% of sequence)
#                             if (min(end_time, event_end) - max(start_time, event_start)).total_sec...
#                                0.5 * (end_time - start_time).total_seconds():
#                                 is_zero_curtain = 1
#                                 break
                    
#                     # Store features and labels
#                     batch_features.append(sequence)
#                     batch_labels.append(is_zero_curtain)
#                     total_positive += is_zero_curtain
#                     total_sequences += 1
                    
#                     # Store metadata
#                     meta = {
#                         'source': site,
#                         'soil_temp_depth': temp_depth,
#                         'start_time': start_time,
#                         'end_time': end_time,
#                         'latitude': group.iloc[j]['latitude'] if 'latitude' in group.columns else ...
#                         'longitude': group.iloc[j]['longitude'] if 'longitude' in group.columns el...
#                         'has_moisture_data': has_moisture
#                     }
#                     batch_metadata.append(meta)
                
#                 # Clean up to free memory
#                 del group
#                 gc.collect()
                
#             except Exception as e:
#                 print(f"Error processing {site}, depth {temp_depth}: {str(e)}")
#                 import traceback
#                 traceback.print_exc()
#                 continue
        
#         # Save batch results if output directory provided and we have data
#         if output_dir is not None and batch_features:
#             os.makedirs(output_dir, exist_ok=True)
            
#             # Save batch as numpy files
#             batch_X = np.array(batch_features)
#             batch_y = np.array(batch_labels)
            
#             np.save(os.path.join(output_dir, f'X_batch_{batch_start}_{batch_end}.npy'), batch_X)
#             np.save(os.path.join(output_dir, f'y_batch_{batch_start}_{batch_end}.npy'), batch_y)
            
#             # Save metadata as pickle
#             import pickle
#             with open(os.path.join(output_dir, f'metadata_batch_{batch_start}_{batch_end}.pkl'), '...
#                 pickle.dump(batch_metadata, f)
            
#             # Save progress marker
#             with open(os.path.join(output_dir, 'progress.txt'), 'w') as f:
#                 f.write(f"Last processed batch: {batch_start}-{batch_end}\n")
#                 f.write(f"Total sequences: {total_sequences}\n")
#                 f.write(f"Positive examples: {total_positive} ({total_positive/total_sequences*100...
        
#         # Clean up batch variables - this is key for memory efficiency
#         del batch_features, batch_labels, batch_metadata
#         if 'batch_X' in locals(): del batch_X
#         if 'batch_y' in locals(): del batch_y
#         gc.collect()
        
#         print(f"Memory after batch: {memory_usage():.1f} MB")
#         #print(f"Progress: {total_sequences} sequences processed, {total_positive} positive exampl...
#         positive_percentage = (total_positive/total_sequences*100) if total_sequences > 0 else 0
#         print(f"Progress: {total_sequences} sequences processed, {total_positive} positive example...

#     print("Data preparation complete!")
#     print(f"Total sequences: {total_sequences}")
#     #print(f"Positive examples: {total_positive} ({total_positive/total_sequences*100:.1f}% if tot...
#     positive_percentage = (total_positive/total_sequences*100) if total_sequences > 0 else 0
#     print(f"Positive examples: {total_positive} ({positive_percentage:.1f}%)")
    
#     if output_dir is not None:
#         print(f"Results saved to {output_dir}")
#         print("To merge batch files into final dataset, use merge_batch_files(output_dir)")
    
#     print(f"Memory after preparation: {memory_usage():.1f} MB")
    
#     # Return info instead of data - prevents memory issues
#     return {
#         'total_sequences': total_sequences,
#         'total_positive': total_positive,
#         'positive_percentage': total_positive/total_sequences*100 if total_sequences > 0 else 0,
#         'output_dir': output_dir
#     }


# def merge_batch_files(output_dir):
#     """
#     Merge all batch files into single X_features.npy and y_labels.npy files.
#     """
#     import numpy as np
#     import os
#     import glob
#     import pickle
#     import gc
    
#     print(f"Memory before merging: {memory_usage():.1f} MB")
    
#     # Find all batch files
#     x_batch_files = sorted(glob.glob(os.path.join(output_dir, 'X_batch_*_*.npy')))
#     y_batch_files = sorted(glob.glob(os.path.join(output_dir, 'y_batch_*_*.npy')))
#     metadata_batch_files = sorted(glob.glob(os.path.join(output_dir, 'metadata_batch_*_*.pkl')))
    
#     print(f"Found {len(x_batch_files)} X batches, {len(y_batch_files)} y batches, and {len(metadat...
    
#     # Get dimensions of first batch to initialize arrays
#     if x_batch_files:
#         first_batch = np.load(x_batch_files[0])
#         shape = first_batch.shape
#         del first_batch
#         gc.collect()
        
#         # Count total sequences
#         total_sequences = 0
#         for batch_file in x_batch_files:
#             batch = np.load(batch_file)
#             total_sequences += batch.shape[0]
#             del batch
#             gc.collect()
        
#         # Pre-allocate arrays
#         X = np.zeros((total_sequences, shape[1], shape[2]), dtype=np.float32)
#         y = np.zeros(total_sequences, dtype=np.int32)
        
#         # Load and copy batches
#         idx = 0
#         for i, (x_file, y_file) in enumerate(zip(x_batch_files, y_batch_files)):
#             print(f"Processing batch {i+1}/{len(x_batch_files)}")
#             print(f"Memory: {memory_usage():.1f} MB")
            
#             batch_x = np.load(x_file)
#             batch_y = np.load(y_file)
            
#             batch_size = batch_x.shape[0]
#             X[idx:idx+batch_size] = batch_x
#             y[idx:idx+batch_size] = batch_y
            
#             idx += batch_size
            
#             # Clean up
#             del batch_x, batch_y
#             gc.collect()
        
#         # Save merged X and y
#         print(f"Saving merged arrays: X.shape={X.shape}, y.shape={y.shape}")
#         np.save(os.path.join(output_dir, 'X_features.npy'), X)
#         np.save(os.path.join(output_dir, 'y_labels.npy'), y)
        
#         # Clean up
#         del X, y
#         gc.collect()
        
#         # Load and concatenate metadata batches
#         all_metadata = []
#         for i, batch_file in enumerate(metadata_batch_files):
#             print(f"Processing metadata batch {i+1}/{len(metadata_batch_files)}")
#             print(f"Memory: {memory_usage():.1f} MB")
            
#             with open(batch_file, 'rb') as f:
#                 batch_metadata = pickle.load(f)
#             all_metadata.extend(batch_metadata)
            
#             # Clean up
#             del batch_metadata
#             gc.collect()
        
#         # Save merged metadata
#         print(f"Saving merged metadata: {len(all_metadata)} entries")
#         with open(os.path.join(output_dir, 'metadata.pkl'), 'wb') as f:
#             pickle.dump(all_metadata, f)
        
#         # Final cleanup
#         del all_metadata
#         gc.collect()
        
#         print(f"Memory after merging: {memory_usage():.1f} MB")
#         return {
#             'total_sequences': total_sequences,
#             'output_dir': output_dir
#         }
#     else:
#         print("No batch files found to merge")
#         return None

# def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
#     """
#     Advanced model architecture combining ConvLSTM, Transformers, and 
#     Variational Autoencoder components to better capture complex zero curtain dynamics.
    
#     Parameters:
#     -----------
#     input_shape : tuple
#         Shape of input data (sequence_length, num_features)
#     include_moisture : bool
#         Whether soil moisture features are included
        
#     Returns:
#     --------
#     tensorflow.keras.Model
#         Compiled model ready for training
#     """
#     import os
#     os.environ["DEVICE_COUNT_GPU"] = "0"
#     import tensorflow as tf
#     from tensorflow.keras.models import Model
#     from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
#     from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPoo...
#     from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
#     from tensorflow.keras.optimizers import Adam
    
#     # Input layer
#     inputs = Input(shape=input_shape)
    
#     # Reshape for ConvLSTM (add spatial dimension)
#     # From (sequence_length, features) to (sequence_length, 1, features)
    
#     #x = Reshape((input_shape[0], 1, input_shape[1]))(inputs)
#     x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
#     # ConvLSTM layer to capture spatiotemporal patterns
#     from tensorflow.keras.layers import ConvLSTM2D
#     convlstm = ConvLSTM2D(
#         filters=64,
#         kernel_size=(3, 1),
#         padding='same',
#         return_sequences=True,
#         activation='tanh',
#         recurrent_dropout=0.2
#     )(x)
    
#     # Reshape back to (sequence_length, features)
#     convlstm = Reshape((input_shape[0], 64))(convlstm)
    
#     # Add positional encoding for transformer
#     def positional_encoding(length, depth):
#         positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
#         depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
#         angle_rates = 1 / tf.pow(10000.0, depths)
#         angle_rads = positions * angle_rates
        
#         pos_encoding = tf.concat(
#             [tf.sin(angle_rads), tf.cos(angle_rads)], axis=-1)
        
#         return pos_encoding
    
#     # Add positional encoding
#     pos_encoding = positional_encoding(input_shape[0], 64)
#     transformer_input = convlstm + pos_encoding
    
#     # Transformer encoder block
#     def transformer_encoder(x, num_heads=8, key_dim=64, ff_dim=128):
#         # Multi-head attention
#         attention_output = MultiHeadAttention(
#             num_heads=num_heads, key_dim=key_dim
#         )(x, x)
        
#         # Skip connection 1
#         x1 = Add()([attention_output, x])
#         x1 = LayerNormalization(epsilon=1e-6)(x1)
        
#         # Feed-forward network
#         ff_output = Dense(ff_dim, activation='relu')(x1)
#         ff_output = Dropout(0.1)(ff_output)
#         ff_output = Dense(64)(ff_output)
        
#         # Skip connection 2
#         x2 = Add()([ff_output, x1])
#         return LayerNormalization(epsilon=1e-6)(x2)
    
#     # Apply transformer encoder
#     transformer_output = transformer_encoder(transformer_input)
    
#     # Parallel CNN paths for multi-scale feature extraction
#     cnn_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(inputs)
#     cnn_1 = BatchNormalization()(cnn_1)
    
#     cnn_2 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(inputs)
#     cnn_2 = BatchNormalization()(cnn_2)
    
#     cnn_3 = Conv1D(filters=32, kernel_size=7, padding='same', activation='relu')(inputs)
#     cnn_3 = BatchNormalization()(cnn_3)
    
#     # Variational Autoencoder components
#     def sampling(args):
#         z_mean, z_log_var = args
#         batch = tf.shape(z_mean)[0]
#         dim = tf.shape(z_mean)[1]
#         epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
#         return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
#     # Global temporal features
#     global_max = GlobalMaxPooling1D()(transformer_output)
#     global_avg = GlobalAveragePooling1D()(transformer_output)
    
#     # VAE encoding
#     z_mean = Dense(32)(Concatenate()([global_max, global_avg]))
#     z_log_var = Dense(32)(Concatenate()([global_max, global_avg]))
#     z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
#     # Combine all features
#     merged_features = Concatenate()(
#         [
#             GlobalMaxPooling1D()(cnn_1),
#             GlobalMaxPooling1D()(cnn_2),
#             GlobalMaxPooling1D()(cnn_3),
#             global_max,
#             global_avg,
#             z
#         ]
#     )
    
#     # Final classification layers
#     x = Dense(128, activation='relu')(merged_features)
#     x = Dropout(0.3)(x)
#     x = BatchNormalization()(x)
#     x = Dense(64, activation='relu')(x)
#     x = Dropout(0.2)(x)
    
#     # Output layer
#     outputs = Dense(1, activation='sigmoid')(x)
    
#     # Create model
#     model = Model(inputs=inputs, outputs=outputs)
    
#     # Add VAE loss
#     kl_loss = -0.5 * tf.reduce_mean(
#         z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
#     )
#     model.add_loss(0.001 * kl_loss)  # Small weight for VAE loss
    
#     # Compile model with appropriate metrics
#     model.compile(
#         optimizer=Adam(learning_rate=0.001),
#         loss='binary_crossentropy',
#         metrics=[
#             'accuracy',
#             tf.keras.metrics.AUC(name='auc'),
#             tf.keras.metrics.Precision(name='precision'),
#             tf.keras.metrics.Recall(name='recall')
#         ]
#     )
    
#     return model

# # Time-based split rather than random split
# def temporal_train_test_split(X, y, metadata, val_ratio=0.2, test_ratio=0.1):
#     """
#     Split data temporally for time series modeling.
    
#     Parameters:
#     -----------
#     X : numpy.ndarray
#         Input features
#     y : numpy.ndarray
#         Output labels
#     metadata : list
#         Metadata containing timestamps for each sequence
#     val_ratio : float
#         Proportion of data for validation
#     test_ratio : float
#         Proportion of data for testing
        
#     Returns:
#     --------
#     tuple
#         (X_train, X_val, X_test, y_train, y_val, y_test)
#     """
#     # Extract timestamps from metadata
#     timestamps = [meta['start_time'] for meta in metadata]
    
#     # Sort indices by timestamp
#     sorted_indices = sorted(range(len(timestamps)), key=lambda i: timestamps[i])
    
#     # Calculate split points
#     n_samples = len(sorted_indices)
#     test_start = int(n_samples * (1 - test_ratio))
#     val_start = int(n_samples * (1 - test_ratio - val_ratio))
    
#     # Split indices into train, validation, and test sets
#     train_indices = sorted_indices[:val_start]
#     val_indices = sorted_indices[val_start:test_start]
#     test_indices = sorted_indices[test_start:]
    
#     # Create the splits
#     X_train = X[train_indices]
#     y_train = y[train_indices]
    
#     X_val = X[val_indices]
#     y_val = y[val_indices]
    
#     X_test = X[test_indices]
#     y_test = y[test_indices]
    
#     return X_train, X_val, X_test, y_train, y_val, y_test

# def train_zero_curtain_model_efficiently(X, y, metadata=None, output_dir=None):
#     """
#     Memory-efficient version of train_zero_curtain_model that implements
#     batch training and model checkpointing with temporal data splitting.
    
#     Parameters:
#     -----------
#     X : numpy.ndarray
#         Input features
#     y : numpy.ndarray
#         Output labels
#     metadata : list, optional
#         Metadata about each sequence (must contain timestamps)
#     output_dir : str, optional
#         Directory to save model and results
        
#     Returns:
#     --------
#     tuple
#         (trained_model, training_history, evaluation_results)
#     """
#     import tensorflow as tf
#     from tensorflow.keras.callbacks import (
#         EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, 
#         CSVLogger, TensorBoard
#     )
#     import matplotlib.pyplot as plt
#     import os
#     import gc
#     import numpy as np
    
#     # Enable memory growth to avoid pre-allocating all GPU memory
#     physical_devices = tf.config.list_physical_devices('GPU')
#     if physical_devices:
#         for device in physical_devices:
#             tf.config.experimental.set_memory_growth(device, True)
#             print(f"Enabled memory growth for {device}")
    
#     print("Training zero curtain model...")
#     print(f"Memory before training: {memory_usage():.1f} MB")
    
#     # Create output directory
#     if output_dir:
#         os.makedirs(output_dir, exist_ok=True)
    
#     # Temporal split for time series data
#     print("Performing temporal split for train/validation/test sets...")
#     if metadata is None:
#         raise ValueError("Metadata with timestamps is required for temporal splitting")
    
#     # Extract timestamps from metadata
#     timestamps = np.array([meta['start_time'] for meta in metadata])
    
#     # Sort indices by timestamp
#     sorted_indices = np.argsort(timestamps)
    
#     # Calculate split points (70% train, 15% validation, 15% test)
#     n_samples = len(sorted_indices)
#     test_ratio = 0.15
#     val_ratio = 0.15
    
#     test_start = int(n_samples * (1 - test_ratio))
#     val_start = int(n_samples * (1 - test_ratio - val_ratio))
    
#     # Split indices into train, validation, and test sets
#     train_indices = sorted_indices[:val_start]
#     val_indices = sorted_indices[val_start:test_start]
#     test_indices = sorted_indices[test_start:]
    
#     print(f"Training on data from {timestamps[train_indices[0]]} to {timestamps[train_indices[-1]]...
#     print(f"Validating on data from {timestamps[val_indices[0]]} to {timestamps[val_indices[-1]]}"...
#     print(f"Testing on data from {timestamps[test_indices[0]]} to {timestamps[test_indices[-1]]}")
    
#     # Create the splits
#     X_train = X[train_indices]
#     y_train = y[train_indices]
    
#     X_val = X[val_indices]
#     y_val = y[val_indices]
    
#     X_test = X[test_indices]
#     y_test = y[test_indices]
    
#     print(f"Split sizes: Train={len(X_train)}, Validation={len(X_val)}, Test={len(X_test)}")
    
#     # Check class balance in each split
#     train_pos = np.sum(y_train)
#     val_pos = np.sum(y_val)
#     test_pos = np.sum(y_test)
    
#     print(f"Positive examples: Train={train_pos} ({train_pos/len(y_train)*100:.1f}%), " +
#           f"Val={val_pos} ({val_pos/len(y_val)*100:.1f}%), " +
#           f"Test={test_pos} ({test_pos/len(y_test)*100:.1f}%)")
    
#     # Clean up to free memory
#     del sorted_indices, timestamps
#     gc.collect()
    
#     # Build model with appropriate input shape
#     print("Building model...")
#     input_shape = (X_train.shape[1], X_train.shape[2])
    
#     model = build_advanced_zero_curtain_model(input_shape)
    
#     # If output directory exists, check for existing model checkpoint
#     model_checkpoint_path = None
#     if output_dir:
#         model_checkpoint_path = os.path.join(output_dir, 'checkpoint.h5')
#         if os.path.exists(model_checkpoint_path):
#             print(f"Loading existing model checkpoint from {model_checkpoint_path}")
#             try:
#                 model = tf.keras.models.load_model(model_checkpoint_path)
#                 print("Checkpoint loaded successfully")
#             except Exception as e:
#                 print(f"Error loading checkpoint: {str(e)}")
    
#     # Set up callbacks with additional memory management
#     callbacks = [
#         # Stop early if validation performance plateaus
#         EarlyStopping(
#             patience=15,  # Increased patience for temporal data
#             restore_best_weights=True, 
#             monitor='val_auc', 
#             mode='max'
#         ),
#         # Reduce learning rate when improvement slows
#         ReduceLROnPlateau(
#             factor=0.5, 
#             patience=7,  # Increased patience for temporal data
#             min_lr=1e-6, 
#             monitor='val_auc', 
#             mode='max'
#         ),
#         # Manual garbage collection after each epoch
#         tf.keras.callbacks.LambdaCallback(
#             on_epoch_end=lambda epoch, logs: gc.collect()
#         )
#     ]
    
#     # Add additional callbacks if output directory provided
#     if output_dir:
#         callbacks.extend([
#             # Save best model
#             ModelCheckpoint(
#                 os.path.join(output_dir, 'checkpoint.h5'),
#                 save_best_only=True,
#                 monitor='val_auc',
#                 mode='max'
#             ),
#             # Log training progress to CSV
#             CSVLogger(
#                 os.path.join(output_dir, 'training_log.csv'),
#                 append=True
#             ),
#             # TensorBoard visualization
#             TensorBoard(
#                 log_dir=os.path.join(output_dir, 'tensorboard_logs'),
#                 histogram_freq=1,
#                 profile_batch=0  # Disable profiling to save memory
#             )
#         ])
    
#     # Calculate class weights to handle imbalance
#     pos_weight = len(y_train) / max(sum(y_train), 1)
#     class_weight = {0: 1, 1: pos_weight}
#     print(f"Using class weight {pos_weight:.2f} for positive class")
    
#     # Train model with memory-efficient settings
#     print("Training model...")
#     batch_size = 32  # Adjust based on available memory
#     epochs = 100
    
#     # Use fit with appropriate memory settings
#     history = model.fit(
#         X_train, y_train,
#         validation_data=(X_val, y_val),
#         epochs=epochs,
#         batch_size=batch_size,
#         callbacks=callbacks,
#         class_weight=class_weight,
#         verbose=1,
#         # Memory efficiency settings
#         shuffle=True,  # Still shuffle within the temporal train split
#         use_multiprocessing=False,  # Avoid extra memory overhead
#         workers=1  # Reduce parallel processing to save memory
#     )
    
#     # Clean up to free memory
#     del X_train, y_train, X_val, y_val
#     gc.collect()
    
#     # Evaluate on test set
#     print("Evaluating model on test set...")
#     evaluation = model.evaluate(X_test, y_test, batch_size=batch_size)
#     print("Test performance:")
#     for metric, value in zip(model.metrics_names, evaluation):
#         print(f"  {metric}: {value:.4f}")
    
#     # Generate predictions for visualization and further analysis
#     y_pred_prob = model.predict(X_test, batch_size=batch_size)
#     y_pred = (y_pred_prob > 0.5).astype(int).flatten()
    
#     # Calculate and save additional evaluation metrics
#     from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
#     report = classification_report(y_test, y_pred)
#     conf_matrix = confusion_matrix(y_test, y_pred)
    
#     print("Classification Report:")
#     print(report)
    
#     print("Confusion Matrix:")
#     print(conf_matrix)
    
#     # Plot and save training history
#     if output_dir:
#         # Save evaluation metrics
#         with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w') as f:
#             f.write("Classification Report:\n")
#             f.write(report)
#             f.write("\n\nConfusion Matrix:\n")
#             f.write(str(conf_matrix))
#             f.write("\n\nTest Metrics:\n")
#             for metric, value in zip(model.metrics_names, evaluation):
#                 f.write(f"{metric}: {value:.4f}\n")
        
#         # Plot training history
#         plt.figure(figsize=(16, 6))
        
#         plt.subplot(1, 3, 1)
#         plt.plot(history.history['auc'])
#         plt.plot(history.history['val_auc'])
#         plt.title('Model AUC')
#         plt.ylabel('AUC')
#         plt.xlabel('Epoch')
#         plt.legend(['Train', 'Validation'], loc='lower right')
        
#         plt.subplot(1, 3, 2)
#         plt.plot(history.history['loss'])
#         plt.plot(history.history['val_loss'])
#         plt.title('Model Loss')
#         plt.ylabel('Loss')
#         plt.xlabel('Epoch')
#         plt.legend(['Train', 'Validation'], loc='upper right')
        
#         # Plot ROC curve
#         plt.subplot(1, 3, 3)
#         fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
#         roc_auc = auc(fpr, tpr)
#         plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')
#         plt.plot([0, 1], [0, 1], 'k--')
#         plt.xlabel('False Positive Rate')
#         plt.ylabel('True Positive Rate')
#         plt.title('ROC Curve (Test Set)')
#         plt.legend(loc='lower right')
        
#         plt.tight_layout()
#         plt.savefig(os.path.join(output_dir, 'training_performance.png'), dpi=300)
        
#         # Save detailed model summary
#         from contextlib import redirect_stdout
#         with open(os.path.join(output_dir, 'model_summary.txt'), 'w') as f:
#             with redirect_stdout(f):
#                 model.summary()
    
#     # Clean up to free memory
#     del X_test, y_test, y_pred, y_pred_prob
#     gc.collect()
    
#     print(f"Memory after training: {memory_usage():.1f} MB")
#     return model, history, evaluation

# def run_full_analysis_pipeline(feather_path, output_base_dir='results', batch_size=50):
#     """
#     Run the complete zero curtain analysis pipeline with progress tracking
#     and memory efficiency.
    
#     Parameters:
#     -----------
#     feather_path : str
#         Path to the feather file with merged data
#     output_base_dir : str
#         Base directory for saving outputs
#     batch_size : int
#         Number of site-depths to process per batch
        
#     Returns:
#     --------
#     dict
#         Dictionary containing analysis results
#     """
#     from tqdm.auto import tqdm
#     import time
#     import os
#     import gc
#     import pickle
    
#     # Create output directories
#     os.makedirs(output_base_dir, exist_ok=True)
#     checkpoint_dir = os.path.join(output_base_dir, 'checkpoints')
#     os.makedirs(checkpoint_dir, exist_ok=True)
    
#     # Function to save checkpoint
#     def save_checkpoint(data, name):
#         with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'wb') as f:
#             pickle.dump(data, f)
    
#     # Function to load checkpoint
#     def load_checkpoint(name):
#         try:
#             with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'rb') as f:
#                 return pickle.load(f)
#         except:
#             return None
    
#     # Initialize results
#     results = load_checkpoint('pipeline_results') or {}
    
#     # Check for completed stages
#     completed_stages = set(results.get('completed_stages', []))
#     print(f"Found {len(completed_stages)} completed stages: {completed_stages}")
    
#     # Add a progress indicator for the overall workflow
#     stages = ['Enhanced Detection', 'Data Preparation', 'Model Training', 
#               'Model Application', 'Visualization', 'Comparison']
    
#     with tqdm(total=len(stages), desc="Overall Progress", initial=len(completed_stages)) as pbar:
#         # Stage 1: Enhanced physical detection
#         if 'Enhanced Detection' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 1/{len(stages)}: {stages[0]}")
            
#             print(f"Current memory usage: {memory_usage():.1f} MB")
            
#             # Run memory-efficient detection using your implementation
#             # This part is already implemented in your code
#             #from zero_curtain_pipeline import run_memory_efficient_pipeline
            
#             enhanced_events = run_memory_efficient_pipeline(
#                 feather_path=feather_path,
#                 output_dir=os.path.join(output_base_dir, 'enhanced'),
#                 site_batch_size=batch_size,
#                 checkpoint_interval=5,
#                 max_gap_hours=6,
#                 interpolation_method='cubic'
#             )
            
#             results['enhanced_events'] = enhanced_events
#             results['enhanced_time'] = time.time() - start_time
            
#             # Save progress
#             completed_stages.add('Enhanced Detection')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             # Clean up memory
#             gc.collect()
#             print(f"Memory after Stage 1: {memory_usage():.1f} MB")
#             pbar.update(1)
#         else:
#             # Load enhanced events if needed
#             if 'enhanced_events' not in results:
#                 enhanced_events = load_checkpoint('enhanced_events')
#                 if enhanced_events is None:
#                     # Try loading from CSV
#                     csv_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events.csv'...
#                     if os.path.exists(csv_path):
#                         enhanced_events = pd.read_csv(csv_path, parse_dates=['datetime_min', 'date...
#                     else:
#                         print("Warning: No enhanced events found, cannot proceed with deep learnin...
#                         enhanced_events = pd.DataFrame()
#                 results['enhanced_events'] = enhanced_events
        
#         # Stage 2: Data Preparation for Deep Learning
#         if 'Data Preparation' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 2/{len(stages)}: {stages[1]}")
            
#             # Get enhanced events
#             enhanced_events = results.get('enhanced_events')
#             if enhanced_events is not None and len(enhanced_events) > 0:
#                 try:
#                     # Prepare data for deep learning with memory efficiency
#                     X, y, metadata = prepare_data_for_deep_learning_efficiently(
#                         feather_path=feather_path,
#                         events_df=enhanced_events,
#                         sequence_length=24,  # Use 24 time steps as in your original code
#                         output_dir=os.path.join(output_base_dir, 'ml_data'),
#                         batch_size=batch_size
#                     )
                    
#                     results['X'] = X.shape  # Store only shape to save memory
#                     results['y'] = y.shape
#                     results['data_preparation_time'] = time.time() - start_time
                    
#                     # Clean up to free memory
#                     del X, y
#                     gc.collect()
#                 except Exception as e:
#                     print(f"Error in data preparation: {str(e)}")
#                     results['data_preparation_error'] = str(e)
#             else:
#                 print("Skipping data preparation: No enhanced events available")
            
#             # Save progress
#             completed_stages.add('Data Preparation')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             # Clean up memory
#             gc.collect()
#             print(f"Memory after Stage 2: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 3: Model Training
#         if 'Model Training' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 3/{len(stages)}: {stages[2]}")
            
#             try:
#                 # Load prepared data
#                 data_dir = os.path.join(output_base_dir, 'ml_data')
#                 X = np.load(os.path.join(data_dir, 'X_features.npy'))
#                 y = np.load(os.path.join(data_dir, 'y_labels.npy'))
                
#                 # Load metadata if needed
#                 with open(os.path.join(data_dir, 'metadata.pkl'), 'rb') as f:
#                     metadata = pickle.load(f)
                
#                 # Train model
#                 model, history, evaluation = train_zero_curtain_model_efficiently(
#                     X=X, 
#                     y=y,
#                     metadata=metadata,
#                     output_dir=os.path.join(output_base_dir, 'model')
#                 )
                
#                 # Store minimal results to save memory
#                 results['model_evaluation'] = evaluation
#                 results['model_training_time'] = time.time() - start_time
                
#                 # Clean up to free memory
#                 del X, y, metadata, model, history
#                 gc.collect()
#             except Exception as e:
#                 print(f"Error in model training: {str(e)}")
#                 results['model_training_error'] = str(e)
            
#             # Save progress
#             completed_stages.add('Model Training')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             # Clean up memory
#             gc.collect()
#             print(f"Memory after Stage 3: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 4: Model Application
#         if 'Model Application' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 4/{len(stages)}: {stages[3]}")
            
#             try:
#                 # Load model
#                 model_path = os.path.join(output_base_dir, 'model', 'checkpoint.h5')
#                 if os.path.exists(model_path):
#                     import tensorflow as tf
#                     model = tf.keras.models.load_model(model_path)
                    
#                     # Create directory for predictions
#                     pred_dir = os.path.join(output_base_dir, 'predictions')
#                     os.makedirs(pred_dir, exist_ok=True)
                    
#                     # Apply model with memory efficiency (batched processing)
#                     #from apply_model_efficiently import apply_model_to_new_data_efficiently
                    
#                     predictions = apply_model_to_new_data_efficiently(
#                         model=model,
#                         feather_path=feather_path,
#                         sequence_length=24,
#                         output_dir=pred_dir,
#                         batch_size=batch_size
#                     )
                    
#                     results['model_predictions_count'] = len(predictions)
#                     results['model_application_time'] = time.time() - start_time
                    
#                     # Clean up
#                     del model, predictions
#                     gc.collect()
#                 else:
#                     print("Skipping model application: No model checkpoint found")
#             except Exception as e:
#                 print(f"Error in model application: {str(e)}")
#                 results['model_application_error'] = str(e)
            
#             # Save progress
#             completed_stages.add('Model Application')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             # Clean up memory
#             gc.collect()
#             print(f"Memory after Stage 4: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stages 5 and 6: Visualization and Comparison
#         # (Follow the same pattern - load data, process, clean up memory)
        
#     # Generate final summary report
#     total_time = sum([
#         results.get('enhanced_time', 0),
#         results.get('data_preparation_time', 0),
#         results.get('model_training_time', 0),
#         results.get('model_application_time', 0),
#         results.get('visualization_time', 0),
#         results.get('comparison_time', 0)
#     ])
    
#     print("\n" + "=" * 80)
#     print("ZERO CURTAIN ANALYSIS COMPLETE")
#     print("=" * 80)
#     print(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
    
#     return results

# def apply_model_to_new_data_efficiently(model, feather_path, sequence_length=6, 
#                                         output_dir=None, batch_size=50):
#     """
#     Apply a trained model to detect zero curtain events in new data with memory efficiency.
    
#     Parameters:
#     -----------
#     model : tensorflow.keras.Model
#         Trained zero curtain detection model
#     feather_path : str
#         Path to the feather file
#     sequence_length : int
#         Length of sequences used for model input
#     output_dir : str, optional
#         Output directory for results
#     batch_size : int
#         Number of site-depths to process per batch
        
#     Returns:
#     --------
#     pandas.DataFrame
#         DataFrame with predictions and probabilities
#     """
#     #from data_loader import get_unique_site_depths, load_site_depth_data
#     import numpy as np
#     from tqdm.auto import tqdm
#     import os
#     import gc
#     import pandas as pd
    
#     print("Applying model to new data...")
#     print(f"Memory before application: {memory_usage():.1f} MB")
    
#     # Create output directory
#     if output_dir:
#         os.makedirs(output_dir, exist_ok=True)
    
#     # Get site-depth combinations
#     site_depths = get_unique_site_depths(feather_path)
#     total_combinations = len(site_depths)
#     print(f"Applying model to {total_combinations} site-depth combinations...")
    
#     # Initialize list for all predictions
#     all_predictions = []
    
#     # Process in batches
#     for batch_start in range(0, total_combinations, batch_size):
#         batch_end = min(batch_start + batch_size, total_combinations)
#         print(f"\nProcessing batch {batch_start+1}-{batch_end}/{total_combinations}")
        
#         batch_predictions = []
        
#         # Process each site-depth in batch
#         for i in tqdm(range(batch_start, batch_end), desc="Making predictions"):
#             site = site_depths.iloc[i]['source']
#             temp_depth = site_depths.iloc[i]['soil_temp_depth']
            
#             try:
#                 # Load data for this site-depth
#                 group = load_site_depth_data(feather_path, site, temp_depth)
                
#                 if len(group) < sequence_length + 1:
#                     continue
                
#                 # Sort by time
#                 group = group.sort_values('datetime')
                
#                 # Prepare features (same as in training)
#                 feature_cols = ['soil_temp_standardized']
#                 group['temp_gradient'] = group['soil_temp_standardized'].diff()
#                 feature_cols.append('temp_gradient')
#                 group['depth_normalized'] = temp_depth / 10.0
#                 feature_cols.append('depth_normalized')
                
#                 # Add soil moisture if available
#                 if 'soil_moist_standardized' in group.columns and not group['soil_moist_standardiz...
#                     feature_cols.append('soil_moist_standardized')
#                     group['moist_gradient'] = group['soil_moist_standardized'].diff()
#                     feature_cols.append('moist_gradient')
                
#                 # Fill missing values
#                 group[feature_cols] = group[feature_cols].fillna(0)
                
#                 # Create sequences and predict in mini-batches to save memory
#                 sequences = []
#                 sequence_meta = []
                
#                 for j in range(len(group) - sequence_length):
#                     # Get time window
#                     start_time = group.iloc[j]['datetime']
#                     end_time = group.iloc[j + sequence_length - 1]['datetime']
                    
#                     # Extract sequence
#                     sequence = group.iloc[j:j+sequence_length][feature_cols].values
#                     sequences.append(sequence)
                    
#                     # Store metadata
#                     meta = {
#                         'source': site,
#                         'soil_temp_depth': temp_depth,
#                         'datetime_min': start_time,
#                         'datetime_max': end_time,
#                         'latitude': group.iloc[j]['latitude'] if 'latitude' in group.columns else ...
#                         'longitude': group.iloc[j]['longitude'] if 'longitude' in group.columns el...
#                     }
#                     sequence_meta.append(meta)
                    
#                     # Process in mini-batches of 1000 sequences
#                     if len(sequences) >= 1000:
#                         # Make predictions
#                         X_batch = np.array(sequences)
#                         pred_probs = model.predict(X_batch, verbose=0)
                        
#                         # Store results
#                         for k, prob in enumerate(pred_probs):
#                             meta = sequence_meta[k]
#                             prediction = {
#                                 'source': meta['source'],
#                                 'soil_temp_depth': meta['soil_temp_depth'],
#                                 'datetime_min': meta['datetime_min'],
#                                 'datetime_max': meta['datetime_max'],
#                                 'zero_curtain_probability': float(prob[0]),
#                                 'is_zero_curtain': 1 if prob[0] > 0.5 else 0,
#                                 'latitude': meta['latitude'],
#                                 'longitude': meta['longitude']
#                             }
#                             batch_predictions.append(prediction)
                        
#                         # Clear mini-batch to free memory
#                         sequences = []
#                         sequence_meta = []
                
#                 # Process any remaining sequences
#                 if sequences:
#                     X_batch = np.array(sequences)
#                     pred_probs = model.predict(X_batch, verbose=0)
                    
#                     for k, prob in enumerate(pred_probs):
#                         meta = sequence_meta[k]
#                         prediction = {
#                             'source': meta['source'],
#                             'soil_temp_depth': meta['soil_temp_depth'],
#                             'datetime_min': meta['datetime_min'],
#                             'datetime_max': meta['datetime_max'],
#                             'zero_curtain_probability': float(prob[0]),
#                             'is_zero_curtain': 1 if prob[0] > 0.5 else 0,
#                             'latitude': meta['latitude'],
#                             'longitude': meta['longitude']
#                         }
#                         batch_predictions.append(prediction)
                
#                 # Clean up to free memory
#                 del group, sequences, sequence_meta, X_batch, pred_probs
#                 gc.collect()
                
#             except Exception as e:
#                 print(f"Error processing {site}, depth {temp_depth}: {str(e)}")
#                 continue
        
#         # Add batch predictions to all predictions
#         all_predictions.extend(batch_predictions)
        
#         # Save batch predictions
#         if output_dir and batch_predictions:
#             batch_df = pd.DataFrame(batch_predictions)
#             batch_df.to_csv(os.path.join(output_dir, f'predictions_batch_{batch_start}_{batch_end}...
        
#         # Clear batch to free memory
#         del batch_predictions
#         gc.collect()
        
#         print(f"Memory after batch: {memory_usage():.1f} MB")
    
#     # Consolidate all predictions
#     print(f"Generated {len(all_predictions)} raw predictions")
    
#     # Convert to DataFrame
#     predictions_df = pd.DataFrame(all_predictions)
    
#     # Save all predictions
#     if output_dir and len(predictions_df) > 0:
#         predictions_df.to_csv(os.path.join(output_dir, 'all_predictions.csv'), index=False)
    
#     # Consolidate overlapping events to get final events
#     if len(predictions_df) > 0:
#         print("Consolidating overlapping events...")
#         consolidated_events = consolidate_overlapping_events(predictions_df)
#         print(f"Consolidated into {len(consolidated_events)} events")
        
#         # Save consolidated events
#         if output_dir:
#             consolidated_events.to_csv(os.path.join(output_dir, 'consolidated_events.csv'), index=...
        
#         print(f"Memory after application: {memory_usage():.1f} MB")
#         return consolidated_events
#     else:
#         print("No predictions generated")
#         print(f"Memory after application: {memory_usage():.1f} MB")
#         return pd.DataFrame()

# def consolidate_overlapping_events(predictions_df, probability_threshold=0.5, gap_threshold=6):
#     """
#     Consolidate overlapping zero curtain events from model predictions.
    
#     Parameters:
#     -----------
#     predictions_df : pandas.DataFrame
#         DataFrame with model predictions
#     probability_threshold : float
#         Minimum probability to consider as zero curtain
#     gap_threshold : float
#         Maximum gap in hours to consider events as continuous
        
#     Returns:
#     --------
#     pandas.DataFrame
#         DataFrame with consolidated zero curtain events
#     """
#     import pandas as pd
#     import numpy as np
#     from tqdm.auto import tqdm
    
#     print(f"Consolidating {len(predictions_df)} predictions...")
    
#     # Filter to likely zero curtain events
#     zero_curtain_events = predictions_df[predictions_df['zero_curtain_probability'] >= probability...
    
#     # Ensure datetime columns are datetime type
#     if not pd.api.types.is_datetime64_dtype(zero_curtain_events['datetime_min']):
#         zero_curtain_events['datetime_min'] = pd.to_datetime(zero_curtain_events['datetime_min'])
#     if not pd.api.types.is_datetime64_dtype(zero_curtain_events['datetime_max']):
#         zero_curtain_events['datetime_max'] = pd.to_datetime(zero_curtain_events['datetime_max'])
    
#     # Process each site and depth separately
#     consolidated_events = []
    
#     # Get unique site-depth combinations
#     site_depths = zero_curtain_events[['source', 'soil_temp_depth']].drop_duplicates()
    
#     # Process each site-depth
#     for _, row in tqdm(site_depths.iterrows(), total=len(site_depths), desc="Consolidating events"...
#         site = row['source']
#         depth = row['soil_temp_depth']
        
#         # Get events for this site-depth
#         group = zero_curtain_events[
#             (zero_curtain_events['source'] == site) & 
#             (zero_curtain_events['soil_temp_depth'] == depth)
#         ].sort_values('datetime_min')
        
#         current_event = None
        
#         for _, event in group.iterrows():
#             if current_event is None:
#                 # Start a new event
#                 current_event = {
#                     'source': site,
#                     'soil_temp_depth': depth,
#                     'datetime_min': event['datetime_min'],
#                     'datetime_max': event['datetime_max'],
#                     'zero_curtain_probability': [event['zero_curtain_probability']],
#                     'latitude': event['latitude'],
#                     'longitude': event['longitude']
#                 }
#             else:
#                 # Check if this event overlaps or is close to the current event
#                 time_gap = (event['datetime_min'] - current_event['datetime_max']).total_seconds()...
                
#                 if time_gap <= gap_threshold:
#                     # Extend the current event
#                     current_event['datetime_max'] = max(current_event['datetime_max'], event['date...
#                     current_event['zero_curtain_probability'].append(event['zero_curtain_probabili...
#                 else:
#                     # Finalize the current event
#                     duration_hours = (current_event['datetime_max'] - current_event['datetime_min'...
                    
#                     if duration_hours >= 12:  # Minimum duration threshold
#                         final_event = {
#                             'source': current_event['source'],
#                             'soil_temp_depth': current_event['soil_temp_depth'],
#                             'datetime_min': current_event['datetime_min'],
#                             'datetime_max': current_event['datetime_max'],
#                             'duration_hours': duration_hours,
#                             'zero_curtain_probability': np.mean(current_event['zero_curtain_probab...
#                             'latitude': current_event['latitude'],
#                             'longitude': current_event['longitude']
#                         }
#                         consolidated_events.append(final_event)
                    
#                     # Start a new event
#                     current_event = {
#                         'source': site,
#                         'soil_temp_depth': depth,
#                         'datetime_min': event['datetime_min'],
#                         'datetime_max': event['datetime_max'],
#                         'zero_curtain_probability': [event['zero_curtain_probability']],
#                         'latitude': event['latitude'],
#                         'longitude': event['longitude']
#                     }
        
#         # Handle the last event
#         if current_event is not None:
#             duration_hours = (current_event['datetime_max'] - current_event['datetime_min']).total...
            
#             if duration_hours >= 12:  # Minimum duration threshold
#                 final_event = {
#                     'source': current_event['source'],
#                     'soil_temp_depth': current_event['soil_temp_depth'],
#                     'datetime_min': current_event['datetime_min'],
#                     'datetime_max': current_event['datetime_max'],
#                     'duration_hours': duration_hours,
#                     'zero_curtain_probability': np.mean(current_event['zero_curtain_probability'])...
#                     'latitude': current_event['latitude'],
#                     'longitude': current_event['longitude']
#                 }
#                 consolidated_events.append(final_event)
    
#     # Convert to DataFrame
#     consolidated_df = pd.DataFrame(consolidated_events)
    
#     # Add region and latitude band classifications if latitude is available
#     if len(consolidated_df) > 0 and 'latitude' in consolidated_df.columns:
#         # Add region classification
#         def assign_region(lat):
#             if lat is None or pd.isna(lat):
#                 return None
#             elif lat >= 66.5:
#                 return 'Arctic'
#             elif lat >= 60:
#                 return 'Subarctic'
#             elif lat >= 50:
#                 return 'Northern Boreal'
#             else:
#                 return 'Other'
        
#         consolidated_df['region'] = consolidated_df['latitude'].apply(assign_region)
        
#         # Add latitude band
#         def assign_lat_band(lat):
#             if lat is None or pd.isna(lat):
#                 return None
#             elif lat < 55:
#                 return '<55°N'
#             elif lat < 60:
#                 return '55-60°N'
#             elif lat < 66.5:
#                 return '60-66.5°N'
#             elif lat < 70:
#                 return '66.5-70°N'
#             elif lat < 75:
#                 return '70-75°N'
#             elif lat < 80:
#                 return '75-80°N'
#             else:
#                 return '>80°N'
        
#         consolidated_df['lat_band'] = consolidated_df['latitude'].apply(assign_lat_band)
    
#     return consolidated_df

# def visualize_events_efficiently(events_df, output_file=None):
#     """
#     Create visualizations for zero curtain events with memory efficiency.
    
#     Parameters:
#     -----------
#     events_df : pandas.DataFrame
#         DataFrame containing zero curtain events
#     output_file : str, optional
#         Path to save the visualization
        
#     Returns:
#     --------
#     dict
#         Statistics about the visualized events
#     """
#     import matplotlib.pyplot as plt
#     import cartopy.crs as ccrs
#     import cartopy.feature as cfeature
#     import numpy as np
#     from matplotlib.colors import PowerNorm
#     import gc
    
#     print(f"Creating visualization for {len(events_df)} events...")
#     print(f"Memory before visualization: {memory_usage():.1f} MB")
    
#     # Calculate percentile boundaries for better scaling
#     p10 = np.percentile(events_df['duration_hours'], 10)
#     p25 = np.percentile(events_df['duration_hours'], 25)
#     p50 = np.percentile(events_df['duration_hours'], 50)  # median
#     p75 = np.percentile(events_df['duration_hours'], 75)
#     p90 = np.percentile(events_df['duration_hours'], 90)
    
#     # Aggregate by site to reduce memory usage and plotting overhead
#     site_data = events_df.groupby(['source', 'latitude', 'longitude']).agg({
#         'duration_hours': ['count', 'mean', 'median', 'min', 'max']
#     }).reset_index()
    
#     # Flatten column names
#     site_data.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col 
#                         for col in site_data.columns]
    
#     # Create figure
#     fig, axes = plt.subplots(1, 2, figsize=(14, 7), 
#                            subplot_kw={'projection': ccrs.NorthPolarStereo()})
    
#     # Set map features
#     for ax in axes:
#         ax.set_extent([-180, 180, 45, 90], ccrs.PlateCarree())
#         ax.add_feature(cfeature.LAND, facecolor='whitesmoke')
#         ax.add_feature(cfeature.OCEAN, facecolor='aliceblue')
#         ax.add_feature(cfeature.COASTLINE, linewidth=0.5)
        
#         # Add Arctic Circle with label
#         ax.plot(
#             np.linspace(-180, 180, 60),
#             np.ones(60) * 66.5,
#             transform=ccrs.PlateCarree(),
#             linestyle='-',
#             color='gray',
#             linewidth=1.0,
#             alpha=0.7
#         )
        
#         ax.text(
#             0, 66.5 + 2,
#             "Arctic Circle",
#             transform=ccrs.PlateCarree(),
#             horizontalalignment='center',
#             verticalalignment='bottom',
#             fontsize=9,
#             bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.2')
#         )
    
#     # Plot 1: Event count
#     count_max = site_data['duration_hours_count'].quantile(0.95)
#     scatter1 = axes[0].scatter(
#         site_data['longitude'],
#         site_data['latitude'],
#         transform=ccrs.PlateCarree(),
#         c=site_data['duration_hours_count'],
#         s=30,
#         cmap='viridis',
#         vmin=1,
#         vmax=count_max,
#         alpha=0.8,
#         edgecolor='none'
#     )
#     plt.colorbar(scatter1, ax=axes[0], shrink=0.7, pad=0.05, label='Event Count')
#     axes[0].set_title('Zero Curtain Event Count', fontsize=12)
    
#     # Plot 2: Mean duration using percentile bounds
#     lower_bound = p10
#     upper_bound = p90
    
#     # Non-linear scaling for better color differentiation
#     scatter2 = axes[1].scatter(
#         site_data['longitude'],
#         site_data['latitude'],
#         transform=ccrs.PlateCarree(),
#         c=site_data['duration_hours_mean'],
#         s=30,
#         cmap='RdYlBu_r',
#         norm=PowerNorm(gamma=0.7, vmin=lower_bound, vmax=upper_bound),
#         alpha=0.8,
#         edgecolor='none'
#     )
    
#     # Create better colorbar with percentile markers
#     cbar = plt.colorbar(scatter2, ax=axes[1], shrink=0.7, pad=0.05, 
#                        label='Mean Duration (hours)')
    
#     # Show percentile ticks
#     percentile_ticks = [p10, p25, p50, p75, p90]
#     cbar.set_ticks(percentile_ticks)
#     cbar.set_ticklabels([f"{h:.0f}h\n({h/24:.1f}d)" for h in percentile_ticks])
    
#     axes[1].set_title('Mean Zero Curtain Duration', fontsize=12)
    
#     # Add comprehensive title with statistics
#     plt.suptitle(
#         f'Zero Curtain Analysis: {len(site_data)} Sites, {len(events_df)} Events\n' +
#         f'Duration: median={p50:.1f}h ({p50/24:.1f}d), 10-90%={p10:.1f}-{p90:.1f}h',
#         fontsize=14
#     )
    
#     plt.tight_layout(rect=[0, 0, 1, 0.93])
    
#     # Save if requested
#     if output_file:
#         plt.savefig(output_file, dpi=200, bbox_inches='tight')
#         print(f"Visualization saved to {output_file}")
    
#     # Clean up to free memory
#     plt.close(fig)
#     del site_data, fig, axes
#     gc.collect()
    
#     print(f"Memory after visualization: {memory_usage():.1f} MB")
    
#     # Return statistics
#     return {
#         'p10': p10,
#         'p25': p25,
#         'p50': p50,
#         'p75': p75,
#         'p90': p90,
#         'mean': events_df['duration_hours'].mean(),
#         'std': events_df['duration_hours'].std(),
#         'min': events_df['duration_hours'].min(),
#         'max': events_df['duration_hours'].max()
#     }

# def compare_detection_methods_efficiently(physical_events_file, model_events_file, output_dir=None...
#     """
#     Compare zero curtain events detected by different methods with memory efficiency.
    
#     Parameters:
#     -----------
#     physical_events_file : str
#         Path to CSV file with events detected by the physics-based method
#     model_events_file : str
#         Path to CSV file with events detected by the deep learning model
#     output_dir : str, optional
#         Directory to save comparison results
        
#     Returns:
#     --------
#     dict
#         Comparison statistics and metrics
#     """
#     import pandas as pd
#     import numpy as np
#     import matplotlib.pyplot as plt
#     import seaborn as sns
#     from datetime import timedelta
#     import os
#     import gc
    
#     print("Comparing detection methods...")
#     print(f"Memory before comparison: {memory_usage():.1f} MB")
    
#     # Load events
#     physical_events = pd.read_csv(physical_events_file, parse_dates=['datetime_min', 'datetime_max...
#     model_events = pd.read_csv(model_events_file, parse_dates=['datetime_min', 'datetime_max'])
    
#     # Calculate basic statistics for each method
#     physical_stats = {
#         'total_events': len(physical_events),
#         'unique_sites': physical_events['source'].nunique(),
#         'median_duration': physical_events['duration_hours'].median(),
#         'mean_duration': physical_events['duration_hours'].mean()
#     }
    
#     model_stats = {
#         'total_events': len(model_events),
#         'unique_sites': model_events['source'].nunique(),
#         'median_duration': model_events['duration_hours'].median(),
#         'mean_duration': model_events['duration_hours'].mean()
#     }
    
#     # Create a site-day matching table for overlap analysis
#     # Process in batches to save memory
#     physical_days = set()
#     model_days = set()
    
#     # Process physical events in batches
#     batch_size = 1000
#     for i in range(0, len(physical_events), batch_size):
#         batch = physical_events.iloc[i:i+batch_size]
        
#         for _, event in batch.iterrows():
#             site = event['source']
#             depth = event['soil_temp_depth']
#             start_day = event['datetime_min'].date()
#             end_day = event['datetime_max'].date()
            
#             # Add each day of the event
#             current_day = start_day
#             while current_day <= end_day:
#                 physical_days.add((site, depth, current_day))
#                 current_day += timedelta(days=1)
        
#         # Clear batch to free memory
#         del batch
#         gc.collect()
    
#     # Process model events in batches
#     for i in range(0, len(model_events), batch_size):
#         batch = model_events.iloc[i:i+batch_size]
        
#         for _, event in batch.iterrows():
#             site = event['source']
#             depth = event['soil_temp_depth']
#             start_day = event['datetime_min'].date()
#             end_day = event['datetime_max'].date()
            
#             # Add each day of the event
#             current_day = start_day
#             while current_day <= end_day:
#                 model_days.add((site, depth, current_day))
#                 current_day += timedelta(days=1)
        
#         # Clear batch to free memory
#         del batch
#         gc.collect()
    
#     # Calculate overlap metrics
#     overlap_days = physical_days.intersection(model_days)
    
#     overlap_metrics = {
#         'physical_only_days': len(physical_days - model_days),
#         'model_only_days': len(model_days - physical_days),
#         'overlap_days': len(overlap_days),
#         'jaccard_index': len(overlap_days) / len(physical_days.union(model_days)) if len(physical_...
#     }
    
#     # Print comparison results
#     print("\n=== DETECTION METHOD COMPARISON ===\n")
    
#     print("Physics-based Detection:")
#     print(f"  Total Events: {physical_stats['total_events']}")
#     print(f"  Unique Sites: {physical_stats['unique_sites']}")
#     print(f"  Median Duration: {physical_stats['median_duration']:.1f} hours ({physical_stats['med...
#     print(f"  Mean Duration: {physical_stats['mean_duration']:.1f} hours ({physical_stats['mean_du...
    
#     print("\nDeep Learning Model Detection:")
#     print(f"  Total Events: {model_stats['total_events']}")
#     print(f"  Unique Sites: {model_stats['unique_sites']}")
#     print(f"  Median Duration: {model_stats['median_duration']:.1f} hours ({model_stats['median_du...
#     print(f"  Mean Duration: {model_stats['mean_duration']:.1f} hours ({model_stats['mean_duration...
    
#     print("\nOverlap Analysis:")
#     print(f"  Days with Events (Physics-based): {len(physical_days)}")
#     print(f"  Days with Events (Deep Learning): {len(model_days)}")
#     print(f"  Days detected by both methods: {overlap_metrics['overlap_days']}")
#     print(f"  Days detected only by Physics-based: {overlap_metrics['physical_only_days']}")
#     print(f"  Days detected only by Deep Learning: {overlap_metrics['model_only_days']}")
#     print(f"  Jaccard Index (overlap): {overlap_metrics['jaccard_index']:.4f}")
    
#     # Generate comparison visualizations
#     if output_dir:
#         os.makedirs(output_dir, exist_ok=True)
        
#         # Create a Venn diagram of detection overlap
#         try:
#             from matplotlib_venn import venn2
            
#             plt.figure(figsize=(8, 6))
#             venn2(subsets=(len(physical_days - model_days), 
#                           len(model_days - physical_days), 
#                           len(overlap_days)),
#                  set_labels=('Physics-based', 'Deep Learning'))
#             plt.title('Overlap between Detection Methods', fontsize=14)
#             plt.savefig(os.path.join(output_dir, 'detection_overlap.png'), dpi=200, bbox_inches='t...
#             plt.close()
#         except ImportError:
#             print("matplotlib_venn not installed. Skipping Venn diagram.")
        
#         # Compare duration distributions
#         plt.figure(figsize=(10, 6))
        
#         sns.histplot(physical_events['duration_hours'], kde=True, alpha=0.5, 
#                     label='Physics-based', color='blue', bins=50)
#         sns.histplot(model_events['duration_hours'], kde=True, alpha=0.5, 
#                     label='Deep Learning', color='red', bins=50)
        
#         plt.xlabel('Duration (hours)')
#         plt.ylabel('Frequency')
#         plt.title('Comparison of Zero Curtain Duration Distributions')
#         plt.legend()
#         plt.grid(alpha=0.3)
        
#         plt.savefig(os.path.join(output_dir, 'duration_comparison.png'), dpi=200, bbox_inches='tig...
#         plt.close()
    
#     # Clean up to free memory
#     del physical_events, model_events, physical_days, model_days, overlap_days
#     gc.collect()
    
#     print(f"Memory after comparison: {memory_usage():.1f} MB")
    
#     comparison_results = {
#         'physical_stats': physical_stats,
#         'model_stats': model_stats,
#         'overlap_metrics': overlap_metrics
#     }
    
#     return comparison_results

# def run_complete_pipeline(feather_path, output_base_dir='results'):
#     """
#     Run the complete zero curtain analysis pipeline with memory efficiency.
    
#     Parameters:
#     -----------
#     feather_path : str
#         Path to the feather file
#     output_base_dir : str
#         Base directory for outputs
        
#     Returns:
#     --------
#     dict
#         Summary of results
#     """
#     import os
#     import time
#     import gc
#     import pickle
#     from tqdm.auto import tqdm
    
#     # Create output directories
#     os.makedirs(output_base_dir, exist_ok=True)
#     checkpoint_dir = os.path.join(output_base_dir, 'checkpoints')
#     os.makedirs(checkpoint_dir, exist_ok=True)
    
#     # Function to save/load checkpoint
#     def save_checkpoint(data, name):
#         with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'wb') as f:
#             pickle.dump(data, f)
    
#     def load_checkpoint(name):
#         try:
#             with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'rb') as f:
#                 return pickle.load(f)
#         except:
#             return None
    
#     # Initialize results
#     results = load_checkpoint('pipeline_results') or {}
    
#     # Check for completed stages
#     completed_stages = set(results.get('completed_stages', []))
#     print(f"Found {len(completed_stages)} completed stages: {completed_stages}")
    
#     # Define stages
#     stages = ['Zero Curtain Detection', 'Data Preparation', 'Model Training', 
#               'Model Application', 'Visualization', 'Comparison']
    
#     # Overall progress bar
#     with tqdm(total=len(stages), desc="Overall Progress", initial=len(completed_stages)) as pbar:
#         # Stage 1: Zero Curtain Detection
#         if 'Zero Curtain Detection' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 1/{len(stages)}: {stages[0]}")
            
#             #from zero_curtain_pipeline import run_memory_efficient_pipeline
            
#             enhanced_events = run_memory_efficient_pipeline(
#                 feather_path=feather_path,
#                 output_dir=os.path.join(output_base_dir, 'enhanced'),
#                 site_batch_size=20,
#                 checkpoint_interval=5,
#                 max_gap_hours=6,
#                 interpolation_method='cubic'
#             )
            
#             results['enhanced_events_count'] = len(enhanced_events)
#             results['enhanced_time'] = time.time() - start_time
            
#             # Save checkpoint
#             completed_stages.add('Zero Curtain Detection')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             # Free memory
#             del enhanced_events
#             gc.collect()
            
#             print(f"Memory after stage 1: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 2: Data Preparation
#         if 'Data Preparation' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 2/{len(stages)}: {stages[1]}")
            
#             # Load events
#             enhanced_events_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events....
#             if os.path.exists(enhanced_events_path):
#                 import pandas as pd
#                 enhanced_events = pd.read_csv(enhanced_events_path, parse_dates=['datetime_min', '...
                
#                 # Prepare data for model
#                 X, y, metadata = prepare_data_for_deep_learning_efficiently(
#                     feather_path=feather_path,
#                     events_df=enhanced_events,
#                     sequence_length=24,
#                     output_dir=os.path.join(output_base_dir, 'ml_data'),
#                     batch_size=20
#                 )
                
#                 results['data_prep_time'] = time.time() - start_time
#                 results['data_shape'] = X.shape
#                 results['positive_examples'] = int(sum(y))
#                 results['positive_percentage'] = float(sum(y)/len(y)*100)
                
#                 # Clean up
#                 del X, y, metadata, enhanced_events
#                 gc.collect()
#             else:
#                 print("No enhanced events file found, cannot proceed with data preparation")
#                 results['data_prep_error'] = "No enhanced events file found"
            
#             # Save checkpoint
#             completed_stages.add('Data Preparation')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             print(f"Memory after stage 2: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 3: Model Training
#         if 'Model Training' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 3/{len(stages)}: {stages[2]}")
            
#             ml_data_dir = os.path.join(output_base_dir, 'ml_data')
#             x_path = os.path.join(ml_data_dir, 'X_features.npy')
#             y_path = os.path.join(ml_data_dir, 'y_labels.npy')
            
#             if os.path.exists(x_path) and os.path.exists(y_path):
#                 import numpy as np
#                 X = np.load(x_path)
#                 y = np.load(y_path)
                
#                 # Train model
#                 model, history, evaluation = train_zero_curtain_model_efficiently(
#                     X=X,
#                     y=y,
#                     output_dir=os.path.join(output_base_dir, 'model')
#                 )
                
#                 results['model_training_time'] = time.time() - start_time
#                 results['model_evaluation'] = evaluation
                
#                 # Clean up
#                 del X, y, model, history, evaluation
#                 gc.collect()
#             else:
#                 print("No prepared data found, cannot proceed with model training")
#                 results['model_training_error'] = "No prepared data found"
            
#             # Save checkpoint
#             completed_stages.add('Model Training')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             print(f"Memory after stage 3: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 4: Model Application
#         if 'Model Application' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 4/{len(stages)}: {stages[3]}")
            
#             model_path = os.path.join(output_base_dir, 'model', 'checkpoint.h5')
            
#             if os.path.exists(model_path):
#                 import tensorflow as tf
#                 model = tf.keras.models.load_model(model_path)
                
#                 # Apply model
#                 predictions = apply_model_to_new_data_efficiently(
#                     model=model,
#                     feather_path=feather_path,
#                     sequence_length=24,
#                     output_dir=os.path.join(output_base_dir, 'predictions'),
#                     batch_size=20
#                 )
                
#                 results['model_application_time'] = time.time() - start_time
#                 results['predictions_count'] = len(predictions)
                
#                 # Clean up
#                 del model, predictions
#                 gc.collect()
#             else:
#                 print("No model checkpoint found, cannot proceed with model application")
#                 results['model_application_error'] = "No model checkpoint found"
            
#             # Save checkpoint
#             completed_stages.add('Model Application')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             print(f"Memory after stage 4: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 5: Visualization
#         if 'Visualization' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 5/{len(stages)}: {stages[4]}")
            
#             import pandas as pd
            
#             # Visualize enhanced events
#             enhanced_events_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events....
#             if os.path.exists(enhanced_events_path):
#                 enhanced_events = pd.read_csv(enhanced_events_path, parse_dates=['datetime_min', '...
                
#                 enhanced_stats = visualize_events_efficiently(
#                     events_df=enhanced_events,
#                     output_file=os.path.join(output_base_dir, 'enhanced_visualization.png')
#                 )
                
#                 results['enhanced_visualization_stats'] = enhanced_stats
                
#                 # Clean up
#                 del enhanced_events, enhanced_stats
#                 gc.collect()
            
#             # Visualize model predictions
#             predictions_path = os.path.join(output_base_dir, 'predictions', 'consolidated_events.c...
#             if os.path.exists(predictions_path):
#                 model_events = pd.read_csv(predictions_path, parse_dates=['datetime_min', 'datetim...
                
#                 model_stats = visualize_events_efficiently(
#                     events_df=model_events,
#                     output_file=os.path.join(output_base_dir, 'model_visualization.png')
#                 )
                
#                 results['model_visualization_stats'] = model_stats
                
#                 # Clean up
#                 del model_events, model_stats
#                 gc.collect()
            
#             results['visualization_time'] = time.time() - start_time
            
#             # Save checkpoint
#             completed_stages.add('Visualization')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             print(f"Memory after stage 5: {memory_usage():.1f} MB")
#             pbar.update(1)
        
#         # Stage 6: Comparison
#         if 'Comparison' not in completed_stages:
#             start_time = time.time()
#             pbar.set_description(f"Stage 6/{len(stages)}: {stages[5]}")
            
#             enhanced_events_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events....
#             predictions_path = os.path.join(output_base_dir, 'predictions', 'consolidated_events.c...
            
#             if os.path.exists(enhanced_events_path) and os.path.exists(predictions_path):
#                 comparison_results = compare_detection_methods_efficiently(
#                     physical_events_file=enhanced_events_path,
#                     model_events_file=predictions_path,
#                     output_dir=os.path.join(output_base_dir, 'comparison')
#                 )
                
#                 results['comparison'] = comparison_results
#             else:
#                 print("Missing events files, cannot perform comparison")
#                 results['comparison_error'] = "Missing events files"
            
#             results['comparison_time'] = time.time() - start_time
            
#             # Save checkpoint
#             completed_stages.add('Comparison')
#             results['completed_stages'] = list(completed_stages)
#             save_checkpoint(results, 'pipeline_results')
            
#             print(f"Memory after stage 6: {memory_usage():.1f} MB")
#             pbar.update(1)
    
#     # Generate summary report
#     total_time = (
#         results.get('enhanced_time', 0) +
#         results.get('data_prep_time', 0) +
#         results.get('model_training_time', 0) +
#         results.get('model_application_time', 0) +
#         results.get('visualization_time', 0) +
#         results.get('comparison_time', 0)
#     )
    
#     # Save summary to file
#     with open(os.path.join(output_base_dir, 'summary.txt'), 'w') as f:
#         f.write("ZERO CURTAIN ANALYSIS SUMMARY\n")
#         f.write("=" * 30 + "\n\n")
        
#         f.write(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\n\n")
        
#         f.write("STAGE TIMINGS:\n")
#         f.write(f"  Zero Curtain Detection: {results.get('enhanced_time', 0):.2f} seconds\n")
#         f.write(f"  Data Preparation: {results.get('data_prep_time', 0):.2f} seconds\n")
#         f.write(f"  Model Training: {results.get('model_training_time', 0):.2f} seconds\n")
#         f.write(f"  Model Application: {results.get('model_application_time', 0):.2f} seconds\n")
#         f.write(f"  Visualization: {results.get('visualization_time', 0):.2f} seconds\n")
#         f.write(f"  Comparison: {results.get('comparison_time', 0):.2f} seconds\n\n")
        
#         f.write("RESULTS SUMMARY:\n")
#         f.write(f"  Enhanced Detection: {results.get('enhanced_events_count', 0)} events\n")
#         f.write(f"  Model Predictions: {results.get('predictions_count', 0)} events\n")
        
#         if 'comparison' in results and 'overlap_metrics' in results['comparison']:
#             overlap = results['comparison']['overlap_metrics']['jaccard_index']
#             f.write(f"  Method Agreement: {overlap*100:.1f}% overlap\n")
    
#     print("\n" + "=" * 80)
#     print("ZERO CURTAIN ANALYSIS COMPLETE")
#     print("=" * 80)
#     print(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
#     print(f"Results saved to {output_base_dir}")
    
#     return results
    
# import os
# import numpy as np
# import pandas as pd
# import gc
# import pickle
# import psutil
# from datetime import datetime, timedelta
# import time
# from scipy.interpolate import interp1d

# def memory_usage():
#     """Monitor memory usage in MB"""
#     process = psutil.Process(os.getpid())
#     mem_info = process.memory_info()
#     return mem_info.rss / 1024**2  # Memory in MB

# def save_checkpoint(data, checkpoint_dir, name):
#     """Save checkpoint data to pickle file"""
#     os.makedirs(checkpoint_dir, exist_ok=True)
#     checkpoint_path = os.path.join(checkpoint_dir, f'{name}.pkl')
#     with open(checkpoint_path, 'wb') as f:
#         pickle.dump(data, f)
#     print(f"Saved checkpoint to {checkpoint_path}")

# def load_checkpoint(checkpoint_dir, name):
#     """Load checkpoint data from pickle file"""
#     checkpoint_path = os.path.join(checkpoint_dir, f'{name}.pkl')
#     try:
#         with open(checkpoint_path, 'rb') as f:
#             data = pickle.load(f)
#         print(f"Loaded checkpoint from {checkpoint_path}")
#         return data
#     except:
#         print(f"No checkpoint found at {checkpoint_path}")
#         return None

# result = prepare_data_for_deep_learning_efficiently(
#     feather_path='merged_compressed.feather',
#     events_df=pd.read_csv('zero_curtain_pipeline/zero_curtain_events.csv', parse_dates=['datetime_...
#     sequence_length=6,
#     output_dir='zero_curtain_pipeline/modeling/ml_data',
#     batch_size=50
# )

# merge_result = merge_batch_files('zero_curtain_pipeline/modeling/ml_data')

# output_base_dir = '/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling'
# data_dir = os.path.join(output_base_dir, 'ml_data')
# X = np.load(os.path.join(data_dir, 'X_features.npy'))
# y = np.load(os.path.join(data_dir, 'y_labels.npy'))
# with open(os.path.join(data_dir, 'metadata.pkl'), 'rb') as f:
#     metadata = pickle.load(f)
# # y labels are binary, i.e., 0 is False for has_moisture_data and 1 is True for has_moisture_data

# import os
# import gc
# import json
# import pickle
# import numpy as np
# from scipy.spatial import cKDTree
# from tqdm import tqdm
# import psutil
# import threading
# import time
# from contextlib import contextmanager
# from joblib import Parallel, delayed

# # Configuration parameters
# CHECKPOINT_DIR = "zero_curtain_pipeline/modeling/checkpoints"
# MEMORY_THRESHOLD = 75  # Memory usage percentage threshold
# METADATA_SAVE_FREQUENCY = 5  # Save metadata every N batches
# MONITOR_INTERVAL = 10  # Memory monitoring interval in seconds

# # Create checkpoint directory
# os.makedirs(CHECKPOINT_DIR, exist_ok=True)
# CHECKPOINT_BASE = os.path.join(CHECKPOINT_DIR, "geo_density_checkpoint")
# METADATA_PATH = os.path.join(CHECKPOINT_DIR, "checkpoint_metadata.json")

# @contextmanager
# def measure_time(description="Operation"):
#     """Context manager to measure and print execution time of code blocks."""
#     start_time = time.time()
#     try:
#         yield
#     finally:
#         elapsed = time.time() - start_time
#         print(f"{description} completed in {elapsed:.2f} seconds")

# class MemoryMonitor:
#     """Class for monitoring memory usage in a separate thread."""
#     def __init__(self, interval=10, threshold=75):
#         self.interval = interval
#         self.threshold = threshold
#         self.stop_flag = threading.Event()
#         self.max_usage = 0
#         self.monitor_thread = None
        
#     def memory_usage(self):
#         """Get current memory usage percentage."""
#         return psutil.virtual_memory().percent
        
#     def monitor(self):
#         """Monitor memory usage periodically."""
#         while not self.stop_flag.is_set():
#             usage = self.memory_usage()
#             self.max_usage = max(self.max_usage, usage)
#             if usage > self.threshold:
#                 print(f"WARNING: High memory usage detected: {usage:.1f}%")
#             time.sleep(self.interval)
                
#     def start(self):
#         """Start the memory monitoring thread."""
#         self.monitor_thread = threading.Thread(target=self.monitor, daemon=True)
#         self.monitor_thread.start()
        
#     def stop(self):
#         """Stop the memory monitoring thread."""
#         self.stop_flag.set()
#         if self.monitor_thread:
#             self.monitor_thread.join(timeout=1.0)
#         print(f"Maximum memory usage: {self.max_usage:.1f}%")

# class CheckpointManager:
#     """Class for managing checkpoints and metadata."""
#     def __init__(self, base_path, metadata_path, save_frequency=5):
#         self.base_path = base_path
#         self.metadata_path = metadata_path
#         self.save_frequency = save_frequency
#         self.completed_batches = self.load_metadata()
        
#     def save_checkpoint(self, batch_data, batch_index):
#         """Save a batch checkpoint to disk."""
#         filepath = f"{self.base_path}_batch_{batch_index}.pkl"
#         with open(filepath, "wb") as f:
#             pickle.dump(batch_data, f)
        
#     def load_checkpoint(self, batch_index):
#         """Load a batch checkpoint from disk."""
#         filepath = f"{self.base_path}_batch_{batch_index}.pkl"
#         if os.path.exists(filepath):
#             with open(filepath, "rb") as f:
#                 return pickle.load(f)
#         return None
        
#     def save_metadata(self, force_save=False):
#         """Save checkpoint metadata to disk."""
#         if force_save or len(self.completed_batches) % self.save_frequency == 0:
#             with open(self.metadata_path, "w") as f:
#                 json.dump({"completed_batches": sorted(list(self.completed_batches))}, f)
                
#     def load_metadata(self):
#         """Load checkpoint metadata from disk."""
#         if os.path.exists(self.metadata_path):
#             with open(self.metadata_path, "r") as f:
#                 return set(json.load(f).get("completed_batches", []))
#         return set()
        
#     def is_batch_completed(self, batch_index):
#         """Check if a batch has been completed."""
#         return batch_index in self.completed_batches
        
#     def mark_batch_completed(self, batch_index):
#         """Mark a batch as completed."""
#         self.completed_batches.add(batch_index)
#         self.save_metadata()

# def haversine_to_cartesian(lat, lon):
#     """Convert latitude and longitude to 3D cartesian coordinates (x,y,z)."""
#     # Convert to radians
#     lat_rad = np.radians(lat)
#     lon_rad = np.radians(lon)
    
#     # Convert to cartesian coordinates on a unit sphere
#     x = np.cos(lat_rad) * np.cos(lon_rad)
#     y = np.cos(lat_rad) * np.sin(lon_rad)
#     z = np.sin(lat_rad)
    
#     return np.column_stack((x, y, z))

# def cartesian_to_haversine_distance(distance_cartesian):
#     """Convert cartesian distance to angular distance in radians."""
#     # Ensure distance is at most 2.0 (diameter of unit sphere)
#     distance_cartesian = np.minimum(distance_cartesian, 2.0)
    
#     # Convert chord length to angular distance using inverse haversine formula
#     return 2 * np.arcsin(distance_cartesian / 2)

# def process_batch(tree, batch_points, k):
#     """Process a single batch of points using KDTree, excluding self-matches."""
#     # Query k+1 neighbors to include self (which will be removed)
#     distances, _ = tree.query(batch_points, k=k+1)
    
#     # Remove the first neighbor (self with distance 0)
#     nn_distances = distances[:, 1:]
    
#     # Convert cartesian distances to angular distances (radians)
#     return cartesian_to_haversine_distance(nn_distances)

# def calculate_optimal_batch_size(total_points, available_memory_mb=None):
#     """Calculate optimal batch size based on available memory and dataset size."""
#     if available_memory_mb is None:
#         # Use 20% of available memory if not specified (to leave room for parallelism)
#         available_memory_mb = psutil.virtual_memory().available / (1024 * 1024) * 0.2
    
#     # Estimate memory per point (assuming float64 coordinates and distances)
#     memory_per_point_kb = 0.5  # Approximate memory per point in KB
    
#     # Calculate batch size that would use available memory
#     batch_size = int(available_memory_mb * 1024 / memory_per_point_kb)
    
#     # Cap batch size to reasonable limits
#     min_batch_size = 1000
#     max_batch_size = 50000
#     batch_size = max(min(batch_size, max_batch_size), min_batch_size)
    
#     return min(batch_size, total_points)

# def aggregate_checkpoints(checkpoint_mgr, batch_indices):
#     """Load and aggregate checkpoints for the specified batch indices."""
#     all_distances = []
#     for batch_idx in batch_indices:
#         distances = checkpoint_mgr.load_checkpoint(batch_idx)
#         if distances is not None:
#             all_distances.append(distances)
#     return all_distances

# def calculate_spatial_density(latitudes, longitudes, k=5, leaf_size=40, batch_size=None, n_jobs=4, 
#                              checkpoint_dir=CHECKPOINT_DIR, overwrite=False):
#     """
#     Calculate spatial density based on K-nearest neighbors with checkpointing.
    
#     Parameters:
#     -----------
#     latitudes : array-like
#         Latitude values in degrees
#     longitudes : array-like
#         Longitude values in degrees
#     k : int
#         Number of nearest neighbors to find (excluding self)
#     leaf_size : int
#         Leaf size for the KDTree (affects performance)
#     batch_size : int or None
#         Batch size for processing. If None, will be calculated automatically.
#     n_jobs : int
#         Number of parallel jobs to run. -1 means using all processors.
#     checkpoint_dir : str
#         Directory for saving checkpoints
#     overwrite : bool
#         Whether to overwrite existing checkpoints
    
#     Returns:
#     --------
#     density : array
#         Density values for each point
#     distances : array
#         Distances to k nearest neighbors, shape (n_points, k)
#     """
#     global CHECKPOINT_DIR
#     CHECKPOINT_DIR = checkpoint_dir
#     global CHECKPOINT_BASE
#     CHECKPOINT_BASE = os.path.join(CHECKPOINT_DIR, "geo_density_checkpoint")
#     global METADATA_PATH
#     METADATA_PATH = os.path.join(CHECKPOINT_DIR, "checkpoint_metadata.json")
    
#     # Never overwrite existing checkpoints - we need them to resume processing
#     if overwrite:
#         print("WARNING: Overwrite flag ignored - checkpoints are preserved for resuming processing...
#         # Do NOT remove checkpoints regardless of the overwrite flag
    
#     # Convert input arrays to numpy if they aren't already
#     lat_array = np.asarray(latitudes, dtype=np.float64)
#     lon_array = np.asarray(longitudes, dtype=np.float64)
    
#     # Convert to 3D cartesian coordinates for better KDTree performance
#     with measure_time("Coordinate conversion to cartesian"):
#         cartesian_points = haversine_to_cartesian(lat_array, lon_array)
    
#     # Calculate optimal batch size if not provided
#     n_points = len(cartesian_points)
#     if batch_size is None:
#         batch_size = calculate_optimal_batch_size(n_points)
    
#     # Adjust batch size to create a reasonable number of batches (target ~200 batches)
#     if n_points / batch_size > 200:
#         batch_size = max(batch_size, n_points // 200)
    
#     print(f"Using batch size: {batch_size} for {n_points} points")
    
#     # Initialize checkpoint manager
#     checkpoint_mgr = CheckpointManager(
#         CHECKPOINT_BASE, 
#         METADATA_PATH,
#         METADATA_SAVE_FREQUENCY
#     )
    
#     # Start memory monitoring
#     memory_monitor = MemoryMonitor(interval=MONITOR_INTERVAL, threshold=MEMORY_THRESHOLD)
#     memory_monitor.start()
    
#     try:
#         # Build cKDTree for fast spatial queries (much faster than BallTree)
#         with measure_time("KDTree construction"):
#             tree = cKDTree(cartesian_points, leafsize=leaf_size)
#             gc.collect()  # Force garbage collection
#             print(f"Available memory after tree creation: {psutil.virtual_memory().available / (10...
        
#         # Calculate number of batches
#         num_batches = int(np.ceil(n_points / batch_size))
#         print(f"Total number of batches: {num_batches}")
        
#         # Check which batches are already completed
#         completed_batches = set()
#         for batch_index in range(num_batches):
#             if checkpoint_mgr.is_batch_completed(batch_index):
#                 completed_batches.add(batch_index)
                
#         print(f"Found {len(completed_batches)} completed batches")
        
#         # Process remaining batches
#         remaining_batches = set(range(num_batches)) - completed_batches
#         print(f"Processing {len(remaining_batches)} remaining batches")
        
#         if remaining_batches:
#             # Convert to list and sort for deterministic processing
#             remaining_batch_indices = sorted(list(remaining_batches))
            
#             # Test with a small batch first
#             print("Testing KDTree query with a small sample...")
#             sample_idx = remaining_batch_indices[0]
#             start_idx = sample_idx * batch_size
#             end_idx = min((sample_idx + 1) * batch_size, n_points)
            
#             # Take just 10 points for testing
#             test_points = cartesian_points[start_idx:start_idx+10]
            
#             with measure_time("Test KDTree query"):
#                 # Query k+1 neighbors (including self) for test
#                 test_distances, _ = tree.query(test_points, k=k+1)
#                 # Remove first column (self-matches)
#                 test_distances = test_distances[:, 1:]
#                 # Convert to angular distances
#                 test_distances = cartesian_to_haversine_distance(test_distances)
            
#             print(f"Test successful! Sample distances: {test_distances[0]}")
#             print(f"Average test distance: {np.mean(test_distances):.6f} radians")
            
#             # Process batches in parallel or sequentially
#             if n_jobs != 1:
#                 # Process in parallel using joblib
#                 print(f"Processing batches in parallel with {n_jobs} jobs")
                
#                 # Process in smaller chunks to avoid memory issues
#                 chunk_size = min(20, len(remaining_batch_indices))  # Smaller chunks for better mo...
#                 num_chunks = int(np.ceil(len(remaining_batch_indices) / chunk_size))
                
#                 for chunk_idx in range(num_chunks):
#                     chunk_start = chunk_idx * chunk_size
#                     chunk_end = min((chunk_idx + 1) * chunk_size, len(remaining_batch_indices))
#                     current_chunk = remaining_batch_indices[chunk_start:chunk_end]
                    
#                     print(f"Processing chunk {chunk_idx + 1}/{num_chunks} with {len(current_chunk)...
                    
#                     # Prepare batch data
#                     batch_data = []
#                     for batch_idx in current_chunk:
#                         start_idx = batch_idx * batch_size
#                         end_idx = min((batch_idx + 1) * batch_size, n_points)
#                         batch_points = cartesian_points[start_idx:end_idx]
#                         batch_data.append((batch_idx, batch_points))
                    
#                     # Process in parallel with timeout monitoring
#                     start_time = time.time()
#                     results = Parallel(n_jobs=n_jobs, verbose=10, timeout=3600)(
#                         delayed(process_batch)(tree, points, k) for batch_idx, points in batch_dat...
#                     )
#                     end_time = time.time()
                    
#                     # Calculate performance statistics
#                     elapsed_time = end_time - start_time
#                     points_processed = sum(len(points) for _, points in batch_data)
#                     points_per_second = points_processed / elapsed_time
                    
#                     print(f"Chunk processed {points_processed} points in {elapsed_time:.2f} second...
#                     print(f"Performance: {points_per_second:.2f} points/second")
                    
#                     # Save results
#                     for (batch_idx, _), distances in zip(batch_data, results):
#                         checkpoint_mgr.save_checkpoint(distances, batch_idx)
#                         checkpoint_mgr.mark_batch_completed(batch_idx)
                    
#                     # Estimate remaining time
#                     remaining_chunks = num_chunks - (chunk_idx + 1)
#                     if remaining_chunks > 0:
#                         remaining_time = remaining_chunks * elapsed_time
#                         hours = remaining_time // 3600
#                         minutes = (remaining_time % 3600) // 60
#                         print(f"Estimated remaining time: {int(hours)}h {int(minutes)}m")
                    
#                     # Free memory
#                     del batch_data
#                     del results
#                     gc.collect()
#             else:
#                 # Process sequentially
#                 print("Processing batches sequentially")
#                 with tqdm(total=len(remaining_batch_indices), desc="Processing Batches") as pbar:
#                     for batch_idx in remaining_batch_indices:
#                         start_idx = batch_idx * batch_size
#                         end_idx = min((batch_idx + 1) * batch_size, n_points)
#                         batch_points = cartesian_points[start_idx:end_idx]
                        
#                         # Process full batch
#                         distances = process_batch(tree, batch_points, k)
                        
#                         # Save checkpoint
#                         checkpoint_mgr.save_checkpoint(distances, batch_idx)
#                         checkpoint_mgr.mark_batch_completed(batch_idx)
                        
#                         # Free memory
#                         del distances
#                         gc.collect()
                        
#                         pbar.update(1)
        
#         # At this point, all batches should be processed and saved to disk
#         # We have two options:
#         # 1. Load all checkpoints into memory (might be too large)
#         # 2. Calculate density from checkpoints directly (more memory efficient)
        
#         # Option 2: Calculate density directly from checkpoints
#         print("Calculating density from checkpoints...")
#         n_processed = 0
#         density = np.zeros(n_points)
        
#         # Process in chunks to save memory
#         chunk_size = min(1000, num_batches)
#         num_chunks = int(np.ceil(num_batches / chunk_size))
        
#         for chunk_idx in range(num_chunks):
#             chunk_start = chunk_idx * chunk_size
#             chunk_end = min((chunk_idx + 1) * chunk_size, num_batches)
#             current_chunk = list(range(chunk_start, chunk_end))
            
#             print(f"Processing density chunk {chunk_idx+1}/{num_chunks}")
            
#             # Load each batch and compute density
#             for batch_idx in tqdm(current_chunk):
#                 batch_distances = checkpoint_mgr.load_checkpoint(batch_idx)
                
#                 if batch_distances is not None:
#                     start_idx = batch_idx * batch_size
#                     end_idx = min((batch_idx + 1) * batch_size, n_points)
                    
#                     # Mean distance to k nearest neighbors
#                     mean_distances = np.mean(batch_distances, axis=1)
                    
#                     # Density is inversely proportional to mean distance
#                     # Add small epsilon to avoid division by zero
#                     epsilon = 1e-10
#                     density[start_idx:end_idx] = 1.0 / (mean_distances + epsilon)
                    
#                     n_processed += end_idx - start_idx
                    
#                     # Clean up
#                     del batch_distances, mean_distances
#                     gc.collect()
        
#         print(f"Processed density for {n_processed} points")
        
#         # Normalize density for easier interpretation
#         if n_processed > 0:
#             density = density / np.mean(density[0:n_processed])
        
#         # Create weights (inverse of density)
#         weights = 1.0 / (density + 1e-8)
        
#         # Normalize weights
#         weights = weights / np.mean(weights) * n_points
        
#         # Clip extreme weights (beyond 3 std from mean)
#         weights_mean = np.mean(weights)
#         weights_std = np.std(weights)
#         weights = np.clip(weights, 0, weights_mean + 3*weights_std)
        
#         # Final normalization
#         weights = weights / np.mean(weights) * n_points
        
#         # Save the final density and weights
#         with open(os.path.join(CHECKPOINT_DIR, "spatial_density.pkl"), "wb") as f:
#             pickle.dump({"density": density, "weights": weights}, f)
        
#         print(f"Saved final density and weights to {os.path.join(CHECKPOINT_DIR, 'spatial_density....
        
#         # Final metadata save
#         checkpoint_mgr.save_metadata(force_save=True)
        
#         return density, weights
        
#     finally:
#         # Stop memory monitoring
#         memory_monitor.stop()

# def stratified_spatiotemporal_split(X, y, metadata, test_size=0.2, val_size=0.15, 
#                                    random_state=42, checkpoint_dir=CHECKPOINT_DIR):
#     """
#     Split data with balanced spatiotemporal representation in train/val/test sets.
    
#     Parameters:
#     -----------
#     X : array
#         Features array
#     y : array
#         Labels array
#     metadata : list
#         List of metadata dicts containing spatial and temporal information
#     test_size : float
#         Fraction of data for testing
#     val_size : float
#         Fraction of data for validation
#     random_state : int
#         Random seed
#     checkpoint_dir : str
#         Directory for checkpoints
        
#     Returns:
#     --------
#     train_indices, val_indices, test_indices : arrays
#         Indices for each split
#     """
#     print("Performing spatiotemporal split with checkpointing...")
#     global CHECKPOINT_DIR
#     CHECKPOINT_DIR = checkpoint_dir
    
#     # Create checkpoint directory if it doesn't exist
#     os.makedirs(CHECKPOINT_DIR, exist_ok=True)
    
#     # ALWAYS check for existing splits to resume from previous run
#     split_file = os.path.join(CHECKPOINT_DIR, "spatiotemporal_split.pkl")
#     if os.path.exists(split_file):
#         print(f"Loading existing split from {split_file}")
#         with open(split_file, "rb") as f:
#             split_data = pickle.load(f)
#         print(f"Successfully loaded existing train/val/test split with {len(split_data['train_indi...
#               f"{len(split_data['val_indices'])} validation, and {len(split_data['test_indices'])}...
#         return split_data["train_indices"], split_data["val_indices"], split_data["test_indices"]
    
#     # If we get here, no existing split was found, so we'll create a new one
#     print("No existing split found, creating new train/val/test split")
    
#     n_samples = len(X)
#     np.random.seed(random_state)
    
#     # Extract relevant metadata
#     timestamps = np.array([meta['start_time'] for meta in metadata])
    
#     # Geography (handle cases where lat/lon may be missing)
#     latitudes = np.array([
#         meta.get('latitude', 0) if meta.get('latitude') is not None else 0 
#         for meta in metadata
#     ])
#     longitudes = np.array([
#         meta.get('longitude', 0) if meta.get('longitude') is not None else 0 
#         for meta in metadata
#     ])
#     depths = np.array([
#         meta.get('soil_temp_depth', 0) if meta.get('soil_temp_depth') is not None else 0 
#         for meta in metadata
#     ])
    
#     has_geo_info = (np.count_nonzero(latitudes) > 0 and np.count_nonzero(longitudes) > 0)
    
#     # Time-based sorting
#     time_indices = np.argsort(timestamps)
    
#     # Keep the most recent data as a true test set (chronological split)
#     test_count = int(n_samples * test_size)
#     test_indices = time_indices[-test_count:]
    
#     # Remaining data for train/val
#     remaining_indices = time_indices[:-test_count]
    
#     # Check for existing density weights
#     density_file = os.path.join(CHECKPOINT_DIR, "spatial_density.pkl")
#     if os.path.exists(density_file) and has_geo_info:
#         print(f"Loading existing density weights from {density_file}")
#         with open(density_file, "rb") as f:
#             density_data = pickle.load(f)
#         weights = density_data["weights"][remaining_indices]
#     elif has_geo_info:
#         # Calculate density-based weights for remaining data
#         print("Calculating spatial density for weighting...")
#         _, weights = calculate_spatial_density(
#             latitudes[remaining_indices],
#             longitudes[remaining_indices],
#             k=5,
#             checkpoint_dir=CHECKPOINT_DIR
#         )
#     else:
#         # If no geography, use uniform weights
#         weights = np.ones(len(remaining_indices))
    
#     # Calculate validation size
#     val_count = int(n_samples * val_size)
    
#     # Stratification criteria
#     print("Creating stratification features...")
    
#     # 1. Time-based features
#     # Extract year, month, day features
#     years = np.array([ts.year for ts in timestamps[remaining_indices]])
#     months = np.array([ts.month for ts in timestamps[remaining_indices]])
#     # Group months into seasons
#     seasons = np.floor((months - 1) / 3).astype(int)
    
#     # 2. Spatial features
#     if has_geo_info:
#         # Latitude bands
#         lat_bands = np.zeros_like(remaining_indices, dtype=int)
#         lat_bands[(latitudes[remaining_indices] >= 50) & (latitudes[remaining_indices] < 60)] = 1
#         lat_bands[(latitudes[remaining_indices] >= 60) & (latitudes[remaining_indices] < 66.5)] = ...
#         lat_bands[(latitudes[remaining_indices] >= 66.5) & (latitudes[remaining_indices] < 75)] = ...
#         lat_bands[(latitudes[remaining_indices] >= 75)] = 4
        
#         # Longitude sectors (8 sectors of 45 degrees each)
#         lon_sectors = ((longitudes[remaining_indices] + 180) / 45).astype(int) % 8
#     else:
#         # Dummy spatial features if no geographic data
#         lat_bands = np.zeros_like(remaining_indices)
#         lon_sectors = np.zeros_like(remaining_indices)
    
#     # 3. Depth strata
#     depth_strata = np.zeros_like(remaining_indices)
#     depth_strata[(depths[remaining_indices] > 0) & (depths[remaining_indices] <= 0.2)] = 1
#     depth_strata[(depths[remaining_indices] > 0.2) & (depths[remaining_indices] <= 0.5)] = 2
#     depth_strata[(depths[remaining_indices] > 0.5) & (depths[remaining_indices] <= 1.0)] = 3
#     depth_strata[(depths[remaining_indices] > 1.0)] = 4
    
#     # 4. Class labels
#     labels = y[remaining_indices]
    
#     # Create strata by combining features
#     strata = (
#         years * 10000 + 
#         seasons * 1000 + 
#         lat_bands * 100 + 
#         lon_sectors * 10 + 
#         depth_strata
#     )
    
#     # If labels are binary, include them in strata
#     if len(np.unique(labels)) <= 5:  # Few enough classes to use for stratification
#         strata = strata * 10 + labels
    
#     unique_strata = np.unique(strata)
    
#     print(f"Found {len(unique_strata)} unique strata")
    
#     # Initialize val indices with checkpoint
#     val_indices_file = os.path.join(CHECKPOINT_DIR, "val_indices_temp.pkl")
#     if os.path.exists(val_indices_file):
#         print(f"Loading partial validation indices from {val_indices_file}")
#         with open(val_indices_file, "rb") as f:
#             val_indices = pickle.load(f)
        
#         # Remove already sampled from potential pool
#         sampled_mask = np.ones(len(remaining_indices), dtype=bool)
#         sampled_indices = np.where(np.isin(remaining_indices, val_indices))[0]
#         sampled_mask[sampled_indices] = False
        
#         # Update sampling pool
#         remaining_pool = np.arange(len(remaining_indices))[sampled_mask]
        
#         print(f"Loaded {len(val_indices)} validation indices, {len(remaining_pool)} remaining to s...
#     else:
#         val_indices = []
#         remaining_pool = np.arange(len(remaining_indices))
    
#     # Sample from each stratum proportionally to create validation set
#     checkpoint_frequency = 10  # Save every 10 strata
#     for i, stratum in enumerate(tqdm(unique_strata, desc="Sampling validation set")):
#         # Find indices for this stratum
#         stratum_positions = np.where((strata == stratum) & np.isin(np.arange(len(remaining_indices...
        
#         if len(stratum_positions) == 0:
#             continue
            
#         stratum_indices = remaining_indices[stratum_positions]
#         stratum_weights = weights[stratum_positions]
        
#         # Calculate target sample size
#         stratum_weight_sum = np.sum(stratum_weights)
#         total_weight_sum = np.sum(weights)
#         target_val_size = int(val_count * stratum_weight_sum / total_weight_sum)
#         target_val_size = max(1, min(target_val_size, len(stratum_indices) - 1))
        
#         # Sample without replacement, weighted by inverse density
#         if len(stratum_indices) > target_val_size:
#             # Weighted sampling
#             sampled_positions = np.random.choice(
#                 len(stratum_positions),
#                 size=target_val_size,
#                 replace=False,
#                 p=stratum_weights/np.sum(stratum_weights)
#             )
#             sampled_indices = stratum_positions[sampled_positions]
#             val_indices.extend(remaining_indices[sampled_indices])
#         else:
#             # Take all if we need more than available
#             val_indices.extend(stratum_indices)
        
#         # Save checkpoint periodically
#         if (i + 1) % checkpoint_frequency == 0 or i == len(unique_strata) - 1:
#             with open(val_indices_file, "wb") as f:
#                 pickle.dump(val_indices, f)
#             print(f"Saved checkpoint with {len(val_indices)} validation indices")
    
#     # Remaining indices go to train set
#     train_indices = np.setdiff1d(remaining_indices, val_indices)
    
#     # Print statistics about the split
#     print(f"Split sizes: Train={len(train_indices)}, Validation={len(val_indices)}, Test={len(test...
    
#     train_pos = np.sum(y[train_indices])
#     val_pos = np.sum(y[val_indices])
#     test_pos = np.sum(y[test_indices])
    
#     print(f"Positive examples: Train={train_pos} ({train_pos/len(train_indices)*100:.1f}%), " +
#           f"Val={val_pos} ({val_pos/len(val_indices)*100:.1f}%), " +
#           f"Test={test_pos} ({test_pos/len(test_indices)*100:.1f}%)")
    
#     # Save the final split
#     with open(split_file, "wb") as f:
#         pickle.dump({
#             "train_indices": train_indices,
#             "val_indices": val_indices,
#             "test_indices": test_indices
#         }, f)
    
#     print(f"Saved final split to {split_file}")
    
#     # Keep temporary files for potential debugging/recovery
#     # Do NOT remove val_indices_file to preserve all checkpoints
    
#     return train_indices, val_indices, test_indices

# # Determine optimal number of jobs based on available CPU cores and memory
# n_cpus = os.cpu_count()
# available_memory_gb = psutil.virtual_memory().available / (1024*1024*1024)
# recommended_jobs = max(1, min(n_cpus - 1, int(available_memory_gb / 8)))  # More conservative
# print(f"Available CPUs: {n_cpus}, Available memory: {available_memory_gb:.2f} GB")
# print(f"Recommended parallel jobs: {recommended_jobs}")

# densities, weights = calculate_spatial_density(
#     latitudes=latitudes, 
#     longitudes=longitudes, 
#     k=5,
#     leaf_size=40,
#     batch_size=100000,
#     n_jobs=recommended_jobs
#     )
# print(f"Average density: {np.mean(densities):.6f}")
# print(f"Average weight: {np.mean(weights):.6f}")

# train_indices, val_indices, test_indices = stratified_spatiotemporal_split(
#     X=X, 
#     y=y, 
#     metadata=metadata,
#     test_size=0.2, 
#     val_size=0.15
# )

# with open("zero_curtain_pipeline/modeling/checkpoints/spatiotemporal_split.pkl","rb") as f:
#     split = pickle.load(f)

# #split.get('train_indices')
# #split.get('valid_indices')
# #split.get('test_indices')

# with open("zero_curtain_pipeline/modeling/checkpoints/spatial_density.pkl","rb") as f:
#     spatialdensity = pickle.load(f)

# #spatialdensity.get('density')
# #spatialdensity.get('weights')

# events_df derives zero-curtain events from this main dataframe, composed of multiple data sources ...
# pd.read_feather('merged_compressed.feather')

# datetime	year	season	latitude	longitude	thickness_m	thickness_m_standardized	soil_temp	soil_temp_s...
# 0	1891-01-15 12:00:00	1891.0	Winter	52.283299	104.300003	NaN	NaN	-10.5	-10.5	0.8	deep	NaN	NaN	NaN	...
# 1	1891-01-15 12:00:00	1891.0	Winter	52.283299	104.300003	NaN	NaN	2.4	2.4	3.2	very_deep	NaN	NaN	NaN...
# 2	1891-01-15 12:00:00	1891.0	Winter	52.283299	104.300003	NaN	NaN	-0.5	-0.5	1.6	very_deep	NaN	NaN	N...
# 3	1891-01-15 12:00:00	1891.0	Winter	52.283299	104.300003	NaN	NaN	-13.4	-13.4	0.4	intermediate	NaN	...
# 4	1891-02-14 00:00:00	1891.0	Winter	52.283299	104.300003	NaN	NaN	-11.7	-11.7	0.8	deep	NaN	NaN	NaN	...
# ...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
# 62708663	2024-12-31 00:00:00	2024.0	Winter	65.154010	-147.502580	NaN	NaN	NaN	NaN	NaN	None	7.59	0.0...
# 62708664	2024-12-31 00:00:00	2024.0	Winter	63.875800	-149.213350	NaN	NaN	NaN	NaN	NaN	None	0.00	0.0...
# 62708665	2024-12-31 00:00:00	2024.0	Winter	65.154010	-147.502580	NaN	NaN	NaN	NaN	NaN	None	4.69	0.0...
# 62708666	2024-12-31 00:00:00	2024.0	Winter	71.282410	-156.619360	NaN	NaN	NaN	NaN	NaN	None	1.66	0.0...
# 62708667	2024-12-31 00:00:00	2024.0	Winter	65.154010	-147.502580	NaN	NaN	NaN	NaN	NaN	None	28.13	0....
# 62708668 rows × 16 columns

# import os
# import numpy as np
# import pandas as pd
# import gc
# import pickle
# import psutil
# from datetime import datetime, timedelta
# import time
# from scipy.interpolate import interp1d

# def memory_usage():
#     """Monitor memory usage in MB"""
#     process = psutil.Process(os.getpid())
#     mem_info = process.memory_info()
#     return mem_info.rss / 1024**2  # Memory in MB

# def save_checkpoint(data, checkpoint_dir, name):
#     """Save checkpoint data to pickle file"""
#     os.makedirs(checkpoint_dir, exist_ok=True)
#     checkpoint_path = os.path.join(checkpoint_dir, f'{name}.pkl')
#     with open(checkpoint_path, 'wb') as f:
#         pickle.dump(data, f)
#     print(f"Saved checkpoint to {checkpoint_path}")

# def load_checkpoint(checkpoint_dir, name):
#     """Load checkpoint data from pickle file"""
#     checkpoint_path = os.path.join(checkpoint_dir, f'{name}.pkl')
#     try:
#         with open(checkpoint_path, 'rb') as f:
#             data = pickle.load(f)
#         print(f"Loaded checkpoint from {checkpoint_path}")
#         return data
#     except:
#         print(f"No checkpoint found at {checkpoint_path}")
#         return None

# # data_loader.py
# import os
# import pandas as pd
# import numpy as np
# import pyarrow as pa
# import pyarrow.feather as pf
# import pyarrow.compute as pc
# import pyarrow.dataset as ds
# import gc
# from tqdm.auto import tqdm

# def get_time_range(feather_path):
#     """Get min and max datetime from feather file efficiently"""
#     print("Determining dataset time range")
#     try:
#         # Use PyArrow for efficient metadata access
#         table = pf.read_table(feather_path, columns=['datetime'])
#         datetime_col = table['datetime']
#         min_date = pd.Timestamp(pc.min(datetime_col).as_py())
#         max_date = pd.Timestamp(pc.max(datetime_col).as_py())
#         del table, datetime_col
#         gc.collect()
#     except Exception as e:
#         print(f"Error with PyArrow min/max: {str(e)}")
#         print("Falling back to reading sample rows...")
        
#         # Read just first and last rows after sorting
#         # This is more efficient than reading all data
#         try:
#             dataset = ds.dataset(feather_path, format='feather')
            
#             # Get min date from sorted data (first row)
#             table_min = dataset.to_table(
#                 columns=['datetime'],
#                 filter=None,
#                 use_threads=True,
#                 limit=1
#             )
#             min_date = pd.Timestamp(table_min['datetime'][0].as_py())
            
#             # Get max date from reverse sorted data (first row)
#             # Note: PyArrow Dataset API doesn't support reverse sort directly
#             # So we read all datetime values and find max
#             table_all = dataset.to_table(columns=['datetime'])
#             max_date = pd.Timestamp(pc.max(table_all['datetime']).as_py())
            
#             del table_min, table_all, dataset
#             gc.collect()
#         except Exception as inner_e:
#             print(f"Error with optimized approach: {str(inner_e)}")
#             print("Using full scan method (slow)...")
            
#             # Last resort: read all timestamps in chunks
#             min_date = pd.Timestamp.max
#             max_date = pd.Timestamp.min
            
#             # Open file directly to avoid loading everything
#             table = pf.read_table(feather_path, columns=['datetime'])
#             datetime_values = table['datetime'].to_pandas()
            
#             min_date = datetime_values.min()
#             max_date = datetime_values.max()
            
#             del datetime_values, table
#             gc.collect()
    
#     print(f"Data timespan: {min_date} to {max_date}")
#     return min_date, max_date

# def get_unique_site_depths(feather_path):
#     """Get unique site-depth combinations efficiently"""
#     print("Finding unique site-depth combinations")
#     print(f"Memory before processing: {memory_usage():.1f} MB")
    
#     try:
#         # Use PyArrow for efficient operation
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Read only the columns we need
#         table = dataset.to_table(columns=['source', 'soil_temp_depth'])
        
#         # Convert to pandas
#         df = table.to_pandas()
        
#         # Get valid combinations (where temp_depth is not NaN)
#         valid_df = df.dropna(subset=['soil_temp_depth'])
        
#         # Get unique combinations
#         site_depths = valid_df.drop_duplicates(['source', 'soil_temp_depth'])
        
#         # Clean up
#         del table, df, valid_df, dataset
#         gc.collect()
        
#     except Exception as e:
#         print(f"Error with PyArrow approach: {str(e)}")
#         print("Falling back to chunked pandas approach...")
        
#         # Fallback: Process in chunks
#         chunk_size = 1000000
#         unique_combos = set()
        
#         # Read feather in chunks (this is slower but more robust)
#         with pd.read_feather(feather_path, columns=['source', 'soil_temp_depth']) as reader:
#             for chunk in reader:
#                 # Get valid rows and unique combinations
#                 valid_rows = chunk.dropna(subset=['soil_temp_depth'])
#                 chunk_combinations = set(zip(valid_rows['source'], valid_rows['soil_temp_depth']))
#                 unique_combos.update(chunk_combinations)
                
#                 # Clean up
#                 del chunk, valid_rows, chunk_combinations
#                 gc.collect()
        
#         # Convert to DataFrame
#         site_depths = pd.DataFrame(list(unique_combos), columns=['source', 'soil_temp_depth'])
    
#     print(f"Found {len(site_depths)} unique site-depth combinations")
#     print(f"Memory after processing: {memory_usage():.1f} MB")
    
#     return site_depths[['source', 'soil_temp_depth']]

# def load_site_depth_data(feather_path, site, temp_depth):
#     """Load ONLY data for a specific site and depth using PyArrow filtering"""
#     print(f"Loading data for site: {site}, depth: {temp_depth}")
#     print(f"Memory before loading: {memory_usage():.1f} MB")
    
#     try:
#         # Use PyArrow Dataset API for efficient filtering
#         dataset = ds.dataset(feather_path, format='feather')
        
#         # Create filter expressions
#         site_filter = ds.field('source') == site
#         depth_filter = ds.field('soil_temp_depth') == float(temp_depth)
#         combined_filter = site_filter & depth_filter
        
#         # Apply filter and read only filtered data
#         table = dataset.to_table(filter=combined_filter)
#         filtered_df = table.to_pandas()
        
#         # Clean up
#         del table, dataset
#         gc.collect()
        
#     except Exception as e:
#         print(f"Error with PyArrow filtering: {str(e)}")
#         print("Falling back to pandas filtering...")
        
#         # Define chunk size based on available memory
#         available_mem = psutil.virtual_memory().available / (1024**2)  # MB
#         chunk_size = min(500000, max(100000, int(available_mem / 10)))
        
#         filtered_chunks = []
        
#         # Read and filter in chunks
#         for chunk in pd.read_feather(feather_path, columns=None, chunksize=chunk_size):
#             # Filter by site and depth
#             chunk_filtered = chunk[(chunk['source'] == site) & 
#                                    (chunk['soil_temp_depth'] == temp_depth)]
            
#             if len(chunk_filtered) > 0:
#                 filtered_chunks.append(chunk_filtered)
            
#             # Clean up
#             del chunk, chunk_filtered
#             gc.collect()
        
#         # Combine filtered chunks
#         if filtered_chunks:
#             filtered_df = pd.concat(filtered_chunks, ignore_index=True)
#             del filtered_chunks
#         else:
#             filtered_df = pd.DataFrame()
        
#         gc.collect()
    
#     # Ensure datetime is in datetime format
#     if 'datetime' in filtered_df.columns and not pd.api.types.is_datetime64_dtype(filtered_df['dat...
#         filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])
    
#     print(f"Loaded {len(filtered_df)} rows for site-depth")
#     print(f"Memory after loading: {memory_usage():.1f} MB")
    
#     return filtered_df
    
#     # zero_curtain_processing.py
# import numpy as np
# import pandas as pd
# import gc
# from scipy.interpolate import interp1d

# def process_site_for_zero_curtain(site_df, site, temp_depth, max_gap_hours=6, interpolation_method...
#     """Process a single site-depth for zero curtain events"""
#     # Initialize list to store events
#     site_events = []
    
#     # Skip if too few points
#     if len(site_df) < 10:
#         return []
    
#     # Sort by time
#     site_df = site_df.sort_values('datetime')
    
#     # Calculate time differences
#     site_df['time_diff'] = site_df['datetime'].diff().dt.total_seconds() / 3600
    
#     # Identify gaps for interpolation
#     interpolation_needed = (site_df['time_diff'] > 1.0) & (site_df['time_diff'] <= max_gap_hours)
    
#     # Perform interpolation if needed
#     if interpolation_needed.any():
#         interp_rows = []
#         gap_indices = site_df.index[interpolation_needed].tolist()
        
#         for idx in gap_indices:
#             try:
#                 # Get before and after rows
#                 before_row = site_df.loc[site_df.index[site_df.index.get_loc(idx) - 1]]
#                 after_row = site_df.loc[idx]
                
#                 # Calculate gap and intervals
#                 time_gap = after_row['time_diff']
#                 n_intervals = int(time_gap)
                
#                 if n_intervals > 0:
#                     # Create timestamps
#                     timestamps = pd.date_range(
#                         start=before_row['datetime'],
#                         end=after_row['datetime'],
#                         periods=n_intervals + 2  # Include endpoints
#                     )[1:-1]  # Exclude endpoints
                    
#                     # Create temperature values
#                     if interpolation_method == 'linear' or len(site_df) < 5:
#                         # Linear interpolation
#                         temp_start = before_row['soil_temp_standardized']
#                         temp_end = after_row['soil_temp_standardized']
#                         temp_values = np.linspace(temp_start, temp_end, n_intervals + 2)[1:-1]
#                     else:
#                         # Try cubic interpolation
#                         try:
#                             # Get neighboring points
#                             idx_loc = site_df.index.get_loc(idx)
#                             start_idx = max(0, idx_loc - 3)
#                             end_idx = min(len(site_df), idx_loc + 2)
                            
#                             temp_points = site_df.iloc[start_idx:end_idx]['soil_temp_standardized'...
#                             time_points = [(t - before_row['datetime']).total_seconds() / 3600 
#                                          for t in site_df.iloc[start_idx:end_idx]['datetime']]
                            
#                             interp_times = [(t - before_row['datetime']).total_seconds() / 3600 
#                                           for t in timestamps]
                            
#                             # Perform cubic interpolation if enough points
#                             if len(time_points) >= 4:
#                                 interp_func = interp1d(time_points, temp_points, 
#                                                      kind='cubic', bounds_error=False)
#                                 temp_values = interp_func(interp_times)
#                             else:
#                                 # Fallback to linear
#                                 temp_start = before_row['soil_temp_standardized']
#                                 temp_end = after_row['soil_temp_standardized']
#                                 temp_values = np.linspace(temp_start, temp_end, n_intervals + 2)[1...
#                         except:
#                             # Fallback to linear if cubic fails
#                             temp_start = before_row['soil_temp_standardized']
#                             temp_end = after_row['soil_temp_standardized']
#                             temp_values = np.linspace(temp_start, temp_end, n_intervals + 2)[1:-1]
                    
#                     # Check for moisture data
#                     moist_values = None
#                     has_moisture = ('soil_moist_standardized' in site_df.columns and 
#                                    not pd.isna(before_row['soil_moist_standardized']) and 
#                                    not pd.isna(after_row['soil_moist_standardized']))
                    
#                     if has_moisture:
#                         moist_start = before_row['soil_moist_standardized']
#                         moist_end = after_row['soil_moist_standardized']
#                         moist_values = np.linspace(moist_start, moist_end, n_intervals + 2)[1:-1]
                    
#                     # Create new rows
#                     for j, timestamp in enumerate(timestamps):
#                         new_row = before_row.copy()
#                         new_row['datetime'] = timestamp
#                         new_row['soil_temp_standardized'] = temp_values[j]
#                         new_row['interpolated'] = True
                        
#                         if has_moisture and moist_values is not None:
#                             new_row['soil_moist_standardized'] = moist_values[j]
                        
#                         interp_rows.append(new_row)
#             except Exception as e:
#                 print(f"  Error during interpolation at idx {idx}: {str(e)}")
#                 continue
        
#         # Add interpolated rows
#         if interp_rows:
#             interp_df = pd.DataFrame(interp_rows)
#             site_df = pd.concat([site_df, interp_df], ignore_index=True)
#             site_df = site_df.sort_values('datetime')
            
#             # Clean up
#             del interp_df, interp_rows
#             gc.collect()
    
#     # Calculate temperature gradient
#     site_df['temp_gradient'] = site_df['soil_temp_standardized'].diff() / \
#                              (site_df['datetime'].diff().dt.total_seconds() / 3600)
    
#     # Check for phase change periods
#     mask_temp = (site_df['soil_temp_standardized'].abs() <= 0.5)
#     mask_gradient = (site_df['temp_gradient'].abs() <= 0.02)
    
#     # Check for moisture data
#     use_moisture = False
#     if 'soil_moist_standardized' in site_df.columns and not site_df['soil_moist_standardized'].isn...
#         use_moisture = True
        
#         # Calculate moisture gradient
#         site_df['moist_gradient'] = site_df['soil_moist_standardized'].diff() / \
#                                   (site_df['datetime'].diff().dt.total_seconds() / 3600)
        
#         # Add moisture depth info
#         if 'soil_moist_depth' in site_df.columns and not site_df['soil_moist_depth'].isna().all():
#             site_df['closest_moist_depth'] = site_df['soil_moist_depth']
#         else:
#             site_df['closest_moist_depth'] = np.nan
        
#         # Moisture mask
#         mask_moisture = (site_df['moist_gradient'].abs() >= 0.0005)
        
#         # Combined mask
#         combined_mask = mask_temp & (mask_gradient | mask_moisture)
#     else:
#         # Use only temperature
#         combined_mask = mask_temp & mask_gradient
    
#     # Find continuous events
#     site_df['zero_curtain_flag'] = combined_mask
#     site_df['event_start'] = combined_mask & ~combined_mask.shift(1, fill_value=False)
#     site_df['event_end'] = combined_mask & ~combined_mask.shift(-1, fill_value=False)
    
#     # Get event starts and ends
#     event_starts = site_df[site_df['event_start']]['datetime'].tolist()
#     event_ends = site_df[site_df['event_end']]['datetime'].tolist()
    
#     if len(event_starts) == 0 or len(event_ends) == 0:
#         return []
    
#     # Handle mismatched starts/ends
#     if len(event_starts) > len(event_ends):
#         event_starts = event_starts[:len(event_ends)]
#     elif len(event_ends) > len(event_starts):
#         event_ends = event_ends[:len(event_starts)]
    
#     # Process each event
#     for start, end in zip(event_starts, event_ends):
#         event_duration = (end - start).total_seconds() / 3600
        
#         # Skip short events
#         if event_duration < 12:
#             continue
        
#         # Get event data
#         event_data = site_df[(site_df['datetime'] >= start) & (site_df['datetime'] <= end)]
        
#         if len(event_data) < 3:
#             continue
        
#         # Extract event info
#         try:
#             event_info = extract_event_info(event_data, site, temp_depth, start, end, 
#                                            event_duration, use_moisture)
#             site_events.append(event_info)
#         except Exception as e:
#             print(f"  Error extracting event info: {str(e)}")
    
#     # Clean up to free memory
#     del site_df
#     gc.collect()
    
#     return site_events

# def extract_event_info(event_data, site, temp_depth, start, end, event_duration, use_moisture):
#     """Extract comprehensive information about a zero curtain event"""
#     # Basic event info
#     event_info = {
#         'source': site,
#         'soil_temp_depth': temp_depth,
#         'soil_temp_depth_zone': event_data['soil_temp_depth_zone'].iloc[0] if 'soil_temp_depth_zon...
#         'datetime_min': start,
#         'datetime_max': end,
#         'duration_hours': event_duration,
#         'observation_count': len(event_data),
#         'observations_per_day': len(event_data) / (event_duration / 24) if event_duration > 0 else...
#         'soil_temp_mean': event_data['soil_temp_standardized'].mean(),
#         'soil_temp_min': event_data['soil_temp_standardized'].min(),
#         'soil_temp_max': event_data['soil_temp_standardized'].max(),
#         'soil_temp_std': event_data['soil_temp_standardized'].std(),
#         'season': event_data['season'].iloc[0] if 'season' in event_data.columns else None,
#         'latitude': event_data['latitude'].iloc[0] if 'latitude' in event_data.columns else None,
#         'longitude': event_data['longitude'].iloc[0] if 'longitude' in event_data.columns else Non...
#         'year': event_data['year'].iloc[0] if 'year' in event_data.columns else None,
#         'month': start.month
#     }
    
#     # Moisture info
#     if use_moisture and not event_data['soil_moist_standardized'].isna().all():
#         event_info['soil_moist_mean'] = event_data['soil_moist_standardized'].mean()
#         event_info['soil_moist_std'] = event_data['soil_moist_standardized'].std()
#         event_info['soil_moist_min'] = event_data['soil_moist_standardized'].min()
#         event_info['soil_moist_max'] = event_data['soil_moist_standardized'].max()
#         event_info['soil_moist_change'] = event_info['soil_moist_max'] - event_info['soil_moist_mi...
        
#         # Moisture depth
#         if 'closest_moist_depth' in event_data.columns and not event_data['closest_moist_depth'].i...
#             event_info['soil_moist_depth'] = event_data['closest_moist_depth'].iloc[0]
#         else:
#             event_info['soil_moist_depth'] = np.nan
#     else:
#         # Empty moisture values
#         event_info['soil_moist_mean'] = np.nan
#         event_info['soil_moist_std'] = np.nan
#         event_info['soil_moist_min'] = np.nan
#         event_info['soil_moist_max'] = np.nan
#         event_info['soil_moist_change'] = np.nan
#         event_info['soil_moist_depth'] = np.nan
    
#     # Temperature gradient info
#     if 'temp_gradient' in event_data.columns and not event_data['temp_gradient'].isna().all():
#         event_info['temp_gradient_mean'] = event_data['temp_gradient'].mean()
#         event_info['temp_stability'] = event_data['temp_gradient'].abs().mean()
    
#     # Add year-month
#     event_info['year_month'] = f"{event_info['year']}-{event_info['month']:02d}"
    
#     # Add region classification
#     if 'latitude' in event_info and event_info['latitude'] is not None:
#         lat = event_info['latitude']
#         if lat >= 66.5:
#             event_info['region'] = 'Arctic'
#         elif lat >= 60:
#             event_info['region'] = 'Subarctic'
#         elif lat >= 49:
#             event_info['region'] = 'Northern Boreal'
#         else:
#             event_info['region'] = 'Other'
            
#         # Add latitude band
#         if lat < 55:
#             event_info['lat_band'] = '<55°N'
#         elif lat < 60:
#             event_info['lat_band'] = '55-60°N'
#         elif lat < 66.5:
#             event_info['lat_band'] = '60-66.5°N'
#         elif lat < 70:
#             event_info['lat_band'] = '66.5-70°N'
#         elif lat < 75:
#             event_info['lat_band'] = '70-75°N'
#         elif lat < 80:
#             event_info['lat_band'] = '75-80°N'
#         else:
#             event_info['lat_band'] = '>80°N'
#     else:
#         event_info['region'] = None
#         event_info['lat_band'] = None
    
#     return event_info

# # zero_curtain_pipeline.py
# import os
# import numpy as np
# import pandas as pd
# import gc
# import time
# from tqdm.auto import tqdm

# def run_memory_efficient_pipeline(feather_path, output_dir=None, 
#                                   site_batch_size=20, checkpoint_interval=5, 
#                                   max_gap_hours=6, interpolation_method='cubic', 
#                                   force_restart=False):
#     """
#     Run the complete memory-efficient zero curtain detection pipeline
    
#     Parameters:
#     -----------
#     feather_path : str
#         Path to the feather file
#     output_dir : str
#         Directory to save results and checkpoints
#     site_batch_size : int
#         Number of sites to process in each batch (smaller = less memory)
#     checkpoint_interval : int
#         Number of sites between saving checkpoints
#     max_gap_hours : float
#         Maximum gap hours for interpolation
#     interpolation_method : str
#         Interpolation method ('linear', 'cubic')
        
#     Returns:
#     --------
#     pandas.DataFrame
#         DataFrame with detected zero curtain events
#     """
#     print("=" * 80)
#     print("TRULY MEMORY-EFFICIENT ZERO CURTAIN DETECTION")
#     print("=" * 80)
#     print(f"Initial memory usage: {memory_usage():.1f} MB")
    
#     # Set up directories
#     if output_dir is not None:
#         os.makedirs(output_dir, exist_ok=True)
#         checkpoint_dir = os.path.join(output_dir, 'checkpoints')
#         os.makedirs(checkpoint_dir, exist_ok=True)
#     else:
#         checkpoint_dir = None
    
#     # Start timing
#     start_time = time.time()

#     def save_checkpoint(data, name):
#         if checkpoint_dir:
#             backup_path = os.path.join(checkpoint_dir, f'{name}_backup.pkl')
#             target_path = os.path.join(checkpoint_dir, f'{name}.pkl')
            
#             # First save to backup file
#             with open(backup_path, 'wb') as f:
#                 pickle.dump(data, f)
            
#             # Then rename to target (atomic operation)
#             import shutil
#             shutil.move(backup_path, target_path)
            
#             print(f"Saved checkpoint to {target_path}")
    
#     # Function to load checkpoint
#     def load_checkpoint(name):
#         if checkpoint_dir:
#             try:
#                 with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'rb') as f:
#                     data = pickle.load(f)
#                 print(f"Loaded checkpoint from {name}.pkl")
#                 return data
#             except:
#                 print(f"No checkpoint found for {name}.pkl")
#         return None
    
#     # Step 1: Get site-depth combinations
#     site_depths = None
#     if not force_restart:
#         site_depths = load_checkpoint('site_depths')
    
#     if site_depths is None:
#         print("\nStep 1: Finding unique site-depth combinations...")
#         site_depths = get_unique_site_depths(feather_path)
#         save_checkpoint(site_depths, 'site_depths')
    
#     total_combinations = len(site_depths)
#     print(f"Found {total_combinations} unique site-depth combinations")
    
#     # Step 2: Initialize results
#     all_events = []
#     processed_indices = set()
    
#     if not force_restart:
#         saved_events = load_checkpoint('all_events')
#         if saved_events is not None:
#             if isinstance(saved_events, list):
#                 all_events = saved_events
#             else:
#                 # If it's a DataFrame, convert to list of dicts
#                 all_events = saved_events.to_dict('records') if len(saved_events) > 0 else []
        
#         saved_indices = load_checkpoint('processed_indices')
#         if saved_indices is not None:
#             processed_indices = set(saved_indices)
    
#     print(f"Starting from {len(processed_indices)}/{total_combinations} processed sites")
#     print(f"Current event count: {len(all_events)}")

#     # Step 3: Process in batches
#     print("\nProcessing site-depth combinations in batches")
    
#     # For tracking new events in this run
#     new_events_count = 0
    
#     # Create batches for processing
#     total_batches = (total_combinations + site_batch_size - 1) // site_batch_size
    
#     for batch_idx in range(total_batches):
#         batch_start = batch_idx * site_batch_size
#         batch_end = min(batch_start + site_batch_size, total_combinations)
        
#         # Skip if already processed
#         batch_indices = set(range(batch_start, batch_end))
#         if batch_indices.issubset(processed_indices):
#             print(f"Batch {batch_idx+1}/{total_batches} (sites {batch_start+1}-{batch_end}/{total_...
#             continue
        
#         print(f"\nProcessing batch {batch_idx+1}/{total_batches} (sites {batch_start+1}-{batch_end...
#         print(f"Memory before batch: {memory_usage():.1f} MB")
        
#         # Force garbage collection
#         gc.collect()
        
#         # Process each site in batch
#         for site_idx in range(batch_start, batch_end):
#             # Skip if already processed
#             if site_idx in processed_indices:
#                 continue
            
#             site = site_depths.iloc[site_idx]['source']
#             temp_depth = site_depths.iloc[site_idx]['soil_temp_depth']
            
#             print(f"\nProcessing site {site_idx+1}/{total_combinations}: {site}, depth: {temp_dept...
            
#             try:
#                 # Load site data efficiently
#                 site_df = load_site_depth_data(feather_path, site, temp_depth)
                
#                 # Skip if insufficient data
#                 if len(site_df) < 10:
#                     print(f"  Insufficient data ({len(site_df)} rows), skipping")
#                     processed_indices.add(site_idx)
#                     continue
                
#                 # Process for zero curtain
#                 site_events = process_site_for_zero_curtain(
#                     site_df, site, temp_depth, 
#                     max_gap_hours, interpolation_method
#                 )
                
#                 # Add to all events
#                 new_events = len(site_events)
#                 all_events.extend(site_events)
#                 new_events_count += new_events
                
#                 print(f"  Found {new_events} events, new total: {len(all_events)}")
                
#                 # Mark as processed
#                 processed_indices.add(site_idx)
                
#                 # Clean up
#                 #del site_df, site_events
#                 #gc.collect()
            
#                 # Save checkpoint periodically
#                 if site_idx % checkpoint_interval == 0:
#                     save_checkpoint(all_events, 'all_events')
#                     save_checkpoint(list(processed_indices), 'processed_indices')
                    
#                 # Clean up
#                 del site_df, site_events
#                 gc.collect()

#             except Exception as e:
#                 print(f"  Error processing site {site}, depth {temp_depth}: {str(e)}")
#                 continue # Continue with next site
        
#         # Save checkpoint after each batch
#         print(f"Saving checkpoint after batch {batch_idx+1}/{total_batches}")
#         save_checkpoint(all_events, 'all_events')
#         save_checkpoint(list(processed_indices), 'processed_indices')
        
#         # Also save intermediate results as CSV
#         if len(all_events) > 0:
#             interim_df = pd.DataFrame(all_events)
#             if output_dir is not None:
#                 interim_path = os.path.join(output_dir, 'events_checkpoint.csv')
#                 interim_df.to_csv(interim_path, index=False)
#                 print(f"Saved interim results to {interim_path}")
        
#         print(f"Memory after batch: {memory_usage():.1f} MB")
    
#     # Step 4: Create final dataframe
#     print("\nCreating final events dataframe")
    
#     if len(all_events) > 0:
#         events_df = pd.DataFrame(all_events)
#         print(f"Created events dataframe with {len(events_df)} total events ({new_events_count} ne...
#     else:
#         # Create empty dataframe with correct columns
#         events_df = pd.DataFrame(columns=[
#             'source', 'soil_temp_depth', 'soil_temp_depth_zone', 
#             'datetime_min', 'datetime_max', 'duration_hours',
#             'observation_count', 'observations_per_day',
#             'soil_temp_mean', 'soil_temp_min', 'soil_temp_max', 'soil_temp_std',
#             'season', 'latitude', 'longitude', 'year', 'month',
#             'soil_moist_mean', 'soil_moist_std', 'soil_moist_min', 'soil_moist_max', 
#             'soil_moist_change', 'soil_moist_depth',
#             'temp_gradient_mean', 'temp_stability',
#             'year_month', 'region', 'lat_band'
#         ])
#         print("No events found")
    
#     # Save final results
#     if output_dir is not None:
#         final_path = os.path.join(output_dir, 'zero_curtain_events.csv')

#         temp_path = os.path.join(output_dir, 'zero_curtain_events_temp.csv')
#         events_df.to_csv(temp_path, index=False)

#         import shutil
#         shutil.move(temp_path, final_path)
        
#         print(f"Saved final results to {final_path}")
    
#     # Report timing
#     total_time = time.time() - start_time
#     print(f"\nPipeline completed in {total_time:.1f} seconds ({total_time/60:.1f} minutes)")
#     print(f"Final memory usage: {memory_usage():.1f} MB")
    
#     return events_df

# def diagnose_zero_curtain_durations(events_df, output_dir=None):
#     """
#     Comprehensive analysis of zero curtain duration patterns to identify
#     the source of value clustering and recommend fixes.
    
#     Parameters:
#     -----------
#     events_df : pandas.DataFrame
#         DataFrame containing zero curtain events
#     output_dir : str, optional
#         Directory to save diagnostic outputs
        
#     Returns:
#     --------
#     dict
#         Dictionary containing diagnostic results
#     """
#     print("=" * 80)
#     print("ZERO CURTAIN DURATION PATTERN ANALYSIS")
#     print("=" * 80)
    
#     # 1. Basic statistics
#     duration_values = events_df['duration_hours'].values
#     n_events = len(duration_values)
#     n_unique = len(np.unique(duration_values))
    
#     print(f"\nTotal events: {n_events}")
#     print(f"Unique duration values: {n_unique} ({n_unique/n_events*100:.1f}%)")
    
#     # Calculate full statistics
#     stats_df = pd.DataFrame({
#         'Statistic': ['Mean', 'Median', 'Std Dev', 'Min', 'Max', 'Q1', 'Q3', 'IQR'],
#         'Value': [
#             duration_values.mean(),
#             np.median(duration_values),
#             duration_values.std(),
#             duration_values.min(),
#             duration_values.max(),
#             np.percentile(duration_values, 25),
#             np.percentile(duration_values, 75),
#             np.percentile(duration_values, 75) - np.percentile(duration_values, 25)
#         ]
#     })
    
#     print("\nBasic Statistics:")
#     for _, row in stats_df.iterrows():
#         print(f"  {row['Statistic']}: {row['Value']:.2f}")
    
#     # 2. Value frequency analysis
#     from collections import Counter
#     value_counts = Counter(duration_values)
#     common_values = pd.Series(value_counts).sort_values(ascending=False).head(20)
    
#     print("\nMost common duration values:")
#     for val, count in common_values.items():
#         print(f"  {val:.2f} hours: {count} events ({count/n_events*100:.1f}%)")
    
#     # 3. Check for patterns in the values
#     # Are values clustering at specific intervals?
#     rounded_to_hour = np.round(duration_values)
#     rounded_to_day = np.round(duration_values / 24) * 24
    
#     print("\nRounding patterns:")
#     print(f"  Events matching exact hours: {np.sum(duration_values == rounded_to_hour)} ({np.sum(d...
#     print(f"  Events matching exact days: {np.sum(duration_values == rounded_to_day)} ({np.sum(dur...
    
#     # Check for specific hour intervals (6h, 12h, 24h)
#     for interval in [1, 6, 12, 24]:
#         rounded = np.round(duration_values / interval) * interval
#         match_pct = np.sum(duration_values == rounded) / n_events * 100
#         print(f"  Events at {interval}h intervals: {np.sum(duration_values == rounded)} ({match_pc...
    
#     # 4. Create visualizations
#     # Distribution plot
#     plt.figure(figsize=(12, 5))
    
#     plt.subplot(121)
#     sns.histplot(duration_values, bins=50, kde=True)
#     plt.title('Distribution of Duration Values')
#     plt.xlabel('Duration (hours)')
#     plt.ylabel('Frequency')
    
#     # Log scale for better visibility
#     plt.subplot(122)
#     sns.histplot(duration_values, bins=50, kde=True, log_scale=(False, True))
#     plt.title('Distribution (Log Scale)')
#     plt.xlabel('Duration (hours)')
#     plt.ylabel('Log Frequency')
    
#     if output_dir:
#         os.makedirs(output_dir, exist_ok=True)
#         plt.tight_layout()
#         plt.savefig(os.path.join(output_dir, 'distribution.png'), dpi=150)
    
#     # Duration value heatmap
#     # Create a 2D histogram based on:
#     # X-axis: Duration value
#     # Y-axis: Remainder when divided by 24 (to check for daily patterns)
    
#     # Filter to reasonable range for visualization
#     filtered_durations = duration_values[duration_values <= np.percentile(duration_values, 95)]
    
#     plt.figure(figsize=(14, 6))
    
#     # 2D histogram for patterns
#     hours_remainder = filtered_durations % 24
#     plt.subplot(121)
#     plt.hist2d(
#         filtered_durations, 
#         hours_remainder, 
#         bins=[50, 24],
#         cmap='viridis'
#     )
#     plt.colorbar(label='Count')
#     plt.title('Duration Patterns')
#     plt.xlabel('Duration (hours)')
#     plt.ylabel('Hours Remainder (duration % 24)')
    
#     # Plot the relationship between durations and observation count
#     plt.subplot(122)
    
#     # Group by source and calculate stats
#     site_durations = events_df.groupby('source').agg({
#         'duration_hours': ['count', 'mean', 'median']
#     })
    
#     site_durations.columns = ['_'.join(col).strip('_') for col in site_durations.columns]
    
#     plt.scatter(
#         site_durations['duration_hours_count'],
#         site_durations['duration_hours_mean'],
#         alpha=0.5
#     )
#     plt.title('Relationship: Event Count vs Duration')
#     plt.xlabel('Number of Events per Site')
#     plt.ylabel('Mean Duration (hours)')
#     plt.grid(alpha=0.3)
    
#     if output_dir:
#         plt.tight_layout()
#         plt.savefig(os.path.join(output_dir, 'patterns.png'), dpi=150)
    
#     # 5. Check for algorithm artifacts
#     # Look for signs of temporal aliasing or measurement effects
#     temporal_patterns = events_df.copy()
    
#     # Extract detection algorithm artifacts if datetime columns exist
#     if 'datetime_min' in events_df.columns and 'datetime_max' in events_df.columns:
#         if not pd.api.types.is_datetime64_dtype(events_df['datetime_min']):
#             temporal_patterns['datetime_min'] = pd.to_datetime(events_df['datetime_min'])
#         if not pd.api.types.is_datetime64_dtype(events_df['datetime_max']):
#             temporal_patterns['datetime_max'] = pd.to_datetime(events_df['datetime_max'])
            
#         # Extract time of day and day of week
#         temporal_patterns['start_hour'] = temporal_patterns['datetime_min'].dt.hour
#         temporal_patterns['end_hour'] = temporal_patterns['datetime_max'].dt.hour
#         temporal_patterns['start_day'] = temporal_patterns['datetime_min'].dt.dayofweek
#         temporal_patterns['end_day'] = temporal_patterns['datetime_max'].dt.dayofweek
        
#         # Plot patterns
#         fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        
#         sns.histplot(data=temporal_patterns, x='start_hour', kde=True, ax=axes[0, 0])
#         axes[0, 0].set_title('Event Start Hour')
        
#         sns.histplot(data=temporal_patterns, x='end_hour', kde=True, ax=axes[0, 1])
#         axes[0, 1].set_title('Event End Hour')
        
#         sns.histplot(data=temporal_patterns, x='start_day', kde=True, ax=axes[1, 0])
#         axes[1, 0].set_title('Event Start Day')
#         axes[1, 0].set_xticks(range(7))
#         axes[1, 0].set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])
        
#         sns.histplot(data=temporal_patterns, x='end_day', kde=True, ax=axes[1, 1])
#         axes[1, 1].set_title('Event End Day')
#         axes[1, 1].set_xticks(range(7))
#         axes[1, 1].set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])
        
#         if output_dir:
#             plt.tight_layout()
#             plt.savefig(os.path.join(output_dir, 'temporal.png'), dpi=150)
    
#     # 6. Correlation with other variables
#     # Check if duration is correlated with geographic location
#     correlation_data = events_df.copy()
    
#     plt.figure(figsize=(14, 5))
    
#     plt.subplot(121)
#     plt.scatter(
#         correlation_data['latitude'],
#         correlation_data['duration_hours'],
#         alpha=0.1,
#         s=3
#     )
#     plt.title('Duration vs Latitude')
#     plt.xlabel('Latitude')
#     plt.ylabel('Duration (hours)')
#     plt.grid(alpha=0.3)
    
#     plt.subplot(122)
#     plt.scatter(
#         correlation_data['soil_temp_depth'],
#         correlation_data['duration_hours'],
#         alpha=0.1,
#         s=3
#     )
#     plt.title('Duration vs Soil Depth')
#     plt.xlabel('Soil Temperature Depth')
#     plt.ylabel('Duration (hours)')
#     plt.grid(alpha=0.3)
    
#     if output_dir:
#         plt.tight_layout()
#         plt.savefig(os.path.join(output_dir, 'correlations.png'), dpi=150)
    
#     # 7. Generate recommendations based on findings
#     print("\n" + "=" * 80)
#     print("DIAGNOSIS AND RECOMMENDATIONS")
#     print("=" * 80)
    
#     # Check for daily measurement effects
#     day_effect = np.sum(duration_values % 24 == 0) / n_events * 100
#     if day_effect > 50:
#         print("\nDiagnosis: Strong daily measurement effect detected.")
#         print(f"  {day_effect:.1f}% of durations are exact multiples of 24 hours.")
#         print("  This suggests temporal aliasing due to measurement frequency.")
#         print("\nRecommendation:")
#         print("  1. Review zero_curtain_detection temporal parameters")
#         print("  2. Decrease 'max_gap_hours' to allow for more fine-grained detection")
#         print("  3. Apply interpolation to estimate more precise event transitions")
    
#     # Check for binning or rounding
#     if n_unique / n_events < 0.1:
#         print("\nDiagnosis: Severe value binning or rounding detected.")
#         print(f"  Only {n_unique} unique values for {n_events} events ({n_unique/n_events*100:.1f}...
#         print("\nRecommendation:")
#         print("  1. Check for explicit rounding in duration calculations")
#         print("  2. Use higher precision timestamps for event boundaries")
#         print("  3. Consider continuous time representation instead of discrete bins")
    
#     # Check for IQR issues
#     q1 = np.percentile(duration_values, 25)
#     q3 = np.percentile(duration_values, 75)
#     iqr = q3 - q1
    
#     if iqr < 1e-6:
#         print("\nDiagnosis: Zero or near-zero IQR detected.")
#         print(f"  Q1 and Q3 are both {q1:.2f}, creating visualization challenges.")
#         print("\nRecommendation:")
#         print("  1. Use percentile-based visualization bounds instead of IQR")
#         print("  2. For visualization, force a minimum range based on domain knowledge")
#         print("  3. Consider a non-linear transformation of duration values")
    
#     # Return diagnostic results
#     return {
#         'n_events': n_events,
#         'n_unique': n_unique,
#         'statistics': stats_df,
#         'common_values': common_values,
#         'day_effect_pct': day_effect,
#         'q1': q1,
#         'q3': q3,
#         'iqr': iqr
#     }
    
# print(f"  Starting memory: {memory_usage():.1f} MB")

# events_df = run_memory_efficient_pipeline(
#     feather_path='merged_compressed.feather', 
#     output_dir='zero_curtain_pipeline', 
#     site_batch_size=50, 
#     checkpoint_interval=5, 
#     max_gap_hours=6, 
#     interpolation_method='cubic'
# )

# # ALL CODE FOR DEBUGGING

# pd.read_csv('zero_curtain_pipeline/modeling/spatial_model/training_log.csv')

# epoch	accuracy	auc	loss	lr	val_accuracy	val_auc	val_loss
# 0	0	0.598504	0.610038	9246419.00	0.00100	0.857411	0.978768	0.436933
# 1	1	0.866324	0.958747	5390313.00	0.00100	0.856785	0.990086	0.327265
# 2	2	0.867492	0.972548	4335007.00	0.00100	0.856785	0.991446	0.281842
# 3	3	0.867726	0.976552	3801146.50	0.00100	0.856785	0.991654	0.251187
# 4	4	0.867960	0.982853	3362294.25	0.00100	0.856785	0.991592	0.230201
# 5	5	0.868427	0.983168	3142311.75	0.00100	0.856785	0.990961	0.215414
# 6	6	0.868895	0.984761	2929097.25	0.00100	0.860225	0.990162	0.204021
# 7	7	0.872867	0.983782	2771868.75	0.00100	0.862727	0.989316	0.194286
# 8	8	0.881514	0.985154	2643040.00	0.00100	0.875860	0.988497	0.185386
# 9	9	0.891330	0.985028	2519734.25	0.00100	0.895872	0.988350	0.177309
# 10	10	0.905819	0.983766	2429415.25	0.00100	0.912758	0.988077	0.169468
# 11	11	0.920308	0.984604	2325136.00	0.00050	0.919950	0.988076	0.165672
# 12	12	0.924749	0.985428	2290334.75	0.00050	0.928705	0.988062	0.161862
# 13	13	0.929189	0.984720	2249495.50	0.00050	0.936210	0.988175	0.158245
# 14	14	0.933162	0.985094	2187842.75	0.00050	0.941213	0.988185	0.154699
# 15	15	0.937135	0.986027	2141003.25	0.00050	0.944966	0.988177	0.151161
# 16	16	0.938537	0.985536	2085697.50	0.00050	0.949343	0.988206	0.147746
# 17	17	0.946015	0.985611	2046315.00	0.00050	0.952783	0.988482	0.144436
# 18	18	0.947885	0.986006	1998005.25	0.00025	0.954034	0.988558	0.142796

# #test predictions
# pd.DataFrame(np.load('zero_curtain_pipeline/modeling/spatial_model/test_predictions_latest.npy'))

# 0
# 0	0.006279
# 1	0.001496
# 2	0.003124
# 3	0.042740
# 4	0.333528
# ...	...
# 4279421	0.003894
# 4279422	0.003436
# 4279423	0.004367
# 4279424	0.463335
# 4279425	0.003021
# 4279426 rows × 1 columns

# pd.read_csv('zero_curtain_pipeline/zero_curtain_events_sorted.csv')

# datetime_min	datetime_max	duration_hours	year	month	year_month	season	latitude	longitude	lat_band	...
# 0	1907-04-15 00:00:00	1907-06-15 00:00:00	1464.0	1907.0	4	1907.0-04	Spring	52.283299	104.300003	<5...
# 1	1910-01-15 12:00:00	1910-06-15 00:00:00	3612.0	1910.0	1	1910.0-01	Winter	52.283299	104.300003	<5...
# 2	1910-07-15 12:00:00	1911-02-14 00:00:00	5124.0	1910.0	7	1910.0-07	Summer	54.716701	128.933304	<5...
# 3	1911-08-15 12:00:00	1912-01-15 12:00:00	3672.0	1911.0	8	1911.0-08	Summer	54.716701	128.933304	<5...
# 4	1913-04-15 00:00:00	1913-06-15 00:00:00	1464.0	1913.0	4	1913.0-04	Spring	52.283299	104.300003	<5...
# ...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
# 87858	2023-05-12 00:00:00.000000000	2023-05-25 00:00:00.000000000	312.0	2023.0	5	2023.0-05	Spring	...
# 87859	2023-05-20 00:00:00.000000000	2023-07-19 00:00:00.000000000	1440.0	2023.0	5	2023.0-05	Spring...
# 87860	2023-05-23 00:00:00.000000000	2023-05-30 00:00:00.000000000	168.0	2023.0	5	2023.0-05	Spring	...
# 87861	2023-06-07 00:00:00.000000000	2023-06-20 00:00:00.000000000	312.0	2023.0	6	2023.0-06	Summer	...
# 87862	2023-07-15 00:00:00.000000000	2023-07-31 00:00:00.000000000	384.0	2023.0	7	2023.0-07	Summer	...
# 87863 rows × 28 columns


# import os,sys
# import cartopy.crs as ccrs
# import cartopy.feature as cfeature
# import cmocean
# import gc
# import glob
# import json
# import logging
# import matplotlib.gridspec as gridspec
# import matplotlib.pyplot as plt
# import numpy as np
# import os
# import pandas as pd
# import pathlib
# import pickle
# import psutil
# import re
# import gc
# import dask
# import dask.dataframe as dd
# import scipy.interpolate as interpolate
# import scipy.stats as stats
# import seaborn as sns
# from pyproj import Proj
# import sklearn.experimental
# import sklearn.impute
# import sklearn.linear_model
# import sklearn.preprocessing
# import tqdm
# import xarray as xr
# import warnings
# warnings.filterwarnings('ignore')

# from osgeo import gdal, osr
# from matplotlib.colors import LinearSegmentedColormap
# from concurrent.futures import ThreadPoolExecutor
# from datetime import datetime, timedelta
# from pathlib import Path
# from scipy.spatial import cKDTree
# from tqdm import tqdm
# from tqdm.notebook import tqdm

# import tensorflow as tf
# try:
#     physical_devices = tf.config.list_physical_devices('GPU')
#     for device in physical_devices:
#         tf.config.experimental.set_memory_growth(device, True)
# except:
#     pass

# print("TensorFlow version:", tf.__version__)

# import keras_tuner as kt
# from keras_tuner.tuners import BayesianOptimization
# #os.environ["TF_FORCE_GPU_ALLOW_GROWTH"] = "true"
# #os.environ["DEVICE_COUNT_GPU"] = "0"
# os.environ["CUDA_VISIBLE_DEVICES"] = "-1" # DISABLE GPU
# import tensorflow as tf
# import json
# import glob
# import keras
# from keras import layers
# from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
# import seaborn as sns
# from sklearn.metrics import confusion_matrix, classification_report

# TensorFlow version: 2.9.0

# from packaging import version

# print("TensorFlow version: ", tf.__version__)
# assert version.parse(tf.__version__).release[0] >= 2, \
#     "This notebook requires TensorFlow 2.0 or above."

# print("==========================")

# print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
# print(sys.getrecursionlimit())
# sys.setrecursionlimit(1000000000)
# print(sys.getrecursionlimit())

# TensorFlow version:  2.9.0
# ==========================
# Num GPUs Available:  1
# 3000
# 1000000000

# import os
# import gc
# import json
# import pickle
# import numpy as np
# import tensorflow as tf
# from tensorflow.keras.callbacks import (
#     EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, 
#     CSVLogger, TensorBoard
# )
# import matplotlib.pyplot as plt
# from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
# from tqdm import tqdm
# import psutil
# from datetime import datetime

# def memory_usage():
#     """Get current memory usage in MB"""
#     process = psutil.Process(os.getpid())
#     mem_info = process.memory_info()
#     return mem_info.rss / 1024**2  # Memory in MB

# def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
#     """
#     Build the advanced zero curtain detection model.
#     """
#     # This is your existing model building function
#     from tensorflow.keras.models import Model
#     from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
#     from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPoo...
#     from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
#     from tensorflow.keras.optimizers import Adam
    
#     # Input layer
#     inputs = Input(shape=input_shape)
    
#     # Reshape for ConvLSTM (add spatial dimension)
#     # From (sequence_length, features) to (sequence_length, 1, features)
#     x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
#     # ConvLSTM layer to capture spatiotemporal patterns
#     from tensorflow.keras.layers import ConvLSTM2D
#     convlstm = ConvLSTM2D(
#         filters=64,
#         kernel_size=(3, 1),
#         padding='same',
#         return_sequences=True,
#         activation='tanh',
#         recurrent_dropout=0.2
#     )(x)
    
#     # Reshape back to (sequence_length, features)
#     convlstm = Reshape((input_shape[0], 64))(convlstm)
    
#     # Add positional encoding for transformer
#     def positional_encoding(length, depth):
#         positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
#         depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
#         angle_rates = 1 / tf.pow(10000.0, depths)
#         angle_rads = positions * angle_rates
        
#         pos_encoding = tf.concat(
#             [tf.sin(angle_rads), tf.cos(angle_rads)], axis=-1)
        
#         return pos_encoding
    
#     # Add positional encoding
#     pos_encoding = positional_encoding(input_shape[0], 64)
#     transformer_input = convlstm + pos_encoding
    
#     # Transformer encoder block
#     def transformer_encoder(x, num_heads=8, key_dim=64, ff_dim=128):
#         # Multi-head attention
#         attention_output = MultiHeadAttention(
#             num_heads=num_heads, key_dim=key_dim
#         )(x, x)
        
#         # Skip connection 1
#         x1 = Add()([attention_output, x])
#         x1 = LayerNormalization(epsilon=1e-6)(x1)
        
#         # Feed-forward network
#         ff_output = Dense(ff_dim, activation='relu')(x1)
#         ff_output = Dropout(0.1)(ff_output)
#         ff_output = Dense(64)(ff_output)
        
#         # Skip connection 2
#         x2 = Add()([ff_output, x1])
#         return LayerNormalization(epsilon=1e-6)(x2)
    
#     # Apply transformer encoder
#     transformer_output = transformer_encoder(transformer_input)
    
#     # Parallel CNN paths for multi-scale feature extraction
#     cnn_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(inputs)
#     cnn_1 = BatchNormalization()(cnn_1)
    
#     cnn_2 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(inputs)
#     cnn_2 = BatchNormalization()(cnn_2)
    
#     cnn_3 = Conv1D(filters=32, kernel_size=7, padding='same', activation='relu')(inputs)
#     cnn_3 = BatchNormalization()(cnn_3)
    
#     # Variational Autoencoder components
#     def sampling(args):
#         z_mean, z_log_var = args
#         batch = tf.shape(z_mean)[0]
#         dim = tf.shape(z_mean)[1]
#         epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
#         return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
#     # Global temporal features
#     global_max = GlobalMaxPooling1D()(transformer_output)
#     global_avg = GlobalAveragePooling1D()(transformer_output)
    
#     # VAE encoding
#     z_mean = Dense(32)(Concatenate()([global_max, global_avg]))
#     z_log_var = Dense(32)(Concatenate()([global_max, global_avg]))
#     z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
#     # Combine all features
#     merged_features = Concatenate()(
#         [
#             GlobalMaxPooling1D()(cnn_1),
#             GlobalMaxPooling1D()(cnn_2),
#             GlobalMaxPooling1D()(cnn_3),
#             global_max,
#             global_avg,
#             z
#         ]
#     )
    
#     # Final classification layers
#     x = Dense(128, activation='relu')(merged_features)
#     x = Dropout(0.3)(x)
#     x = BatchNormalization()(x)
#     x = Dense(64, activation='relu')(x)
#     x = Dropout(0.2)(x)
    
#     # Output layer
#     outputs = Dense(1, activation='sigmoid')(x)
    
#     # Create model
#     model = Model(inputs=inputs, outputs=outputs)
    
#     # Add VAE loss
#     kl_loss = -0.5 * tf.reduce_mean(
#         z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
#     )
#     model.add_loss(0.001 * kl_loss)  # Small weight for VAE loss
    
#     # Compile model with appropriate metrics
#     model.compile(
#         optimizer=Adam(learning_rate=0.001),
#         loss='binary_crossentropy',
#         metrics=[
#             'accuracy',
#             tf.keras.metrics.AUC(name='auc'),
#             tf.keras.metrics.Precision(name='precision'),
#             tf.keras.metrics.Recall(name='recall')
#         ]
#     )
    
#     return model

# def train_with_spatial_balancing(X, y, metadata, output_dir=None, checkpoint_dir=None):
#     """
#     Train a model with spatially balanced sampling.
    
#     Parameters:
#     -----------
#     X : numpy.ndarray
#         Input features
#     y : numpy.ndarray
#         Output labels
#     metadata : list
#         Metadata containing timestamps and spatial information
#     output_dir : str, optional
#         Directory to save model and results
#     checkpoint_dir : str, optional
#         Directory for saving checkpoints
        
#     Returns:
#     --------
#     trained_model, training_history
#     """
#     print("Training zero curtain model with spatial balancing...")
#     print(f"Memory before training: {memory_usage():.1f} MB")
    
#     # Create output directory
#     if output_dir:
#         os.makedirs(output_dir, exist_ok=True)
    
#     if checkpoint_dir is None:
#         checkpoint_dir = os.path.join(output_dir, 'checkpoints') if output_dir else 'checkpoints'
#     os.makedirs(checkpoint_dir, exist_ok=True)
    
#     # Enable memory growth to avoid pre-allocating all GPU memory
#     physical_devices = tf.config.list_physical_devices('GPU')
#     if physical_devices:
#         for device in physical_devices:
#             try:
#                 tf.config.experimental.set_memory_growth(device, True)
#                 print(f"Enabled memory growth for {device}")
#             except:
#                 print(f"Could not set memory growth for {device}")
    
#     # Create spatiotemporally balanced train/val/test split
#     print("Creating spatiotemporally balanced split...")
#     train_indices, val_indices, test_indices = stratified_spatiotemporal_split(
#         X, y, metadata, test_size=0.2, val_size=0.15, checkpoint_dir=checkpoint_dir
#     )
    
#     # Create the splits
#     X_train = X[train_indices]
#     y_train = y[train_indices]
    
#     X_val = X[val_indices]
#     y_val = y[val_indices]
    
#     X_test = X[test_indices]
#     y_test = y[test_indices]
    
#     # Load spatial density weights if available
#     weights_file = os.path.join(checkpoint_dir, "spatial_density.pkl")
#     if os.path.exists(weights_file):
#         print(f"Loading spatial weights from {weights_file}")
#         with open(weights_file, "rb") as f:
#             weights_data = pickle.load(f)
#         sample_weights = weights_data["weights"][train_indices]
        
#         # Normalize weights
#         sample_weights = sample_weights / np.mean(sample_weights) * len(sample_weights)
#     else:
#         print("No spatial weights found, using uniform weights")
#         sample_weights = np.ones(len(train_indices))
    
#     # Print info about the splits
#     print(f"Train/val/test sizes: {len(X_train)}/{len(X_val)}/{len(X_test)}")
#     print(f"Positive examples: Train={np.sum(y_train)} ({np.sum(y_train)/len(y_train)*100:.1f}%), ...
#           f"Val={np.sum(y_val)} ({np.sum(y_val)/len(y_val)*100:.1f}%), " +
#           f"Test={np.sum(y_test)} ({np.sum(y_test)/len(y_test)*100:.1f}%)")
    
#     # Combine sample weights with class weights for imbalanced data
#     pos_weight = (len(y_train) - np.sum(y_train)) / max(1, np.sum(y_train))
#     class_weight = {0: 1.0, 1: pos_weight}
#     print(f"Using class weight {pos_weight:.2f} for positive examples")
    
#     # Build model with appropriate input shape
#     input_shape = (X_train.shape[1], X_train.shape[2])
#     model = build_advanced_zero_curtain_model(input_shape)
    
#     # Always check for existing model checkpoint to resume training
#     model_checkpoint_paths = []
#     if output_dir:
#         # Look for checkpoint files in multiple locations
#         model_checkpoint_path1 = os.path.join(output_dir, 'model_checkpoint.h5')
#         model_checkpoint_path2 = os.path.join(output_dir, 'checkpoint.h5')
#         model_checkpoint_path3 = os.path.join(checkpoint_dir, 'model_checkpoint.h5')
        
#         model_checkpoint_paths = [p for p in [model_checkpoint_path1, model_checkpoint_path2, mode...
#                                 if os.path.exists(p)]
        
#         if model_checkpoint_paths:
#             print(f"Found {len(model_checkpoint_paths)} existing model checkpoints")
#             # Use the most recent checkpoint based on modification time
#             latest_checkpoint = max(model_checkpoint_paths, key=os.path.getmtime)
#             print(f"Loading most recent checkpoint: {latest_checkpoint}")
#             try:
#                 model = tf.keras.models.load_model(latest_checkpoint)
#                 print("Checkpoint loaded successfully - will resume training from this point")
#             except Exception as e:
#                 print(f"Error loading checkpoint: {str(e)}")
#                 print("Will start training from scratch")
    
#     # Set up callbacks
#     callbacks = [
#         # Stop training when validation performance plateaus
#         EarlyStopping(
#             patience=15,
#             restore_best_weights=True,
#             monitor='val_auc',
#             mode='max'
#         ),
#         # Reduce learning rate when improvement slows
#         ReduceLROnPlateau(
#             factor=0.5,
#             patience=7,
#             min_lr=1e-6,
#             monitor='val_auc',
#             mode='max'
#         ),
#         # Manual garbage collection after each epoch
#         tf.keras.callbacks.LambdaCallback(
#             on_epoch_end=lambda epoch, logs: gc.collect()
#         )
#     ]
    
#     # Add additional callbacks if output directory provided
#     if output_dir:
#         callbacks.extend([
#             # Save best model
#             ModelCheckpoint(
#                 os.path.join(output_dir, 'model_checkpoint.h5'),
#                 save_best_only=True,
#                 monitor='val_auc',
#                 mode='max'
#             ),
#             # Log training progress to CSV
#             CSVLogger(
#                 os.path.join(output_dir, 'training_log.csv'),
#                 append=True
#             ),
#             # TensorBoard visualization
#             TensorBoard(
#                 log_dir=os.path.join(output_dir, 'tensorboard_logs'),
#                 histogram_freq=1,
#                 profile_batch=0  # Disable profiling to save memory
#             )
#         ])
    
#     # Train model
#     print("Starting model training...")
#     batch_size = 32  # Adjust based on available memory
#     epochs = 100
    
#     history = model.fit(
#         X_train, y_train,
#         validation_data=(X_val, y_val),
#         epochs=epochs,
#         batch_size=batch_size,
#         callbacks=callbacks,
#         class_weight=class_weight,
#         sample_weight=sample_weights,
#         verbose=1,
#         shuffle=True,
#         use_multiprocessing=False,  # Avoid memory overhead
#         workers=1  # Reduce parallel processing
#     )
    
#     # Clean up to free memory
#     del X_train, y_train, X_val, y_val
#     gc.collect()
    
#     # Evaluate on test set
#     print("Evaluating model on test set...")
#     evaluation = model.evaluate(X_test, y_test, batch_size=batch_size)
#     print("Test performance:")
#     for metric, value in zip(model.metrics_names, evaluation):
#         print(f"  {metric}: {value:.4f}")
    
#     # Generate predictions for test set
#     y_pred_prob = model.predict(X_test, batch_size=batch_size)
#     y_pred = (y_pred_prob > 0.5).astype(int).flatten()
    
#     # Calculate additional evaluation metrics
#     report = classification_report(y_test, y_pred)
#     conf_matrix = confusion_matrix(y_test, y_pred)
    
#     print("Classification Report:")
#     print(report)
    
#     print("Confusion Matrix:")
#     print(conf_matrix)
    
#     # Save evaluation results
#     if output_dir:
#         # Save evaluation metrics
#         with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w') as f:
#             f.write("Classification Report:\n")
#             f.write(report)
#             f.write("\n\nConfusion Matrix:\n")
#             f.write(str(conf_matrix))
#             f.write("\n\nTest Metrics:\n")
#             for metric, value in zip(model.metrics_names, evaluation):
#                 f.write(f"{metric}: {value:.4f}\n")
        
#         # Save test set predictions with timestamp to avoid overwriting
#         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#         np.save(os.path.join(output_dir, f'test_predictions_{timestamp}.npy'), y_pred_prob)
#         np.save(os.path.join(output_dir, f'test_indices_{timestamp}.npy'), test_indices)
#         # Also keep a copy with the standard name for easier reference
#         np.save(os.path.join(output_dir, 'test_predictions_latest.npy'), y_pred_prob)
#         np.save(os.path.join(output_dir, 'test_indices_latest.npy'), test_indices)
        
#         # Plot training history
#         plt.figure(figsize=(16, 6))
        
#         plt.subplot(1, 3, 1)
#         plt.plot(history.history['auc'])
#         plt.plot(history.history['val_auc'])
#         plt.title('Model AUC')
#         plt.ylabel('AUC')
#         plt.xlabel('Epoch')
#         plt.legend(['Train', 'Validation'], loc='lower right')
        
#         plt.subplot(1, 3, 2)
#         plt.plot(history.history['loss'])
#         plt.plot(history.history['val_loss'])
#         plt.title('Model Loss')
#         plt.ylabel('Loss')
#         plt.xlabel('Epoch')
#         plt.legend(['Train', 'Validation'], loc='upper right')
        
#         # Plot ROC curve
#         plt.subplot(1, 3, 3)
#         fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
#         roc_auc = auc(fpr, tpr)
#         plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')
#         plt.plot([0, 1], [0, 1], 'k--')
#         plt.xlabel('False Positive Rate')
#         plt.ylabel('True Positive Rate')
#         plt.title('ROC Curve (Test Set)')
#         plt.legend(loc='lower right')
        
#         plt.tight_layout()
#         plt.savefig(os.path.join(output_dir, 'training_performance.png'), dpi=300)
        
#         # Save detailed model summary
#         from contextlib import redirect_stdout
#         with open(os.path.join(output_dir, 'model_summary.txt'), 'w') as f:
#             with redirect_stdout(f):
#                 model.summary()
    
#     # Clean up to free memory
#     del X_test, y_test
#     gc.collect()
    
#     print(f"Memory after training: {memory_usage():.1f} MB")
#     return model, history, evaluation

# def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
#     """Build the advanced zero curtain detection model."""
#     from tensorflow.keras.models import Model
#     from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
#     from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPoo...
#     from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
#     from tensorflow.keras.optimizers import Adam
    
#     # Input layer
#     inputs = Input(shape=input_shape)
    
#     # Reshape for ConvLSTM (add spatial dimension)
#     x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
#     # ConvLSTM layer to capture spatiotemporal patterns
#     from tensorflow.keras.layers import ConvLSTM2D
#     convlstm = ConvLSTM2D(
#         filters=64,
#         kernel_size=(3, 1),
#         padding='same',
#         return_sequences=True,
#         activation='tanh',
#         recurrent_dropout=0.2
#     )(x)
    
#     # Reshape back to (sequence_length, features)
#     convlstm = Reshape((input_shape[0], 64))(convlstm)
    
#     # Modified positional encoding function that ensures correct dimensions
#     def positional_encoding(length, depth):
#         positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
#         depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
#         angle_rates = 1 / tf.pow(10000.0, depths)
#         angle_rads = positions * angle_rates
        
#         # Only create enough positional encodings for the depth we need
#         if 2 * depth <= depth:
#             # If we have enough room for both sin and cos
#             pos_encoding = tf.concat(
#                 [tf.sin(angle_rads), tf.cos(angle_rads)], axis=-1)
#         else:
#             # Otherwise, just use sin and truncate if needed
#             pos_encoding = tf.sin(angle_rads)
#             if pos_encoding.shape[-1] > depth:
#                 pos_encoding = pos_encoding[:, :depth]
        
#         # Ensure the encoding is the exact size we need
#         pos_encoding = tf.ensure_shape(pos_encoding, [length, depth])
        
#         # Add batch dimension to match convlstm output
#         pos_encoding = tf.expand_dims(pos_encoding, 0)  # [1, length, depth]
#         return pos_encoding
    
#     # Add positional encoding with proper dimensions
#     pos_encoding = positional_encoding(input_shape[0], 64)  # Match convlstm's 64 filters
#     transformer_input = convlstm + pos_encoding
    
#     # Transformer encoder block
#     def transformer_encoder(x, num_heads=8, key_dim=64, ff_dim=128):
#         # Multi-head attention
#         attention_output = MultiHeadAttention(
#             num_heads=num_heads, key_dim=key_dim
#         )(x, x)
        
#         # Skip connection 1
#         x1 = Add()([attention_output, x])
#         x1 = LayerNormalization(epsilon=1e-6)(x1)
        
#         # Feed-forward network
#         ff_output = Dense(ff_dim, activation='relu')(x1)
#         ff_output = Dropout(0.1)(ff_output)
#         ff_output = Dense(64)(ff_output)
        
#         # Skip connection 2
#         x2 = Add()([ff_output, x1])
#         return LayerNormalization(epsilon=1e-6)(x2)
    
#     # Apply transformer encoder
#     transformer_output = transformer_encoder(transformer_input)
    
#     # Parallel CNN paths for multi-scale feature extraction
#     cnn_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(inputs)
#     cnn_1 = BatchNormalization()(cnn_1)
    
#     cnn_2 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(inputs)
#     cnn_2 = BatchNormalization()(cnn_2)
    
#     cnn_3 = Conv1D(filters=32, kernel_size=7, padding='same', activation='relu')(inputs)
#     cnn_3 = BatchNormalization()(cnn_3)
    
#     # Variational Autoencoder components
#     def sampling(args):
#         z_mean, z_log_var = args
#         batch = tf.shape(z_mean)[0]
#         dim = tf.shape(z_mean)[1]
#         epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
#         return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
#     # Global temporal features
#     global_max = GlobalMaxPooling1D()(transformer_output)
#     global_avg = GlobalAveragePooling1D()(transformer_output)
    
#     # VAE encoding
#     z_mean = Dense(32)(Concatenate()([global_max, global_avg]))
#     z_log_var = Dense(32)(Concatenate()([global_max, global_avg]))
#     z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
#     # Combine all features
#     merged_features = Concatenate()(
#         [
#             GlobalMaxPooling1D()(cnn_1),
#             GlobalMaxPooling1D()(cnn_2),
#             GlobalMaxPooling1D()(cnn_3),
#             global_max,
#             global_avg,
#             z
#         ]
#     )
    
#     # Final classification layers
#     x = Dense(128, activation='relu')(merged_features)
#     x = Dropout(0.3)(x)
#     x = BatchNormalization()(x)
#     x = Dense(64, activation='relu')(x)
#     x = Dropout(0.2)(x)
    
#     # Output layer
#     outputs = Dense(1, activation='sigmoid')(x)
    
#     # Create model
#     model = Model(inputs=inputs, outputs=outputs)
    
#     # Add VAE loss
#     kl_loss = -0.5 * tf.reduce_mean(
#         z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
#     )
#     model.add_loss(0.001 * kl_loss)  # Small weight for VAE loss
    
#     # Compile model with appropriate metrics
#     model.compile(
#         optimizer=Adam(learning_rate=0.001),
#         loss='binary_crossentropy',
#         metrics=[
#             'accuracy',
#             tf.keras.metrics.AUC(name='auc'),
#             tf.keras.metrics.Precision(name='precision'),
#             tf.keras.metrics.Recall(name='recall')
#         ]
#     )
    
#     return model

# # Load split indices
# with open("zero_curtain_pipeline/modeling/checkpoints/spatiotemporal_split.pkl", "rb") as f:
#     split_data = pickle.load(f)
# train_indices = split_data["train_indices"]
# val_indices = split_data["val_indices"]
# test_indices = split_data["test_indices"]

# # Load spatial weights
# with open("zero_curtain_pipeline/modeling/checkpoints/spatial_density.pkl", "rb") as f:
#     weights_data = pickle.load(f)

# class DataGenerator(tf.keras.utils.Sequence):
#     def __init__(self, X_file, y_file, indices, batch_size=32, shuffle=True, weights=None):
#         """
#         Data generator for efficient loading from memory-mapped arrays
        
#         Parameters:
#         -----------
#         X_file : str
#             Path to features file
#         y_file : str
#             Path to labels file
#         indices : array
#             Indices to sample from
#         batch_size : int
#             Batch size
#         shuffle : bool
#             Whether to shuffle indices
#         weights : array, optional
#             Sample weights (must be same length as indices)
#         """
#         self.X_file = X_file
#         self.y_file = y_file
#         self.indices = np.asarray(indices)  # Ensure array type
#         self.batch_size = batch_size
#         self.shuffle = shuffle
#         self.weights = weights
        
#         # Verify weights array
#         if self.weights is not None:
#             assert len(self.weights) == len(self.indices), "Weights array must match indices lengt...
        
#         # Load as memory-mapped arrays
#         self.X = np.load(self.X_file, mmap_mode='r')
#         self.y = np.load(self.y_file, mmap_mode='r')
        
#         # Get input shape from first sample
#         self.input_shape = self.X[self.indices[0]].shape
        
#         self.on_epoch_end()
    
#     def __len__(self):
#         """Number of batches per epoch"""
#         return int(np.ceil(len(self.indices) / self.batch_size))
    
#     def __getitem__(self, idx):
#         """Get batch at position idx"""
#         start_idx = idx * self.batch_size
#         end_idx = min((idx + 1) * self.batch_size, len(self.indices))
#         batch_indices = self.indices_array[start_idx:end_idx]
        
#         # Load data
#         X_batch = self.X[batch_indices]
#         y_batch = self.y[batch_indices]
        
#         if self.weights is not None:
#             # Get weights for these specific indices
#             batch_positions = np.where(np.isin(self.indices, batch_indices))[0]
#             w_batch = self.weights[batch_positions]
#             return X_batch, y_batch, w_batch
#         else:
#             return X_batch, y_batch
    
#     def on_epoch_end(self):
#         """Called at the end of each epoch"""
#         self.indices_array = np.copy(self.indices)
#         if self.shuffle:
#             np.random.shuffle(self.indices_array)
            
#     def get_input_shape(self):
#         """Get input shape of samples"""
#         return self.input_shape

# def positional_encoding(length, depth):
#     positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
#     depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
    
#     angle_rates = 1 / tf.pow(10000.0, depths)
#     angle_rads = positions * angle_rates
    
#     # Create a positional encoding with the same depth as the input
#     pos_encoding = tf.concat([tf.sin(angle_rads), tf.cos(angle_rads)], axis=-1)
    
#     # Slice or pad to match the target depth
#     if pos_encoding.shape[-1] > depth:
#         pos_encoding = pos_encoding[:, :depth]  # Slice to match
#     elif pos_encoding.shape[-1] < depth:
#         # Pad to match
#         padding = depth - pos_encoding.shape[-1]
#         pos_encoding = tf.pad(pos_encoding, [[0, 0], [0, padding]])
    
#     return pos_encoding

# data_dir = os.path.join('/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling', 'ml_da...

# # Initialize generators
# X_file = os.path.join(data_dir, 'X_features.npy')
# y_file = os.path.join(data_dir, 'y_labels.npy')

# # Get sample weights for training set
# sample_weights = weights_data["weights"][train_indices]
# sample_weights = sample_weights / np.mean(sample_weights) * len(sample_weights)

# # Load data and metadata; use memory mapping to reduce memory usage
# print("Loading data...")
# X = np.load(X_file, mmap_mode='r')
# y = np.load(y_file, mmap_mode='r')
# print("Loading metadata...")
# with open(os.path.join(data_dir, 'metadata.pkl'), 'rb') as f:
#     metadata = pickle.load(f)
# print("Done.")

# train_y = y[train_indices]
# val_y = y[val_indices]
# test_y = y[test_indices]

# print(f"Train/val/test sizes: {len(train_indices)}/{len(val_indices)}/{len(test_indices)}")
# print(f"Positive examples: Train={np.sum(train_y)} ({np.sum(train_y)/len(train_y)*100:.1f}%), " +
#       f"Val={np.sum(val_y)} ({np.sum(val_y)/len(val_y)*100:.1f}%), " +
#       f"Test={np.sum(test_y)} ({np.sum(test_y)/len(test_y)*100:.1f}%)")

# Train/val/test sizes: 13918936/3198770/4279426
# Positive examples: Train=2017601 (14.5%), Val=462932 (14.5%), Test=576220 (13.5%)

# # Combine sample weights with class weights for imbalanced data
# pos_weight = (len(train_y) - np.sum(train_y)) / max(1, np.sum(train_y))
# class_weight = {0: 1.0, 1: pos_weight}
# print(f"Using class weight {pos_weight:.2f} for positive examples")
# # Free memory
# del train_y, val_y, test_y
# gc.collect()

# def build_simplified_model(input_shape):
#     inputs = Input(shape=input_shape)
#     x = Conv1D(64, 3, activation='relu', padding='same')(inputs)
#     x = GlobalAveragePooling1D()(x)
#     x = Dense(32, activation='relu')(x)
#     x = Dropout(0.3)(x)
#     outputs = Dense(1, activation='sigmoid')(x)
#     model = Model(inputs, outputs)
#     model.compile(
#         optimizer='adam',
#         loss='binary_crossentropy',
#         metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]
#     )
#     return model

# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras.layers import Input
# from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, Dense, Dropout
# from tensorflow.keras.models import Model
# from tensorflow.keras.layers import Layer
# input_shape = X[train_indices[0]].shape
# model = build_simplified_model(input_shape)

# Metal device set to: Apple M2 Max
# 2025-03-10 14:42:39.493972: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_fac...
# 2025-03-10 14:42:39.494128: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_fac...

# model.summary()

# Model: "model"
# _________________________________________________________________
#  Layer (type)                Output Shape              Param #   
# =================================================================
#  input_1 (InputLayer)        [(None, 6, 3)]            0         
                                                                 
#  conv1d (Conv1D)             (None, 6, 64)             640       
                                                                 
#  global_average_pooling1d (G  (None, 64)               0         
#  lobalAveragePooling1D)                                          
                                                                 
#  dense (Dense)               (None, 32)                2080      
                                                                 
#  dropout (Dropout)           (None, 32)                0         
                                                                 
#  dense_1 (Dense)             (None, 1)                 33        
                                                                 
# =================================================================
# Total params: 2,753
# Trainable params: 2,753
# Non-trainable params: 0

# # Custom data generator
# from tensorflow.keras.utils import Sequence

# class DataGenerator(Sequence):
#     def __init__(self, X, y, indices, batch_size=32, shuffle=True, weights=None):
#         self.X = X
#         self.y = y
#         self.indices = indices
#         self.batch_size = batch_size
#         self.shuffle = shuffle
#         self.weights = weights
#         self.on_epoch_end()
        
#     def __len__(self):
#         return int(np.ceil(len(self.indices) / self.batch_size))
        
#     def __getitem__(self, idx):
#         start_idx = idx * self.batch_size
#         end_idx = min((idx + 1) * self.batch_size, len(self.indices))
#         batch_indices = self.indices_array[start_idx:end_idx]
        
#         X_batch = self.X[batch_indices]
#         y_batch = self.y[batch_indices]
        
#         if self.weights is not None:
#             weights_batch = np.array([self.weights[i] for i in range(len(self.indices)) 
#                                      if self.indices[i] in batch_indices])
#             return X_batch, y_batch, weights_batch
#         else:
#             return X_batch, y_batch
        
#     def on_epoch_end(self):
#         self.indices_array = np.array(self.indices)
#         if self.shuffle:
#             np.random.shuffle(self.indices_array)

# # Create generators
# train_gen = DataGenerator(X, y, train_indices, batch_size=1024, shuffle=True, weights=sample_weigh...
# val_gen = DataGenerator(X, y, val_indices, batch_size=1024, shuffle=False)
# test_gen = DataGenerator(X, y, test_indices, batch_size=1024, shuffle=False)

# def create_tf_dataset_from_generator(data_generator, output_signature, buffer_size=10000):
#     """
#     Create a TensorFlow Dataset from a Keras Sequence generator.
    
#     Parameters:
#     -----------
#     data_generator : Sequence
#         Keras Sequence generator
#     output_signature : tuple
#         Output signature for the dataset
#     buffer_size : int
#         Size of shuffle buffer
        
#     Returns:
#     --------
#     tf.data.Dataset
#         Dataset ready for model training/evaluation
#     """
#     # Define generator function that wraps the Keras Sequence
#     def tf_generator():
#         for batch_index in range(len(data_generator)):
#             batch = data_generator[batch_index]
#             # If batch is a tuple, yield elements individually
#             if isinstance(batch, tuple):
#                 for i in range(len(batch[0])):  # For each example in the batch
#                     # Extract individual items from the batch
#                     if len(batch) == 2:  # (x, y)
#                         yield batch[0][i], batch[1][i]
#                     elif len(batch) == 3:  # (x, y, weights)
#                         yield batch[0][i], batch[1][i], batch[2][i]
    
#     # Create dataset
#     dataset = tf.data.Dataset.from_generator(
#         tf_generator,
#         output_signature=output_signature
#     )
    
#     # Determine if shuffling is needed based on the generator
#     if data_generator.shuffle:
#         dataset = dataset.shuffle(buffer_size)
    
#     # Batch and prefetch
#     dataset = dataset.batch(data_generator.batch_size)
#     dataset = dataset.prefetch(tf.data.AUTOTUNE)
    
#     return dataset

# # Create output signatures
# features_signature = tf.TensorSpec(shape=input_shape, dtype=tf.float32)
# label_signature = tf.TensorSpec(shape=(), dtype=tf.int32)
# weight_signature = tf.TensorSpec(shape=(), dtype=tf.float32)

# # Create datasets from existing generators
# train_ds = create_tf_dataset_from_generator(
#     train_gen,
#     output_signature=(features_signature, label_signature, weight_signature)
# )

# val_ds = create_tf_dataset_from_generator(
#     val_gen,
#     output_signature=(features_signature, label_signature)
# )

# test_ds = create_tf_dataset_from_generator(
#     test_gen,
#     output_signature=(features_signature, label_signature)
# )

# # Create datasets from existing generators
# train_ds = create_tf_dataset_from_generator(
#     train_gen,
#     output_signature=(features_signature, label_signature, weight_signature)
# )

# val_ds = create_tf_dataset_from_generator(
#     val_gen,
#     output_signature=(features_signature, label_signature)
# )

# test_ds = create_tf_dataset_from_generator(
#     test_gen,
#     output_signature=(features_signature, label_signature)
# )

# output_dir = os.path.join('/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling', 'spa...

# # Set up callbacks
# callbacks = [
#     # Stop training when validation performance plateaus
#     tf.keras.callbacks.EarlyStopping(
#         patience=15,
#         restore_best_weights=True,
#         monitor='val_auc',
#         mode='max'
#     ),
#     # Reduce learning rate when improvement slows
#     tf.keras.callbacks.ReduceLROnPlateau(
#         factor=0.5,
#         patience=7,
#         min_lr=1e-6,
#         monitor='val_auc',
#         mode='max'
#     ),
#     # Manual garbage collection after each epoch
#     tf.keras.callbacks.LambdaCallback(
#         on_epoch_end=lambda epoch, logs: gc.collect()
#     )
# ]

# # Add additional callbacks if output directory provided
# if output_dir:
#     callbacks.extend([
#         # Save best model
#         tf.keras.callbacks.ModelCheckpoint(
#             os.path.join(output_dir, 'model_checkpoint.h5'),
#             save_best_only=True,
#             monitor='val_auc',
#             mode='max'
#         ),
#         # Log training progress to CSV
#         tf.keras.callbacks.CSVLogger(
#             os.path.join(output_dir, 'training_log.csv'),
#             append=True
#         )
#     ])

# # Train model
# print("Starting model training...")
# epochs = 100

# # Note: class_weight is handled by the sample weights in train_gen
# history = model.fit(
#     train_ds,
#     validation_data=val_ds,
#     epochs=epochs,
#     callbacks=callbacks,
#     verbose=1
# )

# # Clean up to free memory
# del train_ds, val_ds
# gc.collect()

# # Combine sample weights with class weights for imbalanced data
# pos_weight = (len(train_y) - np.sum(train_y)) / max(1, np.sum(train_y))
# class_weight = {0: 1.0, 1: pos_weight}
# print(f"Using class weight {pos_weight:.2f} for positive examples")
# # Free memory
# del train_y, val_y, test_y
# gc.collect()

# def build_simplified_model(input_shape):
#     inputs = Input(shape=input_shape)
#     x = Conv1D(64, 3, activation='relu', padding='same')(inputs)
#     x = GlobalAveragePooling1D()(x)
#     x = Dense(32, activation='relu')(x)
#     x = Dropout(0.3)(x)
#     outputs = Dense(1, activation='sigmoid')(x)
#     model = Model(inputs, outputs)
#     model.compile(
#         optimizer='adam',
#         loss='binary_crossentropy',
#         metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]
#     )
#     return model

# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras.layers import Input
# from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, Dense, Dropout
# from tensorflow.keras.models import Model
# from tensorflow.keras.layers import Layer
# input_shape = X[train_indices[0]].shape
# model = build_simplified_model(input_shape)

# model.summary()

# Model: "model"
# _________________________________________________________________
#  Layer (type)                Output Shape              Param #   
# =================================================================
#  input_1 (InputLayer)        [(None, 6, 3)]            0         
                                                                 
#  conv1d (Conv1D)             (None, 6, 64)             640       
                                                                 
#  global_average_pooling1d (G  (None, 64)               0         
#  lobalAveragePooling1D)                                          
                                                                 
#  dense (Dense)               (None, 32)                2080      
                                                                 
#  dropout (Dropout)           (None, 32)                0         
                                                                 
#  dense_1 (Dense)             (None, 1)                 33        
                                                                 
# =================================================================
# Total params: 2,753
# Trainable params: 2,753
# Non-trainable params: 0

# from tensorflow.keras.utils import plot_model

# plot_model(model, to_file='zero_curtain_pipeline/modeling/spatial_model/simplified_insitu_model_pl...
#            show_layer_names=True, expand_nested=True, dpi=300, layer_range=None, show_layer_activa...

# #LETS CREATE A SAMPLE FIRST

# def create_tf_dataset_from_generator(data_generator, output_signature, has_weights=True, buffer_si...
#     """
#     Create a TensorFlow Dataset from a Keras Sequence generator.
    
#     Parameters:
#     -----------
#     data_generator : Sequence
#         Keras Sequence generator
#     output_signature : tuple
#         Output signature for the dataset
#     has_weights : bool
#         Whether the generator produces sample weights
#     buffer_size : int
#         Size of shuffle buffer
        
#     Returns:
#     --------
#     tf.data.Dataset
#         Dataset ready for model training/evaluation
#     """
#     # Define generator function that wraps the Keras Sequence
#     def tf_generator():
#         for batch_index in range(len(data_generator)):
#             batch = data_generator[batch_index]
#             # Process the batch based on its structure
#             if isinstance(batch, tuple):
#                 if len(batch) == 2:  # (x, y) format
#                     X_batch, y_batch = batch
#                     for i in range(len(X_batch)):
#                         # If weights are expected but not provided, use dummy weights
#                         if has_weights:
#                             yield X_batch[i], y_batch[i], 1.0
#                         else:
#                             yield X_batch[i], y_batch[i]
#                 elif len(batch) == 3:  # (x, y, weights) format
#                     X_batch, y_batch, w_batch = batch
#                     for i in range(len(X_batch)):
#                         yield X_batch[i], y_batch[i], w_batch[i]
#             else:
#                 raise ValueError("Unexpected batch format")
    
#     # Create dataset
#     dataset = tf.data.Dataset.from_generator(
#         tf_generator,
#         output_signature=output_signature
#     )
    
#     # Determine if shuffling is needed based on the generator
#     if data_generator.shuffle:
#         dataset = dataset.shuffle(buffer_size)
    
#     # Batch and prefetch
#     dataset = dataset.batch(data_generator.batch_size)
#     dataset = dataset.prefetch(tf.data.AUTOTUNE)
    
#     return dataset

# # When creating subset indices
# subset_indices = np.random.choice(train_indices, subset_size, replace=False)

# # Get only the weights for the subset
# subset_weights = np.array([sample_weights[np.where(train_indices == idx)[0][0]] 
#                           for idx in subset_indices])

# # Create generator with the subset and its corresponding weights
# train_gen = DataGenerator(X, y, subset_indices, batch_size=512, shuffle=True, weights=subset_weigh...

# subset_fraction = 0.001
# subset_size = int(len(val_indices) * subset_fraction)
# subset_indices = np.random.choice(val_indices, subset_size, replace=False)
# val_gen = DataGenerator(X, y, subset_indices, batch_size=1024, shuffle=False)

# subset_fraction = 0.001
# subset_size = int(len(test_indices) * subset_fraction)
# subset_indices = np.random.choice(test_indices, subset_size, replace=False)
# test_gen = DataGenerator(X, y, test_indices, batch_size=1024, shuffle=False)

# # Create datasets from existing generators - specify if weights are expected
# train_ds = create_tf_dataset_from_generator(
#     train_gen,
#     output_signature=(features_signature, label_signature, weight_signature),
#     has_weights=True  # Train generator has weights
# )

# val_ds = create_tf_dataset_from_generator(
#     val_gen,
#     output_signature=(features_signature, label_signature),
#     has_weights=False  # Validation generator doesn't have weights
# )

# test_ds = create_tf_dataset_from_generator(
#     test_gen,
#     output_signature=(features_signature, label_signature),
#     has_weights=False  # Test generator doesn't have weights
# )

# # Train model
# print("Starting model training...")
# epochs = 100

# # Note: class_weight is handled by the sample weights in train_gen
# history = model.fit(
#     train_ds,
#     validation_data=val_ds,
#     epochs=epochs,
#     callbacks=callbacks,
#     verbose=1
# )

# # Clean up to free memory
# del train_ds, val_ds
# gc.collect()

# Starting model training...
# Epoch 1/100
#       9/Unknown - 1s 80ms/step - loss: 9246419.0000 - accuracy: 0.5985 - auc: 0.6100  
# 2025-03-10 17:18:47.088847: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry....
# 9/9 [==============================] - 9s 1s/step - loss: 9246419.0000 - accuracy: 0.5985 - auc: 0...
# Epoch 2/100
# 9/9 [==============================] - 7s 809ms/step - loss: 5390313.0000 - accuracy: 0.8663 - auc...
# Epoch 3/100
# 9/9 [==============================] - 7s 792ms/step - loss: 4335007.0000 - accuracy: 0.8675 - auc...
# Epoch 4/100
# 9/9 [==============================] - 7s 790ms/step - loss: 3801146.5000 - accuracy: 0.8677 - auc...
# Epoch 5/100
# 9/9 [==============================] - 7s 783ms/step - loss: 3362294.2500 - accuracy: 0.8680 - auc...
# Epoch 6/100
# 9/9 [==============================] - 7s 809ms/step - loss: 3142311.7500 - accuracy: 0.8684 - auc...
# Epoch 7/100
# 9/9 [==============================] - 7s 807ms/step - loss: 2929097.2500 - accuracy: 0.8689 - auc...
# Epoch 8/100
# 9/9 [==============================] - 7s 786ms/step - loss: 2771868.7500 - accuracy: 0.8729 - auc...
# Epoch 9/100
# 9/9 [==============================] - 7s 786ms/step - loss: 2643040.0000 - accuracy: 0.8815 - auc...
# Epoch 10/100
# 9/9 [==============================] - 7s 781ms/step - loss: 2519734.2500 - accuracy: 0.8913 - auc...
# Epoch 11/100
# 9/9 [==============================] - 7s 782ms/step - loss: 2429415.2500 - accuracy: 0.9058 - auc...
# Epoch 12/100
# 9/9 [==============================] - 6s 779ms/step - loss: 2325136.0000 - accuracy: 0.9203 - auc...
# Epoch 13/100
# 9/9 [==============================] - 7s 779ms/step - loss: 2290334.7500 - accuracy: 0.9247 - auc...
# Epoch 14/100
# 9/9 [==============================] - 6s 777ms/step - loss: 2249495.5000 - accuracy: 0.9292 - auc...
# Epoch 15/100
# 9/9 [==============================] - 7s 783ms/step - loss: 2187842.7500 - accuracy: 0.9332 - auc...
# Epoch 16/100
# 9/9 [==============================] - 6s 777ms/step - loss: 2141003.2500 - accuracy: 0.9371 - auc...
# Epoch 17/100
# 9/9 [==============================] - 7s 787ms/step - loss: 2085697.5000 - accuracy: 0.9385 - auc...
# Epoch 18/100
# 9/9 [==============================] - 6s 777ms/step - loss: 2046315.0000 - accuracy: 0.9460 - auc...
# Epoch 19/100
# 9/9 [==============================] - 7s 782ms/step - loss: 1998005.2500 - accuracy: 0.9479 - auc...

# # Evaluate on test set in batches
# print("Evaluating model on test set...")
# evaluation = model.evaluate(test_gen, verbose=1)

# Evaluating model on test set...
#    1/4180 [..............................] - ETA: 19:36 - loss: 0.3147 - accuracy: 0.8379 - auc: 0...
# 2025-03-10 17:29:54.545697: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry....
# 4180/4180 [==============================] - 20s 5ms/step - loss: 0.2406 - accuracy: 0.8654 - auc:...

# print("Test performance:")
# for metric, value in zip(model.metrics_names, evaluation):
#     print(f"  {metric}: {value:.4f}")

# Test performance:
#   loss: 0.2406
#   accuracy: 0.8654
#   auc: 0.9903

# # Generate predictions in batches
# print("Generating predictions...")

# all_preds = []
# all_true = []

# for i in range(len(test_gen)):
#     X_batch, y_batch = test_gen[i]
#     pred_batch = model.predict(X_batch, verbose=0)
#     all_preds.append(pred_batch)
#     all_true.append(y_batch)

# y_pred_prob = np.vstack(all_preds)
# y_test = np.concatenate(all_true)
# y_pred = (y_pred_prob > 0.5).astype(int).flatten()

# # Calculate additional evaluation metrics
# from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc as sk_auc

# report = classification_report(y_test, y_pred)
# conf_matrix = confusion_matrix(y_test, y_pred)

# print("Classification Report:")
# print(report)

# print("Confusion Matrix:")
# print(conf_matrix)

# Classification Report:
#               precision    recall  f1-score   support

#            0       0.87      1.00      0.93   3703206
#            1       0.00      0.00      0.00    576220

#     accuracy                           0.87   4279426
#    macro avg       0.43      0.50      0.46   4279426
# weighted avg       0.75      0.87      0.80   4279426

# Confusion Matrix:
# [[3703206       0]
#  [ 576220       0]]

# # Save evaluation results
# if output_dir:
#     # Save evaluation metrics
#     with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w') as f:
#         f.write("Classification Report:\n")
#         f.write(report)
#         f.write("\n\nConfusion Matrix:\n")
#         f.write(str(conf_matrix))
#         f.write("\n\nTest Metrics:\n")
#         for metric, value in zip(model.metrics_names, evaluation):
#             f.write(f"{metric}: {value:.4f}\n")
    
#     # Save test set predictions with timestamp to avoid overwriting
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     np.save(os.path.join(output_dir, f'test_predictions_{timestamp}.npy'), y_pred_prob)
#     np.save(os.path.join(output_dir, f'test_indices_{timestamp}.npy'), test_indices)
#     # Also keep a copy with the standard name for easier reference
#     np.save(os.path.join(output_dir, 'test_predictions_latest.npy'), y_pred_prob)
#     np.save(os.path.join(output_dir, 'test_indices_latest.npy'), test_indices)
    
#     # Plot training history
#     plt.figure(figsize=(16, 6))
    
#     plt.subplot(1, 3, 1)
#     plt.plot(history.history['auc'])
#     plt.plot(history.history['val_auc'])
#     plt.title('Model AUC')
#     plt.ylabel('AUC')
#     plt.xlabel('Epoch')
#     plt.legend(['Train', 'Validation'], loc='lower right')
    
#     plt.subplot(1, 3, 2)
#     plt.plot(history.history['loss'])
#     plt.plot(history.history['val_loss'])
#     plt.title('Model Loss')
#     plt.ylabel('Loss')
#     plt.xlabel('Epoch')
#     plt.legend(['Train', 'Validation'], loc='upper right')
    
#     # Plot ROC curve
#     plt.subplot(1, 3, 3)
#     fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
#     roc_auc = sk_auc(fpr, tpr)
#     plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')
#     plt.plot([0, 1], [0, 1], 'k--')
#     plt.xlabel('False Positive Rate')
#     plt.ylabel('True Positive Rate')
#     plt.title('ROC Curve (Test Set)')
#     plt.legend(loc='lower right')
    
#     plt.tight_layout()
#     plt.savefig(os.path.join(output_dir, 'training_performance.png'), dpi=300)

# print(f"Memory after training: {memory_usage():.1f} MB")

# def compare_detection_methods(physical_events_file, model_events_file, output_dir=None):
#     """
#     Compare zero curtain events detected by different methods with enhanced
#     analysis and visualizations.
    
#     Parameters:
#     -----------
#     physical_events_file : str
#         Path to CSV file with events detected by the physics-based method
#     model_events_file : str
#         Path to CSV file with events detected by the deep learning model
#     output_dir : str, optional
#         Directory to save comparison results
        
#     Returns:
#     --------
#     dict
#         Comparison statistics and metrics
#     """
#     import pandas as pd
#     import numpy as np
#     import matplotlib.pyplot as plt
#     import seaborn as sns
#     from datetime import timedelta
#     import os
#     import gc
    
#     print("Comparing detection methods...")
#     print(f"Memory before comparison: {memory_usage():.1f} MB")
    
#     # Load events
#     physical_events = pd.read_csv(physical_events_file, parse_dates=['datetime_min', 'datetime_max...
#     model_events = pd.read_csv(model_events_file, parse_dates=['datetime_min', 'datetime_max'])
    
#     print(f"Loaded {len(physical_events)} physics-based events and {len(model_events)} model-detec...
    
#     # Calculate basic statistics for each method
#     physical_stats = {
#         'total_events': len(physical_events),
#         'unique_sites': physical_events['source'].nunique(),
#         'unique_depths': physical_events['soil_temp_depth'].nunique(),
#         'median_duration': physical_events['duration_hours'].median(),
#         'mean_duration': physical_events['duration_hours'].mean(),
#         'std_duration': physical_events['duration_hours'].std(),
#         'min_duration': physical_events['duration_hours'].min(),
#         'max_duration': physical_events['duration_hours'].max(),
#         'total_days': sum([(event['datetime_max'] - event['datetime_min']).total_seconds()/86400 
#                            for _, event in physical_events.iterrows()])
#     }
    
#     model_stats = {
#         'total_events': len(model_events),
#         'unique_sites': model_events['source'].nunique(),
#         'unique_depths': model_events['soil_temp_depth'].nunique(),
#         'median_duration': model_events['duration_hours'].median(),
#         'mean_duration': model_events['duration_hours'].mean(),
#         'std_duration': model_events['duration_hours'].std(),
#         'min_duration': model_events['duration_hours'].min(),
#         'max_duration': model_events['duration_hours'].max(),
#         'total_days': sum([(event['datetime_max'] - event['datetime_min']).total_seconds()/86400 
#                           for _, event in model_events.iterrows()])
#     }
    
#     # Create a site-day matching table for overlap analysis
#     print("Building day-by-day event registry for detailed comparison...")
#     physical_days = set()
#     model_days = set()
    
#     # Process physical events in batches
#     batch_size = 1000
#     for i in range(0, len(physical_events), batch_size):
#         batch = physical_events.iloc[i:i+batch_size]
        
#         for _, event in batch.iterrows():
#             site = event['source']
#             depth = event['soil_temp_depth']
#             start_day = event['datetime_min'].date()
#             end_day = event['datetime_max'].date()
            
#             # Add each day of the event
#             current_day = start_day
#             while current_day <= end_day:
#                 physical_days.add((site, depth, current_day))
#                 current_day += timedelta(days=1)
        
#         # Clear batch to free memory
#         del batch
#         gc.collect()
    
#     # Process model events in batches
#     for i in range(0, len(model_events), batch_size):
#         batch = model_events.iloc[i:i+batch_size]
        
#         for _, event in batch.iterrows():
#             site = event['source']
#             depth = event['soil_temp_depth']
#             start_day = event['datetime_min'].date()
#             end_day = event['datetime_max'].date()
            
#             # Add each day of the event
#             current_day = start_day
#             while current_day <= end_day:
#                 model_days.add((site, depth, current_day))
#                 current_day += timedelta(days=1)
        
#         # Clear batch to free memory
#         del batch
#         gc.collect()
    
#     # Calculate overlap metrics
#     overlap_days = physical_days.intersection(model_days)
    
#     overlap_metrics = {
#         'physical_only_days': len(physical_days - model_days),
#         'model_only_days': len(model_days - physical_days),
#         'overlap_days': len(overlap_days),
#         'jaccard_index': len(overlap_days) / len(physical_days.union(model_days)) if len(physical_...
#         'precision': len(overlap_days) / len(model_days) if len(model_days) > 0 else 0,
#         'recall': len(overlap_days) / len(physical_days) if len(physical_days) > 0 else 0,
#         'f1_score': 2 * len(overlap_days) / (len(physical_days) + len(model_days)) if (len(physica...
#     }
    
#     # Calculate site-specific overlap
#     site_level_comparison = {}
    
#     # Extract unique sites for analysis
#     all_sites = set([s for s, _, _ in physical_days]).union(set([s for s, _, _ in model_days]))
#     print(f"Analyzing {len(all_sites)} unique sites...")
    
#     for site in all_sites:
#         # Get days for this site
#         site_physical = set([(s, d, day) for s, d, day in physical_days if s == site])
#         site_model = set([(s, d, day) for s, d, day in model_days if s == site])
#         site_overlap = site_physical.intersection(site_model)
        
#         if len(site_physical) == 0 and len(site_model) == 0:
#             continue
            
#         site_level_comparison[site] = {
#             'physical_days': len(site_physical),
#             'model_days': len(site_model),
#             'overlap_days': len(site_overlap),
#             'jaccard': len(site_overlap) / len(site_physical.union(site_model)) if len(site_physic...
#             'precision': len(site_overlap) / len(site_model) if len(site_model) > 0 else 0,
#             'recall': len(site_overlap) / len(site_physical) if len(site_physical) > 0 else 0
#         }
    
#     # Analyze temporal distribution
#     print("Analyzing temporal distributions...")
    
#     # Extract month from each event day
#     physical_months = [day.month for _, _, day in physical_days]
#     model_months = [day.month for _, _, day in model_days]
    
#     # Count events by month
#     month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'D...
#     physical_month_counts = [physical_months.count(i+1) for i in range(12)]
#     model_month_counts = [model_months.count(i+1) for i in range(12)]
    
#     # Print comparison results
#     print("\n=== DETECTION METHOD COMPARISON ===\n")
    
#     print("Physics-based Detection:")
#     print(f"  Total Events: {physical_stats['total_events']}")
#     print(f"  Unique Sites: {physical_stats['unique_sites']}")
#     print(f"  Unique Depths: {physical_stats['unique_depths']}")
#     print(f"  Median Duration: {physical_stats['median_duration']:.1f} hours ({physical_stats['med...
#     print(f"  Mean Duration: {physical_stats['mean_duration']:.1f} hours ({physical_stats['mean_du...
#     print(f"  Duration Range: {physical_stats['min_duration']:.1f} - {physical_stats['max_duration...
    
#     print("\nDeep Learning Model Detection:")
#     print(f"  Total Events: {model_stats['total_events']}")
#     print(f"  Unique Sites: {model_stats['unique_sites']}")
#     print(f"  Unique Depths: {model_stats['unique_depths']}")
#     print(f"  Median Duration: {model_stats['median_duration']:.1f} hours ({model_stats['median_du...
#     print(f"  Mean Duration: {model_stats['mean_duration']:.1f} hours ({model_stats['mean_duration...
#     print(f"  Duration Range: {model_stats['min_duration']:.1f} - {model_stats['max_duration']:.1f...
    
#     print("\nOverlap Analysis:")
#     print(f"  Days with Events (Physics-based): {len(physical_days)}")
#     print(f"  Days with Events (Deep Learning): {len(model_days)}")
#     print(f"  Days detected by both methods: {overlap_metrics['overlap_days']}")
#     print(f"  Days detected only by Physics-based: {overlap_metrics['physical_only_days']}")
#     print(f"  Days detected only by Deep Learning: {overlap_metrics['model_only_days']}")
#     print(f"  Jaccard Index (overlap): {overlap_metrics['jaccard_index']:.4f}")
#     print(f"  Precision: {overlap_metrics['precision']:.4f}")
#     print(f"  Recall: {overlap_metrics['recall']:.4f}")
#     print(f"  F1 Score: {overlap_metrics['f1_score']:.4f}")
    
#     # Generate comparison visualizations
#     if output_dir:
#         os.makedirs(output_dir, exist_ok=True)
        
#         # Create a Venn diagram of detection overlap
#         try:
#             from matplotlib_venn import venn2
            
#             plt.figure(figsize=(10, 8))
#             venn2(subsets=(len(physical_days - model_days), 
#                           len(model_days - physical_days), 
#                           len(overlap_days)),
#                  set_labels=('Physics-based', 'Deep Learning'))
#             plt.title('Overlap between Detection Methods', fontsize=16)
#             plt.savefig(os.path.join(output_dir, 'detection_overlap.png'), dpi=300, bbox_inches='t...
#             plt.close()
#         except ImportError:
#             print("matplotlib_venn not installed. Skipping Venn diagram.")
        
#         # Compare duration distributions
#         plt.figure(figsize=(12, 8))
        
#         sns.histplot(physical_events['duration_hours'], kde=True, alpha=0.5, 
#                     label='Physics-based', color='blue', bins=50, log_scale=(False, True))
#         sns.histplot(model_events['duration_hours'], kde=True, alpha=0.5, 
#                     label='Deep Learning', color='red', bins=50, log_scale=(False, True))
        
#         plt.xlabel('Duration (hours)', fontsize=12)
#         plt.ylabel('Frequency (log scale)', fontsize=12)
#         plt.title('Comparison of Zero Curtain Duration Distributions', fontsize=16)
#         plt.legend(fontsize=12)
#         plt.grid(alpha=0.3)
#         plt.savefig(os.path.join(output_dir, 'duration_comparison.png'), dpi=300, bbox_inches='tig...
#         plt.close()
        
#         # Plot monthly distribution
#         plt.figure(figsize=(12, 8))
#         width = 0.35
#         x = np.arange(len(month_names))
        
#         plt.bar(x - width/2, physical_month_counts, width, label='Physics-based', color='blue', al...
#         plt.bar(x + width/2, model_month_counts, width, label='Deep Learning', color='red', alpha=...
        
#         plt.xlabel('Month', fontsize=12)
#         plt.ylabel('Number of Event Days', fontsize=12)
#         plt.title('Monthly Distribution of Zero Curtain Events', fontsize=16)
#         plt.xticks(x, month_names)
#         plt.legend(fontsize=12)
#         plt.grid(axis='y', alpha=0.3)
#         plt.savefig(os.path.join(output_dir, 'monthly_distribution.png'), dpi=300, bbox_inches='ti...
#         plt.close()
        
#         # Site-level agreement analysis (top 20 sites by event count)
#         site_df = pd.DataFrame.from_dict(site_level_comparison, orient='index')
#         top_sites = site_df.sort_values(by=['physical_days', 'model_days'], ascending=False).head(...
        
#         plt.figure(figsize=(14, 8))
#         ax = plt.subplot(111)
#         x = np.arange(len(top_sites))
        
#         p1 = ax.bar(x - width/2, top_sites['physical_days'], width, label='Physics-based', color='...
#         p2 = ax.bar(x + width/2, top_sites['model_days'], width, label='Deep Learning', color='red...
#         p3 = ax.bar(x, top_sites['overlap_days'], width/2, label='Overlap', color='purple', alpha=...
        
#         ax.set_xlabel('Site ID', fontsize=12)
#         ax.set_ylabel('Number of Event Days', fontsize=12)
#         ax.set_title('Site-level Agreement: Top 20 Sites by Event Count', fontsize=16)
#         ax.set_xticks(x)
#         ax.set_xticklabels(top_sites.index, rotation=45, ha='right')
#         ax.legend(fontsize=12)
#         ax.grid(axis='y', alpha=0.3)
        
#         plt.tight_layout()
#         plt.savefig(os.path.join(output_dir, 'site_level_agreement.png'), dpi=300, bbox_inches='ti...
#         plt.close()
        
#         # Save detailed statistics to CSV
#         site_df.to_csv(os.path.join(output_dir, 'site_level_metrics.csv'))
        
#         # Create a summary report
#         with open(os.path.join(output_dir, 'comparison_report.txt'), 'w') as f:
#             f.write("# Zero Curtain Detection Method Comparison\n\n")
            
#             f.write("## Physics-based Detection\n")
#             for key, value in physical_stats.items():
#                 f.write(f"- {key}: {value}\n")
            
#             f.write("\n## Deep Learning Model Detection\n")
#             for key, value in model_stats.items():
#                 f.write(f"- {key}: {value}\n")
            
#             f.write("\n## Overlap Analysis\n")
#             for key, value in overlap_metrics.items():
#                 f.write(f"- {key}: {value}\n")
            
#             f.write("\n## Summary\n")
#             f.write(f"The two methods show a Jaccard similarity index of {overlap_metrics['jaccard...
#             f.write(f"with a precision of {overlap_metrics['precision']:.4f} and recall of {overla...
            
#             f.write("\nThis indicates that ")
#             if overlap_metrics['jaccard_index'] > 0.7:
#                 f.write("the methods have strong agreement in detecting zero curtain events.\n")
#             elif overlap_metrics['jaccard_index'] > 0.4:
#                 f.write("the methods have moderate agreement in detecting zero curtain events.\n")
#             else:
#                 f.write("the methods have relatively low agreement in detecting zero curtain event...
                
#             f.write("\nPossible explanations for differences include:\n")
#             f.write("1. Physics-based detection using fixed thresholds vs. ML pattern recognition\...
#             f.write("2. Different sensitivities to signal noise\n")
#             f.write("3. Model's ability to recognize patterns that may not strictly adhere to phys...
#             f.write("4. Seasonal variability in detection accuracy\n")
            
#     # Clean up to free memory
#     del physical_events, model_events, physical_days, model_days, overlap_days
#     gc.collect()
    
#     print(f"Memory after comparison: {memory_usage():.1f} MB")
    
#     comparison_results = {
#         'physical_stats': physical_stats,
#         'model_stats': model_stats,
#         'overlap_metrics': overlap_metrics,
#         'site_level_comparison': site_level_comparison,
#         'temporal_distribution': {
#             'month_names': month_names,
#             'physical_month_counts': physical_month_counts,
#             'model_month_counts': model_month_counts
#         }
#     }
    
#     return comparison_results

# Evaluate on test set in batches
print("Evaluating model on test set...")
test_gen = DataGenerator(X, y, test_indices, batch_size=16, shuffle=False)
evaluation = model.evaluate(test_gen, verbose=1)

print("Test performance:")
for metric, value in zip(model.metrics_names, evaluation):
    print(f"  {metric}: {value:.4f}")

# Generate predictions in batches
print("Generating predictions...")
all_preds = []
all_true = []

for i in range(len(test_gen)):
    X_batch, y_batch = test_gen[i]
    pred_batch = model.predict(X_batch, verbose=0)
    all_preds.append(pred_batch)
    all_true.append(y_batch)

y_pred_prob = np.vstack(all_preds)
y_test = np.concatenate(all_true)
y_pred = (y_pred_prob > 0.5).astype(int).flatten()

# Calculate additional evaluation metrics
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc as sk_auc

report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print("Classification Report:")
print(report)

print("Confusion Matrix:")
print(conf_matrix)

# Save evaluation results
if output_dir:
    # Save evaluation metrics
    with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w') as f:
        f.write("Classification Report:\n")
        f.write(report)
        f.write("\n\nConfusion Matrix:\n")
        f.write(str(conf_matrix))
        f.write("\n\nTest Metrics:\n")
        for metric, value in zip(model.metrics_names, evaluation):
            f.write(f"{metric}: {value:.4f}\n")
    
    # Save test set predictions with timestamp to avoid overwriting
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    np.save(os.path.join(output_dir, f'test_predictions_{timestamp}.npy'), y_pred_prob)
    np.save(os.path.join(output_dir, f'test_indices_{timestamp}.npy'), test_indices)
    # Also keep a copy with the standard name for easier reference
    np.save(os.path.join(output_dir, 'test_predictions_latest.npy'), y_pred_prob)
    np.save(os.path.join(output_dir, 'test_indices_latest.npy'), test_indices)
    
    # Plot training history
    plt.figure(figsize=(16, 6))
    
    plt.subplot(1, 3, 1)
    plt.plot(history.history['auc'])
    plt.plot(history.history['val_auc'])
    plt.title('Model AUC')
    plt.ylabel('AUC')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='lower right')
    
    plt.subplot(1, 3, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper right')
    
    # Plot ROC curve
    plt.subplot(1, 3, 3)
    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
    roc_auc = sk_auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve (Test Set)')
    plt.legend(loc='lower right')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'training_performance.png'), dpi=300)

print(f"Memory after training: {memory_usage():.1f} MB")
return model, history, evaluation

# output_base_dir = '/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling'
# data_dir = os.path.join(output_base_dir, 'ml_data')
# X = np.load(os.path.join(data_dir, 'X_features.npy'))
# y = np.load(os.path.join(data_dir, 'y_labels.npy'))
# with open(os.path.join(data_dir, 'metadata.pkl'), 'rb') as f:
#     metadata = pickle.load(f)
# # y labels are binary, i.e., 0 is False for has_moisture_data and 1 is True for has_moisture_data

# import os
# import gc
# import json
# import pickle
# import numpy as np
# from scipy.spatial import cKDTree
# from tqdm import tqdm
# import psutil
# import threading
# import time
# from contextlib import contextmanager
# from joblib import Parallel, delayed

# # Configuration parameters
# CHECKPOINT_DIR = "zero_curtain_pipeline/modeling/checkpoints"
# MEMORY_THRESHOLD = 75  # Memory usage percentage threshold
# METADATA_SAVE_FREQUENCY = 5  # Save metadata every N batches
# MONITOR_INTERVAL = 10  # Memory monitoring interval in seconds

# # Create checkpoint directory
# os.makedirs(CHECKPOINT_DIR, exist_ok=True)
# CHECKPOINT_BASE = os.path.join(CHECKPOINT_DIR, "geo_density_checkpoint")
# METADATA_PATH = os.path.join(CHECKPOINT_DIR, "checkpoint_metadata.json")

# @contextmanager
# def measure_time(description="Operation"):
#     """Context manager to measure and print execution time of code blocks."""
#     start_time = time.time()
#     try:
#         yield
#     finally:
#         elapsed = time.time() - start_time
#         print(f"{description} completed in {elapsed:.2f} seconds")

# class MemoryMonitor:
#     """Class for monitoring memory usage in a separate thread."""
#     def __init__(self, interval=10, threshold=75):
#         self.interval = interval
#         self.threshold = threshold
#         self.stop_flag = threading.Event()
#         self.max_usage = 0
#         self.monitor_thread = None
        
#     def memory_usage(self):
#         """Get current memory usage percentage."""
#         return psutil.virtual_memory().percent
        
#     def monitor(self):
#         """Monitor memory usage periodically."""
#         while not self.stop_flag.is_set():
#             usage = self.memory_usage()
#             self.max_usage = max(self.max_usage, usage)
#             if usage > self.threshold:
#                 print(f"WARNING: High memory usage detected: {usage:.1f}%")
#             time.sleep(self.interval)
                
#     def start(self):
#         """Start the memory monitoring thread."""
#         self.monitor_thread = threading.Thread(target=self.monitor, daemon=True)
#         self.monitor_thread.start()
        
#     def stop(self):
#         """Stop the memory monitoring thread."""
#         self.stop_flag.set()
#         if self.monitor_thread:
#             self.monitor_thread.join(timeout=1.0)
#         print(f"Maximum memory usage: {self.max_usage:.1f}%")

# class CheckpointManager:
#     """Class for managing checkpoints and metadata."""
#     def __init__(self, base_path, metadata_path, save_frequency=5):
#         self.base_path = base_path
#         self.metadata_path = metadata_path
#         self.save_frequency = save_frequency
#         self.completed_batches = self.load_metadata()
        
#     def save_checkpoint(self, batch_data, batch_index):
#         """Save a batch checkpoint to disk."""
#         filepath = f"{self.base_path}_batch_{batch_index}.pkl"
#         with open(filepath, "wb") as f:
#             pickle.dump(batch_data, f)
        
#     def load_checkpoint(self, batch_index):
#         """Load a batch checkpoint from disk."""
#         filepath = f"{self.base_path}_batch_{batch_index}.pkl"
#         if os.path.exists(filepath):
#             with open(filepath, "rb") as f:
#                 return pickle.load(f)
#         return None
        
#     def save_metadata(self, force_save=False):
#         """Save checkpoint metadata to disk."""
#         if force_save or len(self.completed_batches) % self.save_frequency == 0:
#             with open(self.metadata_path, "w") as f:
#                 json.dump({"completed_batches": sorted(list(self.completed_batches))}, f)
                
#     def load_metadata(self):
#         """Load checkpoint metadata from disk."""
#         if os.path.exists(self.metadata_path):
#             with open(self.metadata_path, "r") as f:
#                 return set(json.load(f).get("completed_batches", []))
#         return set()
        
#     def is_batch_completed(self, batch_index):
#         """Check if a batch has been completed."""
#         return batch_index in self.completed_batches
        
#     def mark_batch_completed(self, batch_index):
#         """Mark a batch as completed."""
#         self.completed_batches.add(batch_index)
#         self.save_metadata()

# def haversine_to_cartesian(lat, lon):
#     """Convert latitude and longitude to 3D cartesian coordinates (x,y,z)."""
#     # Convert to radians
#     lat_rad = np.radians(lat)
#     lon_rad = np.radians(lon)
    
#     # Convert to cartesian coordinates on a unit sphere
#     x = np.cos(lat_rad) * np.cos(lon_rad)
#     y = np.cos(lat_rad) * np.sin(lon_rad)
#     z = np.sin(lat_rad)
    
#     return np.column_stack((x, y, z))

# def cartesian_to_haversine_distance(distance_cartesian):
#     """Convert cartesian distance to angular distance in radians."""
#     # Ensure distance is at most 2.0 (diameter of unit sphere)
#     distance_cartesian = np.minimum(distance_cartesian, 2.0)
    
#     # Convert chord length to angular distance using inverse haversine formula
#     return 2 * np.arcsin(distance_cartesian / 2)

# def process_batch(tree, batch_points, k):
#     """Process a single batch of points using KDTree, excluding self-matches."""
#     # Query k+1 neighbors to include self (which will be removed)
#     distances, _ = tree.query(batch_points, k=k+1)
    
#     # Remove the first neighbor (self with distance 0)
#     nn_distances = distances[:, 1:]
    
#     # Convert cartesian distances to angular distances (radians)
#     return cartesian_to_haversine_distance(nn_distances)

# def calculate_optimal_batch_size(total_points, available_memory_mb=None):
#     """Calculate optimal batch size based on available memory and dataset size."""
#     if available_memory_mb is None:
#         # Use 20% of available memory if not specified (to leave room for parallelism)
#         available_memory_mb = psutil.virtual_memory().available / (1024 * 1024) * 0.2
    
#     # Estimate memory per point (assuming float64 coordinates and distances)
#     memory_per_point_kb = 0.5  # Approximate memory per point in KB
    
#     # Calculate batch size that would use available memory
#     batch_size = int(available_memory_mb * 1024 / memory_per_point_kb)
    
#     # Cap batch size to reasonable limits
#     min_batch_size = 1000
#     max_batch_size = 50000
#     batch_size = max(min(batch_size, max_batch_size), min_batch_size)
    
#     return min(batch_size, total_points)

# def aggregate_checkpoints(checkpoint_mgr, batch_indices):
#     """Load and aggregate checkpoints for the specified batch indices."""
#     all_distances = []
#     for batch_idx in batch_indices:
#         distances = checkpoint_mgr.load_checkpoint(batch_idx)
#         if distances is not None:
#             all_distances.append(distances)
#     return all_distances

# def calculate_spatial_density(latitudes, longitudes, k=5, leaf_size=40, batch_size=None, n_jobs=4, 
#                              checkpoint_dir=CHECKPOINT_DIR, overwrite=False):
#     """
#     Calculate spatial density based on K-nearest neighbors with checkpointing.
    
#     Parameters:
#     -----------
#     latitudes : array-like
#         Latitude values in degrees
#     longitudes : array-like
#         Longitude values in degrees
#     k : int
#         Number of nearest neighbors to find (excluding self)
#     leaf_size : int
#         Leaf size for the KDTree (affects performance)
#     batch_size : int or None
#         Batch size for processing. If None, will be calculated automatically.
#     n_jobs : int
#         Number of parallel jobs to run. -1 means using all processors.
#     checkpoint_dir : str
#         Directory for saving checkpoints
#     overwrite : bool
#         Whether to overwrite existing checkpoints
    
#     Returns:
#     --------
#     density : array
#         Density values for each point
#     distances : array
#         Distances to k nearest neighbors, shape (n_points, k)
#     """
#     global CHECKPOINT_DIR
#     CHECKPOINT_DIR = checkpoint_dir
#     global CHECKPOINT_BASE
#     CHECKPOINT_BASE = os.path.join(CHECKPOINT_DIR, "geo_density_checkpoint")
#     global METADATA_PATH
#     METADATA_PATH = os.path.join(CHECKPOINT_DIR, "checkpoint_metadata.json")
    
#     # Never overwrite existing checkpoints - we need them to resume processing
#     if overwrite:
#         print("WARNING: Overwrite flag ignored - checkpoints are preserved for resuming processing...
#         # Do NOT remove checkpoints regardless of the overwrite flag
    
#     # Convert input arrays to numpy if they aren't already
#     lat_array = np.asarray(latitudes, dtype=np.float64)
#     lon_array = np.asarray(longitudes, dtype=np.float64)
    
#     # Convert to 3D cartesian coordinates for better KDTree performance
#     with measure_time("Coordinate conversion to cartesian"):
#         cartesian_points = haversine_to_cartesian(lat_array, lon_array)
    
#     # Calculate optimal batch size if not provided
#     n_points = len(cartesian_points)
#     if batch_size is None:
#         batch_size = calculate_optimal_batch_size(n_points)
    
#     # Adjust batch size to create a reasonable number of batches (target ~200 batches)
#     if n_points / batch_size > 200:
#         batch_size = max(batch_size, n_points // 200)
    
#     print(f"Using batch size: {batch_size} for {n_points} points")
    
#     # Initialize checkpoint manager
#     checkpoint_mgr = CheckpointManager(
#         CHECKPOINT_BASE, 
#         METADATA_PATH,
#         METADATA_SAVE_FREQUENCY
#     )
    
#     # Start memory monitoring
#     memory_monitor = MemoryMonitor(interval=MONITOR_INTERVAL, threshold=MEMORY_THRESHOLD)
#     memory_monitor.start()
    
#     try:
#         # Build cKDTree for fast spatial queries (much faster than BallTree)
#         with measure_time("KDTree construction"):
#             tree = cKDTree(cartesian_points, leafsize=leaf_size)
#             gc.collect()  # Force garbage collection
#             print(f"Available memory after tree creation: {psutil.virtual_memory().available / (10...
        
#         # Calculate number of batches
#         num_batches = int(np.ceil(n_points / batch_size))
#         print(f"Total number of batches: {num_batches}")
        
#         # Check which batches are already completed
#         completed_batches = set()
#         for batch_index in range(num_batches):
#             if checkpoint_mgr.is_batch_completed(batch_index):
#                 completed_batches.add(batch_index)
                
#         print(f"Found {len(completed_batches)} completed batches")
        
#         # Process remaining batches
#         remaining_batches = set(range(num_batches)) - completed_batches
#         print(f"Processing {len(remaining_batches)} remaining batches")
        
#         if remaining_batches:
#             # Convert to list and sort for deterministic processing
#             remaining_batch_indices = sorted(list(remaining_batches))
            
#             # Test with a small batch first
#             print("Testing KDTree query with a small sample...")
#             sample_idx = remaining_batch_indices[0]
#             start_idx = sample_idx * batch_size
#             end_idx = min((sample_idx + 1) * batch_size, n_points)
            
#             # Take just 10 points for testing
#             test_points = cartesian_points[start_idx:start_idx+10]
            
#             with measure_time("Test KDTree query"):
#                 # Query k+1 neighbors (including self) for test
#                 test_distances, _ = tree.query(test_points, k=k+1)
#                 # Remove first column (self-matches)
#                 test_distances = test_distances[:, 1:]
#                 # Convert to angular distances
#                 test_distances = cartesian_to_haversine_distance(test_distances)
            
#             print(f"Test successful! Sample distances: {test_distances[0]}")
#             print(f"Average test distance: {np.mean(test_distances):.6f} radians")
            
#             # Process batches in parallel or sequentially
#             if n_jobs != 1:
#                 # Process in parallel using joblib
#                 print(f"Processing batches in parallel with {n_jobs} jobs")
                
#                 # Process in smaller chunks to avoid memory issues
#                 chunk_size = min(20, len(remaining_batch_indices))  # Smaller chunks for better mo...
#                 num_chunks = int(np.ceil(len(remaining_batch_indices) / chunk_size))
                
#                 for chunk_idx in range(num_chunks):
#                     chunk_start = chunk_idx * chunk_size
#                     chunk_end = min((chunk_idx + 1) * chunk_size, len(remaining_batch_indices))
#                     current_chunk = remaining_batch_indices[chunk_start:chunk_end]
                    
#                     print(f"Processing chunk {chunk_idx + 1}/{num_chunks} with {len(current_chunk)...
                    
#                     # Prepare batch data
#                     batch_data = []
#                     for batch_idx in current_chunk:
#                         start_idx = batch_idx * batch_size
#                         end_idx = min((batch_idx + 1) * batch_size, n_points)
#                         batch_points = cartesian_points[start_idx:end_idx]
#                         batch_data.append((batch_idx, batch_points))
                    
#                     # Process in parallel with timeout monitoring
#                     start_time = time.time()
#                     results = Parallel(n_jobs=n_jobs, verbose=10, timeout=3600)(
#                         delayed(process_batch)(tree, points, k) for batch_idx, points in batch_dat...
#                     )
#                     end_time = time.time()
                    
#                     # Calculate performance statistics
#                     elapsed_time = end_time - start_time
#                     points_processed = sum(len(points) for _, points in batch_data)
#                     points_per_second = points_processed / elapsed_time
                    
#                     print(f"Chunk processed {points_processed} points in {elapsed_time:.2f} second...
#                     print(f"Performance: {points_per_second:.2f} points/second")
                    
#                     # Save results
#                     for (batch_idx, _), distances in zip(batch_data, results):
#                         checkpoint_mgr.save_checkpoint(distances, batch_idx)
#                         checkpoint_mgr.mark_batch_completed(batch_idx)
                    
#                     # Estimate remaining time
#                     remaining_chunks = num_chunks - (chunk_idx + 1)
#                     if remaining_chunks > 0:
#                         remaining_time = remaining_chunks * elapsed_time
#                         hours = remaining_time // 3600
#                         minutes = (remaining_time % 3600) // 60
#                         print(f"Estimated remaining time: {int(hours)}h {int(minutes)}m")
                    
#                     # Free memory
#                     del batch_data
#                     del results
#                     gc.collect()
#             else:
#                 # Process sequentially
#                 print("Processing batches sequentially")
#                 with tqdm(total=len(remaining_batch_indices), desc="Processing Batches") as pbar:
#                     for batch_idx in remaining_batch_indices:
#                         start_idx = batch_idx * batch_size
#                         end_idx = min((batch_idx + 1) * batch_size, n_points)
#                         batch_points = cartesian_points[start_idx:end_idx]
                        
#                         # Process full batch
#                         distances = process_batch(tree, batch_points, k)
                        
#                         # Save checkpoint
#                         checkpoint_mgr.save_checkpoint(distances, batch_idx)
#                         checkpoint_mgr.mark_batch_completed(batch_idx)
                        
#                         # Free memory
#                         del distances
#                         gc.collect()
                        
#                         pbar.update(1)
        
#         # At this point, all batches should be processed and saved to disk
#         # We have two options:
#         # 1. Load all checkpoints into memory (might be too large)
#         # 2. Calculate density from checkpoints directly (more memory efficient)
        
#         # Option 2: Calculate density directly from checkpoints
#         print("Calculating density from checkpoints...")
#         n_processed = 0
#         density = np.zeros(n_points)
        
#         # Process in chunks to save memory
#         chunk_size = min(1000, num_batches)
#         num_chunks = int(np.ceil(num_batches / chunk_size))
        
#         for chunk_idx in range(num_chunks):
#             chunk_start = chunk_idx * chunk_size
#             chunk_end = min((chunk_idx + 1) * chunk_size, num_batches)
#             current_chunk = list(range(chunk_start, chunk_end))
            
#             print(f"Processing density chunk {chunk_idx+1}/{num_chunks}")
            
#             # Load each batch and compute density
#             for batch_idx in tqdm(current_chunk):
#                 batch_distances = checkpoint_mgr.load_checkpoint(batch_idx)
                
#                 if batch_distances is not None:
#                     start_idx = batch_idx * batch_size
#                     end_idx = min((batch_idx + 1) * batch_size, n_points)
                    
#                     # Mean distance to k nearest neighbors
#                     mean_distances = np.mean(batch_distances, axis=1)
                    
#                     # Density is inversely proportional to mean distance
#                     # Add small epsilon to avoid division by zero
#                     epsilon = 1e-10
#                     density[start_idx:end_idx] = 1.0 / (mean_distances + epsilon)
                    
#                     n_processed += end_idx - start_idx
                    
#                     # Clean up
#                     del batch_distances, mean_distances
#                     gc.collect()
        
#         print(f"Processed density for {n_processed} points")
        
#         # Normalize density for easier interpretation
#         if n_processed > 0:
#             density = density / np.mean(density[0:n_processed])
        
#         # Create weights (inverse of density)
#         weights = 1.0 / (density + 1e-8)
        
#         # Normalize weights
#         weights = weights / np.mean(weights) * n_points
        
#         # Clip extreme weights (beyond 3 std from mean)
#         weights_mean = np.mean(weights)
#         weights_std = np.std(weights)
#         weights = np.clip(weights, 0, weights_mean + 3*weights_std)
        
#         # Final normalization
#         weights = weights / np.mean(weights) * n_points
        
#         # Save the final density and weights
#         with open(os.path.join(CHECKPOINT_DIR, "spatial_density.pkl"), "wb") as f:
#             pickle.dump({"density": density, "weights": weights}, f)
        
#         print(f"Saved final density and weights to {os.path.join(CHECKPOINT_DIR, 'spatial_density....
        
#         # Final metadata save
#         checkpoint_mgr.save_metadata(force_save=True)
        
#         return density, weights
        
#     finally:
#         # Stop memory monitoring
#         memory_monitor.stop()

# def stratified_spatiotemporal_split(X, y, metadata, test_size=0.2, val_size=0.15, 
#                                    random_state=42, checkpoint_dir=CHECKPOINT_DIR):
#     """
#     Split data with balanced spatiotemporal representation in train/val/test sets.
    
#     Parameters:
#     -----------
#     X : array
#         Features array
#     y : array
#         Labels array
#     metadata : list
#         List of metadata dicts containing spatial and temporal information
#     test_size : float
#         Fraction of data for testing
#     val_size : float
#         Fraction of data for validation
#     random_state : int
#         Random seed
#     checkpoint_dir : str
#         Directory for checkpoints
        
#     Returns:
#     --------
#     train_indices, val_indices, test_indices : arrays
#         Indices for each split
#     """
#     print("Performing spatiotemporal split with checkpointing...")
#     global CHECKPOINT_DIR
#     CHECKPOINT_DIR = checkpoint_dir
    
#     # Create checkpoint directory if it doesn't exist
#     os.makedirs(CHECKPOINT_DIR, exist_ok=True)
    
#     # ALWAYS check for existing splits to resume from previous run
#     split_file = os.path.join(CHECKPOINT_DIR, "spatiotemporal_split.pkl")
#     if os.path.exists(split_file):
#         print(f"Loading existing split from {split_file}")
#         with open(split_file, "rb") as f:
#             split_data = pickle.load(f)
#         print(f"Successfully loaded existing train/val/test split with {len(split_data['train_indi...
#               f"{len(split_data['val_indices'])} validation, and {len(split_data['test_indices'])}...
#         return split_data["train_indices"], split_data["val_indices"], split_data["test_indices"]
    
#     # If we get here, no existing split was found, so we'll create a new one
#     print("No existing split found, creating new train/val/test split")
    
#     n_samples = len(X)
#     np.random.seed(random_state)
    
#     # Extract relevant metadata
#     timestamps = np.array([meta['start_time'] for meta in metadata])
    
#     # Geography (handle cases where lat/lon may be missing)
#     latitudes = np.array([
#         meta.get('latitude', 0) if meta.get('latitude') is not None else 0 
#         for meta in metadata
#     ])
#     longitudes = np.array([
#         meta.get('longitude', 0) if meta.get('longitude') is not None else 0 
#         for meta in metadata
#     ])
#     depths = np.array([
#         meta.get('soil_temp_depth', 0) if meta.get('soil_temp_depth') is not None else 0 
#         for meta in metadata
#     ])
    
#     has_geo_info = (np.count_nonzero(latitudes) > 0 and np.count_nonzero(longitudes) > 0)
    
#     # Time-based sorting
#     time_indices = np.argsort(timestamps)
    
#     # Keep the most recent data as a true test set (chronological split)
#     test_count = int(n_samples * test_size)
#     test_indices = time_indices[-test_count:]
    
#     # Remaining data for train/val
#     remaining_indices = time_indices[:-test_count]
    
#     # Check for existing density weights
#     density_file = os.path.join(CHECKPOINT_DIR, "spatial_density.pkl")
#     if os.path.exists(density_file) and has_geo_info:
#         print(f"Loading existing density weights from {density_file}")
#         with open(density_file, "rb") as f:
#             density_data = pickle.load(f)
#         weights = density_data["weights"][remaining_indices]
#     elif has_geo_info:
#         # Calculate density-based weights for remaining data
#         print("Calculating spatial density for weighting...")
#         _, weights = calculate_spatial_density(
#             latitudes[remaining_indices],
#             longitudes[remaining_indices],
#             k=5,
#             checkpoint_dir=CHECKPOINT_DIR
#         )
#     else:
#         # If no geography, use uniform weights
#         weights = np.ones(len(remaining_indices))
    
#     # Calculate validation size
#     val_count = int(n_samples * val_size)
    
#     # Stratification criteria
#     print("Creating stratification features...")
    
#     # 1. Time-based features
#     # Extract year, month, day features
#     years = np.array([ts.year for ts in timestamps[remaining_indices]])
#     months = np.array([ts.month for ts in timestamps[remaining_indices]])
#     # Group months into seasons
#     seasons = np.floor((months - 1) / 3).astype(int)
    
#     # 2. Spatial features
#     if has_geo_info:
#         # Latitude bands
#         lat_bands = np.zeros_like(remaining_indices, dtype=int)
#         lat_bands[(latitudes[remaining_indices] >= 50) & (latitudes[remaining_indices] < 60)] = 1
#         lat_bands[(latitudes[remaining_indices] >= 60) & (latitudes[remaining_indices] < 66.5)] = ...
#         lat_bands[(latitudes[remaining_indices] >= 66.5) & (latitudes[remaining_indices] < 75)] = ...
#         lat_bands[(latitudes[remaining_indices] >= 75)] = 4
        
#         # Longitude sectors (8 sectors of 45 degrees each)
#         lon_sectors = ((longitudes[remaining_indices] + 180) / 45).astype(int) % 8
#     else:
#         # Dummy spatial features if no geographic data
#         lat_bands = np.zeros_like(remaining_indices)
#         lon_sectors = np.zeros_like(remaining_indices)
    
#     # 3. Depth strata
#     depth_strata = np.zeros_like(remaining_indices)
#     depth_strata[(depths[remaining_indices] > 0) & (depths[remaining_indices] <= 0.2)] = 1
#     depth_strata[(depths[remaining_indices] > 0.2) & (depths[remaining_indices] <= 0.5)] = 2
#     depth_strata[(depths[remaining_indices] > 0.5) & (depths[remaining_indices] <= 1.0)] = 3
#     depth_strata[(depths[remaining_indices] > 1.0)] = 4
    
#     # 4. Class labels
#     labels = y[remaining_indices]
    
#     # Create strata by combining features
#     strata = (
#         years * 10000 + 
#         seasons * 1000 + 
#         lat_bands * 100 + 
#         lon_sectors * 10 + 
#         depth_strata
#     )
    
#     # If labels are binary, include them in strata
#     if len(np.unique(labels)) <= 5:  # Few enough classes to use for stratification
#         strata = strata * 10 + labels
    
#     unique_strata = np.unique(strata)
    
#     print(f"Found {len(unique_strata)} unique strata")
    
#     # Initialize val indices with checkpoint
#     val_indices_file = os.path.join(CHECKPOINT_DIR, "val_indices_temp.pkl")
#     if os.path.exists(val_indices_file):
#         print(f"Loading partial validation indices from {val_indices_file}")
#         with open(val_indices_file, "rb") as f:
#             val_indices = pickle.load(f)
        
#         # Remove already sampled from potential pool
#         sampled_mask = np.ones(len(remaining_indices), dtype=bool)
#         sampled_indices = np.where(np.isin(remaining_indices, val_indices))[0]
#         sampled_mask[sampled_indices] = False
        
#         # Update sampling pool
#         remaining_pool = np.arange(len(remaining_indices))[sampled_mask]
        
#         print(f"Loaded {len(val_indices)} validation indices, {len(remaining_pool)} remaining to s...
#     else:
#         val_indices = []
#         remaining_pool = np.arange(len(remaining_indices))
    
#     # Sample from each stratum proportionally to create validation set
#     checkpoint_frequency = 10  # Save every 10 strata
#     for i, stratum in enumerate(tqdm(unique_strata, desc="Sampling validation set")):
#         # Find indices for this stratum
#         stratum_positions = np.where((strata == stratum) & np.isin(np.arange(len(remaining_indices...
        
#         if len(stratum_positions) == 0:
#             continue
            
#         stratum_indices = remaining_indices[stratum_positions]
#         stratum_weights = weights[stratum_positions]
        
#         # Calculate target sample size
#         stratum_weight_sum = np.sum(stratum_weights)
#         total_weight_sum = np.sum(weights)
#         target_val_size = int(val_count * stratum_weight_sum / total_weight_sum)
#         target_val_size = max(1, min(target_val_size, len(stratum_indices) - 1))
        
#         # Sample without replacement, weighted by inverse density
#         if len(stratum_indices) > target_val_size:
#             # Weighted sampling
#             sampled_positions = np.random.choice(
#                 len(stratum_positions),
#                 size=target_val_size,
#                 replace=False,
#                 p=stratum_weights/np.sum(stratum_weights)
#             )
#             sampled_indices = stratum_positions[sampled_positions]
#             val_indices.extend(remaining_indices[sampled_indices])
#         else:
#             # Take all if we need more than available
#             val_indices.extend(stratum_indices)
        
#         # Save checkpoint periodically
#         if (i + 1) % checkpoint_frequency == 0 or i == len(unique_strata) - 1:
#             with open(val_indices_file, "wb") as f:
#                 pickle.dump(val_indices, f)
#             print(f"Saved checkpoint with {len(val_indices)} validation indices")
    
#     # Remaining indices go to train set
#     train_indices = np.setdiff1d(remaining_indices, val_indices)
    
#     # Print statistics about the split
#     print(f"Split sizes: Train={len(train_indices)}, Validation={len(val_indices)}, Test={len(test...
    
#     train_pos = np.sum(y[train_indices])
#     val_pos = np.sum(y[val_indices])
#     test_pos = np.sum(y[test_indices])
    
#     print(f"Positive examples: Train={train_pos} ({train_pos/len(train_indices)*100:.1f}%), " +
#           f"Val={val_pos} ({val_pos/len(val_indices)*100:.1f}%), " +
#           f"Test={test_pos} ({test_pos/len(test_indices)*100:.1f}%)")
    
#     # Save the final split
#     with open(split_file, "wb") as f:
#         pickle.dump({
#             "train_indices": train_indices,
#             "val_indices": val_indices,
#             "test_indices": test_indices
#         }, f)
    
#     print(f"Saved final split to {split_file}")
    
#     # Keep temporary files for potential debugging/recovery
#     # Do NOT remove val_indices_file to preserve all checkpoints
    
#     return train_indices, val_indices, test_indices

# # Determine optimal number of jobs based on available CPU cores and memory
# n_cpus = os.cpu_count()
# available_memory_gb = psutil.virtual_memory().available / (1024*1024*1024)
# recommended_jobs = max(1, min(n_cpus - 1, int(available_memory_gb / 8)))  # More conservative
# print(f"Available CPUs: {n_cpus}, Available memory: {available_memory_gb:.2f} GB")
# print(f"Recommended parallel jobs: {recommended_jobs}")

# densities, weights = calculate_spatial_density(
#     latitudes=latitudes, 
#     longitudes=longitudes, 
#     k=5,
#     leaf_size=40,
#     batch_size=100000,
#     n_jobs=recommended_jobs
#     )
# print(f"Average density: {np.mean(densities):.6f}")
# print(f"Average weight: {np.mean(weights):.6f}")

# train_indices, val_indices, test_indices = stratified_spatiotemporal_split(
#     X=X, 
#     y=y, 
#     metadata=metadata,
#     test_size=0.2, 
#     val_size=0.15
# )

# with open("zero_curtain_pipeline/modeling/checkpoints/spatiotemporal_split.pkl","rb") as f:
#     split = pickle.load(f)

# #split.get('train_indices')
# #split.get('valid_indices')
# #split.get('test_indices')

# with open("zero_curtain_pipeline/modeling/checkpoints/spatial_density.pkl","rb") as f:
#     spatialdensity = pickle.load(f)

# #spatialdensity.get('density')
# #spatialdensity.get('weights')

# import os
# import gc
# import json
# import pickle
# import numpy as np
# import tensorflow as tf
# from tensorflow.keras.callbacks import (
#     EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, 
#     CSVLogger, TensorBoard
# )
# import matplotlib.pyplot as plt
# from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
# from tqdm import tqdm
# import psutil
# from datetime import datetime

# def memory_usage():
#     """Get current memory usage in MB"""
#     process = psutil.Process(os.getpid())
#     mem_info = process.memory_info()
#     return mem_info.rss / 1024**2  # Memory in MB

# def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
#     """
#     Build the advanced zero curtain detection model.
#     """
#     # This is your existing model building function
#     from tensorflow.keras.models import Model
#     from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
#     from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPoo...
#     from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
#     from tensorflow.keras.optimizers import Adam
    
#     # Input layer
#     inputs = Input(shape=input_shape)
    
#     # Reshape for ConvLSTM (add spatial dimension)
#     # From (sequence_length, features) to (sequence_length, 1, features)
#     x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
#     # ConvLSTM layer to capture spatiotemporal patterns
#     from tensorflow.keras.layers import ConvLSTM2D
#     convlstm = ConvLSTM2D(
#         filters=64,
#         kernel_size=(3, 1),
#         padding='same',
#         return_sequences=True,
#         activation='tanh',
#         recurrent_dropout=0.2
#     )(x)
    
#     # Reshape back to (sequence_length, features)
#     convlstm = Reshape((input_shape[0], 64))(convlstm)
    
#     # Add positional encoding for transformer
#     def positional_encoding(length, depth):
#         positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
#         depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
#         angle_rates = 1 / tf.pow(10000.0, depths)
#         angle_rads = positions * angle_rates
        
#         pos_encoding = tf.concat(
#             [tf.sin(angle_rads), tf.cos(angle_rads)], axis=-1)
        
#         return pos_encoding
    
#     # Add positional encoding
#     pos_encoding = positional_encoding(input_shape[0], 64)
#     transformer_input = convlstm + pos_encoding
    
#     # Transformer encoder block
#     def transformer_encoder(x, num_heads=8, key_dim=64, ff_dim=128):
#         # Multi-head attention
#         attention_output = MultiHeadAttention(
#             num_heads=num_heads, key_dim=key_dim
#         )(x, x)
        
#         # Skip connection 1
#         x1 = Add()([attention_output, x])
#         x1 = LayerNormalization(epsilon=1e-6)(x1)
        
#         # Feed-forward network
#         ff_output = Dense(ff_dim, activation='relu')(x1)
#         ff_output = Dropout(0.1)(ff_output)
#         ff_output = Dense(64)(ff_output)
        
#         # Skip connection 2
#         x2 = Add()([ff_output, x1])
#         return LayerNormalization(epsilon=1e-6)(x2)
    
#     # Apply transformer encoder
#     transformer_output = transformer_encoder(transformer_input)
    
#     # Parallel CNN paths for multi-scale feature extraction
#     cnn_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(inputs)
#     cnn_1 = BatchNormalization()(cnn_1)
    
#     cnn_2 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(inputs)
#     cnn_2 = BatchNormalization()(cnn_2)
    
#     cnn_3 = Conv1D(filters=32, kernel_size=7, padding='same', activation='relu')(inputs)
#     cnn_3 = BatchNormalization()(cnn_3)
    
#     # Variational Autoencoder components
#     def sampling(args):
#         z_mean, z_log_var = args
#         batch = tf.shape(z_mean)[0]
#         dim = tf.shape(z_mean)[1]
#         epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
#         return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
#     # Global temporal features
#     global_max = GlobalMaxPooling1D()(transformer_output)
#     global_avg = GlobalAveragePooling1D()(transformer_output)
    
#     # VAE encoding
#     z_mean = Dense(32)(Concatenate()([global_max, global_avg]))
#     z_log_var = Dense(32)(Concatenate()([global_max, global_avg]))
#     z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
#     # Combine all features
#     merged_features = Concatenate()(
#         [
#             GlobalMaxPooling1D()(cnn_1),
#             GlobalMaxPooling1D()(cnn_2),
#             GlobalMaxPooling1D()(cnn_3),
#             global_max,
#             global_avg,
#             z
#         ]
#     )
    
#     # Final classification layers
#     x = Dense(128, activation='relu')(merged_features)
#     x = Dropout(0.3)(x)
#     x = BatchNormalization()(x)
#     x = Dense(64, activation='relu')(x)
#     x = Dropout(0.2)(x)
    
#     # Output layer
#     outputs = Dense(1, activation='sigmoid')(x)
    
#     # Create model
#     model = Model(inputs=inputs, outputs=outputs)
    
#     # Add VAE loss
#     kl_loss = -0.5 * tf.reduce_mean(
#         z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
#     )
#     model.add_loss(0.001 * kl_loss)  # Small weight for VAE loss
    
#     # Compile model with appropriate metrics
#     model.compile(
#         optimizer=Adam(learning_rate=0.001),
#         loss='binary_crossentropy',
#         metrics=[
#             'accuracy',
#             tf.keras.metrics.AUC(name='auc'),
#             tf.keras.metrics.Precision(name='precision'),
#             tf.keras.metrics.Recall(name='recall')
#         ]
#     )
    
#     return model

# def train_with_spatial_balancing(X, y, metadata, output_dir=None, checkpoint_dir=None):
#     """
#     Train a model with spatially balanced sampling.
    
#     Parameters:
#     -----------
#     X : numpy.ndarray
#         Input features
#     y : numpy.ndarray
#         Output labels
#     metadata : list
#         Metadata containing timestamps and spatial information
#     output_dir : str, optional
#         Directory to save model and results
#     checkpoint_dir : str, optional
#         Directory for saving checkpoints
        
#     Returns:
#     --------
#     trained_model, training_history
#     """
#     print("Training zero curtain model with spatial balancing...")
#     print(f"Memory before training: {memory_usage():.1f} MB")
    
#     # Create output directory
#     if output_dir:
#         os.makedirs(output_dir, exist_ok=True)
    
#     if checkpoint_dir is None:
#         checkpoint_dir = os.path.join(output_dir, 'checkpoints') if output_dir else 'checkpoints'
#     os.makedirs(checkpoint_dir, exist_ok=True)
    
#     # Enable memory growth to avoid pre-allocating all GPU memory
#     physical_devices = tf.config.list_physical_devices('GPU')
#     if physical_devices:
#         for device in physical_devices:
#             try:
#                 tf.config.experimental.set_memory_growth(device, True)
#                 print(f"Enabled memory growth for {device}")
#             except:
#                 print(f"Could not set memory growth for {device}")
    
#     # Create spatiotemporally balanced train/val/test split
#     print("Creating spatiotemporally balanced split...")
#     train_indices, val_indices, test_indices = stratified_spatiotemporal_split(
#         X, y, metadata, test_size=0.2, val_size=0.15, checkpoint_dir=checkpoint_dir
#     )
    
#     # Create the splits
#     X_train = X[train_indices]
#     y_train = y[train_indices]
    
#     X_val = X[val_indices]
#     y_val = y[val_indices]
    
#     X_test = X[test_indices]
#     y_test = y[test_indices]
    
#     # Load spatial density weights if available
#     weights_file = os.path.join(checkpoint_dir, "spatial_density.pkl")
#     if os.path.exists(weights_file):
#         print(f"Loading spatial weights from {weights_file}")
#         with open(weights_file, "rb") as f:
#             weights_data = pickle.load(f)
#         sample_weights = weights_data["weights"][train_indices]
        
#         # Normalize weights
#         sample_weights = sample_weights / np.mean(sample_weights) * len(sample_weights)
#     else:
#         print("No spatial weights found, using uniform weights")
#         sample_weights = np.ones(len(train_indices))
    
#     # Print info about the splits
#     print(f"Train/val/test sizes: {len(X_train)}/{len(X_val)}/{len(X_test)}")
#     print(f"Positive examples: Train={np.sum(y_train)} ({np.sum(y_train)/len(y_train)*100:.1f}%), ...
#           f"Val={np.sum(y_val)} ({np.sum(y_val)/len(y_val)*100:.1f}%), " +
#           f"Test={np.sum(y_test)} ({np.sum(y_test)/len(y_test)*100:.1f}%)")
    
#     # Combine sample weights with class weights for imbalanced data
#     pos_weight = (len(y_train) - np.sum(y_train)) / max(1, np.sum(y_train))
#     class_weight = {0: 1.0, 1: pos_weight}
#     print(f"Using class weight {pos_weight:.2f} for positive examples")
    
#     # Build model with appropriate input shape
#     input_shape = (X_train.shape[1], X_train.shape[2])
#     model = build_advanced_zero_curtain_model(input_shape)
    
#     # Always check for existing model checkpoint to resume training
#     model_checkpoint_paths = []
#     if output_dir:
#         # Look for checkpoint files in multiple locations
#         model_checkpoint_path1 = os.path.join(output_dir, 'model_checkpoint.h5')
#         model_checkpoint_path2 = os.path.join(output_dir, 'checkpoint.h5')
#         model_checkpoint_path3 = os.path.join(checkpoint_dir, 'model_checkpoint.h5')
        
#         model_checkpoint_paths = [p for p in [model_checkpoint_path1, model_checkpoint_path2, mode...
#                                 if os.path.exists(p)]
        
#         if model_checkpoint_paths:
#             print(f"Found {len(model_checkpoint_paths)} existing model checkpoints")
#             # Use the most recent checkpoint based on modification time
#             latest_checkpoint = max(model_checkpoint_paths, key=os.path.getmtime)
#             print(f"Loading most recent checkpoint: {latest_checkpoint}")
#             try:
#                 model = tf.keras.models.load_model(latest_checkpoint)
#                 print("Checkpoint loaded successfully - will resume training from this point")
#             except Exception as e:
#                 print(f"Error loading checkpoint: {str(e)}")
#                 print("Will start training from scratch")
    
#     # Set up callbacks
#     callbacks = [
#         # Stop training when validation performance plateaus
#         EarlyStopping(
#             patience=15,
#             restore_best_weights=True,
#             monitor='val_auc',
#             mode='max'
#         ),
#         # Reduce learning rate when improvement slows
#         ReduceLROnPlateau(
#             factor=0.5,
#             patience=7,
#             min_lr=1e-6,
#             monitor='val_auc',
#             mode='max'
#         ),
#         # Manual garbage collection after each epoch
#         tf.keras.callbacks.LambdaCallback(
#             on_epoch_end=lambda epoch, logs: gc.collect()
#         )
#     ]
    
#     # Add additional callbacks if output directory provided
#     if output_dir:
#         callbacks.extend([
#             # Save best model
#             ModelCheckpoint(
#                 os.path.join(output_dir, 'model_checkpoint.h5'),
#                 save_best_only=True,
#                 monitor='val_auc',
#                 mode='max'
#             ),
#             # Log training progress to CSV
#             CSVLogger(
#                 os.path.join(output_dir, 'training_log.csv'),
#                 append=True
#             ),
#             # TensorBoard visualization
#             TensorBoard(
#                 log_dir=os.path.join(output_dir, 'tensorboard_logs'),
#                 histogram_freq=1,
#                 profile_batch=0  # Disable profiling to save memory
#             )
#         ])
    
#     # Train model
#     print("Starting model training...")
#     batch_size = 32  # Adjust based on available memory
#     epochs = 100
    
#     history = model.fit(
#         X_train, y_train,
#         validation_data=(X_val, y_val),
#         epochs=epochs,
#         batch_size=batch_size,
#         callbacks=callbacks,
#         class_weight=class_weight,
#         sample_weight=sample_weights,
#         verbose=1,
#         shuffle=True,
#         use_multiprocessing=False,  # Avoid memory overhead
#         workers=1  # Reduce parallel processing
#     )
    
#     # Clean up to free memory
#     del X_train, y_train, X_val, y_val
#     gc.collect()
    
#     # Evaluate on test set
#     print("Evaluating model on test set...")
#     evaluation = model.evaluate(X_test, y_test, batch_size=batch_size)
#     print("Test performance:")
#     for metric, value in zip(model.metrics_names, evaluation):
#         print(f"  {metric}: {value:.4f}")
    
#     # Generate predictions for test set
#     y_pred_prob = model.predict(X_test, batch_size=batch_size)
#     y_pred = (y_pred_prob > 0.5).astype(int).flatten()
    
#     # Calculate additional evaluation metrics
#     report = classification_report(y_test, y_pred)
#     conf_matrix = confusion_matrix(y_test, y_pred)
    
#     print("Classification Report:")
#     print(report)
    
#     print("Confusion Matrix:")
#     print(conf_matrix)
    
#     # Save evaluation results
#     if output_dir:
#         # Save evaluation metrics
#         with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w') as f:
#             f.write("Classification Report:\n")
#             f.write(report)
#             f.write("\n\nConfusion Matrix:\n")
#             f.write(str(conf_matrix))
#             f.write("\n\nTest Metrics:\n")
#             for metric, value in zip(model.metrics_names, evaluation):
#                 f.write(f"{metric}: {value:.4f}\n")
        
#         # Save test set predictions with timestamp to avoid overwriting
#         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#         np.save(os.path.join(output_dir, f'test_predictions_{timestamp}.npy'), y_pred_prob)
#         np.save(os.path.join(output_dir, f'test_indices_{timestamp}.npy'), test_indices)
#         # Also keep a copy with the standard name for easier reference
#         np.save(os.path.join(output_dir, 'test_predictions_latest.npy'), y_pred_prob)
#         np.save(os.path.join(output_dir, 'test_indices_latest.npy'), test_indices)
        
#         # Plot training history
#         plt.figure(figsize=(16, 6))
        
#         plt.subplot(1, 3, 1)
#         plt.plot(history.history['auc'])
#         plt.plot(history.history['val_auc'])
#         plt.title('Model AUC')
#         plt.ylabel('AUC')
#         plt.xlabel('Epoch')
#         plt.legend(['Train', 'Validation'], loc='lower right')
        
#         plt.subplot(1, 3, 2)
#         plt.plot(history.history['loss'])
#         plt.plot(history.history['val_loss'])
#         plt.title('Model Loss')
#         plt.ylabel('Loss')
#         plt.xlabel('Epoch')
#         plt.legend(['Train', 'Validation'], loc='upper right')
        
#         # Plot ROC curve
#         plt.subplot(1, 3, 3)
#         fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
#         roc_auc = auc(fpr, tpr)
#         plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')
#         plt.plot([0, 1], [0, 1], 'k--')
#         plt.xlabel('False Positive Rate')
#         plt.ylabel('True Positive Rate')
#         plt.title('ROC Curve (Test Set)')
#         plt.legend(loc='lower right')
        
#         plt.tight_layout()
#         plt.savefig(os.path.join(output_dir, 'training_performance.png'), dpi=300)
        
#         # Save detailed model summary
#         from contextlib import redirect_stdout
#         with open(os.path.join(output_dir, 'model_summary.txt'), 'w') as f:
#             with redirect_stdout(f):
#                 model.summary()
    
#     # Clean up to free memory
#     del X_test, y_test
#     gc.collect()
    
#     print(f"Memory after training: {memory_usage():.1f} MB")
#     return model, history, evaluation

# # Example usage
# if __name__ == "__main__":
#     # Load data
#     data_dir = os.path.join('/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling', 'm...
#     X = np.load(os.path.join(data_dir, 'X_features.npy'))
#     y = np.load(os.path.join(data_dir, 'y_labels.npy'))
#     with open(os.path.join(data_dir, 'metadata.pkl'), 'rb') as f:
#         metadata = pickle.load(f)
    
#     # Train model with spatial balancing
#     output_dir = os.path.join('/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling', ...
#     os.makedirs(output_dir, exist_ok=True)
    
#     model, history, evaluation = train_with_spatial_balancing(
#         X=X,
#         y=y,
#         metadata=metadata,
#         output_dir=output_dir
#     )
    
#     print("Training complete!")

# # Retain all previous imports
# import os
# import sys
# import tensorflow as tf
# import cartopy.crs as ccrs
# import cartopy.feature as cfeature
# import cmocean
# import gc
# import glob
# import json
# import logging
# import matplotlib.gridspec as gridspec
# import matplotlib.pyplot as plt
# import numpy as np
# import pathlib
# import pickle
# import psutil
# import time
# import re
# import scipy.interpolate as interpolate
# import scipy.stats as stats
# import seaborn as sns
# from pyproj import Proj
# import sklearn.experimental
# import sklearn.impute
# import sklearn.linear_model
# import sklearn.preprocessing
# import tqdm
# import xarray as xr
# import warnings
# warnings.filterwarnings('ignore')

# from osgeo import gdal, osr
# from matplotlib.colors import LinearSegmentedColormap
# from concurrent.futures import ThreadPoolExecutor
# from datetime import datetime, timedelta
# from pathlib import Path
# from scipy.spatial import cKDTree
# from tqdm import tqdm
# from tqdm.notebook import tqdm

# import keras_tuner as kt
# from keras_tuner.tuners import BayesianOptimization

# # Configure TensorFlow memory growth
# def configure_tensorflow_memory():
#     """Configure TensorFlow to use memory growth and limit GPU memory allocation"""
#     physical_devices = tf.config.list_physical_devices('GPU')
#     if physical_devices:
#         for device in physical_devices:
#             try:
#                 # Allow memory growth - prevents TF from allocating all GPU memory at once
#                 tf.config.experimental.set_memory_growth(device, True)
#                 print(f"Memory growth enabled for {device}")
#             except Exception as e:
#                 print(f"Error configuring GPU: {e}")
    
#     # Limit CPU threads
#     tf.config.threading.set_intra_op_parallelism_threads(4)
#     tf.config.threading.set_inter_op_parallelism_threads(2)
    
#     # Set soft device placement
#     tf.config.set_soft_device_placement(True)

# # Retain existing utility functions
# def memory_usage():
#     """Monitor memory usage in MB"""
#     process = psutil.Process(os.getpid())
#     mem_info = process.memory_info()
#     return mem_info.rss / 1024**2  # Memory in MB

# def save_checkpoint(data, checkpoint_dir, name):
#     """Save checkpoint data to pickle file"""
#     os.makedirs(checkpoint_dir, exist_ok=True)
#     checkpoint_path = os.path.join(checkpoint_dir, f'{name}.pkl')
#     with open(checkpoint_path, 'wb') as f:
#         pickle.dump(data, f)
#     print(f"Saved checkpoint to {checkpoint_path}")

# def load_checkpoint(checkpoint_dir, name):
#     """Load checkpoint data from pickle file"""
#     checkpoint_path = os.path.join(checkpoint_dir, f'{name}.pkl')
#     try:
#         with open(checkpoint_path, 'rb') as f:
#             data = pickle.load(f)
#         print(f"Loaded checkpoint from {checkpoint_path}")
#         return data
#     except:
#         print(f"No checkpoint found at {checkpoint_path}")
#         return None

# # Updated model building function with recommended improvements
# def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
#     """
#     Advanced zero curtain detection model with recommended improvements.
    
#     Parameters:
#     -----------
#     input_shape : tuple
#         Shape of input data (sequence_length, num_features)
#     include_moisture : bool
#         Whether soil moisture features are included
    
#     Improvements:
#     - Increased dropout rates
#     - L2 regularization
#     - Gradient clipping
#     - Simplified model components
#     - Adjustable VAE weight
#     """
#     from tensorflow.keras.models import Model
#     from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
#     from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPoo...
#     from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
#     from tensorflow.keras.optimizers import Adam
#     from tensorflow.keras.regularizers import l2
    
#     # Regularization parameters
#     l2_lambda = 1e-4  # Lightweight L2 regularization
#     dropout_rate = 0.35  # Increased dropout as recommended
    
#     # Input layer
#     inputs = Input(shape=input_shape)
    
#     # Reshape for ConvLSTM (add spatial dimension)
#     x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
#     # ConvLSTM layer with simplified architecture
#     from tensorflow.keras.layers import ConvLSTM2D
#     convlstm = ConvLSTM2D(
#         filters=48,  # Reduced complexity
#         kernel_size=(3, 1),
#         padding='same',
#         return_sequences=True,
#         activation='tanh',
#         recurrent_dropout=dropout_rate,
#         kernel_regularizer=l2(l2_lambda)
#     )(x)
    
#     # Reshape back to (sequence_length, features)
#     convlstm = Reshape((input_shape[0], 48))(convlstm)
    
#     # Simplified positional encoding
#     def positional_encoding(length, depth):
#         positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
#         depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
#         angle_rates = 1 / tf.pow(10000.0, depths)
#         angle_rads = positions * angle_rates
        
#         return tf.sin(angle_rads)
    
#     # Add positional encoding
#     pos_encoding = positional_encoding(input_shape[0], 48)
#     transformer_input = convlstm + pos_encoding
    
#     # Simplified transformer encoder block
#     def transformer_encoder(x, num_heads=6, key_dim=48, ff_dim=96):
#         # Multi-head attention with regularization
#         attention_output = MultiHeadAttention(
#             num_heads=num_heads, 
#             key_dim=key_dim,
#             kernel_regularizer=l2(l2_lambda)
#         )(x, x)
        
#         # Skip connection with dropout and regularization
#         x1 = Add()([attention_output, x])
#         x1 = Dropout(dropout_rate)(x1)
#         x1 = LayerNormalization(epsilon=1e-6)(x1)
        
#         # Simplified feed-forward network
#         ff_output = Dense(
#             ff_dim, 
#             activation='relu', 
#             kernel_regularizer=l2(l2_lambda)
#         )(x1)
#         ff_output = Dropout(dropout_rate)(ff_output)
#         ff_output = Dense(
#             48, 
#             kernel_regularizer=l2(l2_lambda)
#         )(ff_output)
        
#         # Skip connection
#         x2 = Add()([ff_output, x1])
#         return LayerNormalization(epsilon=1e-6)(x2)
    
#     # Apply transformer encoder
#     transformer_output = transformer_encoder(transformer_input)
    
#     # Simplified CNN paths with regularization
#     cnn_paths = []
#     for kernel_size in [3, 5, 7]:
#         cnn = Conv1D(
#             filters=32, 
#             kernel_size=kernel_size, 
#             padding='same', 
#             activation='relu',
#             kernel_regularizer=l2(l2_lambda)
#         )(inputs)
#         cnn = BatchNormalization()(cnn)
#         cnn = Dropout(dropout_rate)(cnn)
#         cnn_paths.append(GlobalMaxPooling1D()(cnn))
    
#     # Variational Autoencoder components with adjustable weight
#     def sampling(args):
#         z_mean, z_log_var = args
#         batch = tf.shape(z_mean)[0]
#         dim = tf.shape(z_mean)[1]
#         epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
#         return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
#     # Global temporal features
#     global_max = GlobalMaxPooling1D()(transformer_output)
#     global_avg = GlobalAveragePooling1D()(transformer_output)
    
#     # VAE encoding with regularization
#     z_mean = Dense(
#         32, 
#         kernel_regularizer=l2(l2_lambda)
#     )(Concatenate()([global_max, global_avg]))
#     z_log_var = Dense(
#         32, 
#         kernel_regularizer=l2(l2_lambda)
#     )(Concatenate()([global_max, global_avg]))
#     z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
#     # Combine features
#     merged_features = Concatenate()(
#         cnn_paths + [global_max, global_avg, z]
#     )
    
#     # Simplified classification layers with dropout and regularization
#     x = Dense(
#         96, 
#         activation='relu', 
#         kernel_regularizer=l2(l2_lambda)
#     )(merged_features)
#     x = Dropout(dropout_rate)(x)
#     x = BatchNormalization()(x)
    
#     x = Dense(
#         48, 
#         activation='relu', 
#         kernel_regularizer=l2(l2_lambda)
#     )(x)
#     x = Dropout(0.25)(x)  # Slightly lower dropout for final layer
    
#     # Output layer
#     outputs = Dense(1, activation='sigmoid')(x)
    
#     # Create model
#     model = Model(inputs=inputs, outputs=outputs)
    
#     # Adjustable VAE loss weight (can be tuned)
#     vae_weight = 0.001  # Configurable based on recommendations
#     kl_loss = -0.5 * tf.reduce_mean(
#         z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
#     )
#     model.add_loss(vae_weight * kl_loss)
    
#     # Compile with gradient clipping
#     optimizer = Adam(
#         learning_rate=0.001, 
#         clipnorm=1.0  # Gradient clipping to stabilize training
#     )
    
#     model.compile(
#         optimizer=optimizer,
#         loss='binary_crossentropy',
#         metrics=[
#             'accuracy',
#             tf.keras.metrics.AUC(name='auc'),
#             tf.keras.metrics.Precision(name='precision'),
#             tf.keras.metrics.Recall(name='recall')
#         ]
#     )
    
#     return model

# def train_model_with_enhancements(model, X, y, val_X, val_y, output_dir):
#     """
#     Training function with recommended enhancements.
    
#     Improvements:
#     - Cyclic learning rate
#     - Extended early stopping patience
#     - Stratified k-fold cross-validation
#     - Feature importance analysis
#     """
#     import tensorflow as tf
#     from tensorflow.keras.callbacks import (
#         EarlyStopping, 
#         ReduceLROnPlateau, 
#         ModelCheckpoint, 
#         CSVLogger
#     )
#     from sklearn.model_selection import StratifiedKFold
#     import numpy as np
#     import os
    
#     # Create output directory
#     os.makedirs(output_dir, exist_ok=True)
    
#     # Cyclic learning rate callback
#     def cyclic_lr_schedule(epoch, lr):
#         """Implement a simple triangular learning rate schedule"""
#         cycle_length = 10
#         base_lr = 1e-4
#         max_lr = 1e-3
        
#         cycle = np.floor(1 + epoch / (2 * cycle_length))
#         x = np.abs(epoch / cycle_length - 2 * cycle + 1)
#         lr = base_lr + (max_lr - base_lr) * np.maximum(0, (1 - x))
#         return lr
    
#     cyclic_lr = tf.keras.callbacks.LearningRateScheduler(cyclic_lr_schedule)
    
#     # Callbacks with extended patience
#     early_stopping = EarlyStopping(
#         monitor='val_auc',
#         patience=25,  # Extended patience as recommended
#         restore_best_weights=True,
#         mode='max'
#     )
    
#     reduce_lr = ReduceLROnPlateau(
#         monitor='val_auc',
#         factor=0.5,
#         patience=10,
#         min_lr=1e-6,
#         mode='max'
#     )
    
#     # Model checkpoint
#     checkpoint_path = os.path.join(output_dir, 'best_model.h5')
#     model_checkpoint = ModelCheckpoint(
#         checkpoint_path,
#         monitor='val_auc',
#         save_best_only=True,
#         mode='max'
#     )
    
#     # CSV logger for detailed tracking
#     csv_logger = CSVLogger(
#         os.path.join(output_dir, 'training_log.csv'),
#         append=True
#     )
    
#     # Perform stratified k-fold cross-validation
#     stratified_kfold = StratifiedKFold(
#         n_splits=5,  # 5-fold cross-validation 
#         shuffle=True,
#         random_state=42
#     )
    
#     # Store fold results
#     fold_results = []
    
#     # Iterate through folds
#     for fold, (train_idx, val_idx) in enumerate(
#         stratified_kfold.split(X, y), 1
#     ):
#         print(f"\nTraining Fold {fold}")
        
#         # Split data for this fold
#         X_train_fold, X_val_fold = X[train_idx], X[val_idx]
#         y_train_fold, y_val_fold = y[train_idx], y[val_idx]
        
#         # Train on this fold
#         fold_history = model.fit(
#             X_train_fold, y_train_fold,
#             validation_data=(X_val_fold, y_val_fold),
#             epochs=100,  # Max epochs, will stop early
#             batch_size=32,
#             callbacks=[
#                 early_stopping, 
#                 reduce_lr, 
#                 model_checkpoint, 
#                 csv_logger,
#                 cyclic_lr
#             ]
#         )
        
#         # Evaluate and store results
#         fold_results.append({
#             'fold': fold,
#             'val_auc': max(fold_history.history['val_auc']),
#             'val_accuracy': max(fold_history.history['val_accuracy'])
#         })
    
#     # Analyze feature importance (basic method)
#     def visualize_feature_importance(model, X):
#         """
#         Simple feature importance visualization using model gradients.
#         Requires further refinement for more accurate insights.
#         """
#         import numpy as np
#         import matplotlib.pyplot as plt
        
#         # Create gradient tape to track input gradients
#         with tf.GradientTape() as tape:
#             tape.watch(X)
#             predictions = model(X)
        
#         # Compute gradients of predictions with respect to input
#         input_gradients = tape.gradient(predictions, X)
        
#         # Aggregate gradient importance
#         feature_importance = np.abs(input_gradients).mean(axis=(0,1))
        
#         # Visualization
#         plt.figure(figsize=(10, 6))
#         plt.bar(range(len(feature_importance)), feature_importance)
#         plt.title('Feature Importance')
#         plt.xlabel('Feature Index')
#         plt.ylabel('Gradient-based Importance')
#         plt.tight_layout()
#         plt.savefig(os.path.join(output_dir, 'feature_importance.png'))
        
#         return feature_importance
    
#     # Visualize feature importance for the last fold
#     feature_importance = visualize_feature_importance(model, X_val_fold)
    
#     # Save cross-validation results
#     import json
#     with open(os.path.join(output_dir, 'cross_validation_results.json'), 'w') as f:
#         json.dump(fold_results, f, indent=2)
    
#     return model, fold_results, feature_importance

# def create_optimized_tf_dataset(X_file, y_file, indices, batch_size=256, shuffle=True, 
#                                weights=None, cache=False, prefetch_factor=tf.data.AUTOTUNE):
#     """
#     Create an optimized TensorFlow dataset with detailed progress reporting.
    
#     Parameters:
#     -----------
#     X_file : str
#         Path to features numpy file
#     y_file : str
#         Path to labels numpy file
#     indices : array-like
#         Indices to use for dataset
#     batch_size : int
#         Batch size for training
#     shuffle : bool
#         Whether to shuffle the data
#     weights : array-like, optional
#         Sample weights
#     cache : bool
#         Whether to cache the dataset
#     prefetch_factor : int
#         Prefetch buffer size
    
#     Returns:
#     --------
#     tf.data.Dataset
#         Optimized TensorFlow dataset
#     """
#     import numpy as np
#     import tensorflow as tf
#     import time
    
#     # Log start time for loading data
#     load_start = time.time()
#     print(f"  Loading memory-mapped arrays...")
    
#     # Load as memory-mapped arrays
#     X = np.load(X_file, mmap_mode='r')
#     y = np.load(y_file, mmap_mode='r')
    
#     print(f"  Arrays loaded in {time.time() - load_start:.2f} seconds")
    
#     # Get input shape from first sample
#     sample_start = time.time()
#     input_shape = X[indices[0]].shape
#     print(f"  Input shape: {input_shape}, obtained in {time.time() - sample_start:.2f} seconds")
    
#     # Prepare data tensors
#     X_subset = np.array([X[idx] for idx in indices])
#     y_subset = np.array([y[idx] for idx in indices])
    
#     # Create TensorFlow dataset
#     dataset = tf.data.Dataset.from_tensor_slices(
#         (X_subset, y_subset)
#     )
    
#     # Add sample weights if provided
#     if weights is not None:
#         weights_subset = np.array([weights[np.where(indices == idx)[0][0]] for idx in indices])
#         dataset = tf.data.Dataset.from_tensor_slices(
#             (X_subset, y_subset, weights_subset)
#         )
    
#     # Apply dataset optimizations
#     print(f"  Applying dataset optimizations...")
#     opt_start = time.time()
    
#     if shuffle:
#         buffer_size = min(len(indices), 10000)
#         print(f"  Shuffling with buffer size {buffer_size}...")
#         dataset = dataset.shuffle(buffer_size)
    
#     print(f"  Batching with size {batch_size}...")
#     dataset = dataset.batch(batch_size)
    
#     if cache:
#         print(f"  Caching dataset...")
#         dataset = dataset.cache()
    
#     print(f"  Setting prefetch to {prefetch_factor}...")
#     dataset = dataset.prefetch(prefetch_factor)
    
#     print(f"  Optimizations applied in {time.time() - opt_start:.2f} seconds")
    
#     return dataset

# def resumable_efficient_training(model, X_file, y_file, train_indices, val_indices, test_indices,
#                                output_dir, batch_size=32, chunk_size=25000, epochs_per_chunk=2, 
#                                save_frequency=5, class_weight=None, start_chunk=0):
#     """
#     Resumable training function with TensorFlow Datasets for efficient processing.
    
#     Parameters are the same as the previous implementation, with enhanced dataset handling.
#     """
#     import os
#     import gc
#     import json
#     import numpy as np
#     import tensorflow as tf
#     from datetime import datetime, timedelta
#     import time
#     import psutil
    
#     # Create output directories
#     os.makedirs(output_dir, exist_ok=True)
#     predictions_dir = os.path.join(output_dir, "predictions")
#     os.makedirs(predictions_dir, exist_ok=True)
#     checkpoints_dir = os.path.join(output_dir, "checkpoints")
#     os.makedirs(checkpoints_dir, exist_ok=True)
    
#     # Process in chunks
#     num_chunks = int(np.ceil(len(train_indices) / chunk_size))
#     print(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
#     print(f"Starting from chunk {start_chunk+1}")
    
#     # Load validation data using TensorFlow Dataset
#     print("Creating validation dataset...")
#     val_limit = min(2000, len(val_indices))
#     val_indices_subset = val_indices[:val_limit]
    
#     val_dataset = create_optimized_tf_dataset(
#         X_file, y_file, val_indices_subset, 
#         batch_size=batch_size, shuffle=False
#     )
    
#     # Setup callbacks
#     callbacks = [
#         tf.keras.callbacks.EarlyStopping(
#             patience=3, 
#             restore_best_weights=True,
#             monitor='val_loss',
#             min_delta=0.01
#         ),
#         tf.keras.callbacks.ReduceLROnPlateau(
#             factor=0.5,
#             patience=2,
#             min_lr=1e-6,
#             monitor='val_loss'
#         ),
#         # Memory cleanup after each epoch
#         tf.keras.callbacks.LambdaCallback(
#             on_epoch_end=lambda epoch, logs: gc.collect()
#         )
#     ]
    
#     # Track metrics across chunks
#     history_log = []
#     start_time = time.time()
    
#     # Load existing history if resuming
#     history_path = os.path.join(output_dir, "training_history.json")
#     if start_chunk > 0 and os.path.exists(history_path):
#         try:
#             with open(history_path, "r") as f:
#                 history_log = json.load(f)
#         except Exception as e:
#             print(f"Could not load existing history: {e}")
    
#     # Process each chunk
#     for chunk_idx in range(start_chunk, num_chunks):
#         # Get chunk indices
#         start_idx = chunk_idx * chunk_size
#         end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
#         chunk_indices = train_indices[start_idx:end_idx]
        
#         # Create TensorFlow Dataset for this chunk
#         train_dataset = create_optimized_tf_dataset(
#             X_file, y_file, chunk_indices,
#             batch_size=batch_size, shuffle=True
#         )
        
#         # Train on chunk
#         print(f"Training for {epochs_per_chunk} epochs on chunk {chunk_idx+1}...")
#         history = model.fit(
#             train_dataset,
#             validation_data=val_dataset,
#             epochs=epochs_per_chunk,
#             callbacks=callbacks,
#             class_weight=class_weight,
#             verbose=1
#         )
        
#         # Store metrics (similar to previous implementation)
#         chunk_metrics = {}
#         for k, v in history.history.items():
#             chunk_metrics[k] = [float(val) for val in v]
#         history_log.append(chunk_metrics)
        
#         # Save model and history periodically
#         if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
#             model_path = os.path.join(checkpoints_dir, f"model_checkpoint_{chunk_idx+1}.h5")
#             model.save(model_path)
            
#             # Save history
#             try:
#                 with open(history_path, "w") as f:
#                     json.dump(history_log, f)
#             except Exception as e:
#                 print(f"Warning: Could not save history: {e}")
        
#         # Clean up
#         del train_dataset
#         gc.collect()
    
#     # Save final model
#     final_model_path = os.path.join(output_dir, "final_model.h5")
#     model.save(final_model_path)
    
#     # Final evaluation on test set
#     print("\nPerforming final evaluation on test set...")
    
#     # Process test data in batches
#     test_batch_size = 5000
#     num_test_batches = int(np.ceil(len(test_indices) / test_batch_size))
    
#     all_test_predictions = []
#     all_test_true = []
#     test_metrics = {'loss': 0, 'accuracy': 0, 'samples': 0}
    
#     for test_batch_idx in range(num_test_batches):
#         start_idx = test_batch_idx * test_batch_size
#         end_idx = min((test_batch_idx + 1) * test_batch_size, len(test_indices))
#         batch_indices = test_indices[start_idx:end_idx]
        
#         # Load batch data
#         test_X = np.array([X_mmap[idx] for idx in batch_indices])
#         test_y = np.array([y_mmap[idx] for idx in batch_indices])
        
#         # Evaluate
#         metrics = model.evaluate(test_X, test_y, verbose=1)
#         metrics_dict = {name: value for name, value in zip(model.metrics_names, metrics)}
        
#         # Weight metrics by batch size
#         for key in ['loss', 'accuracy']:
#             if key in metrics_dict:
#                 test_metrics[key] += metrics_dict[key] * len(batch_indices)
#         test_metrics['samples'] += len(batch_indices)
        
#         # Get predictions
#         test_preds = model.predict(test_X, batch_size=batch_size)
        
#         # Store
#         all_test_predictions.append(test_preds.flatten())
#         all_test_true.append(test_y)
        
#         # Clean up
#         del test_X, test_y, test_preds
#         gc.collect()
    
#     # Combine results
#     all_test_predictions = np.concatenate(all_test_predictions)
#     all_test_true = np.concatenate(all_test_true)
    
#     # Calculate final test metrics
#     test_loss = test_metrics['loss'] / test_metrics['samples']
#     test_accuracy = test_metrics['accuracy'] / test_metrics['samples']
    
#     # Calculate additional metrics
#     test_preds_binary = (all_test_predictions > 0.5).astype(int)
#     report = classification_report(all_test_true, test_preds_binary)
#     conf_matrix = confusion_matrix(all_test_true, test_preds_binary)
#     fpr, tpr, _ = roc_curve(all_test_true, all_test_predictions)
#     roc_auc = auc(fpr, tpr)
    
#     # Save results
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     np.save(os.path.join(output_dir, f'test_predictions_{timestamp}.npy'), all_test_predictions)
#     np.save(os.path.join(output_dir, 'test_predictions_latest.npy'), all_test_predictions)
    
#     with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w') as f:
#         f.write("Classification Report:\n")
#         f.write(report)
#         f.write("\n\nConfusion Matrix:\n")
#         f.write(str(conf_matrix))
#         f.write("\n\nTest Metrics:\n")
#         f.write(f"loss: {test_loss:.4f}\n")
#         f.write(f"accuracy: {test_accuracy:.4f}\n")
#         f.write(f"AUC: {roc_auc:.4f}\n")
    
#     return model, os.path.join(output_dir, "final_model.h5")

# def optimized_resumable_efficient_training(
#     model, 
#     X_file, 
#     y_file, 
#     train_indices, 
#     val_indices, 
#     test_indices,
#     output_dir, 
#     batch_size=32, 
#     chunk_size=10000, 
#     epochs_per_chunk=2, 
#     save_frequency=3, 
#     class_weight=None, 
#     start_chunk=0
# ):
#     """
#     Optimized training function with extensive memory management and logging
#     """
#     import os
#     import gc
#     import json
#     import numpy as np
#     import tensorflow as tf
#     import psutil
#     import time
#     import logging
#     from datetime import datetime
#     from sklearn.metrics import (
#         classification_report, 
#         confusion_matrix, 
#         roc_auc_score, 
#         precision_recall_curve, 
#         average_precision_score
#     )
#     import matplotlib.pyplot as plt

#     def aggressive_memory_cleanup():
#         """Ultra-aggressive memory cleanup"""
#         # Clear Keras backend
#         tf.keras.backend.clear_session()
        
#         # Garbage collection with multiple passes
#         for _ in range(3):
#             gc.collect()
        
#         # Optional: Release memory back to system
#         try:
#             import ctypes
#             libc = ctypes.CDLL('libc.so.6')
#             libc.malloc_trim(0)
#         except:
#             pass
        
#         # Force NumPy to release memory
#         import numpy as np
#         np.get_default_dtype()
    
#     # Configure logging
#     logging.basicConfig(
#         filename=os.path.join(output_dir, 'training_log.txt'),
#         level=logging.INFO,
#         format='%(asctime)s - %(levelname)s: %(message)s'
#     )
    
#     # Create output directories
#     os.makedirs(output_dir, exist_ok=True)
#     checkpoints_dir = os.path.join(output_dir, "checkpoints")
#     os.makedirs(checkpoints_dir, exist_ok=True)
#     predictions_dir = os.path.join(output_dir, "predictions")
#     os.makedirs(predictions_dir, exist_ok=True)

#     validation_dir = os.path.join(output_dir, "validation")
#     os.makedirs(validation_dir, exist_ok=True)
    
#     # System memory tracking
#     total_memory = psutil.virtual_memory().total / (1024**3)  # GB
#     memory_limit = total_memory * 0.6  # 60% of total memory
#     logging.info(f"Total System Memory: {total_memory:.2f} GB")
#     logging.info(f"Memory Limit Set: {memory_limit:.2f} GB")

#     aggressive_memory_cleanup()
    
#     # Load memory-mapped data
#     X_mmap = np.load(X_file, mmap_mode='r')
#     y_mmap = np.load(y_file, mmap_mode='r')
    
#     # Process in chunks
#     num_chunks = int(np.ceil(len(train_indices) / chunk_size))
#     logging.info(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}"...
    
#     # Prepare validation data (small subset)
#     val_limit = min(2000, len(val_indices))
#     val_indices_subset = val_indices[:val_limit]
#     val_X = np.array([X_mmap[idx] for idx in val_indices_subset], dtype=np.float32)
#     val_y = np.array([y_mmap[idx] for idx in val_indices_subset], dtype=np.float32)

#     model.reset_states()

#     tf.config.run_functions_eagerly(False)
    
#     # Callbacks for memory-aware training
#     callbacks = [
#         tf.keras.callbacks.EarlyStopping(
#             patience=3, 
#             restore_best_weights=True,
#             monitor='val_loss'
#         ),
#         tf.keras.callbacks.ReduceLROnPlateau(
#             factor=0.5, 
#             patience=2, 
#             min_lr=1e-6,
#             monitor='val_loss'
#         ),
#         # Memory cleanup callback
#         tf.keras.callbacks.LambdaCallback(
#             on_epoch_end=lambda epoch, logs: aggressive_memory_cleanup()
#         )
#     ]
    
#     # Comprehensive tracking
#     training_history = {
#         'chunks': [],
#         'validation_metrics': [],
#         'model_checkpoints': []
#     }
#     start_time = time.time()
    
#     # Recovery and resumption
#     recovery_file = os.path.join(output_dir, "last_completed_chunk.txt")
    
#     # Process each chunk
#     for chunk_idx in range(start_chunk, num_chunks):
#         aggressive_memory_cleanup()
#         # Chunk indices
#         start_idx = chunk_idx * chunk_size
#         end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
#         chunk_indices = train_indices[start_idx:end_idx]
        
#         # Memory and performance logging
#         process = psutil.Process(os.getpid())
#         memory_before = process.memory_info().rss / (1024 * 1024)  # MB
#         logging.info(f"Chunk {chunk_idx+1}/{num_chunks}: Memory Before = {memory_before:.2f} MB")
        
#         # Load chunk data
#         chunk_X = np.array([X_mmap[idx] for idx in chunk_indices], dtype=np.float32)
#         chunk_y = np.array([y_mmap[idx] for idx in chunk_indices], dtype=np.float32)
        
#         # Checkpoint paths
#         checkpoint_path = os.path.join(
#             checkpoints_dir, 
#             f'model_checkpoint_chunk_{chunk_idx+1}.h5'
#         )
        
#         # Model checkpoint callback
#         model_checkpoint = tf.keras.callbacks.ModelCheckpoint(
#             checkpoint_path,
#             monitor='val_loss',
#             save_best_only=True,
#             save_weights_only=False
#         )
        
#         # Train on chunk
#         logging.info(f"Training chunk {chunk_idx+1}/{num_chunks}")
#         history = model.fit(
#             chunk_X, chunk_y,
#             validation_data=(val_X, val_y),
#             epochs=epochs_per_chunk,
#             batch_size=batch_size,
#             class_weight=class_weight,
#             callbacks=callbacks + [model_checkpoint],
#             verbose=1
#         )

#         # Store chunk metrics
#         chunk_metrics = {
#             'chunk': chunk_idx+1,
#             'metrics': {k: [float(v) for v in vals] for k, vals in history.history.items()}
#         }
#         training_history['chunks'].append(chunk_metrics)
        
#         # Comprehensive validation
#         val_predictions = model.predict(val_X)
#         val_binary_preds = (val_predictions > 0.5).astype(int)
        
#         val_metrics = {
#             'chunk': chunk_idx+1,
#             'val_auc': roc_auc_score(val_y, val_predictions),
#             'val_avg_precision': average_precision_score(val_y, val_predictions),
#             'val_classification_report': classification_report(val_y, val_binary_preds, output_dic...
#         }
#         training_history['validation_metrics'].append(val_metrics)

#         # Memory after training
#         memory_after = process.memory_info().rss / (1024 * 1024)
#         logging.info(f"Chunk {chunk_idx+1}: Memory After = {memory_after:.2f} MB")
#         logging.info(f"Memory Change: {memory_after - memory_before:.2f} MB")

#         if memory_after > memory_limit * 1024:  # Convert GB to MB
#             logging.warning(f"Memory limit exceeded at chunk {chunk_idx+1}. Stopping training.")
#             break
        
#         # Strategic checkpointing (less frequent)
#         if (chunk_idx + 1) % save_frequency == 0:
#             checkpoint_path = os.path.join(
#                 checkpoints_dir, 
#                 f'model_checkpoint_chunk_{chunk_idx+1}.h5'
#             )
#             model.save(checkpoint_path)
#             training_history['model_checkpoints'].append({
#                 'chunk': chunk_idx+1,
#                 'path': checkpoint_path,
#                 'validation_auc': val_metrics['val_auc']
#             })
        
#         # Save history periodically
#         if (chunk_idx + 1) % save_frequency == 0:
#             with open(os.path.join(output_dir, 'training_history.json'), 'w') as f:
#                 json.dump(training_history, f, indent=2)
            
#             # Update recovery file
#             with open(recovery_file, 'w') as f:
#                 f.write(str(chunk_idx + 1))

#         # Visualize validation performance
#         plt.figure(figsize=(15, 5))
        
#         # ROC Curve
#         plt.subplot(131)
#         from sklearn.metrics import roc_curve
#         fpr, tpr, _ = roc_curve(val_y, val_predictions)
#         plt.plot(fpr, tpr, label=f'AUC = {val_metrics["val_auc"]:.2f}')
#         plt.title('ROC Curve')
#         plt.xlabel('False Positive Rate')
#         plt.ylabel('True Positive Rate')
        
#         # Precision-Recall Curve
#         plt.subplot(132)
#         precision, recall, _ = precision_recall_curve(val_y, val_predictions)
#         plt.plot(recall, precision)
#         plt.title('Precision-Recall Curve')
#         plt.xlabel('Recall')
#         plt.ylabel('Precision')
        
#         # Confusion Matrix Heatmap
#         plt.subplot(133)
#         cm = confusion_matrix(val_y, val_binary_preds)
#         plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
#         plt.title('Confusion Matrix')
#         plt.colorbar()
        
#         plt.tight_layout()
#         plt.savefig(os.path.join(validation_dir, f'validation_chunk_{chunk_idx+1}.png'))
#         plt.close()
        
#         # Aggressive memory cleanup
#         del chunk_X, chunk_y, val_predictions, val_binary_preds
#         aggressive_memory_cleanup()
        
#         # Optional: Break if memory threshold exceeded
#         #if memory_after > total_memory * 0.8 * 1024:
#         #    logging.warning("Memory threshold exceeded. Stopping training.")
#         #    break
    
#     # Save final model
#     final_model_path = os.path.join(output_dir, 'final_model.h5')
#     model.save(final_model_path)
    
#     # Save final training history
#     with open(os.path.join(output_dir, 'final_training_history.json'), 'w') as f:
#         json.dump(training_history, f, indent=2)

#     # Save comprehensive training history
#     with open(os.path.join(output_dir, 'comprehensive_training_history.json'), 'w') as f:
#         json.dump(training_history, f, indent=2)
    
#     # Log total processing time
#     total_time = time.time() - start_time
#     logging.info(f"Total Processing Time: {total_time/3600:.2f} hours")
    
#     return model, final_model_path

# # def resumable_efficient_training(model, X_file, y_file, train_indices, val_indices, test_indices...
# #                                output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
# #                                save_frequency=5, class_weight=None, start_chunk=0):
# #     """
# #     Resumable training function with memory-efficient processing and advanced techniques.
    
# #     Parameters:
# #     -----------
# #     model : tf.keras.Model
# #         Pre-compiled model
# #     X_file, y_file : str
# #         Paths to feature and label files
# #     train_indices, val_indices, test_indices : array
# #         Training, validation, and test indices
# #     output_dir : str
# #         Directory to save results
# #     batch_size : int
# #         Batch size for training
# #     chunk_size : int
# #         Number of samples to process at once
# #     epochs_per_chunk : int
# #         Epochs to train each chunk
# #     save_frequency : int
# #         Save model every N chunks
# #     class_weight : dict, optional
# #         Class weights for handling imbalanced data
# #     start_chunk : int
# #         Chunk to resume training from
    
# #     Returns:
# #     --------
# #     tuple
# #         Trained model and final model path
# #     """
# #     import os
# #     import gc
# #     import json
# #     import numpy as np
# #     import tensorflow as tf
# #     import matplotlib.pyplot as plt
# #     from datetime import datetime, timedelta
# #     import time
# #     import psutil
# #     from sklearn.metrics import (
# #         classification_report, 
# #         confusion_matrix, 
# #         roc_curve, 
# #         auc
# #     )
    
# #     # Create output directories
# #     os.makedirs(output_dir, exist_ok=True)
# #     predictions_dir = os.path.join(output_dir, "predictions")
# #     os.makedirs(predictions_dir, exist_ok=True)
# #     checkpoints_dir = os.path.join(output_dir, "checkpoints")
# #     os.makedirs(checkpoints_dir, exist_ok=True)
    
# #     # Process in chunks
# #     num_chunks = int(np.ceil(len(train_indices) / chunk_size))
# #     print(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
    
# #     # Create validation set once (small size)
# #     val_limit = min(2000, len(val_indices))
# #     val_indices_subset = val_indices[:val_limit]
    
# #     # Open data files
# #     X_mmap = np.load(X_file, mmap_mode='r')
# #     y_mmap = np.load(y_file, mmap_mode='r')
    
# #     # Load validation data once
# #     val_X = np.array([X_mmap[idx] for idx in val_indices_subset])
# #     val_y = np.array([y_mmap[idx] for idx in val_indices_subset])
# #     print(f"Loaded {len(val_X)} validation samples")
    
# #     # Setup callbacks
# #     callbacks = [
# #         tf.keras.callbacks.EarlyStopping(
# #             patience=3, 
# #             restore_best_weights=True,
# #             monitor='val_loss',
# #             min_delta=0.01
# #         ),
# #         tf.keras.callbacks.ReduceLROnPlateau(
# #             factor=0.5,
# #             patience=2,
# #             min_lr=1e-6,
# #             monitor='val_loss'
# #         ),
# #         # Memory cleanup after each epoch
# #         tf.keras.callbacks.LambdaCallback(
# #             on_epoch_end=lambda epoch, logs: gc.collect()
# #         )
# #     ]
    
# #     # Track metrics across chunks
# #     history_log = []
# #     start_time = time.time()
    
# #     # Load existing history if resuming
# #     history_path = os.path.join(output_dir, "training_history.json")
# #     if start_chunk > 0 and os.path.exists(history_path):
# #         try:
# #             with open(history_path, "r") as f:
# #                 history_log = json.load(f)
# #         except Exception as e:
# #             print(f"Could not load existing history: {e}")
    
# #     # For safe recovery
# #     recovery_file = os.path.join(output_dir, "last_completed_chunk.txt")
    
# #     # Process each chunk
# #     for chunk_idx in range(start_chunk, num_chunks):
# #         # Get chunk indices
# #         start_idx = chunk_idx * chunk_size
# #         end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
# #         chunk_indices = train_indices[start_idx:end_idx]
        
# #         # Report memory
# #         memory_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
# #         print(f"\n{'='*50}")
# #         print(f"Processing chunk {chunk_idx+1}/{num_chunks} with {len(chunk_indices)} samples")
# #         print(f"Memory before: {memory_before:.1f} MB")
        
# #         # Force garbage collection before loading new data
# #         gc.collect()
        
# #         # Load chunk data
# #         chunk_X = np.array([X_mmap[idx] for idx in chunk_indices])
# #         chunk_y = np.array([y_mmap[idx] for idx in chunk_indices])
        
# #         print(f"Data loaded. Memory: {psutil.Process(os.getpid()).memory_info().rss / (1024 * 10...
        
# #         # Train on chunk
# #         print(f"Training for {epochs_per_chunk} epochs...")
# #         history = model.fit(
# #             chunk_X, chunk_y,
# #             validation_data=(val_X, val_y),
# #             epochs=epochs_per_chunk,
# #             batch_size=batch_size,
# #             class_weight=class_weight,
# #             callbacks=callbacks,
# #             verbose=1
# #         )
        
# #         # Store serializable metrics
# #         chunk_metrics = {}
# #         for k, v in history.history.items():
# #             chunk_metrics[k] = [float(val) for val in v]
# #         history_log.append(chunk_metrics)
        
# #         # Save model periodically
# #         if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
# #             model_path = os.path.join(checkpoints_dir, f"model_checkpoint_{chunk_idx+1}.h5")
# #             model.save(model_path)
# #             print(f"Model saved to {model_path}")
            
# #             # Save history
# #             try:
# #                 with open(history_path, "w") as f:
# #                     json.dump(history_log, f)
# #             except Exception as e:
# #                 print(f"Warning: Could not save history: {e}")
        
# #         # Generate predictions 
# #         if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
# #             chunk_preds = model.predict(chunk_X, batch_size=batch_size)
# #             np.save(os.path.join(predictions_dir, f"chunk_{chunk_idx+1}_predictions.npy"), chunk...
# #             np.save(os.path.join(predictions_dir, f"chunk_{chunk_idx+1}_indices.npy"), chunk_ind...
        
# #         # Write recovery file with last completed chunk
# #         with open(recovery_file, "w") as f:
# #             f.write(str(chunk_idx + 1))
        
# #         # Clean up
# #         del chunk_X, chunk_y
# #         gc.collect()
    
# #     # Save final model
# #     final_model_path = os.path.join(output_dir, "final_model.h5")
# #     model.save(final_model_path)
    
# #     # Final evaluation on test set
# #     print("\nPerforming final evaluation on test set...")
    
# #     # Process test data in batches
# #     test_batch_size = 5000
# #     num_test_batches = int(np.ceil(len(test_indices) / test_batch_size))
    
# #     all_test_predictions = []
# #     all_test_true = []
# #     test_metrics = {'loss': 0, 'accuracy': 0, 'samples': 0}
    
# #     for test_batch_idx in range(num_test_batches):
# #         start_idx = test_batch_idx * test_batch_size
# #         end_idx = min((test_batch_idx + 1) * test_batch_size, len(test_indices))
# #         batch_indices = test_indices[start_idx:end_idx]
        
# #         # Load batch data
# #         test_X = np.array([X_mmap[idx] for idx in batch_indices])
# #         test_y = np.array([y_mmap[idx] for idx in batch_indices])
        
# #         # Evaluate
# #         metrics = model.evaluate(test_X, test_y, verbose=1)
# #         metrics_dict = {name: value for name, value in zip(model.metrics_names, metrics)}
        
# #         # Weight metrics by batch size
# #         for key in ['loss', 'accuracy']:
# #             if key in metrics_dict:
# #                 test_metrics[key] += metrics_dict[key] * len(batch_indices)
# #         test_metrics['samples'] += len(batch_indices)
        
# #         # Get predictions
# #         test_preds = model.predict(test_X, batch_size=batch_size)
        
# #         # Store
# #         all_test_predictions.append(test_preds.flatten())
# #         all_test_true.append(test_y)
        
# #         # Clean up
# #         del test_X, test_y, test_preds
# #         gc.collect()
    
# #     # Combine results
# #     all_test_predictions = np.concatenate(all_test_predictions)
# #     all_test_true = np.concatenate(all_test_true)
    
# #     # Calculate final test metrics
# #     test_loss = test_metrics['loss'] / test_metrics['samples']
# #     test_accuracy = test_metrics['accuracy'] / test_metrics['samples']
    
# #     # Calculate additional metrics
# #     test_preds_binary = (all_test_predictions > 0.5).astype(int)
# #     report = classification_report(all_test_true, test_preds_binary)
# #     conf_matrix = confusion_matrix(all_test_true, test_preds_binary)
# #     fpr, tpr, _ = roc_curve(all_test_true, all_test_predictions)
# #     roc_auc = auc(fpr, tpr)
    
# #     # Save results
# #     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
# #     np.save(os.path.join(output_dir, f'test_predictions_{timestamp}.npy'), all_test_predictions)
# #     np.save(os.path.join(output_dir, 'test_predictions_latest.npy'), all_test_predictions)
    
# #     with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w') as f:
# #         f.write("Classification Report:\n")
# #         f.write(report)
# #         f.write("\n\nConfusion Matrix:\n")
# #         f.write(str(conf_matrix))
# #         f.write("\n\nTest Metrics:\n")
# #         f.write(f"loss: {test_loss:.4f}\n")
# #         f.write(f"accuracy: {test_accuracy:.4f}\n")
# #         f.write(f"AUC: {roc_auc:.4f}\n")
    
# #     return model, final_model_path

# def plot_training_history(output_dir):
#     """
#     Visualize learning curves across all training chunks
    
#     Parameters:
#     -----------
#     output_dir : str
#         Directory containing training history files
    
#     Returns:
#     --------
#     dict
#         Aggregated metrics across training
#     """
#     import matplotlib.pyplot as plt
#     import numpy as np
#     import json
#     import os
    
#     # Load training history
#     history_path = os.path.join(output_dir, "training_history.json")
#     if not os.path.exists(history_path):
#         history_path = os.path.join(output_dir, "final_training_metrics.json")
    
#     try:
#         with open(history_path, "r") as f:
#             history_log = json.load(f)
#     except:
#         # Try pickle format
#         import pickle
#         with open(os.path.join(output_dir, "training_history.pkl"), "rb") as f:
#             history_log = pickle.load(f)
    
#     # Extract metrics across all chunks
#     metrics = ['loss', 'accuracy', 'auc', 'precision', 'recall']
#     val_metrics = [f'val_{m}' for m in metrics]
    
#     # Prepare aggregated metrics
#     all_metrics = {m: [] for m in metrics + val_metrics}
    
#     # Collect metrics across chunks
#     for chunk_history in history_log:
#         for metric in metrics:
#             # Training metrics
#             if metric in chunk_history:
#                 all_metrics[metric].extend(chunk_history[metric])
            
#             # Validation metrics
#             val_metric = f'val_{metric}'
#             if val_metric in chunk_history:
#                 all_metrics[val_metric].extend(chunk_history[val_metric])
    
#     # Create learning curve visualizations
#     plt.figure(figsize=(18, 12))
    
#     # Plot loss
#     plt.subplot(2, 2, 1)
#     plt.plot(all_metrics['loss'], label='Training Loss')
#     plt.plot(all_metrics['val_loss'], label='Validation Loss')
#     plt.title('Loss Over Time')
#     plt.xlabel('Epoch')
#     plt.ylabel('Loss')
#     plt.legend()
#     plt.grid(True, alpha=0.3)
    
#     # Plot accuracy
#     plt.subplot(2, 2, 2)
#     plt.plot(all_metrics['accuracy'], label='Training Accuracy')
#     plt.plot(all_metrics['val_accuracy'], label='Validation Accuracy')
#     plt.title('Accuracy Over Time')
#     plt.xlabel('Epoch')
#     plt.ylabel('Accuracy')
#     plt.legend()
#     plt.grid(True, alpha=0.3)
    
#     # Plot AUC
#     plt.subplot(2, 2, 3)
#     plt.plot(all_metrics['auc'], label='Training AUC')
#     plt.plot(all_metrics['val_auc'], label='Validation AUC')
#     plt.title('AUC Over Time')
#     plt.xlabel('Epoch')
#     plt.ylabel('AUC')
#     plt.legend()
#     plt.grid(True, alpha=0.3)
    
#     # Plot precision-recall
#     plt.subplot(2, 2, 4)
#     plt.plot(all_metrics['precision'], label='Training Precision')
#     plt.plot(all_metrics['recall'], label='Training Recall')
#     plt.plot(all_metrics['val_precision'], label='Validation Precision')
#     plt.plot(all_metrics['val_recall'], label='Validation Recall')
#     plt.title('Precision-Recall Over Time')
#     plt.xlabel('Epoch')
#     plt.ylabel('Score')
#     plt.legend()
#     plt.grid(True, alpha=0.3)
    
#     plt.tight_layout()
#     plt.savefig(os.path.join(output_dir, 'learning_curves.png'), dpi=300)
#     plt.close()
    
#     return all_metrics

# def ensemble_model_predictions(model_directory, X_test, y_test):
#     """
#     Create an ensemble of models from different checkpoints.
    
#     Parameters:
#     -----------
#     model_directory : str
#         Directory containing model checkpoints
#     X_test : numpy.ndarray
#         Test features
#     y_test : numpy.ndarray
#         Test labels
    
#     Returns:
#     --------
#     dict
#         Ensemble prediction results
#     """
#     import os
#     import numpy as np
#     import tensorflow as tf
#     from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
    
#     # Find all model checkpoints
#     checkpoint_files = [
#         f for f in os.listdir(model_directory) 
#         if f.startswith('model_checkpoint_') and f.endswith('.h5')
#     ]
    
#     # Collect predictions from each model
#     all_predictions = []
#     model_performances = []
    
#     for checkpoint_file in checkpoint_files:
#         # Load model
#         model_path = os.path.join(model_directory, checkpoint_file)
#         model = tf.keras.models.load_model(model_path)
        
#         # Make predictions
#         predictions = model.predict(X_test)
        
#         # Evaluate model performance
#         binary_preds = (predictions > 0.5).astype(int)
#         auc = roc_auc_score(y_test, predictions)
        
#         model_performances.append({
#             'filename': checkpoint_file,
#             'auc': auc
#         })
        
#         all_predictions.append(predictions)
        
#         # Clear model from memory
#         tf.keras.backend.clear_session()
    
#     # Ensemble prediction (weighted average)
#     # Sort models by performance
#     model_performances.sort(key=lambda x: x['auc'], reverse=True)
    
#     # Use top models for ensemble
#     top_n = min(5, len(all_predictions))
#     top_predictions = all_predictions[:top_n]
#     top_weights = [perf['auc'] for perf in model_performances[:top_n]]
    
#     # Normalize weights
#     weights_sum = sum(top_weights)
#     normalized_weights = [w / weights_sum for w in top_weights]
    
#     # Weighted ensemble prediction
#     ensemble_predictions = np.average(top_predictions, axis=0, weights=normalized_weights)
    
#     # Evaluate ensemble
#     ensemble_binary_preds = (ensemble_predictions > 0.5).astype(int)
#     ensemble_report = classification_report(y_test, ensemble_binary_preds)
#     ensemble_confusion = confusion_matrix(y_test, ensemble_binary_preds)
#     ensemble_auc = roc_auc_score(y_test, ensemble_predictions)
    
#     # Save results
#     results = {
#         'ensemble_predictions': ensemble_predictions,
#         'ensemble_binary_predictions': ensemble_binary_preds,
#         'classification_report': ensemble_report,
#         'confusion_matrix': ensemble_confusion,
#         'ensemble_auc': ensemble_auc,
#         'model_performances': model_performances
#     }
    
#     return results

# def cross_validation_feature_importance(X, y, feature_names=None):
#     """
#     Perform cross-validation to assess feature importance robustly.
    
#     Parameters:
#     -----------
#     X : numpy.ndarray
#         Input features
#     y : numpy.ndarray
#         Target labels
#     feature_names : list, optional
#         Names of features for more interpretable output
    
#     Returns:
#     --------
#     dict
#         Feature importance results
#     """
#     from sklearn.model_selection import StratifiedKFold
#     from sklearn.inspection import permutation_importance
#     import numpy as np
#     import matplotlib.pyplot as plt
    
#     # Use stratified k-fold
#     cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
#     # Prepare storage for feature importances
#     feature_importances = []
    
#     # Perform cross-validation
#     for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):
#         # Split data
#         X_train, X_val = X[train_idx], X[val_idx]
#         y_train, y_val = y[train_idx], y[val_idx]
        
#         # Build and train model
#         model = build_advanced_zero_curtain_model((X_train.shape[1], X_train.shape[2]))
#         model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, verbose=0)
        
#         # Compute permutation importance
#         perm_importance = permutation_importance(
#             model, X_val, y_val, 
#             n_repeats=10, 
#             random_state=42
#         )
        
#         feature_importances.append(perm_importance.importances_mean)
        
#         # Clear model from memory
#         del model
    
#     # Aggregate feature importances across folds
#     avg_feature_importance = np.mean(feature_importances, axis=0)
#     std_feature_importance = np.std(feature_importances, axis=0)
    
#     # Visualization
#     plt.figure(figsize=(12, 6))
    
#     # Use feature names if provided, otherwise use indices
#     if feature_names is None:
#         feature_names = [f'Feature {i}' for i in range(len(avg_feature_importance))]
    
#     # Sort features by importance
#     sorted_idx = np.argsort(avg_feature_importance)
#     plt.barh(
#         [feature_names[i] for i in sorted_idx], 
#         avg_feature_importance[sorted_idx],
#         xerr=std_feature_importance[sorted_idx],
#         capsize=5
#     )
#     plt.title('Cross-Validated Feature Importance')
#     plt.xlabel('Permutation Importance')
#     plt.tight_layout()
#     plt.savefig('feature_importance_cv.png')
#     plt.close()
    
#     # Prepare results
#     results = {
#         'avg_importance': avg_feature_importance,
#         'std_importance': std_feature_importance,
#         'feature_names': feature_names,
#         'sorted_features': [feature_names[i] for i in sorted_idx]
#     }
    
#     return results

# import traceback

# def main_zero_curtain_analysis(
#     feather_path, 
#     output_base_dir='results', 
#     batch_size=50, 
#     start_chunk=0
# ):
#     """
#     Comprehensive zero curtain analysis pipeline with extensive error tracking.
#     """
#     import os
#     import gc
#     import numpy as np
#     import tensorflow as tf
#     import pickle
#     import pyarrow.feather as pf
#     import pandas as pd
#     import traceback
#     import sys
    
#     # Detailed error logging function
#     def log_error(error_message):
#         print("=" * 80)
#         print("ERROR DETAILS:")
#         print(error_message)
#         print("=" * 80)
#         traceback.print_exc()

#     logging.basicConfig(
#         filename=os.path.join(output_base_dir, 'training_log.txt'),
#         level=logging.INFO,
#         format='%(asctime)s - %(levelname)s: %(message)s'
#     )
    
#     try:
#         # Create base output directory
#         os.makedirs(output_base_dir, exist_ok=True)
        
#         # Configure TensorFlow
#         configure_tensorflow_memory()
        
#         # Print input file details for debugging
#         print(f"Feather File Path: {feather_path}")
#         print(f"Output Base Directory: {output_base_dir}")
        
#         # First, verify the Feather file
#         try:
#             print("Attempting to read Feather file...")
#             df = pf.read_table(feather_path).to_pandas()
#             print(f"Feather file loaded successfully. Shape: {df.shape}")
#             print("Columns:", list(df.columns))
            
#             # Check required columns
#             required_columns = ['datetime', 'source', 'soil_temp_standardized', 'soil_temp_depth']
#             missing_columns = [col for col in required_columns if col not in df.columns]
#             if missing_columns:
#                 raise ValueError(f"Missing required columns: {missing_columns}")
        
#         except Exception as file_error:
#             log_error(f"Error reading Feather file: {file_error}")
#             raise
        
#         # Prepare data directory
#         data_dir = os.path.join(output_base_dir, 'ml_data')
#         os.makedirs(data_dir, exist_ok=True)
        
#         # Data preparation steps
#         X_file = os.path.join(data_dir, 'X_features.npy')
#         y_file = os.path.join(data_dir, 'y_labels.npy')
        
#         # Check if prepared data exists
#         if not (os.path.exists(X_file) and os.path.exists(y_file)):
#             print("Prepared data not found. Running data preparation...")
#             # You'll need to implement or import the data preparation function
#             # This might be a function like prepare_data_for_deep_learning_efficiently()
#             # from your previous code
#             raise NotImplementedError("Data preparation function needs to be implemented")
        
#         # Load split indices and weights
#         split_indices_path = '/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling/che...
#         try:
#             with open(split_indices_path, 'rb') as f:
#                 split_data = pickle.load(f)
            
#             train_indices = split_data["train_indices"]
#             val_indices = split_data["val_indices"]
#             test_indices = split_data["test_indices"]
#         except Exception as split_error:
#             log_error(f"Error loading split indices: {split_error}")
#             raise
        
#         # Load memory-mapped data
#         X = np.load(X_file, mmap_mode='r')
#         y = np.load(y_file, mmap_mode='r')
        
#         # Load metadata
#         try:
#             with open(os.path.join(data_dir, 'metadata.pkl'), 'rb') as f:
#                 metadata = pickle.load(f)
#         except Exception as metadata_error:
#             log_error(f"Error loading metadata: {metadata_error}")
#             raise
        
#         # Prepare class weights
#         train_y = y[train_indices]
#         pos_weight = (len(train_y) - np.sum(train_y)) / max(1, np.sum(train_y))
#         class_weight = {0: 1.0, 1: pos_weight}
        
#         # Build model
#         input_shape = X[train_indices[0]].shape
#         print(f"Input shape: {input_shape}")
#         model = build_advanced_zero_curtain_model(input_shape)
        
#         # Output directory for this run
#         run_output_dir = os.path.join(output_base_dir, 'efficient_model')
#         os.makedirs(run_output_dir, exist_ok=True)
        
#         # Train model with verbose error handling
#         try:
#             start_time = time.time()
#             start_chunk = 0
#             checkpoint_path = os.path.join(output_base_dir, 'last_completed_chunk.txt')
#             if os.path.exists(checkpoint_path):
#                 with open(checkpoint_path, 'r') as f:
#                     start_chunk = int(f.read().strip())
            
#             trained_model, final_model_path = optimized_resumable_efficient_training(
#                 model, 
#                 X_file, 
#                 y_file,
#                 train_indices, 
#                 val_indices, 
#                 test_indices,
#                 output_dir=run_output_dir,
#                 batch_size=32,  # Reduced batch size
#                 chunk_size=10000,  # Smaller chunks
#                 epochs_per_chunk=1,  # Fewer epochs per chunk
#                 save_frequency=3,
#                 class_weight=class_weight,
#                 start_chunk=start_chunk
#             )
        
#             total_time = time.time() - start_time
#             logging.info(f"Total Processing Time: {total_time/3600:.2f} hours")
            
#             # Save last completed chunk
#             num_chunks = int(np.ceil(len(train_indices) / 10000))  # Use consistent chunk size
#             with open(checkpoint_path, 'w') as f:
#                 f.write(str(num_chunks))
            
#         except Exception as e:
#             log_error(f"Error during model training: {e}")
#             logging.error(f"Critical Error: {e}")
#             logging.error(traceback.format_exc())
#             raise
        
#         # Rest of the analysis remains the same...
#         learning_metrics = plot_training_history(run_output_dir)
        
#         feature_importance = cross_validation_feature_importance(
#             X[train_indices], 
#             y[train_indices]
#         )
        
#         # Ensemble predictions
#         test_X = np.array([X[idx] for idx in test_indices])
#         test_y = np.array([y[idx] for idx in test_indices])
        
#         ensemble_results = ensemble_model_predictions(
#             os.path.join(run_output_dir, 'checkpoints'), 
#             test_X, 
#             test_y
#         )
        
#         # Combine and return results
#         results = {
#             'model_path': final_model_path,
#             'learning_metrics': learning_metrics,
#             'feature_importance': feature_importance,
#             'ensemble_results': ensemble_results
#         }
        
#         return results
    
#     except Exception as e:
#         log_error(f"Unhandled error in main_zero_curtain_analysis: {e}")
#         raise

# # Add a main block with additional error handling
# if __name__ == "__main__":
#     try:
#         # Example parameters
#         feather_path = '/Users/bgay/Desktop/Research/Code/merged_compressed.feather'
#         output_base_dir = '/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling'
        
#         # Detailed exception handling
#         try:
#             # Run full analysis
#             results = main_zero_curtain_analysis(
#                 feather_path, 
#                 output_base_dir=output_base_dir, 
#                 batch_size=50, 
#                 start_chunk=0
#             )
            
#             # You can now access various results
#             print("Final model saved at:", results['model_path'])
#             print("Ensemble AUC:", results['ensemble_results']['ensemble_auc'])
        
#         except Exception as analysis_error:
#             print("=" * 80)
#             print("ANALYSIS FAILED:")
#             print(analysis_error)
#             traceback.print_exc()
#             print("=" * 80)
    
#     except Exception as e:
#         print(f"Unexpected error: {e}")
#         traceback.print_exc()

import os
import gc
import json
import logging
import numpy as np
import tensorflow as tf
import psutil
import time
import traceback
from datetime import datetime
from sklearn.metrics import (
    classification_report, 
    confusion_matrix, 
    roc_curve, 
    auc,
    roc_auc_score,
    precision_recall_curve, 
    average_precision_score
)
import matplotlib.pyplot as plt

import os
import gc
import json
import logging
import numpy as np
import tensorflow as tf
import psutil
import time
import traceback
from datetime import datetime
from sklearn.metrics import (
    classification_report, 
    confusion_matrix, 
    roc_curve, 
    auc,
    roc_auc_score,
    precision_recall_curve, 
    average_precision_score
)
import matplotlib.pyplot as plt

def configure_tensorflow_memory():
    """Configure TensorFlow memory usage safely"""
    # Clear any existing session
    tf.keras.backend.clear_session()
    
    # Force garbage collection
    gc.collect()
    
    # Configure GPU memory usage - CHOOSE ONLY ONE METHOD:
    physical_devices = tf.config.list_physical_devices('GPU')
    
    if physical_devices:
        print(f"Found {len(physical_devices)} GPU devices")
        
        try:
            # OPTION 1: Dynamic memory growth (comment if using Option 2)
            for device in physical_devices:
                tf.config.experimental.set_memory_growth(device, True)
                print(f"Enabled memory growth for {device}")
                
            # OPTION 2: Fixed memory limit (uncomment if needed instead of Option 1)
            # GPU_MEMORY_LIMIT_MB = 4096  # 4GB limit
            # for device in physical_devices:
            #     tf.config.set_logical_device_configuration(
            #         device, [tf.config.LogicalDeviceConfiguration(memory_limit=GPU_MEMORY_LIMIT_MB)]
            #     )
            #     print(f"Set memory limit of {GPU_MEMORY_LIMIT_MB}MB for {device}")
                
        except Exception as e:
            print(f"Error configuring GPU: {e}")
            print("Falling back to CPU")
    else:
        print("No GPU devices found. Using CPU only.")
    
    # Limit CPU threads
    tf.config.threading.set_intra_op_parallelism_threads(4)
    tf.config.threading.set_inter_op_parallelism_threads(2)
    
    # Set soft device placement
    tf.config.set_soft_device_placement(True)
    
    # Reset backend again to ensure configurations are applied
    tf.keras.backend.clear_session()
    gc.collect()
    
    # Return information about TensorFlow configuration
    return {
        "physical_devices": len(physical_devices),
        "version": tf.__version__,
        "eager_execution": tf.executing_eagerly(),
        "device_policy": tf.config.get_soft_device_placement()
    }

def aggressive_memory_cleanup():
    """Ultra-aggressive memory cleanup to reclaim memory"""
    # Clear Keras backend
    tf.keras.backend.clear_session()
    
    # Garbage collection with multiple passes
    for _ in range(3):
        gc.collect()
    
    # Optional: Release memory back to system on Linux
    try:
        import ctypes
        libc = ctypes.CDLL('libc.so.6')
        libc.malloc_trim(0)
    except:
        pass


def memory_usage():
    """Monitor memory usage in MB"""
    process = psutil.Process(os.getpid())
    mem_info = process.memory_info()
    return mem_info.rss / 1024**2  # Memory in MB


def get_memory_summary():
    """Get detailed memory usage summary"""
    try:
        # Process memory
        process = psutil.Process(os.getpid())
        mem_info = process.memory_info()
        
        # System memory
        system = psutil.virtual_memory()
        
        # GPU memory if available
        gpu_memory = "N/A"
        try:
            import subprocess
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'], 
                stdout=subprocess.PIPE
            )
            gpu_memory = result.stdout.decode('utf-8').strip()
        except:
            pass
        
        return {
            'process_rss_mb': mem_info.rss / (1024**2),
            'process_vms_mb': mem_info.vms / (1024**2),
            'system_available_mb': system.available / (1024**2),
            'system_percent': system.percent,
            'gpu_memory': gpu_memory
        }
    except:
        return {'error': 'Could not get memory summary'}


def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
    """
    Advanced zero curtain detection model with optimized memory footprint.
    
    Parameters:
    -----------
    input_shape : tuple
        Shape of input data (sequence_length, num_features)
    include_moisture : bool
        Whether soil moisture features are included
    """
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
    from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D
    from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.regularizers import l2
    
    # Regularization parameters - increased for better generalization and reduced memory
    l2_lambda = 1e-4
    dropout_rate = 0.35
    
    # Input layer
    inputs = Input(shape=input_shape)
    
    # Reshape for ConvLSTM (add spatial dimension)
    x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
    # ConvLSTM layer with simplified architecture
    from tensorflow.keras.layers import ConvLSTM2D
    convlstm = ConvLSTM2D(
        filters=32,  # Reduced from 48 to save memory
        kernel_size=(3, 1),
        padding='same',
        return_sequences=True,
        activation='tanh',
        recurrent_dropout=dropout_rate,
        kernel_regularizer=l2(l2_lambda)
    )(x)
    
    # Reshape back to (sequence_length, features)
    convlstm = Reshape((input_shape[0], 32))(convlstm)
    
    # Simplified positional encoding
    def positional_encoding(length, depth):
        positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
        angle_rates = 1 / tf.pow(10000.0, depths)
        angle_rads = positions * angle_rates
        
        return tf.sin(angle_rads)
    
    # Add positional encoding
    pos_encoding = positional_encoding(input_shape[0], 32)
    transformer_input = convlstm + pos_encoding
    
    # Simplified transformer encoder block
    def transformer_encoder(x, num_heads=4, key_dim=32, ff_dim=64):  # Reduced from 6 heads, 48 key_dim, 96 ff_dim
        # Multi-head attention with regularization
        attention_output = MultiHeadAttention(
            num_heads=num_heads, 
            key_dim=key_dim,
            kernel_regularizer=l2(l2_lambda)
        )(x, x)
        
        # Skip connection with dropout and regularization
        x1 = Add()([attention_output, x])
        x1 = Dropout(dropout_rate)(x1)
        x1 = LayerNormalization(epsilon=1e-6)(x1)
        
        # Simplified feed-forward network
        ff_output = Dense(
            ff_dim, 
            activation='relu', 
            kernel_regularizer=l2(l2_lambda)
        )(x1)
        ff_output = Dropout(dropout_rate)(ff_output)
        ff_output = Dense(
            32,  # Reduced from 48
            kernel_regularizer=l2(l2_lambda)
        )(ff_output)
        
        # Skip connection
        x2 = Add()([ff_output, x1])
        return LayerNormalization(epsilon=1e-6)(x2)
    
    # Apply transformer encoder
    transformer_output = transformer_encoder(transformer_input)
    
    # Simplified CNN paths with regularization - reduced number of paths
    cnn_paths = []
    for kernel_size in [3, 7]:  # Removed the middle kernel size (5)
        cnn = Conv1D(
            filters=24,  # Reduced from 32
            kernel_size=kernel_size, 
            padding='same', 
            activation='relu',
            kernel_regularizer=l2(l2_lambda)
        )(inputs)
        cnn = BatchNormalization()(cnn)
        cnn = Dropout(dropout_rate)(cnn)
        cnn_paths.append(GlobalMaxPooling1D()(cnn))
    
    # Variational Autoencoder components with adjustable weight
    def sampling(args):
        z_mean, z_log_var = args
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
    # Global temporal features
    global_max = GlobalMaxPooling1D()(transformer_output)
    global_avg = GlobalAveragePooling1D()(transformer_output)
    
    # VAE encoding with regularization
    z_mean = Dense(
        24,  # Reduced from 32
        kernel_regularizer=l2(l2_lambda)
    )(Concatenate()([global_max, global_avg]))
    z_log_var = Dense(
        24,  # Reduced from 32
        kernel_regularizer=l2(l2_lambda)
    )(Concatenate()([global_max, global_avg]))
    z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
    # Combine features
    merged_features = Concatenate()(
        cnn_paths + [global_max, global_avg, z]
    )
    
    # Simplified classification layers with dropout and regularization
    x = Dense(
        64,  # Reduced from 96
        activation='relu', 
        kernel_regularizer=l2(l2_lambda)
    )(merged_features)
    x = Dropout(dropout_rate)(x)
    x = BatchNormalization()(x)
    
    x = Dense(
        32,  # Reduced from 48
        activation='relu', 
        kernel_regularizer=l2(l2_lambda)
    )(x)
    x = Dropout(0.25)(x)
    
    # Output layer
    outputs = Dense(1, activation='sigmoid')(x)
    
    # Create model
    model = Model(inputs=inputs, outputs=outputs)
    
    # Adjustable VAE loss weight
    vae_weight = 0.001
    kl_loss = -0.5 * tf.reduce_mean(
        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
    )
    model.add_loss(vae_weight * kl_loss)
    
    # Compile with gradient clipping
    optimizer = Adam(
        learning_rate=0.001, 
        clipnorm=1.0
    )
    
    model.compile(
        optimizer=optimizer,
        loss='binary_crossentropy',
        metrics=[
            'accuracy',
            tf.keras.metrics.AUC(name='auc'),
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall')
        ]
    )
    
    return model


def create_optimized_tf_dataset(X_file, y_file, indices, batch_size=256, shuffle=True, 
                               weights=None, cache=False, prefetch_factor=2):
    """
    Create an optimized TensorFlow dataset with memory-efficient loading.
    
    Parameters:
    -----------
    X_file : str
        Path to features numpy file
    y_file : str
        Path to labels numpy file
    indices : array-like
        Indices to use for dataset
    batch_size : int
        Batch size for training
    shuffle : bool
        Whether to shuffle the data
    weights : array-like, optional
        Sample weights
    cache : bool
        Whether to cache the dataset
    prefetch_factor : int
        Prefetch buffer size
    
    Returns:
    --------
    tf.data.Dataset
        Optimized TensorFlow dataset
    """
    import numpy as np
    import tensorflow as tf
    import time
    
    # Memory-efficient generator function that loads data on-demand
    def data_generator():
        X_mmap = np.load(X_file, mmap_mode='r')
        y_mmap = np.load(y_file, mmap_mode='r')
        
        # Use smaller subsets of indices to reduce memory pressure
        chunk_size = 1000
        num_chunks = (len(indices) + chunk_size - 1) // chunk_size
        
        for chunk_idx in range(num_chunks):
            start_idx = chunk_idx * chunk_size
            end_idx = min((chunk_idx + 1) * chunk_size, len(indices))
            chunk_indices = indices[start_idx:end_idx]
            
            # Load in smaller batches
            mini_batch_size = 100
            for i in range(0, len(chunk_indices), mini_batch_size):
                batch_indices = chunk_indices[i:i + mini_batch_size]
                
                # Extract features and labels for this mini-batch
                batch_X = np.array([X_mmap[idx] for idx in batch_indices], dtype=np.float32)
                batch_y = np.array([y_mmap[idx] for idx in batch_indices], dtype=np.float32)
                
                # Yield one sample at a time
                for j in range(len(batch_indices)):
                    if weights is not None:
                        yield batch_X[j], batch_y[j], weights[batch_indices[j]]
                    else:
                        yield batch_X[j], batch_y[j]
                
                # Clear references to mini-batch data
                del batch_X, batch_y
                gc.collect()
    
    # Get input shape from first sample
    X_mmap = np.load(X_file, mmap_mode='r')
    input_shape = X_mmap[indices[0]].shape
    del X_mmap  # Release reference
    
    # Create dataset from generator
    if weights is not None:
        output_types = (tf.float32, tf.float32, tf.float32)
        output_shapes = (input_shape, (), ())
    else:
        output_types = (tf.float32, tf.float32)
        output_shapes = (input_shape, ())
    
    dataset = tf.data.Dataset.from_generator(
        data_generator,
        output_types=output_types,
        output_shapes=output_shapes
    )
    
    # Apply dataset optimizations
    if shuffle:
        buffer_size = min(5000, len(indices))  # Reduced buffer size to save memory
        dataset = dataset.shuffle(buffer_size)
    
    dataset = dataset.batch(batch_size)
    
    # Limit prefetch to save memory
    dataset = dataset.prefetch(prefetch_factor)
    
    return dataset


import os
import gc
import json
import logging
import numpy as np
import tensorflow as tf
import psutil
import time
import traceback
from datetime import datetime
from sklearn.metrics import (
    classification_report, 
    confusion_matrix, 
    roc_curve, 
    auc,
    roc_auc_score,
    precision_recall_curve, 
    average_precision_score
)
import matplotlib.pyplot as plt

# Configure TensorFlow memory growth
def configure_tensorflow_memory():
    """Configure TensorFlow to use memory growth and limit GPU memory allocation"""
    # Clear any existing session
    tf.keras.backend.clear_session()
    
    # Configure GPU memory growth
    physical_devices = tf.config.list_physical_devices('GPU')
    if physical_devices:
        for device in physical_devices:
            try:
                # Allow memory growth - prevents TF from allocating all GPU memory at once
                tf.config.experimental.set_memory_growth(device, True)
                # Optionally limit GPU memory for stability
                tf.config.set_logical_device_configuration(
                    device,
                    [tf.config.LogicalDeviceConfiguration(memory_limit=4096)]  # Limit to 4GB
                )
                print(f"Memory growth enabled for {device}")
            except Exception as e:
                print(f"Error configuring GPU: {e}")
    
    # Limit CPU threads
    tf.config.threading.set_intra_op_parallelism_threads(4)
    tf.config.threading.set_inter_op_parallelism_threads(2)
    
    # Set soft device placement
    tf.config.set_soft_device_placement(True)
    
    # Set memory cleanup for malloc
    os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'
    
    # Run garbage collection
    gc.collect()


def aggressive_memory_cleanup():
    """Ultra-aggressive memory cleanup to reclaim memory"""
    # Clear Keras backend
    tf.keras.backend.clear_session()
    
    # Garbage collection with multiple passes
    for _ in range(3):
        gc.collect()
    
    # Optional: Release memory back to system on Linux
    try:
        import ctypes
        libc = ctypes.CDLL('libc.so.6')
        libc.malloc_trim(0)
    except:
        pass


def memory_usage():
    """Monitor memory usage in MB"""
    process = psutil.Process(os.getpid())
    mem_info = process.memory_info()
    return mem_info.rss / 1024**2  # Memory in MB


def get_memory_summary():
    """Get detailed memory usage summary"""
    try:
        # Process memory
        process = psutil.Process(os.getpid())
        mem_info = process.memory_info()
        
        # System memory
        system = psutil.virtual_memory()
        
        # GPU memory if available
        gpu_memory = "N/A"
        try:
            import subprocess
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'], 
                stdout=subprocess.PIPE
            )
            gpu_memory = result.stdout.decode('utf-8').strip()
        except:
            pass
        
        return {
            'process_rss_mb': mem_info.rss / (1024**2),
            'process_vms_mb': mem_info.vms / (1024**2),
            'system_available_mb': system.available / (1024**2),
            'system_percent': system.percent,
            'gpu_memory': gpu_memory
        }
    except:
        return {'error': 'Could not get memory summary'}


def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
    """
    Advanced zero curtain detection model with optimized memory footprint.
    
    Parameters:
    -----------
    input_shape : tuple
        Shape of input data (sequence_length, num_features)
    include_moisture : bool
        Whether soil moisture features are included
    """
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
    from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D
    from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.regularizers import l2
    
    # Regularization parameters - increased for better generalization and reduced memory
    l2_lambda = 1e-4
    dropout_rate = 0.35
    
    # Input layer
    inputs = Input(shape=input_shape)
    
    # Reshape for ConvLSTM (add spatial dimension)
    x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
    # ConvLSTM layer with simplified architecture
    from tensorflow.keras.layers import ConvLSTM2D
    convlstm = ConvLSTM2D(
        filters=32,  # Reduced from 48 to save memory
        kernel_size=(3, 1),
        padding='same',
        return_sequences=True,
        activation='tanh',
        recurrent_dropout=dropout_rate,
        kernel_regularizer=l2(l2_lambda)
    )(x)
    
    # Reshape back to (sequence_length, features)
    convlstm = Reshape((input_shape[0], 32))(convlstm)
    
    # Simplified positional encoding
    def positional_encoding(length, depth):
        positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
        angle_rates = 1 / tf.pow(10000.0, depths)
        angle_rads = positions * angle_rates
        
        return tf.sin(angle_rads)
    
    # Add positional encoding
    pos_encoding = positional_encoding(input_shape[0], 32)
    transformer_input = convlstm + pos_encoding
    
    # Simplified transformer encoder block
    def transformer_encoder(x, num_heads=4, key_dim=32, ff_dim=64):  # Reduced from 6 heads, 48 key_dim, 96 ff_dim
        # Multi-head attention with regularization
        attention_output = MultiHeadAttention(
            num_heads=num_heads, 
            key_dim=key_dim,
            kernel_regularizer=l2(l2_lambda)
        )(x, x)
        
        # Skip connection with dropout and regularization
        x1 = Add()([attention_output, x])
        x1 = Dropout(dropout_rate)(x1)
        x1 = LayerNormalization(epsilon=1e-6)(x1)
        
        # Simplified feed-forward network
        ff_output = Dense(
            ff_dim, 
            activation='relu', 
            kernel_regularizer=l2(l2_lambda)
        )(x1)
        ff_output = Dropout(dropout_rate)(ff_output)
        ff_output = Dense(
            32,  # Reduced from 48
            kernel_regularizer=l2(l2_lambda)
        )(ff_output)
        
        # Skip connection
        x2 = Add()([ff_output, x1])
        return LayerNormalization(epsilon=1e-6)(x2)
    
    # Apply transformer encoder
    transformer_output = transformer_encoder(transformer_input)
    
    # Simplified CNN paths with regularization - reduced number of paths
    cnn_paths = []
    for kernel_size in [3, 7]:  # Removed the middle kernel size (5)
        cnn = Conv1D(
            filters=24,  # Reduced from 32
            kernel_size=kernel_size, 
            padding='same', 
            activation='relu',
            kernel_regularizer=l2(l2_lambda)
        )(inputs)
        cnn = BatchNormalization()(cnn)
        cnn = Dropout(dropout_rate)(cnn)
        cnn_paths.append(GlobalMaxPooling1D()(cnn))
    
    # Variational Autoencoder components with adjustable weight
    def sampling(args):
        z_mean, z_log_var = args
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
    # Global temporal features
    global_max = GlobalMaxPooling1D()(transformer_output)
    global_avg = GlobalAveragePooling1D()(transformer_output)
    
    # VAE encoding with regularization
    z_mean = Dense(
        24,  # Reduced from 32
        kernel_regularizer=l2(l2_lambda)
    )(Concatenate()([global_max, global_avg]))
    z_log_var = Dense(
        24,  # Reduced from 32
        kernel_regularizer=l2(l2_lambda)
    )(Concatenate()([global_max, global_avg]))
    z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
    # Combine features
    merged_features = Concatenate()(
        cnn_paths + [global_max, global_avg, z]
    )
    
    # Simplified classification layers with dropout and regularization
    x = Dense(
        64,  # Reduced from 96
        activation='relu', 
        kernel_regularizer=l2(l2_lambda)
    )(merged_features)
    x = Dropout(dropout_rate)(x)
    x = BatchNormalization()(x)
    
    x = Dense(
        32,  # Reduced from 48
        activation='relu', 
        kernel_regularizer=l2(l2_lambda)
    )(x)
    x = Dropout(0.25)(x)
    
    # Output layer
    outputs = Dense(1, activation='sigmoid')(x)
    
    # Create model
    model = Model(inputs=inputs, outputs=outputs)
    
    # Adjustable VAE loss weight
    vae_weight = 0.001
    kl_loss = -0.5 * tf.reduce_mean(
        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
    )
    model.add_loss(vae_weight * kl_loss)
    
    # Compile with gradient clipping
    optimizer = Adam(
        learning_rate=0.001, 
        clipnorm=1.0
    )
    
    model.compile(
        optimizer=optimizer,
        loss='binary_crossentropy',
        metrics=[
            'accuracy',
            tf.keras.metrics.AUC(name='auc'),
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall')
        ]
    )
    
    return model


def create_optimized_tf_dataset(X_file, y_file, indices, batch_size=256, shuffle=True, 
                               weights=None, cache=False, prefetch_factor=2):
    """
    Create an optimized TensorFlow dataset with memory-efficient loading.
    
    Parameters:
    -----------
    X_file : str
        Path to features numpy file
    y_file : str
        Path to labels numpy file
    indices : array-like
        Indices to use for dataset
    batch_size : int
        Batch size for training
    shuffle : bool
        Whether to shuffle the data
    weights : array-like, optional
        Sample weights
    cache : bool
        Whether to cache the dataset
    prefetch_factor : int
        Prefetch buffer size
    
    Returns:
    --------
    tf.data.Dataset
        Optimized TensorFlow dataset
    """
    import numpy as np
    import tensorflow as tf
    import time
    
    # Memory-efficient generator function that loads data on-demand
    def data_generator():
        X_mmap = np.load(X_file, mmap_mode='r')
        y_mmap = np.load(y_file, mmap_mode='r')
        
        # Use smaller subsets of indices to reduce memory pressure
        chunk_size = 1000
        num_chunks = (len(indices) + chunk_size - 1) // chunk_size
        
        for chunk_idx in range(num_chunks):
            start_idx = chunk_idx * chunk_size
            end_idx = min((chunk_idx + 1) * chunk_size, len(indices))
            chunk_indices = indices[start_idx:end_idx]
            
            # Load in smaller batches
            mini_batch_size = 100
            for i in range(0, len(chunk_indices), mini_batch_size):
                batch_indices = chunk_indices[i:i + mini_batch_size]
                
                # Extract features and labels for this mini-batch
                batch_X = np.array([X_mmap[idx] for idx in batch_indices], dtype=np.float32)
                batch_y = np.array([y_mmap[idx] for idx in batch_indices], dtype=np.float32)
                
                # Yield one sample at a time
                for j in range(len(batch_indices)):
                    if weights is not None:
                        yield batch_X[j], batch_y[j], weights[batch_indices[j]]
                    else:
                        yield batch_X[j], batch_y[j]
                
                # Clear references to mini-batch data
                del batch_X, batch_y
                gc.collect()
    
    # Get input shape from first sample
    X_mmap = np.load(X_file, mmap_mode='r')
    input_shape = X_mmap[indices[0]].shape
    del X_mmap  # Release reference
    
    # Create dataset from generator
    if weights is not None:
        output_types = (tf.float32, tf.float32, tf.float32)
        output_shapes = (input_shape, (), ())
    else:
        output_types = (tf.float32, tf.float32)
        output_shapes = (input_shape, ())
    
    dataset = tf.data.Dataset.from_generator(
        data_generator,
        output_types=output_types,
        output_shapes=output_shapes
    )
    
    # Apply dataset optimizations
    if shuffle:
        buffer_size = min(5000, len(indices))  # Reduced buffer size to save memory
        dataset = dataset.shuffle(buffer_size)
    
    dataset = dataset.batch(batch_size)
    
    # Limit prefetch to save memory
    dataset = dataset.prefetch(prefetch_factor)
    
    return dataset

def memory_optimized_training(
    model, 
    X_file, 
    y_file, 
    train_indices, 
    val_indices, 
    test_indices,
    output_dir, 
    batch_size=32, 
    chunk_size=5000,  # Reduced chunk size
    epochs_per_chunk=2, 
    save_frequency=5, 
    class_weight=None, 
    start_chunk=0,
    max_chunks=None,  # Optional limit on number of chunks to process
    memory_limit_gb=None  # Optional memory limit in GB
):
    """
    Memory-optimized training function with strategic checkpointing.
    
    Parameters:
    -----------
    model : tf.keras.Model
        Pre-compiled model
    X_file, y_file : str
        Paths to feature and label files
    train_indices, val_indices, test_indices : array
        Training, validation, and test indices
    output_dir : str
        Directory to save results
    batch_size : int
        Batch size for training
    chunk_size : int
        Number of samples to process at once
    epochs_per_chunk : int
        Epochs to train each chunk
    save_frequency : int
        Save model every N chunks
    class_weight : dict, optional
        Class weights for handling imbalanced data
    start_chunk : int
        Chunk to resume training from
    max_chunks : int, optional
        Maximum number of chunks to process
    memory_limit_gb : float, optional
        Memory limit in GB
    
    Returns:
    --------
    tuple
        Trained model and final model path
    """
    # Create output directories
    os.makedirs(output_dir, exist_ok=True)
    checkpoints_dir = os.path.join(output_dir, "checkpoints")
    os.makedirs(checkpoints_dir, exist_ok=True)
    metrics_dir = os.path.join(output_dir, "metrics")
    os.makedirs(metrics_dir, exist_ok=True)
    
    # Configure logging
    log_file = os.path.join(output_dir, 'training_log.txt')
    logging.basicConfig(
        filename=log_file,
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s: %(message)s',
        filemode='a'  # Append mode for resuming training
    )
    
    # Log start info
    logging.info("=" * 50)
    logging.info(f"Starting/resuming training from chunk {start_chunk}")
    logging.info(f"Memory usage at start: {memory_usage():.2f} MB")
    
    # Process in chunks
    num_chunks = int(np.ceil(len(train_indices) / chunk_size))
    if max_chunks is not None:
        num_chunks = min(num_chunks, start_chunk + max_chunks)
    
    logging.info(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
    
    # Prepare small validation set (to reduce memory footprint)
    val_limit = min(1000, len(val_indices))  # Reduced from 2000
    val_indices_subset = val_indices[:val_limit]
    
    # Create validation dataset once
    logging.info(f"Preparing validation dataset with {val_limit} samples")
    X_mmap = np.load(X_file, mmap_mode='r')
    val_X = np.array([X_mmap[idx] for idx in val_indices_subset], dtype=np.float32)
    del X_mmap
    
    y_mmap = np.load(y_file, mmap_mode='r')
    val_y = np.array([y_mmap[idx] for idx in val_indices_subset], dtype=np.float32)
    del y_mmap
    
    # Track metrics across chunks
    if start_chunk > 0:
        # Try to load existing history
        history_path = os.path.join(metrics_dir, "training_history.json")
        try:
            with open(history_path, "r") as f:
                training_history = json.load(f)
            logging.info(f"Loaded existing training history with {len(training_history['chunks'])} records")
        except:
            logging.warning("Could not load existing history, starting fresh")
            training_history = {'chunks': [], 'validation_metrics': [], 'model_checkpoints': []}
    else:
        training_history = {'chunks': [], 'validation_metrics': [], 'model_checkpoints': []}
    
    # Early stopping tracker
    patience = 5  # Number of chunks with no improvement
    best_val_auc = 0
    patience_counter = 0
    best_model_path = None
    
    # Record starting time
    start_time = time.time()
    
    # Memory limit setting
    if memory_limit_gb is None:
        # Default to 70% of system memory
        memory_limit_gb = psutil.virtual_memory().total / (1024**3) * 0.7
        logging.info(f"Memory limit set to {memory_limit_gb:.2f} GB (70% of system memory)")
    else:
        logging.info(f"Memory limit set to {memory_limit_gb:.2f} GB")
    
    # Recovery file for safe resumption
    recovery_file = os.path.join(output_dir, "last_completed_chunk.txt")
    
    # Process each chunk
    for chunk_idx in range(start_chunk, num_chunks):
        # Memory check before processing chunk
        current_memory_gb = psutil.Process(os.getpid()).memory_info().rss / (1024**3)
        logging.info(f"Memory before chunk {chunk_idx+1}: {current_memory_gb:.2f} GB / {memory_limit_gb:.2f} GB")
        
        if current_memory_gb > memory_limit_gb:
            logging.warning(f"Memory limit exceeded ({current_memory_gb:.2f} GB > {memory_limit_gb:.2f} GB). Stopping.")
            break
        
        # Get chunk indices
        start_idx = chunk_idx * chunk_size
        end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
        chunk_indices = train_indices[start_idx:end_idx]
        
        # Log chunk information
        logging.info(f"Processing chunk {chunk_idx+1}/{num_chunks} with {len(chunk_indices)} samples")
        
        # Load chunk data with memory mapping
        X_mmap = np.load(X_file, mmap_mode='r')
        y_mmap = np.load(y_file, mmap_mode='r')
        
        # Create smaller sub-chunks for processing
        sub_chunk_size = 1000  # Process in smaller batches
        num_sub_chunks = int(np.ceil(len(chunk_indices) / sub_chunk_size))
        
        # Process each sub-chunk
        sub_chunk_histories = []
        for sub_idx in range(num_sub_chunks):
            # Get sub-chunk indices
            sub_start = sub_idx * sub_chunk_size
            sub_end = min((sub_idx + 1) * sub_chunk_size, len(chunk_indices))
            sub_indices = chunk_indices[sub_start:sub_end]
            
            # Load sub-chunk
            sub_X = np.array([X_mmap[idx] for idx in sub_indices], dtype=np.float32)
            sub_y = np.array([y_mmap[idx] for idx in sub_indices], dtype=np.float32)
            
            # Train on sub-chunk
            history = model.fit(
                sub_X, sub_y,
                validation_data=(val_X, val_y),
                epochs=1,  # Train just one epoch per sub-chunk
                batch_size=batch_size,
                class_weight=class_weight,
                verbose=1
            )
            
            # Save sub-chunk metrics
            sub_chunk_histories.append({
                k: [float(v) for v in vals] for k, vals in history.history.items()
            })
            
            # Clear sub-chunk data
            del sub_X, sub_y
            gc.collect()
        
        # Clear memory-mapped arrays
        del X_mmap, y_mmap
        
        # Combine sub-chunk metrics
        chunk_metrics = {}
        for metric in sub_chunk_histories[0].keys():
            chunk_metrics[metric] = []
            for sub_history in sub_chunk_histories:
                chunk_metrics[metric].extend(sub_history[metric])
        
        # Add to overall training history
        training_history['chunks'].append({
            'chunk': chunk_idx+1,
            'metrics': chunk_metrics
        })
        
        # Evaluate on validation set
        val_scores = model.evaluate(val_X, val_y, verbose=0)
        val_metrics = {
            'chunk': chunk_idx+1,
            'loss': float(val_scores[0]),
            'accuracy': float(val_scores[1]),
            'auc': float(val_scores[2]),
            'precision': float(val_scores[3]),
            'recall': float(val_scores[4])
        }
        training_history['validation_metrics'].append(val_metrics)
        
        # Check for improvement
        current_val_auc = val_metrics['auc']
        logging.info(f"Chunk {chunk_idx+1} validation AUC: {current_val_auc:.4f}")
        
        if current_val_auc > best_val_auc:
            best_val_auc = current_val_auc
            patience_counter = 0
            
            # Save best model
            best_model_path = os.path.join(checkpoints_dir, "best_model.h5")
            model.save(best_model_path, save_format='h5')
            logging.info(f"New best model with AUC {best_val_auc:.4f} saved to {best_model_path}")
            
            # Track best model checkpoint
            training_history['model_checkpoints'].append({
                'chunk': chunk_idx+1,
                'path': best_model_path,
                'auc': best_val_auc,
                'is_best': True
            })
        else:
            patience_counter += 1
            logging.info(f"No improvement for {patience_counter} chunks (best: {best_val_auc:.4f})")
        
        # Save periodic checkpoints (less frequently)
        if (chunk_idx + 1) % save_frequency == 0:
            checkpoint_path = os.path.join(checkpoints_dir, f"checkpoint_{chunk_idx+1}.h5")
            model.save(checkpoint_path, save_format='h5')
            logging.info(f"Periodic checkpoint saved to {checkpoint_path}")
            
            # Track checkpoint
            training_history['model_checkpoints'].append({
                'chunk': chunk_idx+1,
                'path': checkpoint_path,
                'auc': current_val_auc,
                'is_best': False
            })
            
            # Cleanup old non-best checkpoints
            if len(training_history['model_checkpoints']) > 3:
                # Keep best model and last checkpoint, remove others
                checkpoints_to_remove = []
                for checkpoint in training_history['model_checkpoints'][:-3]:
                    if not checkpoint.get('is_best', False) and os.path.exists(checkpoint['path']):
                        checkpoints_to_remove.append(checkpoint['path'])
                
                # Remove old checkpoints
                for old_checkpoint in checkpoints_to_remove:
                    try:
                        os.remove(old_checkpoint)
                        logging.info(f"Removed old checkpoint: {old_checkpoint}")
                    except:
                        logging.warning(f"Could not remove checkpoint: {old_checkpoint}")
        
        # Save condensed training history periodically
        if (chunk_idx + 1) % (save_frequency // 2) == 0 or chunk_idx == num_chunks - 1:
            try:
                # Save compact version (only last 10 chunks)
                compact_history = {
                    'chunks': training_history['chunks'][-10:],
                    'validation_metrics': training_history['validation_metrics'],
                    'model_checkpoints': training_history['model_checkpoints']
                }
                
                with open(os.path.join(metrics_dir, "training_history.json"), "w") as f:
                    json.dump(compact_history, f, indent=2)
                
                # Update recovery file
                with open(recovery_file, "w") as f:
                    f.write(str(chunk_idx + 1))
                
                logging.info(f"Saved training history and updated recovery file")
            except Exception as e:
                logging.error(f"Error saving history: {e}")
        
        # Visualize progress (sparingly)
        if (chunk_idx + 1) % (save_frequency * 2) == 0 or chunk_idx == num_chunks - 1:
            try:
                # Create simple progress plot
                val_aucs = [m['auc'] for m in training_history['validation_metrics']]
                chunk_nums = [m['chunk'] for m in training_history['validation_metrics']]
                
                plt.figure(figsize=(10, 5))
                plt.plot(chunk_nums, val_aucs, marker='o')
                plt.xlabel('Chunk')
                plt.ylabel('Validation AUC')
                plt.title('Training Progress')
                plt.grid(True, alpha=0.3)
                plt.savefig(os.path.join(metrics_dir, "training_progress.png"), dpi=100)
                plt.close()
                
                logging.info(f"Generated progress visualization")
            except Exception as e:
                logging.error(f"Error creating visualization: {e}")
        
        # Check for early stopping
        if patience_counter >= patience:
            logging.info(f"Early stopping after {patience} chunks with no improvement")
            break
        
        # Aggressive cleanup
        aggressive_memory_cleanup()
    
    # Training complete
    training_time = time.time() - start_time
    logging.info(f"Training completed in {training_time/3600:.2f} hours")
    
    # Load best model for final evaluation
    if best_model_path and os.path.exists(best_model_path):
        logging.info(f"Loading best model from {best_model_path}")
        model = tf.keras.models.load_model(best_model_path)
    
    # Save final training metrics
    final_metrics_path = os.path.join(metrics_dir, "final_metrics.json")
    with open(final_metrics_path, "w") as f:
        json.dump({
            'training_time_hours': training_time/3600,
            'best_val_auc': best_val_auc,
            'chunks_completed': chunk_idx + 1,
            'best_model_path': best_model_path
        }, f, indent=2)
    
    # Perform lightweight test evaluation if possible
    test_metrics = None
    try:
        # Use a small subset of test data to save memory
        test_limit = min(1000, len(test_indices))
        test_indices_subset = test_indices[:test_limit]
        
        # Load test data
        X_mmap = np.load(X_file, mmap_mode='r')
        test_X = np.array([X_mmap[idx] for idx in test_indices_subset], dtype=np.float32)
        del X_mmap
        
        y_mmap = np.load(y_file, mmap_mode='r')
        test_y = np.array([y_mmap[idx] for idx in test_indices_subset], dtype=np.float32)
        del y_mmap
        
        # Evaluate on test set
        test_scores = model.evaluate(test_X, test_y, verbose=0)
        test_preds = model.predict(test_X)
        
        # Calculate metrics
        test_binary_preds = (test_preds > 0.5).astype(int)
        test_metrics = {
            'loss': float(test_scores[0]),
            'accuracy': float(test_scores[1]),
            'auc': float(test_scores[2]),
            'precision': float(test_scores[3]),
            'recall': float(test_scores[4]),
            'confusion_matrix': confusion_matrix(test_y, test_binary_preds).tolist(),
            'subset_size': test_limit
        }
        
        # Save test metrics
        with open(os.path.join(metrics_dir, "test_metrics.json"), "w") as f:
            json.dump(test_metrics, f, indent=2)
        
        logging.info(f"Test evaluation completed. AUC: {test_metrics['auc']:.4f}")
        
        # Clean up
        del test_X, test_y, test_preds
        gc.collect()
    except Exception as e:
        logging.error(f"Error during test evaluation: {e}")
    
    return model, best_model_path

def evaluate_model_performance(model, X_file, y_file, test_indices, output_dir, batch_size=32):
    """
    Memory-efficient model evaluation function.
    
    Parameters:
    -----------
    model : tf.keras.Model
        Trained model
    X_file, y_file : str
        Paths to feature and label files
    test_indices : array
        Test indices
    output_dir : str
        Directory to save results
    batch_size : int
        Batch size for evaluation
    
    Returns:
    --------
    dict
        Evaluation results
    """
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Configure logging
    logging.basicConfig(
        filename=os.path.join(output_dir, 'evaluation_log.txt'),
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s: %(message)s'
    )
    
    # Process test data in batches
    test_batch_size = 1000  # Small batch size for memory efficiency
    num_test_batches = int(np.ceil(len(test_indices) / test_batch_size))
    
    all_test_predictions = []
    all_test_true = []
    test_metrics = {'loss': 0, 'accuracy': 0, 'auc': 0, 'precision': 0, 'recall': 0, 'samples': 0}
    
    logging.info(f"Evaluating model on {len(test_indices)} test samples in {num_test_batches} batches")
    
    try:
        for test_batch_idx in range(num_test_batches):
            # Get batch indices
            start_idx = test_batch_idx * test_batch_size
            end_idx = min((test_batch_idx + 1) * test_batch_size, len(test_indices))
            batch_indices = test_indices[start_idx:end_idx]
            
            # Load batch data with memory mapping
            X_mmap = np.load(X_file, mmap_mode='r')
            test_X = np.array([X_mmap[idx] for idx in batch_indices], dtype=np.float32)
            del X_mmap
            
            y_mmap = np.load(y_file, mmap_mode='r')
            test_y = np.array([y_mmap[idx] for idx in batch_indices], dtype=np.float32)
            del y_mmap
            
            # Log progress
            logging.info(f"Processing batch {test_batch_idx+1}/{num_test_batches} with {len(batch_indices)} samples")
            
            # Evaluate
            metrics = model.evaluate(test_X, test_y, batch_size=batch_size, verbose=0)
            metrics_dict = {name: value for name, value in zip(model.metrics_names, metrics)}
            
            # Weight metrics by batch size
            for key in ['loss', 'accuracy', 'auc', 'precision', 'recall']:
                if key in metrics_dict:
                    test_metrics[key] += metrics_dict[key] * len(batch_indices)
            test_metrics['samples'] += len(batch_indices)
            
            # Get predictions
            test_preds = model.predict(test_X, batch_size=batch_size)
            
            # Store predictions and labels
            all_test_predictions.append(test_preds.flatten())
            all_test_true.append(test_y)
            
            # Clean up
            del test_X, test_y, test_preds
            gc.collect()
            
            # Save intermediate results
            if (test_batch_idx + 1) % 5 == 0 or test_batch_idx == num_test_batches - 1:
                # Calculate metrics so far
                current_metrics = {k: v / test_metrics['samples'] for k, v in test_metrics.items() if k != 'samples'}
                current_metrics['batches_processed'] = test_batch_idx + 1
                
                # Save intermediate metrics
                with open(os.path.join(output_dir, "evaluation_progress.json"), "w") as f:
                    json.dump(current_metrics, f, indent=2)
        
        # Combine results
        all_test_predictions = np.concatenate(all_test_predictions)
        all_test_true = np.concatenate(all_test_true)
        
        # Calculate final test metrics
        final_metrics = {k: v / test_metrics['samples'] for k, v in test_metrics.items() if k != 'samples'}
        
        # Calculate additional metrics
        test_preds_binary = (all_test_predictions > 0.5).astype(int)
        report = classification_report(all_test_true, test_preds_binary, output_dict=True)
        conf_matrix = confusion_matrix(all_test_true, test_preds_binary)
        
        # Combine all metrics
        results = {
            'metrics': final_metrics,
            'classification_report': report,
            'confusion_matrix': conf_matrix.tolist(),
            'num_samples': len(all_test_true)
        }
        
        # Generate ROC curve data (for later plotting)
        fpr, tpr, _ = roc_curve(all_test_true, all_test_predictions)
        roc_data = {'fpr': fpr.tolist(), 'tpr': tpr.tolist()}
        
        # Save results
        with open(os.path.join(output_dir, 'evaluation_results.json'), 'w') as f:
            json.dump(results, f, indent=2)
        
        with open(os.path.join(output_dir, 'roc_curve_data.json'), 'w') as f:
            json.dump(roc_data, f, indent=2)
        
        # Save a small subset of predictions for analysis (avoid storing all)
        sample_size = min(1000, len(all_test_predictions))
        sample_indices = np.random.choice(len(all_test_predictions), sample_size, replace=False)
        
        sample_data = {
            'predictions': all_test_predictions[sample_indices].tolist(),
            'true_labels': all_test_true[sample_indices].tolist(),
            'indices': test_indices[sample_indices].tolist()
        }
        
        with open(os.path.join(output_dir, 'sample_predictions.json'), 'w') as f:
            json.dump(sample_data, f, indent=2)
        
        # Create a visualization
        plt.figure(figsize=(15, 5))
        
        # ROC Curve
        plt.subplot(131)
        plt.plot(fpr, tpr, label=f'AUC = {final_metrics["auc"]:.3f}')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve')
        plt.legend()
        
        # Confusion Matrix
        plt.subplot(132)
        plt.imshow(conf_matrix, interpolation='nearest', cmap='Blues')
        plt.title('Confusion Matrix')
        plt.colorbar()
        for i in range(conf_matrix.shape[0]):
            for j in range(conf_matrix.shape[1]):
                plt.text(j, i, str(conf_matrix[i, j]),
                         ha="center", va="center", color="white" if conf_matrix[i, j] > conf_matrix.max()/2 else "black")
        plt.xlabel('Predicted Label')
        plt.ylabel('True Label')
        
        # Precision-Recall
        plt.subplot(133)
        precision, recall, _ = precision_recall_curve(all_test_true, all_test_predictions)
        avg_precision = average_precision_score(all_test_true, all_test_predictions)
        plt.plot(recall, precision, label=f'AP = {avg_precision:.3f}')
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curve')
        plt.legend()
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'evaluation_summary.png'), dpi=100)
        plt.close()
        
        logging.info("Evaluation completed successfully")
        return results
        
    except Exception as e:
        logging.error(f"Error during evaluation: {e}")
        logging.error(traceback.format_exc())
        return {'error': str(e)}

import os
import gc
import json
import logging
import numpy as np
import tensorflow as tf
import psutil
import time
import traceback
from datetime import datetime
from sklearn.metrics import (
    classification_report, 
    confusion_matrix, 
    roc_curve, 
    auc,
    roc_auc_score,
    precision_recall_curve, 
    average_precision_score
)
import matplotlib.pyplot as plt

# Configure TensorFlow memory growth
def configure_tensorflow_memory():
    """Configure TensorFlow to use memory growth and limit GPU memory allocation"""
    # Clear any existing session
    tf.keras.backend.clear_session()
    
    # Configure GPU memory growth
    physical_devices = tf.config.list_physical_devices('GPU')
    if physical_devices:
        for device in physical_devices:
            try:
                # Allow memory growth - prevents TF from allocating all GPU memory at once
                tf.config.experimental.set_memory_growth(device, True)
                # Optionally limit GPU memory for stability
                tf.config.set_logical_device_configuration(
                    device,
                    [tf.config.LogicalDeviceConfiguration(memory_limit=4096)]  # Limit to 4GB
                )
                print(f"Memory growth enabled for {device}")
            except Exception as e:
                print(f"Error configuring GPU: {e}")
    
    # Limit CPU threads
    tf.config.threading.set_intra_op_parallelism_threads(4)
    tf.config.threading.set_inter_op_parallelism_threads(2)
    
    # Set soft device placement
    tf.config.set_soft_device_placement(True)
    
    # Set memory cleanup for malloc
    os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'
    
    # Run garbage collection
    gc.collect()


def aggressive_memory_cleanup():
    """Ultra-aggressive memory cleanup to reclaim memory"""
    # Clear Keras backend
    tf.keras.backend.clear_session()
    
    # Garbage collection with multiple passes
    for _ in range(3):
        gc.collect()
    
    # Optional: Release memory back to system on Linux
    try:
        import ctypes
        libc = ctypes.CDLL('libc.so.6')
        libc.malloc_trim(0)
    except:
        pass


def memory_usage():
    """Monitor memory usage in MB"""
    process = psutil.Process(os.getpid())
    mem_info = process.memory_info()
    return mem_info.rss / 1024**2  # Memory in MB


def get_memory_summary():
    """Get detailed memory usage summary"""
    try:
        # Process memory
        process = psutil.Process(os.getpid())
        mem_info = process.memory_info()
        
        # System memory
        system = psutil.virtual_memory()
        
        # GPU memory if available
        gpu_memory = "N/A"
        try:
            import subprocess
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'], 
                stdout=subprocess.PIPE
            )
            gpu_memory = result.stdout.decode('utf-8').strip()
        except:
            pass
        
        return {
            'process_rss_mb': mem_info.rss / (1024**2),
            'process_vms_mb': mem_info.vms / (1024**2),
            'system_available_mb': system.available / (1024**2),
            'system_percent': system.percent,
            'gpu_memory': gpu_memory
        }
    except:
        return {'error': 'Could not get memory summary'}


def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
    """
    Advanced zero curtain detection model with optimized memory footprint.
    
    Parameters:
    -----------
    input_shape : tuple
        Shape of input data (sequence_length, num_features)
    include_moisture : bool
        Whether soil moisture features are included
    """
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
    from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D
    from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.regularizers import l2
    
    # Regularization parameters - increased for better generalization and reduced memory
    l2_lambda = 1e-4
    dropout_rate = 0.35
    
    # Input layer
    inputs = Input(shape=input_shape)
    
    # Reshape for ConvLSTM (add spatial dimension)
    x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
    # ConvLSTM layer with simplified architecture
    from tensorflow.keras.layers import ConvLSTM2D
    convlstm = ConvLSTM2D(
        filters=32,  # Reduced from 48 to save memory
        kernel_size=(3, 1),
        padding='same',
        return_sequences=True,
        activation='tanh',
        recurrent_dropout=dropout_rate,
        kernel_regularizer=l2(l2_lambda)
    )(x)
    
    # Reshape back to (sequence_length, features)
    convlstm = Reshape((input_shape[0], 32))(convlstm)
    
    # Simplified positional encoding
    def positional_encoding(length, depth):
        positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
        angle_rates = 1 / tf.pow(10000.0, depths)
        angle_rads = positions * angle_rates
        
        return tf.sin(angle_rads)
    
    # Add positional encoding
    pos_encoding = positional_encoding(input_shape[0], 32)
    transformer_input = convlstm + pos_encoding
    
    # Simplified transformer encoder block
    def transformer_encoder(x, num_heads=4, key_dim=32, ff_dim=64):  # Reduced from 6 heads, 48 key_dim, 96 ff_dim
        # Multi-head attention with regularization
        attention_output = MultiHeadAttention(
            num_heads=num_heads, 
            key_dim=key_dim,
            kernel_regularizer=l2(l2_lambda)
        )(x, x)
        
        # Skip connection with dropout and regularization
        x1 = Add()([attention_output, x])
        x1 = Dropout(dropout_rate)(x1)
        x1 = LayerNormalization(epsilon=1e-6)(x1)
        
        # Simplified feed-forward network
        ff_output = Dense(
            ff_dim, 
            activation='relu', 
            kernel_regularizer=l2(l2_lambda)
        )(x1)
        ff_output = Dropout(dropout_rate)(ff_output)
        ff_output = Dense(
            32,  # Reduced from 48
            kernel_regularizer=l2(l2_lambda)
        )(ff_output)
        
        # Skip connection
        x2 = Add()([ff_output, x1])
        return LayerNormalization(epsilon=1e-6)(x2)
    
    # Apply transformer encoder
    transformer_output = transformer_encoder(transformer_input)
    
    # Simplified CNN paths with regularization - reduced number of paths
    cnn_paths = []
    for kernel_size in [3, 7]:  # Removed the middle kernel size (5)
        cnn = Conv1D(
            filters=24,  # Reduced from 32
            kernel_size=kernel_size, 
            padding='same', 
            activation='relu',
            kernel_regularizer=l2(l2_lambda)
        )(inputs)
        cnn = BatchNormalization()(cnn)
        cnn = Dropout(dropout_rate)(cnn)
        cnn_paths.append(GlobalMaxPooling1D()(cnn))
    
    # Variational Autoencoder components with adjustable weight
    def sampling(args):
        z_mean, z_log_var = args
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
    # Global temporal features
    global_max = GlobalMaxPooling1D()(transformer_output)
    global_avg = GlobalAveragePooling1D()(transformer_output)
    
    # VAE encoding with regularization
    z_mean = Dense(
        24,  # Reduced from 32
        kernel_regularizer=l2(l2_lambda)
    )(Concatenate()([global_max, global_avg]))
    z_log_var = Dense(
        24,  # Reduced from 32
        kernel_regularizer=l2(l2_lambda)
    )(Concatenate()([global_max, global_avg]))
    z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
    # Combine features
    merged_features = Concatenate()(
        cnn_paths + [global_max, global_avg, z]
    )
    
    # Simplified classification layers with dropout and regularization
    x = Dense(
        64,  # Reduced from 96
        activation='relu', 
        kernel_regularizer=l2(l2_lambda)
    )(merged_features)
    x = Dropout(dropout_rate)(x)
    x = BatchNormalization()(x)
    
    x = Dense(
        32,  # Reduced from 48
        activation='relu', 
        kernel_regularizer=l2(l2_lambda)
    )(x)
    x = Dropout(0.25)(x)
    
    # Output layer
    outputs = Dense(1, activation='sigmoid')(x)
    
    # Create model
    model = Model(inputs=inputs, outputs=outputs)
    
    # Adjustable VAE loss weight
    vae_weight = 0.001
    kl_loss = -0.5 * tf.reduce_mean(
        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
    )
    model.add_loss(vae_weight * kl_loss)
    
    # Compile with gradient clipping
    optimizer = Adam(
        learning_rate=0.001, 
        clipnorm=1.0
    )
    
    model.compile(
        optimizer=optimizer,
        loss='binary_crossentropy',
        metrics=[
            'accuracy',
            tf.keras.metrics.AUC(name='auc'),
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall')
        ]
    )
    
    return model


def create_optimized_tf_dataset(X_file, y_file, indices, batch_size=256, shuffle=True, 
                               weights=None, cache=False, prefetch_factor=2):
    """
    Create an optimized TensorFlow dataset with memory-efficient loading.
    
    Parameters:
    -----------
    X_file : str
        Path to features numpy file
    y_file : str
        Path to labels numpy file
    indices : array-like
        Indices to use for dataset
    batch_size : int
        Batch size for training
    shuffle : bool
        Whether to shuffle the data
    weights : array-like, optional
        Sample weights
    cache : bool
        Whether to cache the dataset
    prefetch_factor : int
        Prefetch buffer size
    
    Returns:
    --------
    tf.data.Dataset
        Optimized TensorFlow dataset
    """
    import numpy as np
    import tensorflow as tf
    import time
    
    # Memory-efficient generator function that loads data on-demand
    def data_generator():
        X_mmap = np.load(X_file, mmap_mode='r')
        y_mmap = np.load(y_file, mmap_mode='r')
        
        # Use smaller subsets of indices to reduce memory pressure
        chunk_size = 1000
        num_chunks = (len(indices) + chunk_size - 1) // chunk_size
        
        for chunk_idx in range(num_chunks):
            start_idx = chunk_idx * chunk_size
            end_idx = min((chunk_idx + 1) * chunk_size, len(indices))
            chunk_indices = indices[start_idx:end_idx]
            
            # Load in smaller batches
            mini_batch_size = 100
            for i in range(0, len(chunk_indices), mini_batch_size):
                batch_indices = chunk_indices[i:i + mini_batch_size]
                
                # Extract features and labels for this mini-batch
                batch_X = np.array([X_mmap[idx] for idx in batch_indices], dtype=np.float32)
                batch_y = np.array([y_mmap[idx] for idx in batch_indices], dtype=np.float32)
                
                # Yield one sample at a time
                for j in range(len(batch_indices)):
                    if weights is not None:
                        yield batch_X[j], batch_y[j], weights[batch_indices[j]]
                    else:
                        yield batch_X[j], batch_y[j]
                
                # Clear references to mini-batch data
                del batch_X, batch_y
                gc.collect()
    
    # Get input shape from first sample
    X_mmap = np.load(X_file, mmap_mode='r')
    input_shape = X_mmap[indices[0]].shape
    del X_mmap  # Release reference
    
    # Create dataset from generator
    if weights is not None:
        output_types = (tf.float32, tf.float32, tf.float32)
        output_shapes = (input_shape, (), ())
    else:
        output_types = (tf.float32, tf.float32)
        output_shapes = (input_shape, ())
    
    dataset = tf.data.Dataset.from_generator(
        data_generator,
        output_types=output_types,
        output_shapes=output_shapes
    )
    
    # Apply dataset optimizations
    if shuffle:
        buffer_size = min(5000, len(indices))  # Reduced buffer size to save memory
        dataset = dataset.shuffle(buffer_size)
    
    dataset = dataset.batch(batch_size)
    
    # Limit prefetch to save memory
    dataset = dataset.prefetch(prefetch_factor)
    
    return dataset


def memory_optimized_training(
    model, 
    X_file, 
    y_file, 
    train_indices, 
    val_indices, 
    test_indices,
    output_dir, 
    batch_size=32, 
    chunk_size=5000,  # Reduced chunk size
    epochs_per_chunk=2, 
    save_frequency=5, 
    class_weight=None, 
    start_chunk=0,
    max_chunks=None,  # Optional limit on number of chunks to process
    memory_limit_gb=None  # Optional memory limit in GB
):
    """
    Memory-optimized training function with strategic checkpointing.
    
    Parameters:
    -----------
    model : tf.keras.Model
        Pre-compiled model
    X_file, y_file : str
        Paths to feature and label files
    train_indices, val_indices, test_indices : array
        Training, validation, and test indices
    output_dir : str
        Directory to save results
    batch_size : int
        Batch size for training
    chunk_size : int
        Number of samples to process at once
    epochs_per_chunk : int
        Epochs to train each chunk
    save_frequency : int
        Save model every N chunks
    class_weight : dict, optional
        Class weights for handling imbalanced data
    start_chunk : int
        Chunk to resume training from
    max_chunks : int, optional
        Maximum number of chunks to process
    memory_limit_gb : float, optional
        Memory limit in GB
    
    Returns:
    --------
    tuple
        Trained model and final model path
    """
    # Create output directories
    os.makedirs(output_dir, exist_ok=True)
    checkpoints_dir = os.path.join(output_dir, "checkpoints")
    os.makedirs(checkpoints_dir, exist_ok=True)
    metrics_dir = os.path.join(output_dir, "metrics")
    os.makedirs(metrics_dir, exist_ok=True)
    
    # Configure logging
    log_file = os.path.join(output_dir, 'training_log.txt')
    logging.basicConfig(
        filename=log_file,
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s: %(message)s',
        filemode='a'  # Append mode for resuming training
    )
    
    # Log start info
    logging.info("=" * 50)
    logging.info(f"Starting/resuming training from chunk {start_chunk}")
    logging.info(f"Memory usage at start: {memory_usage():.2f} MB")
    
    # Process in chunks
    num_chunks = int(np.ceil(len(train_indices) / chunk_size))
    if max_chunks is not None:
        num_chunks = min(num_chunks, start_chunk + max_chunks)
    
    logging.info(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
    
    # Prepare small validation set (to reduce memory footprint)
    val_limit = min(1000, len(val_indices))  # Reduced from 2000
    val_indices_subset = val_indices[:val_limit]
    
    # Create validation dataset once
    logging.info(f"Preparing validation dataset with {val_limit} samples")
    X_mmap = np.load(X_file, mmap_mode='r')
    val_X = np.array([X_mmap[idx] for idx in val_indices_subset], dtype=np.float32)
    del X_mmap
    
    y_mmap = np.load(y_file, mmap_mode='r')
    val_y = np.array([y_mmap[idx] for idx in val_indices_subset], dtype=np.float32)
    del y_mmap
    
    # Track metrics across chunks
    if start_chunk > 0:
        # Try to load existing history
        history_path = os.path.join(metrics_dir, "training_history.json")
        try:
            with open(history_path, "r") as f:
                training_history = json.load(f)
            logging.info(f"Loaded existing training history with {len(training_history['chunks'])} records")
        except:
            logging.warning("Could not load existing history, starting fresh")
            training_history = {'chunks': [], 'validation_metrics': [], 'model_checkpoints': []}
    else:
        training_history = {'chunks': [], 'validation_metrics': [], 'model_checkpoints': []}
    
    # Early stopping tracker
    patience = 5  # Number of chunks with no improvement
    best_val_auc = 0
    patience_counter = 0
    best_model_path = None
    
    # Record starting time
    start_time = time.time()
    
    # Memory limit setting
    if memory_limit_gb is None:
        # Default to 70% of system memory
        memory_limit_gb = psutil.virtual_memory().total / (1024**3) * 0.7
        logging.info(f"Memory limit set to {memory_limit_gb:.2f} GB (70% of system memory)")
    else:
        logging.info(f"Memory limit set to {memory_limit_gb:.2f} GB")
    
    # Recovery file for safe resumption
    recovery_file = os.path.join(output_dir, "last_completed_chunk.txt")
    
    # Process each chunk
    for chunk_idx in range(start_chunk, num_chunks):
        # Memory check before processing chunk
        current_memory_gb = psutil.Process(os.getpid()).memory_info().rss / (1024**3)
        logging.info(f"Memory before chunk {chunk_idx+1}: {current_memory_gb:.2f} GB / {memory_limit_gb:.2f} GB")
        
        if current_memory_gb > memory_limit_gb:
            logging.warning(f"Memory limit exceeded ({current_memory_gb:.2f} GB > {memory_limit_gb:.2f} GB). Stopping.")
            break
        
        # Get chunk indices
        start_idx = chunk_idx * chunk_size
        end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
        chunk_indices = train_indices[start_idx:end_idx]
        
        # Log chunk information
        logging.info(f"Processing chunk {chunk_idx+1}/{num_chunks} with {len(chunk_indices)} samples")
        
        # Load chunk data with memory mapping
        X_mmap = np.load(X_file, mmap_mode='r')
        y_mmap = np.load(y_file, mmap_mode='r')
        
        # Create smaller sub-chunks for processing
        sub_chunk_size = 1000  # Process in smaller batches
        num_sub_chunks = int(np.ceil(len(chunk_indices) / sub_chunk_size))
        
        # Process each sub-chunk
        sub_chunk_histories = []
        for sub_idx in range(num_sub_chunks):
            # Get sub-chunk indices
            sub_start = sub_idx * sub_chunk_size
            sub_end = min((sub_idx + 1) * sub_chunk_size, len(chunk_indices))
            sub_indices = chunk_indices[sub_start:sub_end]
            
            # Load sub-chunk
            sub_X = np.array([X_mmap[idx] for idx in sub_indices], dtype=np.float32)
            sub_y = np.array([y_mmap[idx] for idx in sub_indices], dtype=np.float32)
            
            # Train on sub-chunk
            history = model.fit(
                sub_X, sub_y,
                validation_data=(val_X, val_y),
                epochs=1,  # Train just one epoch per sub-chunk
                batch_size=batch_size,
                class_weight=class_weight,
                verbose=1
            )
            
            # Save sub-chunk metrics
            sub_chunk_histories.append({
                k: [float(v) for v in vals] for k, vals in history.history.items()
            })
            
            # Clear sub-chunk data
            del sub_X, sub_y
            gc.collect()
        
        # Clear memory-mapped arrays
        del X_mmap, y_mmap
        
        # Combine sub-chunk metrics
        chunk_metrics = {}
        for metric in sub_chunk_histories[0].keys():
            chunk_metrics[metric] = []
            for sub_history in sub_chunk_histories:
                chunk_metrics[metric].extend(sub_history[metric])
        
        # Add to overall training history
        training_history['chunks'].append({
            'chunk': chunk_idx+1,
            'metrics': chunk_metrics
        })
        
        # Evaluate on validation set
        val_scores = model.evaluate(val_X, val_y, verbose=0)
        val_metrics = {
            'chunk': chunk_idx+1,
            'loss': float(val_scores[0]),
            'accuracy': float(val_scores[1]),
            'auc': float(val_scores[2]),
            'precision': float(val_scores[3]),
            'recall': float(val_scores[4])
        }
        training_history['validation_metrics'].append(val_metrics)
        
        # Check for improvement
        current_val_auc = val_metrics['auc']
        logging.info(f"Chunk {chunk_idx+1} validation AUC: {current_val_auc:.4f}")
        
        if current_val_auc > best_val_auc:
            best_val_auc = current_val_auc
            patience_counter = 0
            
            # Save best model
            best_model_path = os.path.join(checkpoints_dir, "best_model.h5")
            model.save(best_model_path, save_format='h5')
            logging.info(f"New best model with AUC {best_val_auc:.4f} saved to {best_model_path}")
            
            # Track best model checkpoint
            training_history['model_checkpoints'].append({
                'chunk': chunk_idx+1,
                'path': best_model_path,
                'auc': best_val_auc,
                'is_best': True
            })
        else:
            patience_counter += 1
            logging.info(f"No improvement for {patience_counter} chunks (best: {best_val_auc:.4f})")
        
        # Save periodic checkpoints (less frequently)
        if (chunk_idx + 1) % save_frequency == 0:
            checkpoint_path = os.path.join(checkpoints_dir, f"checkpoint_{chunk_idx+1}.h5")
            model.save(checkpoint_path, save_format='h5')
            logging.info(f"Periodic checkpoint saved to {checkpoint_path}")
            
            # Track checkpoint
            training_history['model_checkpoints'].append({
                'chunk': chunk_idx+1,
                'path': checkpoint_path,
                'auc': current_val_auc,
                'is_best': False
            })
            
            # Cleanup old non-best checkpoints
            if len(training_history['model_checkpoints']) > 3:
                # Keep best model and last checkpoint, remove others
                checkpoints_to_remove = []
                for checkpoint in training_history['model_checkpoints'][:-3]:
                    if not checkpoint.get('is_best', False) and os.path.exists(checkpoint['path']):
                        checkpoints_to_remove.append(checkpoint['path'])
                
                # Remove old checkpoints
                for old_checkpoint in checkpoints_to_remove:
                    try:
                        os.remove(old_checkpoint)
                        logging.info(f"Removed old checkpoint: {old_checkpoint}")
                    except:
                        logging.warning(f"Could not remove checkpoint: {old_checkpoint}")
        
        # Save condensed training history periodically
        if (chunk_idx + 1) % (save_frequency // 2) == 0 or chunk_idx == num_chunks - 1:
            try:
                # Save compact version (only last 10 chunks)
                compact_history = {
                    'chunks': training_history['chunks'][-10:],
                    'validation_metrics': training_history['validation_metrics'],
                    'model_checkpoints': training_history['model_checkpoints']
                }
                
                with open(os.path.join(metrics_dir, "training_history.json"), "w") as f:
                    json.dump(compact_history, f, indent=2)
                
                # Update recovery file
                with open(recovery_file, "w") as f:
                    f.write(str(chunk_idx + 1))
                
                logging.info(f"Saved training history and updated recovery file")
            except Exception as e:
                logging.error(f"Error saving history: {e}")
        
        # Visualize progress (sparingly)
        if (chunk_idx + 1) % (save_frequency * 2) == 0 or chunk_idx == num_chunks - 1:
            try:
                # Create simple progress plot
                val_aucs = [m['auc'] for m in training_history['validation_metrics']]
                chunk_nums = [m['chunk'] for m in training_history['validation_metrics']]
                
                plt.figure(figsize=(10, 5))
                plt.plot(chunk_nums, val_aucs, marker='o')
                plt.xlabel('Chunk')
                plt.ylabel('Validation AUC')
                plt.title('Training Progress')
                plt.grid(True, alpha=0.3)
                plt.savefig(os.path.join(metrics_dir, "training_progress.png"), dpi=100)
                plt.close()
                
                logging.info(f"Generated progress visualization")
            except Exception as e:
                logging.error(f"Error creating visualization: {e}")
        
        # Check for early stopping
        if patience_counter >= patience:
            logging.info(f"Early stopping after {patience} chunks with no improvement")
            break
        
        # Aggressive cleanup
        aggressive_memory_cleanup()
    
    # Training complete
    training_time = time.time() - start_time
    logging.info(f"Training completed in {training_time/3600:.2f} hours")
    
    # Load best model for final evaluation
    if best_model_path and os.path.exists(best_model_path):
        logging.info(f"Loading best model from {best_model_path}")
        model = tf.keras.models.load_model(best_model_path)
    
    # Save final training metrics
    final_metrics_path = os.path.join(metrics_dir, "final_metrics.json")
    with open(final_metrics_path, "w") as f:
        json.dump({
            'training_time_hours': training_time/3600,
            'best_val_auc': best_val_auc,
            'chunks_completed': chunk_idx + 1,
            'best_model_path': best_model_path
        }, f, indent=2)
    
    # Perform lightweight test evaluation if possible
    test_metrics = None
    try:
        # Use a small subset of test data to save memory
        test_limit = min(1000, len(test_indices))
        test_indices_subset = test_indices[:test_limit]
        
        # Load test data
        X_mmap = np.load(X_file, mmap_mode='r')
        test_X = np.array([X_mmap[idx] for idx in test_indices_subset], dtype=np.float32)
        del X_mmap
        
        y_mmap = np.load(y_file, mmap_mode='r')
        test_y = np.array([y_mmap[idx] for idx in test_indices_subset], dtype=np.float32)
        del y_mmap
        
        # Evaluate on test set
        with tf.device('/CPU:0'):
            test_scores = model.evaluate(test_X, test_y, verbose=0)
            test_preds = model.predict(test_X)
        
            # Calculate metrics
            test_binary_preds = (test_preds > 0.5).astype(int)
            test_metrics = {
                'loss': float(test_scores[0]),
                'accuracy': float(test_scores[1]),
                'auc': float(test_scores[2]),
                'precision': float(test_scores[3]),
                'recall': float(test_scores[4]),
                'confusion_matrix': confusion_matrix(test_y, test_binary_preds).tolist(),
                'subset_size': test_limit
            }
        
        # Save test metrics
        with open(os.path.join(metrics_dir, "test_metrics.json"), "w") as f:
            json.dump(test_metrics, f, indent=2)
        
        logging.info(f"Test evaluation completed. AUC: {test_metrics['auc']:.4f}")
        
        # Clean up
        del test_X, test_y, test_preds
        gc.collect()
    except Exception as e:
        logging.error(f"Error during test evaluation: {e}")
    
    return model, best_model_path


def evaluate_model_performance(model, X_file, y_file, test_indices, output_dir, batch_size=32):
    """
    Memory-efficient model evaluation function.
    
    Parameters:
    -----------
    model : tf.keras.Model
        Trained model
    X_file, y_file : str
        Paths to feature and label files
    test_indices : array
        Test indices
    output_dir : str
        Directory to save results
    batch_size : int
        Batch size for evaluation
    
    Returns:
    --------
    dict
        Evaluation results
    """
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Configure logging
    logging.basicConfig(
        filename=os.path.join(output_dir, 'evaluation_log.txt'),
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s: %(message)s'
    )
    
    # Process test data in batches
    test_batch_size = 1000  # Small batch size for memory efficiency
    num_test_batches = int(np.ceil(len(test_indices) / test_batch_size))
    
    all_test_predictions = []
    all_test_true = []
    test_metrics = {'loss': 0, 'accuracy': 0, 'auc': 0, 'precision': 0, 'recall': 0, 'samples': 0}
    
    logging.info(f"Evaluating model on {len(test_indices)} test samples in {num_test_batches} batches")
    
    try:
        for test_batch_idx in range(num_test_batches):
            # Get batch indices
            start_idx = test_batch_idx * test_batch_size
            end_idx = min((test_batch_idx + 1) * test_batch_size, len(test_indices))
            batch_indices = test_indices[start_idx:end_idx]
            
            # Load batch data with memory mapping
            X_mmap = np.load(X_file, mmap_mode='r')
            test_X = np.array([X_mmap[idx] for idx in batch_indices], dtype=np.float32)
            del X_mmap
            
            y_mmap = np.load(y_file, mmap_mode='r')
            test_y = np.array([y_mmap[idx] for idx in batch_indices], dtype=np.float32)
            del y_mmap
            
            # Log progress
            logging.info(f"Processing batch {test_batch_idx+1}/{num_test_batches} with {len(batch_indices)} samples")
            
            # Evaluate
            metrics = model.evaluate(test_X, test_y, batch_size=batch_size, verbose=0)
            metrics_dict = {name: value for name, value in zip(model.metrics_names, metrics)}
            
            # Weight metrics by batch size
            for key in ['loss', 'accuracy', 'auc', 'precision', 'recall']:
                if key in metrics_dict:
                    test_metrics[key] += metrics_dict[key] * len(batch_indices)
            test_metrics['samples'] += len(batch_indices)
            
            # Get predictions
            test_preds = model.predict(test_X, batch_size=batch_size)
            
            # Store predictions and labels
            all_test_predictions.append(test_preds.flatten())
            all_test_true.append(test_y)
            
            # Clean up
            del test_X, test_y, test_preds
            gc.collect()
            
            # Save intermediate results
            if (test_batch_idx + 1) % 5 == 0 or test_batch_idx == num_test_batches - 1:
                # Calculate metrics so far
                current_metrics = {k: v / test_metrics['samples'] for k, v in test_metrics.items() if k != 'samples'}
                current_metrics['batches_processed'] = test_batch_idx + 1
                
                # Save intermediate metrics
                with open(os.path.join(output_dir, "evaluation_progress.json"), "w") as f:
                    json.dump(current_metrics, f, indent=2)
        
        # Combine results
        all_test_predictions = np.concatenate(all_test_predictions)
        all_test_true = np.concatenate(all_test_true)
        
        # Calculate final test metrics
        final_metrics = {k: v / test_metrics['samples'] for k, v in test_metrics.items() if k != 'samples'}
        
        # Calculate additional metrics
        test_preds_binary = (all_test_predictions > 0.5).astype(int)
        report = classification_report(all_test_true, test_preds_binary, output_dict=True)
        conf_matrix = confusion_matrix(all_test_true, test_preds_binary)
        
        # Combine all metrics
        results = {
            'metrics': final_metrics,
            'classification_report': report,
            'confusion_matrix': conf_matrix.tolist(),
            'num_samples': len(all_test_true)
        }
        
        # Generate ROC curve data (for later plotting)
        fpr, tpr, _ = roc_curve(all_test_true, all_test_predictions)
        roc_data = {'fpr': fpr.tolist(), 'tpr': tpr.tolist()}
        
        # Save results
        with open(os.path.join(output_dir, 'evaluation_results.json'), 'w') as f:
            json.dump(results, f, indent=2)
        
        with open(os.path.join(output_dir, 'roc_curve_data.json'), 'w') as f:
            json.dump(roc_data, f, indent=2)
        
        # Save a small subset of predictions for analysis (avoid storing all)
        sample_size = min(1000, len(all_test_predictions))
        sample_indices = np.random.choice(len(all_test_predictions), sample_size, replace=False)
        
        sample_data = {
            'predictions': all_test_predictions[sample_indices].tolist(),
            'true_labels': all_test_true[sample_indices].tolist(),
            'indices': test_indices[sample_indices].tolist()
        }
        
        with open(os.path.join(output_dir, 'sample_predictions.json'), 'w') as f:
            json.dump(sample_data, f, indent=2)
        
        # Create a visualization
        plt.figure(figsize=(15, 5))
        
        # ROC Curve
        plt.subplot(131)
        plt.plot(fpr, tpr, label=f'AUC = {final_metrics["auc"]:.3f}')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve')
        plt.legend()
        
        # Confusion Matrix
        plt.subplot(132)
        plt.imshow(conf_matrix, interpolation='nearest', cmap='Blues')
        plt.title('Confusion Matrix')
        plt.colorbar()
        for i in range(conf_matrix.shape[0]):
            for j in range(conf_matrix.shape[1]):
                plt.text(j, i, str(conf_matrix[i, j]),
                         ha="center", va="center", color="white" if conf_matrix[i, j] > conf_matrix.max()/2 else "black")
        plt.xlabel('Predicted Label')
        plt.ylabel('True Label')
        
        # Precision-Recall
        plt.subplot(133)
        precision, recall, _ = precision_recall_curve(all_test_true, all_test_predictions)
        avg_precision = average_precision_score(all_test_true, all_test_predictions)
        plt.plot(recall, precision, label=f'AP = {avg_precision:.3f}')
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curve')
        plt.legend()
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'evaluation_summary.png'), dpi=100)
        plt.close()
        
        logging.info("Evaluation completed successfully")
        return results
        
    except Exception as e:
        logging.error(f"Error during evaluation: {e}")
        logging.error(traceback.format_exc())
        return {'error': str(e)}


def main_zero_curtain_analysis(
    feather_path, 
    output_base_dir, 
    batch_size=16, 
    start_chunk=0,
    max_chunks=None,
    memory_limit_gb=None,
    use_cpu_only=False
):
    """
    Memory-optimized zero curtain analysis pipeline.
    
    Parameters:
    -----------
    feather_path : str
        Path to input feather file
    output_base_dir : str
        Base directory for outputs
    batch_size : int
        Batch size for training
    start_chunk : int
        Chunk to resume training from
    max_chunks : int, optional
        Maximum number of chunks to process
    memory_limit_gb : float, optional
        Memory limit in GB
    
    Returns:
    --------
    dict
        Analysis results
    """

    if use_cpu_only:
        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
        print("Forcing CPU-only mode as requested")
    
    # Create base output directory
    os.makedirs(output_base_dir, exist_ok=True)
    
    # Configure logging
    log_file = os.path.join(output_base_dir, 'pipeline_log.txt')
    logging.basicConfig(
        filename=log_file,
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s: %(message)s',
        filemode='a'  # Append mode for resuming
    )

    # Console handler for better visibility
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    logging.getLogger('').addHandler(console_handler)
    
    try:
        # Log start time and memory
        start_time = time.time()
        initial_memory = memory_usage()
        logging.info(f"Starting zero curtain analysis pipeline")
        logging.info(f"Initial memory usage: {initial_memory:.2f} MB")
        
        # Configure TensorFlow - do this early
        tf_config = configure_tensorflow_memory()
        logging.info(f"TensorFlow configuration: {tf_config}")
        
        # Verify input files exist
        logging.info(f"Verifying input file: {feather_path}")
        if not os.path.exists(feather_path):
            raise FileNotFoundError(f"Input file not found: {feather_path}")
        
        if not os.path.exists(feather_path):
            raise FileNotFoundError(f"Input file not found: {feather_path}")
        
        # Verify prepared data files
        data_dir = os.path.join(output_base_dir, 'ml_data')
        X_file = os.path.join(data_dir, 'X_features.npy')
        y_file = os.path.join(data_dir, 'y_labels.npy')
        
        if not (os.path.exists(X_file) and os.path.exists(y_file)):
            logging.error("Prepared data files not found. Please run data preparation first.")
            raise FileNotFoundError("Prepared data files not found")
        
        # Load split indices
        split_indices_path = os.path.join(output_base_dir, 'checkpoints/spatiotemporal_split.pkl')
        
        try:
            with open(split_indices_path, 'rb') as f:
                split_data = pickle.load(f)
            
            train_indices = split_data["train_indices"]
            val_indices = split_data["val_indices"]
            test_indices = split_data["test_indices"]
            
            logging.info(f"Loaded split indices - Train: {len(train_indices)}, Val: {len(val_indices)}, Test: {len(test_indices)}")
        except Exception as e:
            logging.error(f"Error loading split indices: {e}")
            raise
        
        # Prepare class weights
        y_mmap = np.load(y_file, mmap_mode='r')
        train_y = np.array([y_mmap[idx] for idx in train_indices[:5000]])  # Sample for class balance
        del y_mmap
        
        pos_weight = (len(train_y) - np.sum(train_y)) / max(1, np.sum(train_y))
        class_weight = {0: 1.0, 1: pos_weight}
        logging.info(f"Class weights calculated: {class_weight}")
        
        # Get input shape
        X_mmap = np.load(X_file, mmap_mode='r')
        input_shape = X_mmap[train_indices[0]].shape
        del X_mmap
        
        logging.info(f"Input shape: {input_shape}")

        # Force CPU for model building to save GPU memory
        with tf.device('/CPU:0'):
            # Build model with reduced complexity
            model = build_advanced_zero_curtain_model(input_shape)
            model.summary(print_fn=logging.info)
        
        # Output directory for this run
        run_output_dir = os.path.join(output_base_dir, 'efficient_model')
        os.makedirs(run_output_dir, exist_ok=True)
        
        # Train model with memory optimization
        trained_model, best_model_path = memory_optimized_training(
            model, 
            X_file, 
            y_file,
            train_indices, 
            val_indices, 
            test_indices,
            output_dir=run_output_dir,
            batch_size=batch_size,
            chunk_size=2500,  # Smaller chunks for memory efficiency
            epochs_per_chunk=2,
            save_frequency=10,
            class_weight=class_weight,
            start_chunk=start_chunk,
            max_chunks=max_chunks,
            memory_limit_gb=memory_limit_gb,
            use_cpu_only=use_cpu_only
        )
        
        # Clear model from memory
        del model
        tf.keras.backend.clear_session()
        gc.collect()
        
        # # Load best model for evaluation
        # if best_model_path and os.path.exists(best_model_path):
        #     logging.info(f"Loading best model from {best_model_path}")
        #     best_model = tf.keras.models.load_model(best_model_path)
            
        #     # Create evaluation directory
        #     eval_dir = os.path.join(run_output_dir, 'evaluation')
        #     os.makedirs(eval_dir, exist_ok=True)
            
        #     # Evaluate on test set
        #     results = evaluate_model_performance(
        #         best_model,
        #         X_file,
        #         y_file,
        #         test_indices,
        #         output_dir=eval_dir,
        #         batch_size=batch_size
        #     )
            
        #     # Clear model
        #     del best_model
        #     gc.collect()
        # else:
        #     logging.warning("Best model not found, skipping evaluation")
        #     results = {'error': 'Best model not found'}
        
        # Log completion
        total_time = time.time() - start_time
        final_memory = memory_usage()
        memory_increase = final_memory - initial_memory
        
        logging.info(f"Pipeline completed in {total_time/3600:.2f} hours")
        logging.info(f"Memory usage: {final_memory:.2f} MB (increase: {memory_increase:.2f} MB)")
        
        # Return summary of results
        return {
            'status': 'completed',
            'time_hours': total_time/3600,
            'best_model_path': best_model_path
            #'evaluation_results': results
        }
        
    except Exception as e:
        logging.error(f"Error in pipeline: {e}")
        logging.error(traceback.format_exc())
        return {'status': 'failed', 'error': str(e)}


# Example usage
if __name__ == "__main__":
    try:

        # Set memory limit to 80% of available system memory
        #system_memory = psutil.virtual_memory().total / (1024**3)  # GB
        #memory_limit = system_memory * 0.8

        results = main_zero_curtain_analysis(
            feather_path = '/Users/bgay/Desktop/Research/Code/merged_compressed.feather',
            output_base_dir = '/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling',
            batch_size=256,
            start_chunk=0,  # Start from beginning or resume from a specific chunk
            #max_chunks=None,  # Set a limit if needed
            #memory_limit_gb=memory_limit
            #memory_limit_gb=None  # Will use 70% of system memory by default
            use_cpu_only=True
        )
        
        print("Analysis complete!")
        print(f"Best model saved at: {results.get('best_model_path', 'Not available')}")    
        
        # Print evaluation summary if available
        #if 'evaluation_results' in results and 'metrics' in results['evaluation_results']:
        #    metrics = results['evaluation_results']['metrics']
        #    print(f"Test AUC: {metrics.get('auc', 'N/A'):.4f}")
        #    print(f"Test Accuracy: {metrics.get('accuracy', 'N/A'):.4f}")
        
    except Exception as e:
        print(f"Pipeline failed: {e}")
        traceback.print_exc()

#!/usr/bin/env python
# coding: utf-8

"""
Zero Curtain Detection Model with Memory-Optimized Training Pipeline
Author: [REDACTED_NAME]
"""

# Force CPU-only mode if needed
import os
# Uncomment the next line to force CPU only:
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

# Basic imports
import gc
import json
import logging
import pickle
import numpy as np
import tensorflow as tf
import psutil
import time
import traceback
from datetime import datetime
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s: %(message)s',
)

# Make sure we have a logger
logger = logging.getLogger(__name__)

# ML-related imports
from sklearn.metrics import (
    classification_report, 
    confusion_matrix, 
    roc_curve, 
    auc,
    roc_auc_score,
    precision_recall_curve, 
    average_precision_score
)
import matplotlib.pyplot as plt

# TensorFlow setup - Do this early to catch configuration issues
print(f"TensorFlow version: {tf.__version__}")
print(f"Devices available: {tf.config.list_physical_devices()}")
print(f"Eager execution: {tf.executing_eagerly()}")


def memory_usage():
    """Monitor memory usage in MB"""
    process = psutil.Process(os.getpid())
    mem_info = process.memory_info()
    return mem_info.rss / 1024**2  # Memory in MB


def aggressive_memory_cleanup():
    """Ultra-aggressive memory cleanup to reclaim memory"""
    # Clear Keras backend
    tf.keras.backend.clear_session()
    
    # Garbage collection with multiple passes
    for _ in range(3):
        gc.collect()
    
    # Optional: Release memory back to system on Linux
    try:
        import ctypes
        libc = ctypes.CDLL('libc.so.6')
        libc.malloc_trim(0)
    except:
        pass

    return memory_usage()


def get_memory_summary():
    """Get detailed memory usage summary"""
    try:
        # Process memory
        process = psutil.Process(os.getpid())
        mem_info = process.memory_info()
        
        # System memory
        system = psutil.virtual_memory()
        
        # GPU memory if available
        gpu_memory = "N/A"
        try:
            import subprocess
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'], 
                stdout=subprocess.PIPE
            )
            gpu_memory = result.stdout.decode('utf-8').strip()
        except:
            pass
        
        return {
            'process_rss_mb': mem_info.rss / (1024**2),
            'process_vms_mb': mem_info.vms / (1024**2),
            'system_available_mb': system.available / (1024**2),
            'system_percent': system.percent,
            'gpu_memory': gpu_memory
        }
    except:
        return {'error': 'Could not get memory summary'}


def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
    """
    Advanced zero curtain detection model with optimized memory footprint.
    
    Parameters:
    -----------
    input_shape : tuple
        Shape of input data (sequence_length, num_features)
    include_moisture : bool
        Whether soil moisture features are included
    """
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
    from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D
    from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.regularizers import l2
    
    # Regularization parameters - increased for better generalization and reduced memory
    l2_lambda = 1e-4
    dropout_rate = 0.35
    
    # Input layer
    inputs = Input(shape=input_shape)
    
    # Reshape for ConvLSTM (add spatial dimension)
    x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
    # ConvLSTM layer with simplified architecture
    from tensorflow.keras.layers import ConvLSTM2D
    convlstm = ConvLSTM2D(
        filters=24,  # Reduced from 32 to save more memory
        kernel_size=(3, 1),
        padding='same',
        return_sequences=True,
        activation='tanh',
        recurrent_dropout=dropout_rate,
        kernel_regularizer=l2(l2_lambda)
    )(x)
    
    # Reshape back to (sequence_length, features)
    convlstm = Reshape((input_shape[0], 24))(convlstm)
    
    # Simplified positional encoding
    def positional_encoding(length, depth):
        positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
        angle_rates = 1 / tf.pow(10000.0, depths)
        angle_rads = positions * angle_rates
        
        return tf.sin(angle_rads)
    
    # Add positional encoding
    pos_encoding = positional_encoding(input_shape[0], 24)
    transformer_input = convlstm + pos_encoding
    
    # Simplified transformer encoder block
    def transformer_encoder(x, num_heads=3, key_dim=24, ff_dim=48):  # Reduced dimensions
        # Multi-head attention with regularization
        attention_output = MultiHeadAttention(
            num_heads=num_heads, 
            key_dim=key_dim,
            kernel_regularizer=l2(l2_lambda)
        )(x, x)
        
        # Skip connection with dropout and regularization
        x1 = Add()([attention_output, x])
        x1 = Dropout(dropout_rate)(x1)
        x1 = LayerNormalization(epsilon=1e-6)(x1)
        
        # Simplified feed-forward network
        ff_output = Dense(
            ff_dim, 
            activation='relu', 
            kernel_regularizer=l2(l2_lambda)
        )(x1)
        ff_output = Dropout(dropout_rate)(ff_output)
        ff_output = Dense(
            24,  # Match input dimension
            kernel_regularizer=l2(l2_lambda)
        )(ff_output)
        
        # Skip connection
        x2 = Add()([ff_output, x1])
        return LayerNormalization(epsilon=1e-6)(x2)
    
    # Apply transformer encoder
    transformer_output = transformer_encoder(transformer_input)
    
    # Simplified CNN paths with regularization - reduced number of paths
    cnn_paths = []
    for kernel_size in [3, 7]:  # Removed the middle kernel size (5)
        cnn = Conv1D(
            filters=16,  # Reduced filters
            kernel_size=kernel_size, 
            padding='same', 
            activation='relu',
            kernel_regularizer=l2(l2_lambda)
        )(inputs)
        cnn = BatchNormalization()(cnn)
        cnn = Dropout(dropout_rate)(cnn)
        cnn_paths.append(GlobalMaxPooling1D()(cnn))
    
    # Variational Autoencoder components with adjustable weight
    def sampling(args):
        z_mean, z_log_var = args
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
    # Global temporal features
    global_max = GlobalMaxPooling1D()(transformer_output)
    global_avg = GlobalAveragePooling1D()(transformer_output)
    
    # VAE encoding with regularization
    z_mean = Dense(
        16,  # Reduced from 24
        kernel_regularizer=l2(l2_lambda)
    )(Concatenate()([global_max, global_avg]))
    z_log_var = Dense(
        16,  # Reduced from 24
        kernel_regularizer=l2(l2_lambda)
    )(Concatenate()([global_max, global_avg]))
    z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
    # Combine features
    merged_features = Concatenate()(
        cnn_paths + [global_max, global_avg, z]
    )
    
    # Simplified classification layers with dropout and regularization
    x = Dense(
        48,  # Reduced from 64
        activation='relu', 
        kernel_regularizer=l2(l2_lambda)
    )(merged_features)
    x = Dropout(dropout_rate)(x)
    x = BatchNormalization()(x)
    
    x = Dense(
        24,  # Reduced from 32
        activation='relu', 
        kernel_regularizer=l2(l2_lambda)
    )(x)
    x = Dropout(0.25)(x)
    
    # Output layer
    outputs = Dense(1, activation='sigmoid')(x)
    
    # Create model
    model = Model(inputs=inputs, outputs=outputs)
    
    # Adjustable VAE loss weight
    vae_weight = 0.001
    kl_loss = -0.5 * tf.reduce_mean(
        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
    )
    model.add_loss(vae_weight * kl_loss)
    
    # Compile with gradient clipping
    optimizer = Adam(
        learning_rate=0.001, 
        clipnorm=1.0
    )
    
    model.compile(
        optimizer=optimizer,
        loss='binary_crossentropy',
        metrics=[
            'accuracy',
            tf.keras.metrics.AUC(name='auc'),
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall')
        ]
    )
    
    return model


def memory_optimized_training(
    model, 
    X_file, 
    y_file, 
    train_indices, 
    val_indices, 
    test_indices,
    output_dir, 
    batch_size=16, 
    chunk_size=2500,  # Smaller chunk size
    epochs_per_chunk=1,  # Reduced epochs per chunk
    save_frequency=5, 
    class_weight=None, 
    start_chunk=0,
    max_chunks=None  # Optional limit on number of chunks to process
):
    """
    Memory-optimized training function with strategic checkpointing.
    
    Parameters:
    -----------
    model : tf.keras.Model
        Pre-compiled model
    X_file, y_file : str
        Paths to feature and label files
    train_indices, val_indices, test_indices : array
        Training, validation, and test indices
    output_dir : str
        Directory to save results
    batch_size : int
        Batch size for training
    chunk_size : int
        Number of samples to process at once
    epochs_per_chunk : int
        Epochs to train each chunk
    save_frequency : int
        Save model every N chunks
    class_weight : dict, optional
        Class weights for handling imbalanced data
    start_chunk : int
        Chunk to resume training from
    max_chunks : int, optional
        Maximum number of chunks to process
    
    Returns:
    --------
    tuple
        Trained model and final model path
    """
    # Create output directories
    os.makedirs(output_dir, exist_ok=True)
    checkpoints_dir = os.path.join(output_dir, "checkpoints")
    os.makedirs(checkpoints_dir, exist_ok=True)
    metrics_dir = os.path.join(output_dir, "metrics")
    os.makedirs(metrics_dir, exist_ok=True)
    
    # Configure logging
    log_file = os.path.join(output_dir, 'training_log.txt')
    file_handler = logging.FileHandler(log_file, mode='a')
    file_handler.setLevel(logging.INFO)
    logger.addHandler(file_handler)
    
    # Memory limit setting - use 70% of system memory
    memory_limit_gb = psutil.virtual_memory().total / (1024**3) * 0.7
    logger.info(f"Memory limit set to {memory_limit_gb:.2f} GB")
    
    # Log start info
    logger.info("=" * 50)
    logger.info(f"Starting/resuming training from chunk {start_chunk}")
    logger.info(f"Memory usage at start: {memory_usage():.2f} MB")
    
    # Process in chunks
    num_chunks = int(np.ceil(len(train_indices) / chunk_size))
    if max_chunks is not None:
        num_chunks = min(num_chunks, start_chunk + max_chunks)
    
    logger.info(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
    
    # Prepare small validation set (to reduce memory footprint)
    val_limit = min(1000, len(val_indices))
    val_indices_subset = val_indices[:val_limit]
    
    # Create validation dataset once
    logger.info(f"Preparing validation dataset with {val_limit} samples")
    X_mmap = np.load(X_file, mmap_mode='r')
    val_X = np.array([X_mmap[idx] for idx in val_indices_subset], dtype=np.float32)
    del X_mmap
    
    y_mmap = np.load(y_file, mmap_mode='r')
    val_y = np.array([y_mmap[idx] for idx in val_indices_subset], dtype=np.float32)
    del y_mmap
    
    # Track metrics across chunks
    if start_chunk > 0:
        # Try to load existing history
        history_path = os.path.join(metrics_dir, "training_history.json")
        try:
            with open(history_path, "r") as f:
                training_history = json.load(f)
            logger.info(f"Loaded existing training history with {len(training_history['chunks'])} records")
        except:
            logger.warning("Could not load existing history, starting fresh")
            training_history = {'chunks': [], 'validation_metrics': [], 'model_checkpoints': []}
    else:
        training_history = {'chunks': [], 'validation_metrics': [], 'model_checkpoints': []}
    
    # Early stopping tracker
    patience = 5  # Number of chunks with no improvement
    best_val_auc = 0
    patience_counter = 0
    best_model_path = None
    
    # Record starting time
    start_time = time.time()
    
    # Recovery file for safe resumption
    recovery_file = os.path.join(output_dir, "last_completed_chunk.txt")
    
    # Process each chunk
    for chunk_idx in range(start_chunk, num_chunks):
        # Memory check before processing chunk
        current_memory_gb = psutil.Process(os.getpid()).memory_info().rss / (1024**3)
        logger.info(f"Memory before chunk {chunk_idx+1}: {current_memory_gb:.2f} GB / {memory_limit_gb:.2f} GB")
        
        if current_memory_gb > memory_limit_gb:
            logger.warning(f"Memory limit exceeded ({current_memory_gb:.2f} GB > {memory_limit_gb:.2f} GB). Stopping.")
            break
        
        # Get chunk indices
        start_idx = chunk_idx * chunk_size
        end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
        chunk_indices = train_indices[start_idx:end_idx]
        
        # Log chunk information
        logger.info(f"Processing chunk {chunk_idx+1}/{num_chunks} with {len(chunk_indices)} samples")
        
        # Load chunk data with memory mapping
        X_mmap = np.load(X_file, mmap_mode='r')
        y_mmap = np.load(y_file, mmap_mode='r')
        
        # Create smaller sub-chunks for processing
        sub_chunk_size = 500  # Process in smaller batches
        num_sub_chunks = int(np.ceil(len(chunk_indices) / sub_chunk_size))
        
        # Process each sub-chunk
        sub_chunk_histories = []
        for sub_idx in range(num_sub_chunks):
            # Get sub-chunk indices
            sub_start = sub_idx * sub_chunk_size
            sub_end = min((sub_idx + 1) * sub_chunk_size, len(chunk_indices))
            sub_indices = chunk_indices[sub_start:sub_end]
            
            # Load sub-chunk
            sub_X = np.array([X_mmap[idx] for idx in sub_indices], dtype=np.float32)
            sub_y = np.array([y_mmap[idx] for idx in sub_indices], dtype=np.float32)
            
            # Train on sub-chunk
            history = model.fit(
                sub_X, sub_y,
                validation_data=(val_X, val_y),
                epochs=1,  # Train just one epoch per sub-chunk
                batch_size=batch_size,
                class_weight=class_weight,
                verbose=1
            )
            
            # Save sub-chunk metrics
            sub_chunk_histories.append({
                k: [float(v) for v in vals] for k, vals in history.history.items()
            })
            
            # Clear sub-chunk data
            del sub_X, sub_y
            gc.collect()
        
        # Clear memory-mapped arrays
        del X_mmap, y_mmap
        
        # Combine sub-chunk metrics
        chunk_metrics = {}
        for metric in sub_chunk_histories[0].keys():
            chunk_metrics[metric] = []
            for sub_history in sub_chunk_histories:
                chunk_metrics[metric].extend(sub_history[metric])
        
        # Add to overall training history
        training_history['chunks'].append({
            'chunk': chunk_idx+1,
            'metrics': chunk_metrics
        })
        
        # Evaluate on validation set
        val_scores = model.evaluate(val_X, val_y, verbose=0)
        val_metrics = {
            'chunk': chunk_idx+1,
            'loss': float(val_scores[0]),
            'accuracy': float(val_scores[1]),
            'auc': float(val_scores[2]),
            'precision': float(val_scores[3]),
            'recall': float(val_scores[4])
        }
        training_history['validation_metrics'].append(val_metrics)
        
        # Check for improvement
        current_val_auc = val_metrics['auc']
        logger.info(f"Chunk {chunk_idx+1} validation AUC: {current_val_auc:.4f}")
        
        if current_val_auc > best_val_auc:
            best_val_auc = current_val_auc
            patience_counter = 0
            
            # Save best model
            best_model_path = os.path.join(checkpoints_dir, "best_model.h5")
            model.save(best_model_path, save_format='h5')
            logger.info(f"New best model with AUC {best_val_auc:.4f} saved to {best_model_path}")
            
            # Track best model checkpoint
            training_history['model_checkpoints'].append({
                'chunk': chunk_idx+1,
                'path': best_model_path,
                'auc': best_val_auc,
                'is_best': True
            })
        else:
            patience_counter += 1
            logger.info(f"No improvement for {patience_counter} chunks (best: {best_val_auc:.4f})")
        
        # Save periodic checkpoints (less frequently)
        if (chunk_idx + 1) % save_frequency == 0:
            checkpoint_path = os.path.join(checkpoints_dir, f"checkpoint_{chunk_idx+1}.h5")
            model.save(checkpoint_path, save_format='h5')
            logger.info(f"Periodic checkpoint saved to {checkpoint_path}")
            
            # Track checkpoint
            training_history['model_checkpoints'].append({
                'chunk': chunk_idx+1,
                'path': checkpoint_path,
                'auc': current_val_auc,
                'is_best': False
            })
            
            # Cleanup old non-best checkpoints
            if len(training_history['model_checkpoints']) > 3:
                # Keep best model and last checkpoint, remove others
                checkpoints_to_remove = []
                for checkpoint in training_history['model_checkpoints'][:-3]:
                    if not checkpoint.get('is_best', False) and os.path.exists(checkpoint['path']):
                        checkpoints_to_remove.append(checkpoint['path'])
                
                # Remove old checkpoints
                for old_checkpoint in checkpoints_to_remove:
                    try:
                        os.remove(old_checkpoint)
                        logger.info(f"Removed old checkpoint: {old_checkpoint}")
                    except:
                        logger.warning(f"Could not remove checkpoint: {old_checkpoint}")
        
        # Save condensed training history periodically
        if (chunk_idx + 1) % (save_frequency // 2) == 0 or chunk_idx == num_chunks - 1:
            try:
                # Save compact version (only last 10 chunks)
                compact_history = {
                    'chunks': training_history['chunks'][-10:],
                    'validation_metrics': training_history['validation_metrics'],
                    'model_checkpoints': training_history['model_checkpoints']
                }
                
                with open(os.path.join(metrics_dir, "training_history.json"), "w") as f:
                    json.dump(compact_history, f, indent=2)
                
                # Update recovery file
                with open(recovery_file, "w") as f:
                    f.write(str(chunk_idx + 1))
                
                logger.info(f"Saved training history and updated recovery file")
            except Exception as e:
                logger.error(f"Error saving history: {e}")
        
        # Visualize progress (sparingly)
        if (chunk_idx + 1) % (save_frequency * 2) == 0 or chunk_idx == num_chunks - 1:
            try:
                # Create simple progress plot
                val_aucs = [m['auc'] for m in training_history['validation_metrics']]
                chunk_nums = [m['chunk'] for m in training_history['validation_metrics']]
                
                plt.figure(figsize=(10, 5))
                plt.plot(chunk_nums, val_aucs, marker='o')
                plt.xlabel('Chunk')
                plt.ylabel('Validation AUC')
                plt.title('Training Progress')
                plt.grid(True, alpha=0.3)
                plt.savefig(os.path.join(metrics_dir, "training_progress.png"), dpi=100)
                plt.close()
                
                logger.info(f"Generated progress visualization")
            except Exception as e:
                logger.error(f"Error creating visualization: {e}")
        
        # Check for early stopping
        if patience_counter >= patience:
            logger.info(f"Early stopping after {patience} chunks with no improvement")
            break
        
        # Aggressive cleanup
        aggressive_memory_cleanup()
    
    # Training complete
    training_time = time.time() - start_time
    logger.info(f"Training completed in {training_time/3600:.2f} hours")
    
    # Save final training metrics
    final_metrics_path = os.path.join(metrics_dir, "final_metrics.json")
    with open(final_metrics_path, "w") as f:
        json.dump({
            'training_time_hours': training_time/3600,
            'best_val_auc': best_val_auc,
            'chunks_completed': chunk_idx + 1,
            'best_model_path': best_model_path
        }, f, indent=2)
    
    # Remove file handler to avoid duplicate logging
    logger.removeHandler(file_handler)
    
    return model, best_model_path

# def main_zero_curtain_analysis(
#     feather_path, 
#     output_base_dir, 
#     batch_size=16, 
#     start_chunk=0,
#     max_chunks=None
# ):
#     """
#     Memory-optimized zero curtain analysis pipeline.
    
#     Parameters:
#     -----------
#     feather_path : str
#         Path to input feather file
#     output_base_dir : str
#         Base directory for outputs
#     batch_size : int
#         Batch size for training
#     start_chunk : int
#         Chunk to resume training from
#     max_chunks : int, optional
#         Maximum number of chunks to process
    
#     Returns:
#     --------
#     dict
#         Analysis results
#     """
#     # Create base output directory
#     os.makedirs(output_base_dir, exist_ok=True)
    
#     # Configure file logging
#     log_file = os.path.join(output_base_dir, 'pipeline_log.txt')
#     file_handler = logging.FileHandler(log_file, mode='a')
#     file_handler.setLevel(logging.INFO)
#     logger.addHandler(file_handler)
    
#     try:
#         # Log start time and memory
#         start_time = time.time()
#         initial_memory = memory_usage()
#         logger.info(f"Starting zero curtain analysis pipeline")
#         logger.info(f"Initial memory usage: {initial_memory:.2f} MB")
        
#         # Verify input files exist
#         logger.info(f"Verifying input file: {feather_path}")
#         if not os.path.exists(feather_path):
#             raise FileNotFoundError(f"Input file not found: {feather_path}")
        
#         # Verify prepared data files
#         data_dir = os.path.join(output_base_dir, 'ml_data')
#         X_file = os.path.join(data_dir, 'X_features.npy')
#         y_file = os.path.join(data_dir, 'y_labels.npy')
        
#         if not (os.path.exists(X_file) and os.path.exists(y_file)):
#             logger.error("Prepared data files not found. Please run data preparation first.")
#             raise FileNotFoundError("Prepared data files not found")
        
#         # Load split indices - adjust the path as needed
#         split_indices_path = os.path.join(output_base_dir, 'checkpoints/spatiotemporal_split.pkl')
        
#         try:
#             with open(split_indices_path, 'rb') as f:
#                 split_data = pickle.load(f)
            
#             train_indices = split_data["train_indices"]
#             val_indices = split_data["val_indices"]
#             test_indices = split_data["test_indices"]
            
#             logger.info(f"Loaded split indices - Train: {len(train_indices)}, Val: {len(val_indice...
#         except Exception as e:
#             logger.error(f"Error loading split indices: {e}")
#             raise
        
#         # Prepare class weights
#         y_mmap = np.load(y_file, mmap_mode='r')
#         train_y = np.array([y_mmap[idx] for idx in train_indices[:5000]])  # Sample for class bala...
#         del y_mmap
        
#         pos_weight = (len(train_y) - np.sum(train_y)) / max(1, np.sum(train_y))
#         class_weight = {0: 1.0, 1: float(pos_weight)}
#         logger.info(f"Class weights calculated: {class_weight}")
        
#         # Get input shape
#         X_mmap = np.load(X_file, mmap_mode='r')
#         input_shape = X_mmap[train_indices[0]].shape
#         del X_mmap
        
#         logger.info(f"Input shape: {input_shape}")
        
#         # Build model with reduced complexity
#         model = build_advanced_zero_curtain_model(input_shape)
        
#         # Output directory for this run
#         run_output_dir = os.path.join(output_base_dir, 'efficient_model')
#         os.makedirs(run_output_dir, exist_ok=True)
        
#         # Train model with memory optimization
#         trained_model, best_model_path = memory_optimized_training(
#             model, 
#             X_file, 
#             y_file,
#             train_indices, 
#             val_indices, 
#             test_indices,
#             output_dir=run_output_dir,
#             batch_size=batch_size,
#             chunk_size=2500,  # Smaller chunks for memory efficiency
#             epochs_per_chunk=1,
#             save_frequency=5,
#             class_weight=class_weight,
#             start_chunk=start_chunk,
#             max_chunks=max_chunks
#         )
        
#         # Clear model from memory
#         del model
#         tf.keras.backend.clear_session()
#         gc.collect()
        
#         # Log completion
#         total_time = time.time() - start_time
#         final_memory = memory_usage()
#         memory_increase = final_memory - initial_memory
        
#         logger.info(f"Pipeline completed in {total_time/3600:.2f} hours")
#         logger.info(f"Memory usage: {final_memory:.2f} MB (increase: {memory_increase:.2f} MB)")
        
#         # Remove file handler to avoid duplicate logging
#         logger.removeHandler(file_handler)
        
#         # Return summary of results
#         return {
#             'status': 'completed',
#             'time_hours': total_time/3600,
#             'best_model_path': best_model_path
#         }
        
#     except Exception as e:
#         logger.error(f"Error in pipeline: {e}")
#         logger.error(traceback.format_exc())
#         # Remove file handler even on error
#         logger.removeHandler(file_handler)
#         return {'status': 'failed', 'error': str(e)}


# # Example usage
# if __name__ == "__main__":
#     try:
#         results = main_zero_curtain_analysis(
#             feather_path = '/Users/bgay/Desktop/Research/Code/merged_compressed.feather',
#             output_base_dir = '/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling',
#             batch_size=16,
#             start_chunk=0,  # Start from beginning or resume from a specific chunk
#             max_chunks=None  # Process all chunks
#         )
        
#         print("Analysis complete!")
#         print(f"Best model saved at: {results.get('best_model_path', 'Not available')}")    
        
#         # Print evaluation summary if available
#         #if 'evaluation_results' in results and 'metrics' in results['evaluation_results']:
#         #    metrics = results['evaluation_results']['metrics']
#         #    print(f"Test AUC: {metrics.get('auc', 'N/A'):.4f}")
#         #    print(f"Test Accuracy: {metrics.get('accuracy', 'N/A'):.4f}")
        
#     except Exception as e:
#         print(f"Pipeline failed: {e}")
#         traceback.print_exc()

def create_balanced_validation_set(X_file, y_file, val_indices, max_samples=1000):
    """
    Create a balanced validation set with equal class representation.
    
    Parameters:
    -----------
    X_file : str
        Path to features numpy file
    y_file : str
        Path to labels numpy file
    val_indices : array
        All validation indices
    max_samples : int
        Maximum size of the validation set (will contain max_samples/2 of each class)
    
    Returns:
    --------
    tuple
        (val_X, val_y) - Balanced validation data
    """
    import numpy as np
    import logging
    
    logger = logging.getLogger(__name__)
    logger.info(f"Creating balanced validation set with up to {max_samples} samples")
    
    # Load all validation labels
    y_mmap = np.load(y_file, mmap_mode='r')
    val_y_all = np.array([y_mmap[idx] for idx in val_indices])
    
    # Find positive and negative indices
    pos_indices = np.where(val_y_all == 1)[0]
    neg_indices = np.where(val_y_all == 0)[0]
    
    logger.info(f"Found {len(pos_indices)} positive and {len(neg_indices)} negative samples in validation set")
    
    # Calculate how many of each class to include
    samples_per_class = max_samples // 2
    
    # Select samples (use all positives if fewer than samples_per_class)
    if len(pos_indices) <= samples_per_class:
        selected_pos = pos_indices
        logger.warning(f"Using all {len(pos_indices)} positive samples (fewer than requested {samples_per_class})")
    else:
        # Randomly select positive samples
        selected_pos = np.random.choice(pos_indices, samples_per_class, replace=False)
    
    # Select equal number of negative samples
    selected_neg = np.random.choice(neg_indices, len(selected_pos), replace=False)
    
    # Combine indices and shuffle
    selected_indices = np.concatenate([selected_pos, selected_neg])
    np.random.shuffle(selected_indices)
    
    # Map back to original validation indices
    balanced_val_indices = [val_indices[idx] for idx in selected_indices]
    
    # Load features for selected samples
    X_mmap = np.load(X_file, mmap_mode='r')
    val_X = np.array([X_mmap[idx] for idx in balanced_val_indices], dtype=np.float32)
    val_y = np.array([y_mmap[idx] for idx in balanced_val_indices], dtype=np.float32)
    
    # Clean up
    del X_mmap, y_mmap
    
    logger.info(f"Created balanced validation set with {len(val_X)} samples")
    logger.info(f"Class distribution: {np.sum(val_y)} positive, {len(val_y) - np.sum(val_y)} negative")
    
    return val_X, val_y


def custom_metrics():
    """
    Create metrics with adjusted thresholds for imbalanced data.
    
    Returns:
    --------
    list
        List of metrics to use
    """
    import tensorflow as tf
    
    # Standard metrics
    metrics = [
        'accuracy',
        tf.keras.metrics.AUC(name='auc', curve='ROC'),
    ]
    
    # Add metrics with multiple thresholds for precision and recall
    thresholds = [0.1, 0.3, 0.5]
    for threshold in thresholds:
        metrics.extend([
            tf.keras.metrics.Precision(name=f'precision_{int(threshold*100)}', thresholds=threshold),
            tf.keras.metrics.Recall(name=f'recall_{int(threshold*100)}', thresholds=threshold)
        ])
    
    return metrics


def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
    """
    Advanced zero curtain detection model with optimized memory footprint and adjusted metrics.
    
    Parameters:
    -----------
    input_shape : tuple
        Shape of input data (sequence_length, num_features)
    include_moisture : bool
        Whether soil moisture features are included
    """
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
    from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D
    from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.regularizers import l2
    
    # Regularization parameters - increased for better generalization and reduced memory
    l2_lambda = 1e-4
    dropout_rate = 0.35
    
    # Input layer
    inputs = Input(shape=input_shape)
    
    # Reshape for ConvLSTM (add spatial dimension)
    x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
    # ConvLSTM layer with simplified architecture
    from tensorflow.keras.layers import ConvLSTM2D
    convlstm = ConvLSTM2D(
        filters=24,  # Reduced from 32 to save more memory
        kernel_size=(3, 1),
        padding='same',
        return_sequences=True,
        activation='tanh',
        recurrent_dropout=dropout_rate,
        kernel_regularizer=l2(l2_lambda)
    )(x)
    
    # Reshape back to (sequence_length, features)
    convlstm = Reshape((input_shape[0], 24))(convlstm)
    
    # Simplified positional encoding
    def positional_encoding(length, depth):
        positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
        angle_rates = 1 / tf.pow(10000.0, depths)
        angle_rads = positions * angle_rates
        
        return tf.sin(angle_rads)
    
    # Add positional encoding
    pos_encoding = positional_encoding(input_shape[0], 24)
    transformer_input = convlstm + pos_encoding
    
    # Simplified transformer encoder block
    def transformer_encoder(x, num_heads=3, key_dim=24, ff_dim=48):  # Reduced dimensions
        # Multi-head attention with regularization
        attention_output = MultiHeadAttention(
            num_heads=num_heads, 
            key_dim=key_dim,
            kernel_regularizer=l2(l2_lambda)
        )(x, x)
        
        # Skip connection with dropout and regularization
        x1 = Add()([attention_output, x])
        x1 = Dropout(dropout_rate)(x1)
        x1 = LayerNormalization(epsilon=1e-6)(x1)
        
        # Simplified feed-forward network
        ff_output = Dense(
            ff_dim, 
            activation='relu', 
            kernel_regularizer=l2(l2_lambda)
        )(x1)
        ff_output = Dropout(dropout_rate)(ff_output)
        ff_output = Dense(
            24,  # Match input dimension
            kernel_regularizer=l2(l2_lambda)
        )(ff_output)
        
        # Skip connection
        x2 = Add()([ff_output, x1])
        return LayerNormalization(epsilon=1e-6)(x2)
    
    # Apply transformer encoder
    transformer_output = transformer_encoder(transformer_input)
    
    # Simplified CNN paths with regularization - reduced number of paths
    cnn_paths = []
    for kernel_size in [3, 7]:  # Removed the middle kernel size (5)
        cnn = Conv1D(
            filters=16,  # Reduced filters
            kernel_size=kernel_size, 
            padding='same', 
            activation='relu',
            kernel_regularizer=l2(l2_lambda)
        )(inputs)
        cnn = BatchNormalization()(cnn)
        cnn = Dropout(dropout_rate)(cnn)
        cnn_paths.append(GlobalMaxPooling1D()(cnn))
    
    # Variational Autoencoder components with adjustable weight
    def sampling(args):
        z_mean, z_log_var = args
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
    # Global temporal features
    global_max = GlobalMaxPooling1D()(transformer_output)
    global_avg = GlobalAveragePooling1D()(transformer_output)
    
    # VAE encoding with regularization
    z_mean = Dense(
        16,  # Reduced from 24
        kernel_regularizer=l2(l2_lambda)
    )(Concatenate()([global_max, global_avg]))
    z_log_var = Dense(
        16,  # Reduced from 24
        kernel_regularizer=l2(l2_lambda)
    )(Concatenate()([global_max, global_avg]))
    z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
    # Combine features
    merged_features = Concatenate()(
        cnn_paths + [global_max, global_avg, z]
    )
    
    # Simplified classification layers with dropout and regularization
    x = Dense(
        48,  # Reduced from 64
        activation='relu', 
        kernel_regularizer=l2(l2_lambda)
    )(merged_features)
    x = Dropout(dropout_rate)(x)
    x = BatchNormalization()(x)
    
    x = Dense(
        24,  # Reduced from 32
        activation='relu', 
        kernel_regularizer=l2(l2_lambda)
    )(x)
    x = Dropout(0.25)(x)
    
    # Output layer
    outputs = Dense(1, activation='sigmoid')(x)
    
    # Create model
    model = Model(inputs=inputs, outputs=outputs)
    
    # Adjustable VAE loss weight
    vae_weight = 0.001
    kl_loss = -0.5 * tf.reduce_mean(
        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
    )
    model.add_loss(vae_weight * kl_loss)
    
    # Compile with gradient clipping and custom metrics
    optimizer = Adam(
        learning_rate=0.001, 
        clipnorm=1.0
    )
    
    model.compile(
        optimizer=optimizer,
        loss='binary_crossentropy',
        metrics=custom_metrics()
    )
    
    return model


def memory_optimized_training(
    model, 
    X_file, 
    y_file, 
    train_indices, 
    val_indices, 
    test_indices,
    output_dir, 
    batch_size=16, 
    chunk_size=2500,  # Smaller chunk size
    epochs_per_chunk=1,  # Reduced epochs per chunk
    save_frequency=5, 
    class_weight=None, 
    start_chunk=0,
    max_chunks=None  # Optional limit on number of chunks to process
):
    """
    Memory-optimized training function with strategic checkpointing.
    
    Parameters:
    -----------
    model : tf.keras.Model
        Pre-compiled model
    X_file, y_file : str
        Paths to feature and label files
    train_indices, val_indices, test_indices : array
        Training, validation, and test indices
    output_dir : str
        Directory to save results
    batch_size : int
        Batch size for training
    chunk_size : int
        Number of samples to process at once
    epochs_per_chunk : int
        Epochs to train each chunk
    save_frequency : int
        Save model every N chunks
    class_weight : dict, optional
        Class weights for handling imbalanced data
    start_chunk : int
        Chunk to resume training from
    max_chunks : int, optional
        Maximum number of chunks to process
    
    Returns:
    --------
    tuple
        Trained model and final model path
    """
    import os
    import gc
    import json
    import numpy as np
    import tensorflow as tf
    import psutil
    import time
    import logging
    import traceback
    from datetime import datetime
    from sklearn.metrics import roc_auc_score
    import matplotlib.pyplot as plt
    
    # Get logger
    logger = logging.getLogger(__name__)
    
    # Create output directories
    os.makedirs(output_dir, exist_ok=True)
    checkpoints_dir = os.path.join(output_dir, "checkpoints")
    os.makedirs(checkpoints_dir, exist_ok=True)
    metrics_dir = os.path.join(output_dir, "metrics")
    os.makedirs(metrics_dir, exist_ok=True)
    
    # Configure file logging
    log_file = os.path.join(output_dir, 'training_log.txt')
    file_handler = logging.FileHandler(log_file, mode='a')
    file_handler.setLevel(logging.INFO)
    logger.addHandler(file_handler)
    
    # Memory limit setting - use 70% of system memory
    memory_limit_gb = psutil.virtual_memory().total / (1024**3) * 0.7
    logger.info(f"Memory limit set to {memory_limit_gb:.2f} GB")
    
    # Log start info
    logger.info("=" * 50)
    logger.info(f"Starting/resuming training from chunk {start_chunk}")
    current_memory = psutil.Process(os.getpid()).memory_info().rss / (1024**3)
    logger.info(f"Memory usage at start: {current_memory:.2f} GB")
    
    # Process in chunks
    num_chunks = int(np.ceil(len(train_indices) / chunk_size))
    if max_chunks is not None:
        num_chunks = min(num_chunks, start_chunk + max_chunks)
    
    logger.info(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
    
    # Create balanced validation set
    val_X, val_y = create_balanced_validation_set(X_file, y_file, val_indices, max_samples=1000)
    
    # Track metrics across chunks
    if start_chunk > 0:
        # Try to load existing history
        history_path = os.path.join(metrics_dir, "training_history.json")
        try:
            with open(history_path, "r") as f:
                training_history = json.load(f)
            logger.info(f"Loaded existing training history with {len(training_history['chunks'])} records")
        except:
            logger.warning("Could not load existing history, starting fresh")
            training_history = {'chunks': [], 'validation_metrics': [], 'model_checkpoints': []}
    else:
        training_history = {'chunks': [], 'validation_metrics': [], 'model_checkpoints': []}
    
    # Early stopping tracker
    patience = 5  # Number of chunks with no improvement
    best_val_auc = 0
    patience_counter = 0
    best_model_path = None
    
    # Record starting time
    start_time = time.time()
    
    # Recovery file for safe resumption
    recovery_file = os.path.join(output_dir, "last_completed_chunk.txt")
    
    # Process each chunk
    for chunk_idx in range(start_chunk, num_chunks):
        # Memory check before processing chunk
        current_memory_gb = psutil.Process(os.getpid()).memory_info().rss / (1024**3)
        logger.info(f"Memory before chunk {chunk_idx+1}: {current_memory_gb:.2f} GB / {memory_limit_gb:.2f} GB")
        
        if current_memory_gb > memory_limit_gb:
            logger.warning(f"Memory limit exceeded ({current_memory_gb:.2f} GB > {memory_limit_gb:.2f} GB). Stopping.")
            break
        
        # Get chunk indices
        start_idx = chunk_idx * chunk_size
        end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
        chunk_indices = train_indices[start_idx:end_idx]
        
        # Log chunk information
        logger.info(f"Processing chunk {chunk_idx+1}/{num_chunks} with {len(chunk_indices)} samples")
        
        # Load chunk data with memory mapping
        X_mmap = np.load(X_file, mmap_mode='r')
        y_mmap = np.load(y_file, mmap_mode='r')
        
        # Create smaller sub-chunks for processing
        sub_chunk_size = 500  # Process in smaller batches
        num_sub_chunks = int(np.ceil(len(chunk_indices) / sub_chunk_size))
        
        # Process each sub-chunk
        sub_chunk_histories = []
        for sub_idx in range(num_sub_chunks):
            # Get sub-chunk indices
            sub_start = sub_idx * sub_chunk_size
            sub_end = min((sub_idx + 1) * sub_chunk_size, len(chunk_indices))
            sub_indices = chunk_indices[sub_start:sub_end]
            
            # Load sub-chunk
            sub_X = np.array([X_mmap[idx] for idx in sub_indices], dtype=np.float32)
            sub_y = np.array([y_mmap[idx] for idx in sub_indices], dtype=np.float32)
            
            # Train on sub-chunk
            try:
                history = model.fit(
                    sub_X, sub_y,
                    validation_data=(val_X, val_y),
                    epochs=1,  # Train just one epoch per sub-chunk
                    batch_size=batch_size,
                    class_weight=class_weight,
                    verbose=1
                )
                
                # Save sub-chunk metrics
                sub_chunk_histories.append({
                    k: [float(v) for v in vals] for k, vals in history.history.items()
                })
            except Exception as e:
                logger.error(f"Error training sub-chunk {sub_idx+1}: {e}")
                logger.error(traceback.format_exc())
                continue
            
            # Clear sub-chunk data
            del sub_X, sub_y
            gc.collect()
        
        # Clear memory-mapped arrays
        del X_mmap, y_mmap
        
        if not sub_chunk_histories:
            logger.error("No successful training in this chunk. Skipping evaluation.")
            continue
        
        # Combine sub-chunk metrics
        chunk_metrics = {}
        for metric in sub_chunk_histories[0].keys():
            chunk_metrics[metric] = []
            for sub_history in sub_chunk_histories:
                if metric in sub_history:
                    chunk_metrics[metric].extend(sub_history[metric])
        
        # Add to overall training history
        training_history['chunks'].append({
            'chunk': chunk_idx+1,
            'metrics': chunk_metrics
        })
        
        # Evaluate on validation set
        try:
            val_scores = model.evaluate(val_X, val_y, verbose=0)
            val_preds = model.predict(val_X)
            
            # Generate metrics dictionary
            val_metrics = {
                'chunk': chunk_idx+1,
                'loss': float(val_scores[0]),
            }
            
            # Add all metrics from evaluation
            for i, name in enumerate(model.metrics_names[1:], 1):
                val_metrics[name] = float(val_scores[i])
            
            # Add custom AUC calculation as a backup
            val_metrics['sklearn_auc'] = float(roc_auc_score(val_y, val_preds))
            
            # Use the best AUC (from TF or scikit-learn)
            current_val_auc = max(val_metrics.get('auc', 0), val_metrics.get('sklearn_auc', 0))
            val_metrics['best_auc'] = current_val_auc
            
            # Add to training history
            training_history['validation_metrics'].append(val_metrics)
            
            # Log validation metrics
            metric_str = ", ".join([f"{k}: {v:.4f}" for k, v in val_metrics.items() if k not in ['chunk', 'sklearn_auc'] and isinstance(v, (int, float))])
            logger.info(f"Chunk {chunk_idx+1} validation metrics: {metric_str}")
            
            # Check for improvement
            if current_val_auc > best_val_auc:
                best_val_auc = current_val_auc
                patience_counter = 0
                
                # Save best model
                best_model_path = os.path.join(checkpoints_dir, "best_model.h5")
                model.save(best_model_path, save_format='h5')
                logger.info(f"New best model with AUC {best_val_auc:.4f} saved to {best_model_path}")
                
                # Track best model checkpoint
                training_history['model_checkpoints'].append({
                    'chunk': chunk_idx+1,
                    'path': best_model_path,
                    'auc': best_val_auc,
                    'is_best': True
                })
            else:
                patience_counter += 1
                logger.info(f"No improvement for {patience_counter} chunks (best: {best_val_auc:.4f})")
        except Exception as e:
            logger.error(f"Error during validation: {e}")
            logger.error(traceback.format_exc())
            patience_counter += 1
        
        # Save periodic checkpoints (less frequently)
        if (chunk_idx + 1) % save_frequency == 0:
            try:
                checkpoint_path = os.path.join(checkpoints_dir, f"checkpoint_{chunk_idx+1}.h5")
                model.save(checkpoint_path, save_format='h5')
                logger.info(f"Periodic checkpoint saved to {checkpoint_path}")
                
                # Track checkpoint
                training_history['model_checkpoints'].append({
                    'chunk': chunk_idx+1,
                    'path': checkpoint_path,
                    'auc': current_val_auc if 'current_val_auc' in locals() else 0,
                    'is_best': False
                })
                
                # Cleanup old non-best checkpoints
                if len(training_history['model_checkpoints']) > 3:
                    # Keep best model and last checkpoint, remove others
                    checkpoints_to_remove = []
                    for checkpoint in training_history['model_checkpoints'][:-3]:
                        if not checkpoint.get('is_best', False) and os.path.exists(checkpoint['path']):
                            checkpoints_to_remove.append(checkpoint['path'])
                    
                    # Remove old checkpoints
                    for old_checkpoint in checkpoints_to_remove:
                        try:
                            os.remove(old_checkpoint)
                            logger.info(f"Removed old checkpoint: {old_checkpoint}")
                        except:
                            logger.warning(f"Could not remove checkpoint: {old_checkpoint}")
            except Exception as e:
                logger.error(f"Error saving checkpoint: {e}")
        
        # Save condensed training history periodically
        if (chunk_idx + 1) % (save_frequency // 2) == 0 or chunk_idx == num_chunks - 1:
            try:
                # Save compact version (only last 10 chunks)
                compact_history = {
                    'chunks': training_history['chunks'][-10:],
                    'validation_metrics': training_history['validation_metrics'],
                    'model_checkpoints': training_history['model_checkpoints']
                }
                
                with open(os.path.join(metrics_dir, "training_history.json"), "w") as f:
                    json.dump(compact_history, f, indent=2)
                
                # Update recovery file
                with open(recovery_file, "w") as f:
                    f.write(str(chunk_idx + 1))
                
                logger.info(f"Saved training history and updated recovery file")
            except Exception as e:
                logger.error(f"Error saving history: {e}")
        
        # Visualize progress (sparingly)
        if (chunk_idx + 1) % (save_frequency * 2) == 0 or chunk_idx == num_chunks - 1:
            try:
                # Create simple progress plot
                val_aucs = [m.get('best_auc', m.get('auc', 0)) for m in training_history['validation_metrics']]
                chunk_nums = [m['chunk'] for m in training_history['validation_metrics']]
                
                plt.figure(figsize=(10, 5))
                plt.plot(chunk_nums, val_aucs, marker='o')
                plt.xlabel('Chunk')
                plt.ylabel('Validation AUC')
                plt.title('Training Progress')
                plt.grid(True, alpha=0.3)
                plt.savefig(os.path.join(metrics_dir, "training_progress.png"), dpi=100)
                plt.close()
                
                logger.info(f"Generated progress visualization")
            except Exception as e:
                logger.error(f"Error creating visualization: {e}")
        
        # Check for early stopping
        if patience_counter >= patience:
            logger.info(f"Early stopping after {patience} chunks with no improvement")
            break
        
        # Aggressive cleanup
        tf.keras.backend.clear_session()
        for _ in range(3):
            gc.collect()
    
    # Training complete
    training_time = time.time() - start_time
    logger.info(f"Training completed in {training_time/3600:.2f} hours")
    
    # Save final training metrics
    final_metrics_path = os.path.join(metrics_dir, "final_metrics.json")
    with open(final_metrics_path, "w") as f:
        json.dump({
            'training_time_hours': training_time/3600,
            'best_val_auc': best_val_auc,
            'chunks_completed': chunk_idx + 1,
            'best_model_path': best_model_path
        }, f, indent=2)
    
    # Remove file handler to avoid duplicate logging
    logger.removeHandler(file_handler)
    
    return model, best_model_path


def main_zero_curtain_analysis(
    feather_path, 
    output_base_dir, 
    batch_size=16, 
    start_chunk=0,
    max_chunks=None
):
    """
    Memory-optimized zero curtain analysis pipeline with balanced validation.
    
    Parameters:
    -----------
    feather_path : str
        Path to input feather file
    output_base_dir : str
        Base directory for outputs
    batch_size : int
        Batch size for training
    start_chunk : int
        Chunk to resume training from
    max_chunks : int, optional
        Maximum number of chunks to process
    
    Returns:
    --------
    dict
        Analysis results
    """
    import os
    import gc
    import pickle
    import numpy as np
    import tensorflow as tf
    import psutil
    import time
    import logging
    import traceback
    
    # Get logger
    logger = logging.getLogger(__name__)
    
    # Create base output directory
    os.makedirs(output_base_dir, exist_ok=True)
    
    # Configure file logging
    log_file = os.path.join(output_base_dir, 'pipeline_log.txt')
    file_handler = logging.FileHandler(log_file, mode='a')
    file_handler.setLevel(logging.INFO)
    logger.addHandler(file_handler)
    
    try:
        # Log start time and memory
        start_time = time.time()
        initial_memory = psutil.Process(os.getpid()).memory_info().rss / (1024**3)
        logger.info(f"Starting zero curtain analysis pipeline")
        logger.info(f"Initial memory usage: {initial_memory:.2f} GB")
        
        # Verify input files exist
        logger.info(f"Verifying input file: {feather_path}")
        if not os.path.exists(feather_path):
            raise FileNotFoundError(f"Input file not found: {feather_path}")
        
        # Verify prepared data files
        data_dir = os.path.join(output_base_dir, 'ml_data')
        X_file = os.path.join(data_dir, 'X_features.npy')
        y_file = os.path.join(data_dir, 'y_labels.npy')
        
        if not (os.path.exists(X_file) and os.path.exists(y_file)):
            logger.error("Prepared data files not found. Please run data preparation first.")
            raise FileNotFoundError("Prepared data files not found")
        
        # Load split indices - adjust the path as needed
        split_indices_path = os.path.join(output_base_dir, 'checkpoints/spatiotemporal_split.pkl')
        
        try:
            with open(split_indices_path, 'rb') as f:
                split_data = pickle.load(f)
            
            train_indices = split_data["train_indices"]
            val_indices = split_data["val_indices"]
            test_indices = split_data["test_indices"]
            
            logger.info(f"Loaded split indices - Train: {len(train_indices)}, Val: {len(val_indices)}, Test: {len(test_indices)}")
        except Exception as e:
            logger.error(f"Error loading split indices: {e}")
            raise
        
        # Prepare class weights
        y_mmap = np.load(y_file, mmap_mode='r')
        train_y = np.array([y_mmap[idx] for idx in train_indices[:5000]])  # Sample for class balance
        del y_mmap
        
        pos_count = np.sum(train_y)
        neg_count = len(train_y) - pos_count
        pos_weight = neg_count / max(1, pos_count)
        class_weight = {0: 1.0, 1: float(pos_weight)}
        logger.info(f"Class weights calculated: {class_weight}")
        logger.info(f"Positive examples in sample: {pos_count}/{len(train_y)} ({pos_count/len(train_y)*100:.2f}%)")
        
        # Get input shape
        X_mmap = np.load(X_file, mmap_mode='r')
        input_shape = X_mmap[train_indices[0]].shape
        del X_mmap
        
        logger.info(f"Input shape: {input_shape}")
        
        # Build model with custom metrics for imbalanced data
        model = build_advanced_zero_curtain_model(input_shape)
        
        # Print model summary to log
        model.summary(print_fn=logger.info)
        
        # Output directory for this run
        run_output_dir = os.path.join(output_base_dir, 'efficient_model')
        os.makedirs(run_output_dir, exist_ok=True)
        
        # Train model with memory optimization and balanced validation
        trained_model, best_model_path = memory_optimized_training(
            model, 
            X_file, 
            y_file,
            train_indices, 
            val_indices, 
            test_indices,
            output_dir=run_output_dir,
            batch_size=batch_size,
            chunk_size=2500,  # Smaller chunks for memory efficiency
            epochs_per_chunk=1,
            save_frequency=5,
            class_weight=class_weight,
            start_chunk=start_chunk,
            max_chunks=max_chunks
        )
        
        # Clear model from memory
        del model
        tf.keras.backend.clear_session()
        gc.collect()
        
        # Log completion
        total_time = time.time() - start_time
        final_memory = psutil.Process(os.getpid()).memory_info().rss / (1024**3)
        memory_increase = final_memory - initial_memory
        
        logger.info(f"Pipeline completed in {total_time/3600:.2f} hours")
        logger.info(f"Memory usage: {final_memory:.2f} GB (increase: {memory_increase:.2f} GB)")
        
        # Remove file handler to avoid duplicate logging
        logger.removeHandler(file_handler)
        
        # Return summary of results
        return {
            'status': 'completed',
            'time_hours': total_time/3600,
            'best_model_path': best_model_path
        }
        
    except Exception as e:
        logger.error(f"Error in pipeline: {e}")
        logger.error(traceback.format_exc())
        # Remove file handler even on error
        logger.removeHandler(file_handler)
        return {'status': 'failed', 'error': str(e)}


# Example usage
if __name__ == "__main__":
    import os
    
    # Force CPU-only mode to avoid GPU issues
    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
    print("Forcing CPU-only mode")
    
    # Set up logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s: %(message)s',
    )
    
    try:
        results = main_zero_curtain_analysis(
            feather_path = '/Users/bgay/Desktop/Research/Code/merged_compressed.feather',
            output_base_dir = '/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling',
            batch_size=16,
            start_chunk=0,  # Start from beginning or resume from a specific chunk
            max_chunks=None  # Process only 5 chunks for testing
        )
        
        print("Analysis complete!")
        print(f"Best model saved at: {results.get('best_model_path', 'Not available')}")
    except Exception as e:
        print(f"Pipeline failed: {e}")
        traceback.print_exc()

# def build_improved_zero_curtain_model_fixed(input_shape, include_moisture=True):
#     """
#     Fixed model architecture that works with the ConvLSTM2D and BatchNormalization
#     """
#     from tensorflow.keras.models import Model
#     from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
#     from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPoo...
#     from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add, Lambda
#     from tensorflow.keras.optimizers import Adam
#     from tensorflow.keras.regularizers import l2
#     import tensorflow as tf
    
#     # Reduced L2 regularization strength
#     reg_strength = 0.00005
#     # Slightly reduced dropout rate
#     dropout_rate = 0.25
    
#     # Input layer
#     inputs = Input(shape=input_shape)
    
#     # Add BatchNormalization to input
#     x = BatchNormalization()(inputs)
    
#     # Reshape for ConvLSTM (add spatial dimension)
#     x = Reshape((input_shape[0], 1, 1, input_shape[1]))(x)
    
#     # ConvLSTM layer with regularization
#     from tensorflow.keras.layers import ConvLSTM2D
#     convlstm = ConvLSTM2D(
#         filters=64,
#         kernel_size=(3, 1),
#         padding='same',
#         return_sequences=True,
#         activation='tanh',
#         recurrent_dropout=dropout_rate,
#         kernel_regularizer=l2(reg_strength)
#     )(x)
    
#     # Use BatchNorm5D for the ConvLSTM output
#     from tensorflow.keras.layers import Layer
    
#     class BatchNorm5D(Layer):
#         def __init__(self, **kwargs):
#             super(BatchNorm5D, self).__init__(**kwargs)
#             self.bn = BatchNormalization()
            
#         def call(self, inputs, training=None):
#             # Get the dimensions
#             input_shape = tf.shape(inputs)
#             batch_size = input_shape[0]
#             time_steps = input_shape[1]
#             height = input_shape[2]
#             width = input_shape[3]
#             channels = input_shape[4]
            
#             # Reshape to 4D for BatchNorm by combining batch and time dimensions
#             x_reshaped = tf.reshape(inputs, [-1, height, width, channels])
            
#             # Apply BatchNorm
#             x_bn = self.bn(x_reshaped, training=training)
            
#             # Reshape back to 5D
#             x_back = tf.reshape(x_bn, [batch_size, time_steps, height, width, channels])
#             return x_back
        
#         def get_config(self):
#             config = super(BatchNorm5D, self).get_config()
#             return config
    
#     convlstm = BatchNorm5D()(convlstm)
    
#     # Reshape back to (sequence_length, features)
#     convlstm = Reshape((input_shape[0], 64))(convlstm)
    
#     # Add positional encoding for transformer
#     def positional_encoding(length, depth):
#         positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
#         depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
#         angle_rates = 1 / tf.pow(10000.0, depths)
#         angle_rads = positions * angle_rates
        
#         # Only use sin to ensure output depth matches input depth
#         pos_encoding = tf.sin(angle_rads)
        
#         # Add batch dimension
#         pos_encoding = tf.expand_dims(pos_encoding, 0)
        
#         return pos_encoding
    
#     # Add positional encoding
#     pos_encoding = positional_encoding(input_shape[0], 64)
#     transformer_input = convlstm + pos_encoding
    
#     # Add BatchNormalization before transformer
#     transformer_input = BatchNormalization()(transformer_input)
    
#     # Improved transformer encoder block
#     def transformer_encoder(x, num_heads=8, key_dim=64, ff_dim=128):
#         # Multi-head attention with regularization
#         attention_output = MultiHeadAttention(
#             num_heads=num_heads, key_dim=key_dim,
#             kernel_regularizer=l2(reg_strength)
#         )(x, x)
        
#         # Skip connection 1 with dropout
#         x1 = Add()([attention_output, x])
#         x1 = LayerNormalization(epsilon=1e-6)(x1)
#         # Add BatchNormalization after LayerNormalization
#         x1 = BatchNormalization()(x1)
#         x1 = Dropout(dropout_rate)(x1)
        
#         # Feed-forward network with regularization
#         ff_output = Dense(ff_dim, activation='relu', kernel_regularizer=l2(reg_strength))(x1)
#         ff_output = BatchNormalization()(ff_output)  # Add BatchNorm
#         ff_output = Dropout(dropout_rate)(ff_output)
#         ff_output = Dense(64, kernel_regularizer=l2(reg_strength))(ff_output)
#         ff_output = BatchNormalization()(ff_output)  # Add BatchNorm
        
#         # Skip connection 2
#         x2 = Add()([ff_output, x1])
#         return LayerNormalization(epsilon=1e-6)(x2)
    
#     # Apply transformer encoder
#     transformer_output = transformer_encoder(transformer_input)
    
#     # Parallel CNN paths for multi-scale feature extraction (with regularization)
#     cnn_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu', 
#                   kernel_regularizer=l2(reg_strength))(inputs)
#     cnn_1 = BatchNormalization()(cnn_1)  
#     cnn_1 = Dropout(dropout_rate/2)(cnn_1)
    
#     cnn_2 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu',
#                   kernel_regularizer=l2(reg_strength))(inputs)
#     cnn_2 = BatchNormalization()(cnn_2)  
#     cnn_2 = Dropout(dropout_rate/2)(cnn_2)
    
#     cnn_3 = Conv1D(filters=32, kernel_size=7, padding='same', activation='relu',
#                   kernel_regularizer=l2(reg_strength))(inputs)
#     cnn_3 = BatchNormalization()(cnn_3)  
#     cnn_3 = Dropout(dropout_rate/2)(cnn_3)
    
#     # VAE components
#     def sampling(args):
#         z_mean, z_log_var = args
#         batch = tf.shape(z_mean)[0]
#         dim = tf.shape(z_mean)[1]
#         epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
#         return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
#     # Global temporal features
#     global_max = GlobalMaxPooling1D()(transformer_output)
#     global_avg = GlobalAveragePooling1D()(transformer_output)
    
#     # VAE encoding with reduced regularization
#     z_concat = Concatenate()([global_max, global_avg])
#     z_concat = BatchNormalization()(z_concat)  # Add BatchNorm
    
#     z_mean = Dense(32, kernel_regularizer=l2(reg_strength))(z_concat)
#     z_log_var = Dense(32, kernel_regularizer=l2(reg_strength))(z_concat)
#     z = Lambda(sampling)([z_mean, z_log_var])
    
#     # Combine all features
#     merged_features = Concatenate()(
#         [
#             GlobalMaxPooling1D()(cnn_1),
#             GlobalMaxPooling1D()(cnn_2),
#             GlobalMaxPooling1D()(cnn_3),
#             global_max,
#             global_avg,
#             z
#         ]
#     )
    
#     # Add BatchNorm to merged features
#     merged_features = BatchNormalization()(merged_features)
    
#     # Final classification layers with regularization
#     x = Dense(128, activation='relu', kernel_regularizer=l2(reg_strength))(merged_features)
#     x = BatchNormalization()(x)  
#     x = Dropout(dropout_rate)(x)
#     x = Dense(64, activation='relu', kernel_regularizer=l2(reg_strength))(x)
#     x = BatchNormalization()(x)  
#     x = Dropout(dropout_rate)(x)
    
#     # Output layer
#     outputs = Dense(1, activation='sigmoid', kernel_regularizer=l2(reg_strength))(x)
    
#     # Create model
#     model = Model(inputs=inputs, outputs=outputs)
    
#     # CRITICAL FIX: Reduce VAE loss weight
#     kl_loss = -0.5 * tf.reduce_mean(
#         z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
#     )
#     model.add_loss(0.0002 * kl_loss)  # Reduced from 0.0005 to 0.0002
    
#     # Compile model with gradient clipping but simpler metrics to avoid errors
#     model.compile(
#         optimizer=Adam(
#             learning_rate=0.0005,
#             clipvalue=0.5,  # Reduced from 1.0 to 0.5 for better stability
#             epsilon=1e-7  # Increased epsilon for better numerical stability
#         ),
#         loss='binary_crossentropy',
#         metrics=[
#             'accuracy',
#             tf.keras.metrics.AUC(name='auc', num_thresholds=200),
#             tf.keras.metrics.Precision(name='precision'),
#             tf.keras.metrics.Recall(name='recall')
#         ]
#     )
    
#     return model

def build_improved_zero_curtain_model_fixed(input_shape, include_moisture=True):
    """
    Fixed model architecture that works with the ConvLSTM2D and BatchNormalization
    """
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
    from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D
    from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add, Lambda
    from tensorflow.keras.layers import ConvLSTM2D
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.regularizers import l2
    import tensorflow as tf
    
    # Import the BatchNorm5D class from the global scope
    # This assumes it's been registered with get_custom_objects()
    BatchNorm5D = tf.keras.utils.get_custom_objects()['BatchNorm5D']
    
    # Reduced L2 regularization strength
    reg_strength = 0.00005
    # Slightly reduced dropout rate
    dropout_rate = 0.25
    
    # Input layer
    inputs = Input(shape=input_shape)
    
    # Add BatchNormalization to input
    x = BatchNormalization()(inputs)
    
    # Reshape for ConvLSTM (add spatial dimension)
    x = Reshape((input_shape[0], 1, 1, input_shape[1]))(x)
    
    # ConvLSTM layer with regularization
    convlstm = ConvLSTM2D(
        filters=64,
        kernel_size=(3, 1),
        padding='same',
        return_sequences=True,
        activation='tanh',
        recurrent_dropout=dropout_rate,
        kernel_regularizer=l2(reg_strength)
    )(x)
    
    # Use BatchNorm5D for the ConvLSTM output
    convlstm = BatchNorm5D()(convlstm)
    
    # Reshape back to (sequence_length, features)
    convlstm = Reshape((input_shape[0], 64))(convlstm)
    
    # Add positional encoding for transformer
    def positional_encoding(length, depth):
        positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
        angle_rates = 1 / tf.pow(10000.0, depths)
        angle_rads = positions * angle_rates
        
        # Only use sin to ensure output depth matches input depth
        pos_encoding = tf.sin(angle_rads)
        
        # Add batch dimension
        pos_encoding = tf.expand_dims(pos_encoding, 0)
        
        return pos_encoding
    
    # Add positional encoding
    pos_encoding = positional_encoding(input_shape[0], 64)
    transformer_input = convlstm + pos_encoding
    
    # Add BatchNormalization before transformer
    transformer_input = BatchNormalization()(transformer_input)
    
    # Improved transformer encoder block
    def transformer_encoder(x, num_heads=8, key_dim=64, ff_dim=128):
        # Multi-head attention with regularization
        attention_output = MultiHeadAttention(
            num_heads=num_heads, key_dim=key_dim,
            kernel_regularizer=l2(reg_strength)
        )(x, x)
        
        # Skip connection 1 with dropout
        x1 = Add()([attention_output, x])
        x1 = LayerNormalization(epsilon=1e-6)(x1)
        # Add BatchNormalization after LayerNormalization
        x1 = BatchNormalization()(x1)
        x1 = Dropout(dropout_rate)(x1)
        
        # Feed-forward network with regularization
        ff_output = Dense(ff_dim, activation='relu', kernel_regularizer=l2(reg_strength))(x1)
        ff_output = BatchNormalization()(ff_output)  # Add BatchNorm
        ff_output = Dropout(dropout_rate)(ff_output)
        ff_output = Dense(64, kernel_regularizer=l2(reg_strength))(ff_output)
        ff_output = BatchNormalization()(ff_output)  # Add BatchNorm
        
        # Skip connection 2
        x2 = Add()([ff_output, x1])
        return LayerNormalization(epsilon=1e-6)(x2)
    
    # Apply transformer encoder
    transformer_output = transformer_encoder(transformer_input)
    
    # Parallel CNN paths for multi-scale feature extraction (with regularization)
    cnn_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu', 
                  kernel_regularizer=l2(reg_strength))(inputs)
    cnn_1 = BatchNormalization()(cnn_1)  
    cnn_1 = Dropout(dropout_rate/2)(cnn_1)
    
    cnn_2 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu',
                  kernel_regularizer=l2(reg_strength))(inputs)
    cnn_2 = BatchNormalization()(cnn_2)  
    cnn_2 = Dropout(dropout_rate/2)(cnn_2)
    
    cnn_3 = Conv1D(filters=32, kernel_size=7, padding='same', activation='relu',
                  kernel_regularizer=l2(reg_strength))(inputs)
    cnn_3 = BatchNormalization()(cnn_3)  
    cnn_3 = Dropout(dropout_rate/2)(cnn_3)
    
    # VAE components
    def sampling(args):
        z_mean, z_log_var = args
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
    # Global temporal features
    global_max = GlobalMaxPooling1D()(transformer_output)
    global_avg = GlobalAveragePooling1D()(transformer_output)
    
    # VAE encoding with reduced regularization
    z_concat = Concatenate()([global_max, global_avg])
    z_concat = BatchNormalization()(z_concat)  # Add BatchNorm
    
    z_mean = Dense(32, kernel_regularizer=l2(reg_strength))(z_concat)
    z_log_var = Dense(32, kernel_regularizer=l2(reg_strength))(z_concat)
    z = Lambda(sampling)([z_mean, z_log_var])
    
    # Combine all features
    merged_features = Concatenate()(
        [
            GlobalMaxPooling1D()(cnn_1),
            GlobalMaxPooling1D()(cnn_2),
            GlobalMaxPooling1D()(cnn_3),
            global_max,
            global_avg,
            z
        ]
    )
    
    # Add BatchNorm to merged features
    merged_features = BatchNormalization()(merged_features)
    
    # Final classification layers with regularization
    x = Dense(128, activation='relu', kernel_regularizer=l2(reg_strength))(merged_features)
    x = BatchNormalization()(x)  
    x = Dropout(dropout_rate)(x)
    x = Dense(64, activation='relu', kernel_regularizer=l2(reg_strength))(x)
    x = BatchNormalization()(x)  
    x = Dropout(dropout_rate)(x)
    
    # Output layer
    outputs = Dense(1, activation='sigmoid', kernel_regularizer=l2(reg_strength))(x)
    
    # Create model
    model = Model(inputs=inputs, outputs=outputs)
    
    # CRITICAL FIX: Reduce VAE loss weight
    kl_loss = -0.5 * tf.reduce_mean(
        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
    )
    model.add_loss(0.0002 * kl_loss)  # Reduced from 0.0005 to 0.0002
    
    # Compile model with gradient clipping but simpler metrics to avoid errors
    model.compile(
        optimizer=Adam(
            learning_rate=0.0005,
            clipvalue=0.5,  # Reduced from 1.0 to 0.5 for better stability
            epsilon=1e-7  # Increased epsilon for better numerical stability
        ),
        loss='binary_crossentropy',
        metrics=[
            'accuracy',
            tf.keras.metrics.AUC(name='auc', num_thresholds=200),
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall')
        ]
    )
    
    return model

# # Main improved training function
# def improved_training_pipeline(model, X_file, y_file, train_indices, val_indices, test_indices,
#                              output_dir, batch_size=256, chunk_size=10000, epochs_per_chunk=2, 
#                              save_frequency=5, class_weight=None, start_chunk=0):
#     """
#     Improved training pipeline with memory management, error handling, and performance monitoring
    
#     Parameters:
#     -----------
#     model : tf.keras.Model
#         The model to train
#     X_file : str
#         Path to features numpy file
#     y_file : str
#         Path to labels numpy file
#     train_indices : numpy.ndarray
#         Indices for training samples
#     val_indices : numpy.ndarray
#         Indices for validation samples
#     test_indices : numpy.ndarray
#         Indices for test samples
#     output_dir : str
#         Directory to save model checkpoints and logs
#     batch_size : int
#         Batch size for training
#     chunk_size : int
#         Number of samples per chunk
#     epochs_per_chunk : int
#         Number of epochs to train each chunk
#     save_frequency : int
#         Number of chunks between model saves
#     class_weight : dict or None
#         Class weights for imbalanced data
#     start_chunk : int
#         Chunk to start training from (for resuming)
        
#     Returns:
#     --------
#     tuple
#         (trained_model, model_path)
#     """
#     import os
#     import gc
#     import json
#     import numpy as np
#     import tensorflow as tf
#     from datetime import datetime, timedelta
#     import time
#     import psutil
    
#     # Create output directories
#     os.makedirs(output_dir, exist_ok=True)
#     os.makedirs(os.path.join(output_dir, "checkpoints"), exist_ok=True)
#     os.makedirs(os.path.join(output_dir, "predictions"), exist_ok=True)
#     os.makedirs(os.path.join(output_dir, "logs"), exist_ok=True)
    
#     # Setup logging
#     log_file = os.path.join(output_dir, "logs", "training_log.txt")
#     def log_message(message):
#         timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
#         with open(log_file, "a") as f:
#             f.write(f"[{timestamp}] {message}\n")
#         print(message)
    
#     # Calculate number of chunks
#     num_chunks = int(np.ceil(len(train_indices) / chunk_size))
#     log_message(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
#     log_message(f"Starting from chunk {start_chunk+1}")
    
#     # Load data in memory-mapped mode
#     X_mmap = np.load(X_file, mmap_mode='r')
#     y_mmap = np.load(y_file, mmap_mode='r')
    
#     # Create validation set (limited size for memory efficiency)
#     val_limit = min(2000, len(val_indices))
#     val_indices_subset = val_indices[:val_limit]
    
#     # Load validation data once
#     log_message("Loading validation data...")
#     val_X = np.array([X_mmap[idx] for idx in val_indices_subset])
#     val_y = np.array([y_mmap[idx] for idx in val_indices_subset])
#     log_message(f"Loaded {len(val_X)} validation samples")
    
#     # Load existing history if resuming
#     history_log = []
#     history_path = os.path.join(output_dir, "training_history.json")
    
#     if start_chunk > 0:
#         if os.path.exists(history_path):
#             try:
#                 with open(history_path, "r") as f:
#                     history_log = json.load(f)
#                 log_message(f"Loaded existing training history with {len(history_log)} entries")
#             except Exception as e:
#                 log_message(f"Could not load existing history JSON: {e}")
#                 # Try pickle format
#                 pickle_path = os.path.join(output_dir, "training_history.pkl")
#                 if os.path.exists(pickle_path):
#                     import pickle
#                     with open(pickle_path, "rb") as f:
#                         history_log = pickle.load(f)
#                     log_message(f"Loaded training history from pickle file")
#                 else:
#                     log_message("No training history found, starting fresh")
        
#         # Load latest model if resuming
#         checkpoint_path = os.path.join(output_dir, "checkpoints", f"model_chunk_{start_chunk}.h5")
#         if os.path.exists(checkpoint_path):
#             log_message(f"Loading model from checkpoint {checkpoint_path}")
#             # Load with custom objects to handle BatchNorm5D
#             model = tf.keras.models.load_model(
#                 checkpoint_path,
#                 custom_objects={'BatchNorm5D': BatchNorm5D, 'f1_score': f1_score}
#             )
#             log_message(f"Model loaded successfully")
#         else:
#             log_message(f"Checkpoint not found. Looking for alternative checkpoints...")
#             # Find the most recent checkpoint before start_chunk
#             checkpoint_indices = []
#             checkpoints_dir = os.path.join(output_dir, "checkpoints")
#             for filename in os.listdir(checkpoints_dir):
#                 if filename.startswith("model_chunk_") and filename.endswith(".h5"):
#                     try:
#                         idx = int(filename.split("_")[-1].split(".")[0])
#                         if idx < start_chunk:
#                             checkpoint_indices.append(idx)
#                     except ValueError:
#                         continue
            
#             if checkpoint_indices:
#                 latest_idx = max(checkpoint_indices)
#                 alt_checkpoint_path = os.path.join(checkpoints_dir, f"model_chunk_{latest_idx}.h5"...
#                 log_message(f"Loading alternative checkpoint: {alt_checkpoint_path}")
#                 model = tf.keras.models.load_model(
#                     alt_checkpoint_path,
#                     custom_objects={'BatchNorm5D': BatchNorm5D, 'f1_score': f1_score}
#                 )
#                 log_message(f"Model loaded from alternative checkpoint")
#             else:
#                 log_message("No checkpoints found, using initial model")
    
#     # Setup enhanced callbacks
#     callbacks = get_enhanced_callbacks(output_dir)
    
#     # Track training progress
#     start_time = time.time()
#     best_val_auc = 0
#     best_model_path = None
    
#     # Recovery file
#     recovery_file = os.path.join(output_dir, "last_completed_chunk.txt")
    
#     # Process each chunk
#     for chunk_idx in range(start_chunk, num_chunks):
#         # Track progress for recovery purposes
#         with open(os.path.join(output_dir, "current_progress.txt"), "w") as f:
#             f.write(f"Processing chunk {chunk_idx+1}/{num_chunks}\n")
#             f.write(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
#         # Get chunk indices
#         start_idx = chunk_idx * chunk_size
#         end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
#         chunk_indices = train_indices[start_idx:end_idx]
        
#         # Report memory usage before processing
#         memory_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
#         log_message(f"\n{'='*50}")
#         log_message(f"Processing chunk {chunk_idx+1}/{num_chunks} with {len(chunk_indices)} sample...
#         log_message(f"Memory before: {memory_before:.1f} MB")
        
#         # Force garbage collection before loading new data
#         gc.collect()
        
#         try:
#             # Load chunk data in smaller batches to avoid memory spikes
#             log_message("Loading chunk data in batches...")
#             chunk_X = []
#             chunk_y = []
            
#             # Use a smaller batch size for loading data
#             load_batch_size = 1000
#             for i in range(0, len(chunk_indices), load_batch_size):
#                 end_i = min(i + load_batch_size, len(chunk_indices))
#                 log_message(f"  Loading batch {i+1}-{end_i} of {len(chunk_indices)}...")
                
#                 # Load this batch
#                 batch_indices = chunk_indices[i:end_i]
#                 try:
#                     X_batch = np.array([X_mmap[idx] for idx in batch_indices])
#                     y_batch = np.array([y_mmap[idx] for idx in batch_indices])
                    
#                     # Append to list
#                     chunk_X.append(X_batch)
#                     chunk_y.append(y_batch)
                    
#                     # Free memory
#                     del X_batch, y_batch
#                     gc.collect()
#                 except Exception as inner_e:
#                     log_message(f"Warning: Error loading batch {i}-{end_i}: {inner_e}")
#                     # Continue to next batch
            
#             # Combine batches
#             try:
#                 chunk_X = np.concatenate(chunk_X)
#                 chunk_y = np.concatenate(chunk_y)
#                 log_message(f"Data loaded. Shape: X={chunk_X.shape}, y={chunk_y.shape}")
#                 log_message(f"Memory after loading: {psutil.Process(os.getpid()).memory_info().rss...
#             except Exception as concat_e:
#                 log_message(f"Error combining data batches: {concat_e}")
#                 # Skip this chunk
#                 continue
            
#             # FIXED: Properly handle training with the process_chunk_with_batch_safety function
#             log_message(f"Training on chunk for {epochs_per_chunk} epochs...")
            
#             try:
#                 # Process chunk with batch safety
#                 chunk_history = process_chunk_with_batch_safety(
#                     model,
#                     chunk_X, 
#                     chunk_y,
#                     val_data=(val_X, val_y),
#                     batch_size=batch_size,
#                     epochs=epochs_per_chunk
#                 )
                
#                 # Store history
#                 if chunk_history and "history" in chunk_history:
#                     # Convert numpy values to Python native types for JSON serialization
#                     serializable_history = {}
#                     for k, v in chunk_history["history"].items():
#                         serializable_history[k] = [float(val) if hasattr(val, 'dtype') else val fo...
                    
#                     history_log.append(serializable_history)
                    
#                     # Save history
#                     try:
#                         with open(history_path, "w") as f:
#                             json.dump(history_log, f)
#                         log_message("Saved training history to JSON")
#                     except Exception as history_e:
#                         log_message(f"Warning: Could not save history to JSON: {history_e}")
#                         # Fallback - save as pickle
#                         import pickle
#                         with open(os.path.join(output_dir, "training_history.pkl"), "wb") as f:
#                             pickle.dump(history_log, f)
#                         log_message("Saved training history as pickle file")
#             except Exception as train_e:
#                 log_message(f"Error during chunk training: {train_e}")
#                 # Try to save model in current state
#                 error_model_path = os.path.join(output_dir, "checkpoints", f"error_recovery_{chunk...
#                 model.save(error_model_path)
#                 log_message(f"Saved model in error state to {error_model_path}")
            
#             # Check validation performance
#             try:
#                 val_metrics = model.evaluate(val_X, val_y, verbose=0)
#                 val_metrics_dict = dict(zip(model.metrics_names, val_metrics))
                
#                 val_metrics_str = " - ".join([f"{k}: {v:.4f}" for k, v in val_metrics_dict.items()...
#                 log_message(f"Validation after chunk: {val_metrics_str}")
                
#                 # Save if better
#                 val_auc = val_metrics_dict.get('auc', 0)
#                 if val_auc > best_val_auc:
#                     improvement = val_auc - best_val_auc
#                     best_val_auc = val_auc
#                     best_model_path = os.path.join(output_dir, "checkpoints", f"best_model_chunk_{...
#                     model.save(best_model_path)
#                     log_message(f"New best model saved with val_auc: {best_val_auc:.4f} (improved ...
#             except Exception as eval_e:
#                 log_message(f"Error during validation: {eval_e}")
            
#             # Save model periodically
#             if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
#                 model_path = os.path.join(output_dir, "checkpoints", f"model_chunk_{chunk_idx+1}.h...
#                 try:
#                     model.save(model_path)
#                     log_message(f"Model saved to {model_path}")
#                 except Exception as save_e:
#                     log_message(f"Error saving model: {save_e}")
#                     # Try saving weights only
#                     try:
#                         weights_path = os.path.join(output_dir, "checkpoints", f"model_weights_chu...
#                         model.save_weights(weights_path)
#                         log_message(f"Saved model weights to {weights_path}")
#                     except Exception as weights_e:
#                         log_message(f"Error saving weights: {weights_e}")
            
#             # Explicitly delete everything from memory
#             del chunk_X, chunk_y
#             gc.collect()
            
#         except Exception as outer_e:
#             log_message(f"Critical error processing chunk {chunk_idx+1}: {outer_e}")
#             import traceback
#             log_message(traceback.format_exc())
            
#             # Write error to file for debugging
#             with open(os.path.join(output_dir, f"error_chunk_{chunk_idx+1}.txt"), "w") as f:
#                 f.write(f"Error processing chunk {chunk_idx+1}:\n")
#                 f.write(traceback.format_exc())
            
#             # Skip to next chunk
#             continue
        
#         # Report memory after cleanup
#         memory_after = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
#         log_message(f"Memory after: {memory_after:.1f} MB (Change: {memory_after - memory_before:....
        
#         # Write recovery file with last completed chunk
#         with open(recovery_file, "w") as f:
#             f.write(str(chunk_idx + 1))
        
#         # Estimate time
#         elapsed = time.time() - start_time
#         avg_time_per_chunk = elapsed / (chunk_idx - start_chunk + 1) if chunk_idx > start_chunk el...
#         remaining = avg_time_per_chunk * (num_chunks - chunk_idx - 1)
#         log_message(f"Estimated remaining time: {timedelta(seconds=int(remaining))}")
        
#         # CRITICAL FIX: Reset TensorFlow session periodically to prevent memory leak
#         if (chunk_idx + 1) % 10 == 0 and chunk_idx > 0:
#             log_message("Resetting TensorFlow session to prevent memory issues")
#             try:
#                 # Save model
#                 temp_model_path = os.path.join(output_dir, "checkpoints", f"temp_reset_{chunk_idx+...
#                 model.save(temp_model_path)
                
#                 # Clear session and collect garbage
#                 tf.keras.backend.clear_session()
#                 gc.collect()
                
#                 # Reload model with custom objects
#                 model = tf.keras.models.load_model(
#                     temp_model_path,
#                     custom_objects={'BatchNorm5D': BatchNorm5D, 'f1_score': f1_score}
#                 )
#                 log_message("TensorFlow session reset complete")
#             except Exception as reset_e:
#                 log_message(f"Error during TensorFlow session reset: {reset_e}")
#                 # Continue with current model
    
#     # Save final model
#     final_model_path = os.path.join(output_dir, "final_model.h5")
#     try:
#         model.save(final_model_path)
#         log_message(f"Final model saved to {final_model_path}")
#     except Exception as final_save_e:
#         log_message(f"Error saving final model: {final_save_e}")
#         try:
#             # Try alternative location
#             alt_path = os.path.join(output_dir, "checkpoints", "final_model_backup.h5")
#             model.save(alt_path)
#             final_model_path = alt_path
#             log_message(f"Saved final model to alternate location: {alt_path}")
#         except Exception as alt_save_e:
#             log_message(f"Could not save final model in any format: {alt_save_e}")
    
#     # Return the best model if it exists
#     if best_model_path and os.path.exists(best_model_path):
#         log_message(f"Loading best model from {best_model_path} for return")
#         try:
#             model = tf.keras.models.load_model(
#                 best_model_path,
#                 custom_objects={'BatchNorm5D': BatchNorm5D, 'f1_score': f1_score}
#             )
#             return model, best_model_path
#         except Exception as load_e:
#             log_message(f"Error loading best model: {load_e}")
#             # Return current model
#             return model, final_model_path
    
#     return model, final_model_path

# def improved_resumable_training_fixed(model, X_file, y_file, train_indices, val_indices, test_indi...
#                                output_dir, batch_size=256, chunk_size=10000, epochs_per_chunk=3, 
#                                save_frequency=5, class_weight=None, start_chunk=0):
#     """
#     Enhanced training function with improved error handling and memory management.
#     """
#     import os
#     import gc
#     import json
#     import numpy as np
#     import tensorflow as tf
#     from datetime import datetime, timedelta
#     import time
#     import psutil
    
#     # Create output directories
#     os.makedirs(output_dir, exist_ok=True)
#     os.makedirs(os.path.join(output_dir, "checkpoints"), exist_ok=True)
#     os.makedirs(os.path.join(output_dir, "logs"), exist_ok=True)
    
#     # Setup logging
#     log_file = os.path.join(output_dir, "logs", "training_log.txt")
#     def log_message(message):
#         with open(log_file, "a") as f:
#             timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
#             f.write(f"[{timestamp}] {message}\n")
#         print(message)
    
#     log_message("Starting robust training with fixed architecture")
    
#     # Process in chunks to manage memory
#     num_chunks = int(np.ceil(len(train_indices) / chunk_size))
#     log_message(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
#     log_message(f"Starting from chunk {start_chunk+1}")
    
#     # Open data files with memory mapping
#     X_mmap = np.load(X_file, mmap_mode='r')
#     y_mmap = np.load(y_file, mmap_mode='r')
    
#     # Create validation set once (limited size for memory efficiency)
#     val_limit = min(1000, len(val_indices))
#     val_indices_subset = val_indices[:val_limit]
    
#     # Load validation data
#     val_X = np.array([X_mmap[idx] for idx in val_indices_subset])
#     val_y = np.array([y_mmap[idx] for idx in val_indices_subset])
#     log_message(f"Loaded {len(val_X)} validation samples")
    
#     # Load existing history if resuming
#     history_log = []
#     history_path = os.path.join(output_dir, "training_history.json")
#     if start_chunk > 0 and os.path.exists(history_path):
#         try:
#             with open(history_path, "r") as f:
#                 history_log = json.load(f)
#         except Exception as e:
#             log_message(f"Could not load existing history: {e}")
#             # Try pickle format
#             pickle_path = os.path.join(output_dir, "training_history.pkl")
#             if os.path.exists(pickle_path):
#                 import pickle
#                 with open(pickle_path, "rb") as f:
#                     history_log = pickle.load(f)
    
#     # If resuming, load latest model
#     if start_chunk > 0:
#         # Find the most recent checkpoint before start_chunk
#         checkpoint_indices = []
#         checkpoints_dir = os.path.join(output_dir, "checkpoints")
#         for filename in os.listdir(checkpoints_dir):
#             if filename.startswith("model_chunk_") and filename.endswith(".h5"):
#                 try:
#                     idx = int(filename.split("_")[-1].split(".")[0])
#                     if idx < start_chunk:
#                         checkpoint_indices.append(idx)
#                 except ValueError:
#                     continue
        
#         if checkpoint_indices:
#             latest_idx = max(checkpoint_indices)
#             model_path = os.path.join(checkpoints_dir, f"model_chunk_{latest_idx}.h5")
#             if os.path.exists(model_path):
#                 log_message(f"Loading model from checkpoint {model_path}")
#                 # Load with custom objects to handle BatchNorm5D
#                 model = tf.keras.models.load_model(model_path, custom_objects={'BatchNorm5D': Batc...
#             else:
#                 log_message(f"Warning: Could not find model checkpoint for chunk {latest_idx}")
    
#     # Setup callbacks
#     callbacks = [
#         # Early stopping with increased patience
#         tf.keras.callbacks.EarlyStopping(
#             patience=10, 
#             restore_best_weights=True,
#             monitor='val_auc',
#             mode='max',
#             min_delta=0.001
#         ),
#         # Reduced LR with increased patience
#         tf.keras.callbacks.ReduceLROnPlateau(
#             factor=0.5,
#             patience=5,
#             min_lr=1e-6,
#             monitor='val_auc',
#             mode='max'
#         ),
#         # Memory cleanup after each epoch
#         tf.keras.callbacks.LambdaCallback(
#             on_epoch_end=lambda epoch, logs: gc.collect()
#         )
#     ]
    
#     # Track metrics across chunks
#     start_time = time.time()
    
#     # For safe recovery
#     recovery_file = os.path.join(output_dir, "last_completed_chunk.txt")
    
#     # Process each chunk
#     for chunk_idx in range(start_chunk, num_chunks):
#         # Track progress in file for recovery purposes
#         with open(os.path.join(output_dir, "current_progress.txt"), "w") as f:
#             f.write(f"Processing chunk {chunk_idx+1}/{num_chunks}\n")
#             f.write(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
#         # Get chunk indices
#         start_idx = chunk_idx * chunk_size
#         end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
#         chunk_indices = train_indices[start_idx:end_idx]
        
#         # Report memory
#         memory_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
#         log_message(f"\n{'='*50}")
#         log_message(f"Processing chunk {chunk_idx+1}/{num_chunks} with {len(chunk_indices)} sample...
#         log_message(f"Memory before: {memory_before:.1f} MB")
        
#         # Force garbage collection before loading new data
#         gc.collect()
        
#         try:
#             # Load chunk data in smaller batches
#             chunk_X = []
#             chunk_y = []
            
#             # Use a smaller batch size for loading
#             load_batch_size = 1000  # Reduced batch size for more stability
#             for i in range(0, len(chunk_indices), load_batch_size):
#                 end_i = min(i + load_batch_size, len(chunk_indices))
#                 log_message(f"  Loading batch {i}-{end_i} of {len(chunk_indices)}...")
                
#                 try:
#                     # Load this batch
#                     batch_indices = chunk_indices[i:end_i]
#                     X_batch = np.array([X_mmap[idx] for idx in batch_indices])
#                     y_batch = np.array([y_mmap[idx] for idx in batch_indices])
                    
#                     # Append to list
#                     chunk_X.append(X_batch)
#                     chunk_y.append(y_batch)
                    
#                     # Free memory
#                     del X_batch, y_batch
#                     gc.collect()
#                 except Exception as inner_e:
#                     log_message(f"Warning: Error loading batch {i}-{end_i}: {inner_e}")
#                     # Continue to next batch
            
#             # Combine batches
#             try:
#                 chunk_X = np.concatenate(chunk_X)
#                 chunk_y = np.concatenate(chunk_y)
#                 log_message(f"Data loaded. Shape: X={chunk_X.shape}, y={chunk_y.shape}")
#                 log_message(f"Memory after loading: {psutil.Process(os.getpid()).memory_info().rss...
#             except Exception as concat_e:
#                 log_message(f"Error combining data batches: {concat_e}")
#                 # Skip this chunk
#                 continue
            
#             # Train on chunk using a safe batch size
#             mini_batch_size = min(batch_size, 64)  # Use smaller mini-batches for training
#             log_message(f"Training with mini-batch size: {mini_batch_size}")
            
#             # Track each epoch separately for better error handling
#             chunk_history = {}
#             for epoch in range(epochs_per_chunk):
#                 try:
#                     log_message(f"Epoch {epoch+1}/{epochs_per_chunk}")
                    
#                     # Train for a single epoch
#                     history = model.fit(
#                         chunk_X, chunk_y,
#                         validation_data=(val_X, val_y),
#                         epochs=1,
#                         batch_size=mini_batch_size,
#                         class_weight=class_weight,
#                         callbacks=callbacks,
#                         verbose=1
#                     )
                    
#                     # Check if loss is NaN and recover if needed
#                     if np.isnan(history.history['loss'][0]):
#                         log_message("WARNING: NaN loss detected! Reducing learning rate...")
#                         current_lr = float(tf.keras.backend.get_value(model.optimizer.lr))
#                         new_lr = current_lr * 0.1
#                         tf.keras.backend.set_value(model.optimizer.lr, new_lr)
#                         log_message(f"Reduced learning rate from {current_lr} to {new_lr}")
#                         continue
                    
#                     # Store history
#                     for k, v in history.history.items():
#                         if k not in chunk_history:
#                             chunk_history[k] = []
#                         chunk_history[k].extend([float(val) for val in v])
                    
#                     # Save after each epoch for safety
#                     epoch_model_path = os.path.join(output_dir, "checkpoints", f"model_chunk_{chun...
#                     model.save(epoch_model_path)
                    
#                 except Exception as epoch_e:
#                     log_message(f"Error during epoch {epoch+1}: {epoch_e}")
#                     import traceback
#                     traceback.print_exc()
#                     # Try to continue with next epoch
            
#             # Store history from all completed epochs
#             if chunk_history:
#                 history_log.append(chunk_history)
                
#                 # Save history
#                 try:
#                     with open(history_path, "w") as f:
#                         json.dump(history_log, f)
#                 except Exception as history_e:
#                     log_message(f"Warning: Could not save history to JSON: {history_e}")
#                     # Fallback - save as pickle
#                     import pickle
#                     with open(os.path.join(output_dir, "training_history.pkl"), "wb") as f:
#                         pickle.dump(history_log, f)
            
#             # Save model periodically
#             if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
#                 try:
#                     model_path = os.path.join(output_dir, "checkpoints", f"model_chunk_{chunk_idx+...
#                     model.save(model_path)
#                     log_message(f"Model saved to {model_path}")
#                 except Exception as save_e:
#                     log_message(f"Error saving model: {save_e}")
#                     # Try saving weights only
#                     try:
#                         weights_path = os.path.join(output_dir, "checkpoints", f"model_weights_{ch...
#                         model.save_weights(weights_path)
#                         log_message(f"Saved model weights to {weights_path}")
#                     except:
#                         log_message("Could not save model in any format")
            
#             # Explicitly delete everything from memory
#             del chunk_X, chunk_y
            
#         except Exception as outer_e:
#             log_message(f"Critical error processing chunk {chunk_idx+1}: {outer_e}")
#             import traceback
#             traceback.print_exc()
            
#             # Write error to file for debugging
#             with open(os.path.join(output_dir, f"error_chunk_{chunk_idx+1}.txt"), "w") as f:
#                 f.write(f"Error processing chunk {chunk_idx+1}:\n")
#                 f.write(traceback.format_exc())
                
#             # Try saving model in error state
#             try:
#                 error_model_path = os.path.join(output_dir, "checkpoints", f"error_recovery_{chunk...
#                 model.save(error_model_path)
#                 log_message(f"Saved model in error state to {error_model_path}")
#             except:
#                 log_message("Could not save model in error state")
                
#             # Continue to next chunk
#             continue
        
#         # Force garbage collection
#         gc.collect()
        
#         # Report memory after cleanup
#         memory_after = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
#         log_message(f"Memory after: {memory_after:.1f} MB (Change: {memory_after - memory_before:....
        
#         # Write recovery file with last completed chunk
#         with open(recovery_file, "w") as f:
#             f.write(str(chunk_idx + 1))
        
#         # Estimate time
#         elapsed = time.time() - start_time
#         avg_time_per_chunk = elapsed / (chunk_idx - start_chunk + 1) if chunk_idx > start_chunk el...
#         remaining = avg_time_per_chunk * (num_chunks - chunk_idx - 1)
#         log_message(f"Estimated remaining time: {timedelta(seconds=int(remaining))}")
        
#         # Reset TensorFlow session periodically to prevent memory growth
#         if (chunk_idx + 1) % 10 == 0:  # Reset more frequently
#             log_message("Resetting TensorFlow session to prevent memory issues")
#             try:
#                 temp_model_path = os.path.join(output_dir, "checkpoints", f"temp_reset_{chunk_idx+...
#                 model.save(temp_model_path)
                
#                 # Clear session
#                 tf.keras.backend.clear_session()
#                 gc.collect()
                
#                 # Reload model
#                 model = tf.keras.models.load_model(temp_model_path, custom_objects={'BatchNorm5D':...
#                 log_message("TensorFlow session reset complete")
#             except Exception as reset_e:
#                 log_message(f"Error during TensorFlow reset: {reset_e}")
#                 # Continue anyway
    
#     # Save final model
#     final_model_path = os.path.join(output_dir, "final_model.h5")
#     try:
#         model.save(final_model_path)
#         log_message(f"Final model saved to {final_model_path}")
#     except Exception as final_save_e:
#         log_message(f"Error saving final model: {final_save_e}")
#         try:
#             # Try alternative location
#             alt_path = os.path.join(output_dir, "checkpoints", "final_model_backup.h5")
#             model.save(alt_path)
#             final_model_path = alt_path
#             log_message(f"Saved final model to alternate location: {alt_path}")
#         except:
#             log_message("Could not save final model in any format")
    
#     return model, final_model_path

# def improved_resumable_training_fixed(model, X_file, y_file, train_indices, val_indices, test_indi...
#                                output_dir, batch_size=256, chunk_size=10000, epochs_per_chunk=3, 
#                                save_frequency=5, class_weight=None, start_chunk=0):
#     """
#     Enhanced training function with improved error handling and memory management.
    
#     This version fixes the module pickling issue by using saved_model format instead of h5
#     """
#     import os
#     import gc
#     import json
#     import numpy as np
#     import tensorflow as tf
#     from datetime import datetime, timedelta
#     import time
#     import psutil
    
#     # Create output directories
#     os.makedirs(output_dir, exist_ok=True)
#     os.makedirs(os.path.join(output_dir, "checkpoints"), exist_ok=True)
#     os.makedirs(os.path.join(output_dir, "logs"), exist_ok=True)
    
#     # Setup logging
#     log_file = os.path.join(output_dir, "logs", "training_log.txt")
#     def log_message(message):
#         with open(log_file, "a") as f:
#             timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
#             f.write(f"[{timestamp}] {message}\n")
#         print(message)
    
#     log_message("Starting robust training with fixed architecture")
    
#     # Process in chunks to manage memory
#     num_chunks = int(np.ceil(len(train_indices) / chunk_size))
#     log_message(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
#     log_message(f"Starting from chunk {start_chunk+1}")
    
#     # Open data files with memory mapping
#     X_mmap = np.load(X_file, mmap_mode='r')
#     y_mmap = np.load(y_file, mmap_mode='r')
    
#     # Create validation set once (limited size for memory efficiency)
#     val_limit = min(1000, len(val_indices))
#     val_indices_subset = val_indices[:val_limit]
    
#     # Load validation data
#     val_X = np.array([X_mmap[idx] for idx in val_indices_subset])
#     val_y = np.array([y_mmap[idx] for idx in val_indices_subset])
#     log_message(f"Loaded {len(val_X)} validation samples")
    
#     # Load existing history if resuming
#     history_log = []
#     history_path = os.path.join(output_dir, "training_history.json")
#     if start_chunk > 0 and os.path.exists(history_path):
#         try:
#             with open(history_path, "r") as f:
#                 history_log = json.load(f)
#         except Exception as e:
#             log_message(f"Could not load existing history: {e}")
#             # Try pickle format
#             pickle_path = os.path.join(output_dir, "training_history.pkl")
#             if os.path.exists(pickle_path):
#                 import pickle
#                 with open(pickle_path, "rb") as f:
#                     history_log = pickle.load(f)
    
#     # If resuming, load latest model
#     if start_chunk > 0:
#         # Find the most recent checkpoint before start_chunk
#         checkpoint_indices = []
#         checkpoints_dir = os.path.join(output_dir, "checkpoints")
        
#         # Look for SavedModel format directories instead of h5 files
#         for dirname in os.listdir(checkpoints_dir):
#             if dirname.startswith("model_chunk_"):
#                 try:
#                     # Extract chunk number from directory name
#                     idx = int(dirname.split("_")[-1])
#                     if idx < start_chunk:
#                         checkpoint_indices.append(idx)
#                 except ValueError:
#                     continue
        
#         if checkpoint_indices:
#             latest_idx = max(checkpoint_indices)
#             model_path = os.path.join(checkpoints_dir, f"model_chunk_{latest_idx}")
#             if os.path.exists(model_path):
#                 log_message(f"Loading model from checkpoint {model_path}")
#                 # Load with custom objects to handle BatchNorm5D
#                 model = tf.keras.models.load_model(model_path)
#             else:
#                 log_message(f"Warning: Could not find model checkpoint for chunk {latest_idx}")
    
#     # Setup callbacks
#     callbacks = [
#         # Early stopping with increased patience
#         tf.keras.callbacks.EarlyStopping(
#             patience=10, 
#             restore_best_weights=True,
#             monitor='val_auc',
#             mode='max',
#             min_delta=0.001
#         ),
#         # Reduced LR with increased patience
#         tf.keras.callbacks.ReduceLROnPlateau(
#             factor=0.5,
#             patience=5,
#             min_lr=1e-6,
#             monitor='val_auc',
#             mode='max'
#         ),
#         # Memory cleanup after each epoch
#         tf.keras.callbacks.LambdaCallback(
#             on_epoch_end=lambda epoch, logs: gc.collect()
#         )
#     ]
    
#     # Track metrics across chunks
#     start_time = time.time()
    
#     # For safe recovery
#     recovery_file = os.path.join(output_dir, "last_completed_chunk.txt")
    
#     # Process each chunk
#     for chunk_idx in range(start_chunk, num_chunks):
#         # Track progress in file for recovery purposes
#         with open(os.path.join(output_dir, "current_progress.txt"), "w") as f:
#             f.write(f"Processing chunk {chunk_idx+1}/{num_chunks}\n")
#             f.write(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
#         # Get chunk indices
#         start_idx = chunk_idx * chunk_size
#         end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
#         chunk_indices = train_indices[start_idx:end_idx]
        
#         # Report memory
#         memory_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
#         log_message(f"\n{'='*50}")
#         log_message(f"Processing chunk {chunk_idx+1}/{num_chunks} with {len(chunk_indices)} sample...
#         log_message(f"Memory before: {memory_before:.1f} MB")
        
#         # Force garbage collection before loading new data
#         gc.collect()
        
#         try:
#             # Load chunk data in smaller batches
#             chunk_X = []
#             chunk_y = []
            
#             # Use a smaller batch size for loading
#             load_batch_size = 1000  # Reduced batch size for more stability
#             for i in range(0, len(chunk_indices), load_batch_size):
#                 end_i = min(i + load_batch_size, len(chunk_indices))
#                 log_message(f"  Loading batch {i}-{end_i} of {len(chunk_indices)}...")
                
#                 try:
#                     # Load this batch
#                     batch_indices = chunk_indices[i:end_i]
#                     X_batch = np.array([X_mmap[idx] for idx in batch_indices])
#                     y_batch = np.array([y_mmap[idx] for idx in batch_indices])
                    
#                     # Append to list
#                     chunk_X.append(X_batch)
#                     chunk_y.append(y_batch)
                    
#                     # Free memory
#                     del X_batch, y_batch
#                     gc.collect()
#                 except Exception as inner_e:
#                     log_message(f"Warning: Error loading batch {i}-{end_i}: {inner_e}")
#                     # Continue to next batch
            
#             # Combine batches
#             try:
#                 chunk_X = np.concatenate(chunk_X)
#                 chunk_y = np.concatenate(chunk_y)
#                 log_message(f"Data loaded. Shape: X={chunk_X.shape}, y={chunk_y.shape}")
#                 log_message(f"Memory after loading: {psutil.Process(os.getpid()).memory_info().rss...
#             except Exception as concat_e:
#                 log_message(f"Error combining data batches: {concat_e}")
#                 # Skip this chunk
#                 continue
            
#             # Train on chunk using a safe batch size
#             mini_batch_size = min(batch_size, 64)  # Use smaller mini-batches for training
#             log_message(f"Training with mini-batch size: {mini_batch_size}")
            
#             # Track each epoch separately for better error handling
#             chunk_history = {}
#             for epoch in range(epochs_per_chunk):
#                 try:
#                     log_message(f"Epoch {epoch+1}/{epochs_per_chunk}")
                    
#                     # Train for a single epoch
#                     history = model.fit(
#                         chunk_X, chunk_y,
#                         validation_data=(val_X, val_y),
#                         epochs=1,
#                         batch_size=mini_batch_size,
#                         class_weight=class_weight,
#                         callbacks=callbacks,
#                         verbose=1
#                     )
                    
#                     # Check if loss is NaN and recover if needed
#                     if np.isnan(history.history['loss'][0]):
#                         log_message("WARNING: NaN loss detected! Reducing learning rate...")
#                         current_lr = float(tf.keras.backend.get_value(model.optimizer.lr))
#                         new_lr = current_lr * 0.1
#                         tf.keras.backend.set_value(model.optimizer.lr, new_lr)
#                         log_message(f"Reduced learning rate from {current_lr} to {new_lr}")
#                         continue
                    
#                     # Store history
#                     for k, v in history.history.items():
#                         if k not in chunk_history:
#                             chunk_history[k] = []
#                         chunk_history[k].extend([float(val) for val in v])
                    
#                     # Save after each epoch for safety using SavedModel format instead of h5
#                     #epoch_model_path = os.path.join(output_dir, "checkpoints", f"model_chunk_{chu...
#                     #model.save(epoch_model_path, save_format='tf')
#                     #log_message(f"Saved model after epoch {epoch+1} to {epoch_model_path}")
#                     # Save weights only after each epoch
#                     epoch_weights_path = os.path.join(output_dir, "checkpoints", f"model_weights_{...
#                     model.save_weights(epoch_weights_path)
#                     log_message(f"Saved model weights after epoch {epoch+1} to {epoch_weights_path...
                    
#                 except Exception as epoch_e:
#                     log_message(f"Error during epoch {epoch+1}: {epoch_e}")
#                     import traceback
#                     traceback.print_exc()
#                     # Try to continue with next epoch
            
#             # Store history from all completed epochs
#             if chunk_history:
#                 history_log.append(chunk_history)
                
#                 # Save history
#                 try:
#                     with open(history_path, "w") as f:
#                         json.dump(history_log, f)
#                 except Exception as history_e:
#                     log_message(f"Warning: Could not save history to JSON: {history_e}")
#                     # Fallback - save as pickle
#                     import pickle
#                     with open(os.path.join(output_dir, "training_history.pkl"), "wb") as f:
#                         pickle.dump(history_log, f)
            
#             # Save model periodically using SavedModel format
#             # if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
#             #     try:
#             #         model_path = os.path.join(output_dir, "checkpoints", f"model_chunk_{chunk_id...
#             #         model.save(model_path, save_format='tf')
#             #         log_message(f"Model saved to {model_path}")
#             #     except Exception as save_e:
#             #         log_message(f"Error saving model: {save_e}")
#             #         # Try saving weights only
#             #         try:
#             #             weights_path = os.path.join(output_dir, "checkpoints", f"model_weights_{...
#             #             model.save_weights(weights_path)
#             #             log_message(f"Saved model weights to {weights_path}")
#             #         except:
#             #             log_message("Could not save model in any format")

#             # Save weights only at regular intervals
#             if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
#                 try:
#                     weights_path = os.path.join(output_dir, "checkpoints", f"model_weights_{chunk_...
#                     model.save_weights(weights_path)
#                     log_message(f"Saved model weights to {weights_path}")
#                 except Exception as save_e:
#                     log_message(f"Error saving weights: {save_e}")
#                     try:
#                         backup_path = os.path.join(output_dir, "checkpoints", f"backup_weights_{ch...
#                         model.save_weights(backup_path)
#                         log_message(f"Saved backup weights to {backup_path}")
#                     except:
#                         log_message("Could not save weights in any format")
            
#             # Explicitly delete everything from memory
#             del chunk_X, chunk_y
            
#         except Exception as outer_e:
#             log_message(f"Critical error processing chunk {chunk_idx+1}: {outer_e}")
#             import traceback
#             traceback.print_exc()
            
#             # Write error to file for debugging
#             with open(os.path.join(output_dir, f"error_chunk_{chunk_idx+1}.txt"), "w") as f:
#                 f.write(f"Error processing chunk {chunk_idx+1}:\n")
#                 f.write(traceback.format_exc())
                
#             # Try saving model in error state with SavedModel format
#             # try:
#             #     error_model_path = os.path.join(output_dir, "checkpoints", f"error_recovery_{chu...
#             #     model.save(error_model_path, save_format='tf')
#             #     log_message(f"Saved model in error state to {error_model_path}")
#             # except:
#             #     log_message("Could not save model in error state")

#             # Try saving weights in error state
#             try:
#                 error_weights_path = os.path.join(output_dir, "checkpoints", f"error_weights_{chun...
#                 model.save_weights(error_weights_path)
#                 log_message(f"Saved weights in error state to {error_weights_path}")
#             except:
#                 log_message("Could not save weights in error state")
                
#             # Continue to next chunk
#             continue
        
#         # Force garbage collection
#         gc.collect()
        
#         # Report memory after cleanup
#         memory_after = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
#         log_message(f"Memory after: {memory_after:.1f} MB (Change: {memory_after - memory_before:....
        
#         # Write recovery file with last completed chunk
#         with open(recovery_file, "w") as f:
#             f.write(str(chunk_idx + 1))
        
#         # Estimate time
#         elapsed = time.time() - start_time
#         avg_time_per_chunk = elapsed / (chunk_idx - start_chunk + 1) if chunk_idx > start_chunk el...
#         remaining = avg_time_per_chunk * (num_chunks - chunk_idx - 1)
#         log_message(f"Estimated remaining time: {timedelta(seconds=int(remaining))}")
        
#         # Reset TensorFlow session periodically to prevent memory growth
#         if (chunk_idx + 1) % 10 == 0:  # Reset more frequently
#             log_message("Resetting TensorFlow session to prevent memory issues")
#             try:
#                 temp_model_path = os.path.join(output_dir, "checkpoints", f"temp_reset_{chunk_idx+...
#                 model.save(temp_model_path, save_format='tf')
                
#                 # Clear session
#                 tf.keras.backend.clear_session()
#                 gc.collect()
                
#                 # Reload model
#                 model = tf.keras.models.load_model(temp_model_path)
#                 log_message("TensorFlow session reset complete")
#             except Exception as reset_e:
#                 log_message(f"Error during TensorFlow reset: {reset_e}")
#                 # Continue anyway
    
#     # Save final model with SavedModel format
#     # final_model_path = os.path.join(output_dir, "final_model")
#     # try:
#     #     model.save(final_model_path, save_format='tf')
#     #     log_message(f"Final model saved to {final_model_path}")
#     # except Exception as final_save_e:
#     #     log_message(f"Error saving final model: {final_save_e}")
#     #     try:
#     #         # Try alternative location
#     #         alt_path = os.path.join(output_dir, "checkpoints", "final_model_backup")
#     #         model.save(alt_path, save_format='tf')
#     #         final_model_path = alt_path
#     #         log_message(f"Saved final model to alternate location: {alt_path}")
#     #     except:
#     #         log_message("Could not save final model in any format")

#     # Save final model weights
#     final_weights_path = os.path.join(output_dir, "final_model_weights")
#     try:
#         model.save_weights(final_weights_path)
#         log_message(f"Final model weights saved to {final_weights_path}")
#     except Exception as final_save_e:
#         log_message(f"Error saving final weights: {final_save_e}")
#         try:
#             # Try alternative location
#             alt_path = os.path.join(output_dir, "checkpoints", "final_weights_backup")
#             model.save_weights(alt_path)
#             final_weights_path = alt_path
#             log_message(f"Saved final weights to alternate location: {alt_path}")
#         except:
#             log_message("Could not save final weights in any format")
    
#     return model, final_model_path

def improved_resumable_training_fixed(model, X_file, y_file, train_indices, val_indices, test_indices,
                                  output_dir, batch_size=256, chunk_size=10000, epochs_per_chunk=3, 
                                  save_frequency=5, class_weight=None, start_chunk=0):
    """
    Enhanced training function with improved error handling and memory management.
    Uses weights-only saving to avoid serialization issues.
    """
    import os
    import gc
    import json
    import numpy as np
    import tensorflow as tf
    from datetime import datetime, timedelta
    import time
    import psutil
    import shutil
    
    # Create output directories
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(os.path.join(output_dir, "checkpoints"), exist_ok=True)
    os.makedirs(os.path.join(output_dir, "logs"), exist_ok=True)
    
    # Setup logging
    log_file = os.path.join(output_dir, "logs", "training_log.txt")
    def log_message(message):
        with open(log_file, "a") as f:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            f.write(f"[{timestamp}] {message}\n")
        print(message)
    
    # Initialize best model tracking
    best_model_info = {
        'val_auc': 0.0,
        'checkpoint': None,
        'epoch': None,
        'chunk': None
    }
    
    # Load previous best model info if it exists
    best_model_file = os.path.join(output_dir, "best_model_info.json")
    if os.path.exists(best_model_file):
        try:
            with open(best_model_file, 'r') as f:
                best_model_info = json.load(f)
            log_message(f"Loaded previous best model info: {best_model_info}")
        except Exception as e:
            log_message(f"Error loading best model info: {e}")
    
    log_message("Starting robust training with fixed architecture")
    
    # Process in chunks to manage memory
    num_chunks = int(np.ceil(len(train_indices) / chunk_size))
    log_message(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
    log_message(f"Starting from chunk {start_chunk+1}")
    
    # Open data files with memory mapping
    X_mmap = np.load(X_file, mmap_mode='r')
    y_mmap = np.load(y_file, mmap_mode='r')
    
    # Create validation set once (limited size for memory efficiency)
    val_limit = min(1000, len(val_indices))
    val_indices_subset = val_indices[:val_limit]
    
    # Load validation data
    val_X = np.array([X_mmap[idx] for idx in val_indices_subset])
    val_y = np.array([y_mmap[idx] for idx in val_indices_subset])
    log_message(f"Loaded {len(val_X)} validation samples")
    
    # Load existing history if resuming
    history_log = []
    history_path = os.path.join(output_dir, "training_history.json")
    if start_chunk > 0 and os.path.exists(history_path):
        try:
            with open(history_path, "r") as f:
                history_log = json.load(f)
        except Exception as e:
            log_message(f"Could not load existing history: {e}")
            # Try pickle format
            pickle_path = os.path.join(output_dir, "training_history.pkl")
            if os.path.exists(pickle_path):
                import pickle
                with open(pickle_path, "rb") as f:
                    history_log = pickle.load(f)
    
    # If resuming, load latest model weights
    if start_chunk > 0:
        # Find the most recent weights file before start_chunk
        checkpoint_indices = []
        checkpoints_dir = os.path.join(output_dir, "checkpoints")
        
        for filename in os.listdir(checkpoints_dir):
            if filename.startswith("model_weights_") and ".index" in filename:
                try:
                    # Extract chunk number
                    chunk_str = filename.split("_")[-1].split(".")[0]
                    idx = int(chunk_str)
                    if idx < start_chunk:
                        checkpoint_indices.append(idx)
                except (ValueError, IndexError):
                    continue
        
        if checkpoint_indices:
            latest_idx = max(checkpoint_indices)
            weights_path = os.path.join(checkpoints_dir, f"model_weights_{latest_idx}")
            if os.path.exists(weights_path + ".index"):
                log_message(f"Loading weights from checkpoint {weights_path}")
                model.load_weights(weights_path)
            else:
                log_message(f"Warning: Could not find model weights for chunk {latest_idx}")
    
    # Setup callbacks
    callbacks = [
        # Early stopping with increased patience
        tf.keras.callbacks.EarlyStopping(
            patience=10, 
            restore_best_weights=True,
            monitor='val_auc',
            mode='max',
            min_delta=0.001
        ),
        # Reduced LR with increased patience
        tf.keras.callbacks.ReduceLROnPlateau(
            factor=0.5,
            patience=5,
            min_lr=1e-6,
            monitor='val_auc',
            mode='max'
        ),
        # Memory cleanup after each epoch
        tf.keras.callbacks.LambdaCallback(
            on_epoch_end=lambda epoch, logs: gc.collect()
        )
    ]
    
    # Track metrics across chunks
    start_time = time.time()
    
    # For safe recovery
    recovery_file = os.path.join(output_dir, "last_completed_chunk.txt")
    
    # Process each chunk
    for chunk_idx in range(start_chunk, num_chunks):
        # Track progress in file for recovery purposes
        with open(os.path.join(output_dir, "current_progress.txt"), "w") as f:
            f.write(f"Processing chunk {chunk_idx+1}/{num_chunks}\n")
            f.write(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # Get chunk indices
        start_idx = chunk_idx * chunk_size
        end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
        chunk_indices = train_indices[start_idx:end_idx]
        
        # Report memory
        memory_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
        log_message(f"\n{'='*50}")
        log_message(f"Processing chunk {chunk_idx+1}/{num_chunks} with {len(chunk_indices)} samples")
        log_message(f"Memory before: {memory_before:.1f} MB")
        
        # Force garbage collection before loading new data
        gc.collect()
        
        try:
            # Load chunk data in smaller batches
            chunk_X = []
            chunk_y = []
            
            # Use a smaller batch size for loading
            load_batch_size = 1000  # Reduced batch size for more stability
            for i in range(0, len(chunk_indices), load_batch_size):
                end_i = min(i + load_batch_size, len(chunk_indices))
                log_message(f"  Loading batch {i}-{end_i} of {len(chunk_indices)}...")
                
                try:
                    # Load this batch
                    batch_indices = chunk_indices[i:end_i]
                    X_batch = np.array([X_mmap[idx] for idx in batch_indices])
                    y_batch = np.array([y_mmap[idx] for idx in batch_indices])
                    
                    # Append to list
                    chunk_X.append(X_batch)
                    chunk_y.append(y_batch)
                    
                    # Free memory
                    del X_batch, y_batch
                    gc.collect()
                except Exception as inner_e:
                    log_message(f"Warning: Error loading batch {i}-{end_i}: {inner_e}")
                    # Continue to next batch
            
            # Combine batches
            try:
                chunk_X = np.concatenate(chunk_X)
                chunk_y = np.concatenate(chunk_y)
                log_message(f"Data loaded. Shape: X={chunk_X.shape}, y={chunk_y.shape}")
                log_message(f"Memory after loading: {psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024):.1f} MB")
            except Exception as concat_e:
                log_message(f"Error combining data batches: {concat_e}")
                # Skip this chunk
                continue
            
            # Train on chunk using a safe batch size
            mini_batch_size = min(batch_size, 64)  # Use smaller mini-batches for training
            log_message(f"Training with mini-batch size: {mini_batch_size}")
            
            # Track each epoch separately for better error handling
            chunk_history = {}
            for epoch in range(epochs_per_chunk):
                try:
                    log_message(f"Epoch {epoch+1}/{epochs_per_chunk}")
                    
                    # Train for a single epoch
                    history = model.fit(
                        chunk_X, chunk_y,
                        validation_data=(val_X, val_y),
                        epochs=1,
                        batch_size=mini_batch_size,
                        class_weight=class_weight,
                        callbacks=callbacks,
                        verbose=1
                    )
                    
                    # Check if loss is NaN and recover if needed
                    if np.isnan(history.history['loss'][0]):
                        log_message("WARNING: NaN loss detected! Reducing learning rate...")
                        current_lr = float(tf.keras.backend.get_value(model.optimizer.lr))
                        new_lr = current_lr * 0.1
                        tf.keras.backend.set_value(model.optimizer.lr, new_lr)
                        log_message(f"Reduced learning rate from {current_lr} to {new_lr}")
                        continue
                    
                    # Store history
                    for k, v in history.history.items():
                        if k not in chunk_history:
                            chunk_history[k] = []
                        chunk_history[k].extend([float(val) for val in v])
                    
                    # Save weights after each epoch
                    epoch_weights_path = os.path.join(output_dir, "checkpoints", f"model_weights_{chunk_idx+1}_epoch_{epoch+1}")
                    model.save_weights(epoch_weights_path)
                    log_message(f"Saved model weights after epoch {epoch+1} to {epoch_weights_path}")
                    
                    # Check if this is the best model so far
                    current_val_auc = float(history.history['val_auc'][0])
                    log_message(f"Epoch {epoch+1} validation AUC: {current_val_auc:.4f}")
                    
                    if current_val_auc > best_model_info.get('val_auc', 0):
                        previous_best = best_model_info.get('val_auc', 0)
                        best_model_info = {
                            'val_auc': current_val_auc,
                            'checkpoint': epoch_weights_path,
                            'epoch': epoch + 1,
                            'chunk': chunk_idx + 1,
                            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                        
                        # Save best model info
                        with open(best_model_file, 'w') as f:
                            json.dump(best_model_info, f, indent=4)
                        
                        # Make a copy with a clear "best" name for easier loading later
                        best_weights_path = os.path.join(output_dir, "checkpoints", "best_model_weights")
                        
                        # Copy all associated weight files
                        for file in os.listdir(os.path.dirname(epoch_weights_path)):
                            if file.startswith(os.path.basename(epoch_weights_path)):
                                src_path = os.path.join(os.path.dirname(epoch_weights_path), file)
                                dst_file = file.replace(os.path.basename(epoch_weights_path), "best_model_weights")
                                dst_path = os.path.join(os.path.dirname(best_weights_path), dst_file)
                                shutil.copy2(src_path, dst_path)
                        
                        log_message(f"New best model! AUC improved from {previous_best:.4f} to {current_val_auc:.4f}")
                        log_message(f"Best model weights copied to {best_weights_path}")
                        
                except Exception as epoch_e:
                    log_message(f"Error during epoch {epoch+1}: {epoch_e}")
                    import traceback
                    traceback.print_exc()
                    # Try to continue with next epoch
            
            # Store history from all completed epochs
            if chunk_history:
                history_log.append(chunk_history)
                
                # Save history
                try:
                    with open(history_path, "w") as f:
                        json.dump(history_log, f)
                except Exception as history_e:
                    log_message(f"Warning: Could not save history to JSON: {history_e}")
                    # Fallback - save as pickle
                    import pickle
                    with open(os.path.join(output_dir, "training_history.pkl"), "wb") as f:
                        pickle.dump(history_log, f)
            
            # Save weights periodically
            if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
                try:
                    weights_path = os.path.join(output_dir, "checkpoints", f"model_weights_{chunk_idx+1}")
                    model.save_weights(weights_path)
                    log_message(f"Saved model weights to {weights_path}")
                except Exception as save_e:
                    log_message(f"Error saving weights: {save_e}")
                    try:
                        backup_path = os.path.join(output_dir, "checkpoints", f"backup_weights_{chunk_idx+1}")
                        model.save_weights(backup_path)
                        log_message(f"Saved backup weights to {backup_path}")
                    except:
                        log_message("Could not save weights in any format")
            
            # Explicitly delete everything from memory
            del chunk_X, chunk_y
            
        except Exception as outer_e:
            log_message(f"Critical error processing chunk {chunk_idx+1}: {outer_e}")
            import traceback
            traceback.print_exc()
            
            # Write error to file for debugging
            with open(os.path.join(output_dir, f"error_chunk_{chunk_idx+1}.txt"), "w") as f:
                f.write(f"Error processing chunk {chunk_idx+1}:\n")
                f.write(traceback.format_exc())
                
            # Try saving weights in error state
            try:
                error_weights_path = os.path.join(output_dir, "checkpoints", f"error_weights_{chunk_idx+1}")
                model.save_weights(error_weights_path)
                log_message(f"Saved weights in error state to {error_weights_path}")
            except:
                log_message("Could not save weights in error state")
                
            # Continue to next chunk
            continue
        
        # Force garbage collection
        gc.collect()
        
        # Report memory after cleanup
        memory_after = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
        log_message(f"Memory after: {memory_after:.1f} MB (Change: {memory_after - memory_before:.1f} MB)")
        
        # Write recovery file with last completed chunk
        with open(recovery_file, "w") as f:
            f.write(str(chunk_idx + 1))
        
        # Estimate time
        elapsed = time.time() - start_time
        avg_time_per_chunk = elapsed / (chunk_idx - start_chunk + 1) if chunk_idx > start_chunk else elapsed
        remaining = avg_time_per_chunk * (num_chunks - chunk_idx - 1)
        log_message(f"Estimated remaining time: {timedelta(seconds=int(remaining))}")
        
        # Reset TensorFlow session periodically to prevent memory growth
        if (chunk_idx + 1) % 10 == 0:  # Reset more frequently
            log_message("Resetting TensorFlow session to prevent memory issues")
            try:
                # First save the weights
                temp_weights_path = os.path.join(output_dir, "checkpoints", f"temp_reset_{chunk_idx+1}")
                model.save_weights(temp_weights_path)
                
                # Get input shape for rebuilding model
                input_shape = model.input_shape[1:]
                
                # Clear session
                tf.keras.backend.clear_session()
                gc.collect()
                
                # Rebuild model and reload weights
                from zero_curtain_pipeline.modeling.fixed_code import build_improved_zero_curtain_model_fixed
                model = build_improved_zero_curtain_model_fixed(input_shape)
                model.load_weights(temp_weights_path)
                log_message("TensorFlow session reset complete")
            except Exception as reset_e:
                log_message(f"Error during TensorFlow reset: {reset_e}")
                # Continue anyway
    
    # Save final model weights
    final_weights_path = os.path.join(output_dir, "final_model_weights")
    try:
        model.save_weights(final_weights_path)
        log_message(f"Final model weights saved to {final_weights_path}")
    except Exception as final_save_e:
        log_message(f"Error saving final weights: {final_save_e}")
        try:
            # Try alternative location
            alt_path = os.path.join(output_dir, "checkpoints", "final_weights_backup")
            model.save_weights(alt_path)
            final_weights_path = alt_path
            log_message(f"Saved final weights to alternate location: {alt_path}")
        except:
            log_message("Could not save final weights in any format")
    
    # Log info about the best model
    if best_model_info.get('checkpoint'):
        log_message("\nBest model summary:")
        log_message(f"  Validation AUC: {best_model_info['val_auc']:.4f}")
        log_message(f"  From chunk {best_model_info['chunk']}, epoch {best_model_info['epoch']}")
        log_message(f"  Weights saved at: {best_model_info['checkpoint']}")
        log_message(f"  And copied to: {os.path.join(output_dir, 'checkpoints', 'best_model_weights')}")
    
    return model, final_weights_path

def evaluate_model_with_visualizations_fixed(model, X_file, y_file, test_indices, output_dir, metadata_file=None):
    """
    Comprehensive evaluation with detailed visualizations and robust error handling.
    """
    import os
    import gc
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.metrics import (
        classification_report, confusion_matrix, roc_curve, 
        auc, precision_recall_curve, average_precision_score
    )
    import traceback
    
    os.makedirs(os.path.join(output_dir, "visualizations"), exist_ok=True)
    
    # Setup logging
    log_file = os.path.join(output_dir, "evaluation_log.txt")
    def log_message(message):
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with open(log_file, "a") as f:
            f.write(f"[{timestamp}] {message}\n")
        print(message)
    
    log_message(f"Starting evaluation on {len(test_indices)} test samples")
    
    # Load metadata if available
    metadata = None
    if metadata_file and os.path.exists(metadata_file):
        try:
            import pickle
            with open(metadata_file, "rb") as f:
                metadata = pickle.load(f)
            log_message(f"Loaded metadata from {metadata_file}")
        except Exception as e:
            log_message(f"Error loading metadata: {str(e)}")
    
    # Load memory-mapped data
    X = np.load(X_file, mmap_mode='r')
    y = np.load(y_file, mmap_mode='r')
    
    # Process test data in batches - use smaller batches for better stability
    batch_size = 100  # Reduced batch size for more reliable processing
    num_batches = int(np.ceil(len(test_indices) / batch_size))
    
    all_preds = []
    all_true = []
    all_meta = []
    
    log_message(f"Processing test data in {num_batches} batches of size {batch_size}")
    
    # Save test indices for reference
    np.save(os.path.join(output_dir, "test_indices.npy"), test_indices)
    
    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size, len(test_indices))
        batch_indices = test_indices[start_idx:end_idx]
        
        log_message(f"Processing batch {batch_idx+1}/{num_batches} with {len(batch_indices)} samples")
        
        try:
            # Load batch data
            batch_X = np.array([X[idx] for idx in batch_indices])
            batch_y = np.array([y[idx] for idx in batch_indices])
            
            # Store metadata if available
            if metadata:
                try:
                    batch_meta = []
                    for idx in batch_indices:
                        if idx in metadata:
                            batch_meta.append(metadata[idx])
                        else:
                            # Create empty metadata if not found
                            batch_meta.append({})
                    all_meta.extend(batch_meta)
                except Exception as meta_e:
                    log_message(f"Error extracting metadata for batch {batch_idx}: {str(meta_e)}")
                    # Continue without metadata for this batch
            
            # Predict with error handling
            try:
                batch_preds = model.predict(batch_X, verbose=0)
            except Exception as pred_e:
                log_message(f"Error during prediction on batch {batch_idx}: {str(pred_e)}")
                # Try with smaller sub-batches
                batch_preds = []
                sub_batch_size = 10
                for i in range(0, len(batch_X), sub_batch_size):
                    end_i = min(i + sub_batch_size, len(batch_X))
                    sub_batch = batch_X[i:end_i]
                    sub_preds = model.predict(sub_batch, verbose=0)
                    batch_preds.append(sub_preds)
                batch_preds = np.concatenate(batch_preds)
            
            # Store results
            all_preds.extend(batch_preds.flatten())
            all_true.extend(batch_y)
            
            # Clean up
            del batch_X, batch_y, batch_preds
            gc.collect()
            
            log_message(f"Completed batch {batch_idx+1}/{num_batches}")
            
        except Exception as e:
            log_message(f"Error processing batch {batch_idx+1}: {str(e)}")
            log_message(traceback.format_exc())
            
            # Try to process the batch in smaller chunks
            try:
                log_message("Attempting to process batch in smaller chunks...")
                sub_batch_size = 10  # Much smaller batch
                sub_batches = int(np.ceil(len(batch_indices) / sub_batch_size))
                
                for sub_idx in range(sub_batches):
                    sub_start = sub_idx * sub_batch_size
                    sub_end = min((sub_idx + 1) * sub_batch_size, len(batch_indices))
                    sub_indices = batch_indices[sub_start:sub_end]
                    
                    # Load and process sub-batch
                    sub_X = np.array([X[idx] for idx in sub_indices])
                    sub_y = np.array([y[idx] for idx in sub_indices])
                    
                    # Add metadata if available
                    if metadata:
                        sub_meta = []
                        for idx in sub_indices:
                            if idx in metadata:
                                sub_meta.append(metadata[idx])
                            else:
                                sub_meta.append({})
                        all_meta.extend(sub_meta)
                    
                    # Predict
                    sub_preds = model.predict(sub_X, verbose=0)
                    
                    # Store results
                    all_preds.extend(sub_preds.flatten())
                    all_true.extend(sub_y)
                    
                    # Clean up
                    del sub_X, sub_y, sub_preds
                    gc.collect()
                
                log_message(f"Successfully processed batch {batch_idx+1} in {sub_batches} sub-batches")
            except Exception as sub_e:
                log_message(f"Error processing sub-batches: {str(sub_e)}")
                log_message(traceback.format_exc())
                log_message(f"Skipping batch {batch_idx+1}")
    
    log_message(f"Prediction complete. Total samples: {len(all_preds)}")
    
    # Save raw predictions for reference
    np.save(os.path.join(output_dir, "test_predictions.npy"), np.array(all_preds))
    np.save(os.path.join(output_dir, "test_true_labels.npy"), np.array(all_true))
    
    # Convert to numpy arrays
    all_preds = np.array(all_preds)
    all_true = np.array(all_true)
    
    try:
        # Create binary predictions
        all_preds_binary = (all_preds > 0.5).astype(int)
        
        # Calculate metrics
        report = classification_report(all_true, all_preds_binary, output_dict=True)
        report_str = classification_report(all_true, all_preds_binary)
        conf_matrix = confusion_matrix(all_true, all_preds_binary)
        
        log_message("\nClassification Report:")
        log_message(report_str)
        
        log_message("\nConfusion Matrix:")
        log_message(str(conf_matrix))
        
        # Calculate ROC curve and AUC
        fpr, tpr, _ = roc_curve(all_true, all_preds)
        roc_auc = auc(fpr, tpr)
        
        # Calculate Precision-Recall curve
        precision, recall, _ = precision_recall_curve(all_true, all_preds)
        avg_precision = average_precision_score(all_true, all_preds)
        
        log_message(f"\nROC AUC: {roc_auc:.4f}")
        log_message(f"Average Precision: {avg_precision:.4f}")
        
        # Save results to file
        with open(os.path.join(output_dir, "evaluation_results.txt"), "w") as f:
            f.write("Classification Report:\n")
            f.write(report_str)
            f.write("\n\nConfusion Matrix:\n")
            f.write(str(conf_matrix))
            f.write(f"\n\nROC AUC: {roc_auc:.4f}")
            f.write(f"\nAverage Precision: {avg_precision:.4f}")
        
        # Create visualizations
        try:
            # 1. Confusion Matrix
            plt.figure(figsize=(10, 8))
            sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                      xticklabels=['Negative', 'Positive'],
                      yticklabels=['Negative', 'Positive'])
            plt.xlabel('Predicted Label')
            plt.ylabel('True Label')
            plt.title('Confusion Matrix')
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, "visualizations", "confusion_matrix.png"), dpi=300)
            plt.close()
            
            # 2. ROC Curve
            plt.figure(figsize=(10, 8))
            plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')
            plt.plot([0, 1], [0, 1], 'k--')
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title('Receiver Operating Characteristic (ROC) Curve')
            plt.legend(loc="lower right")
            plt.grid(alpha=0.3)
            plt.savefig(os.path.join(output_dir, "visualizations", "roc_curve.png"), dpi=300)
            plt.close()
            
            # 3. Precision-Recall Curve
            plt.figure(figsize=(10, 8))
            plt.plot(recall, precision, label=f'Precision-Recall curve (AP = {avg_precision:.3f})')
            plt.xlabel('Recall')
            plt.ylabel('Precision')
            plt.title('Precision-Recall Curve')
            plt.legend(loc="upper right")
            plt.grid(alpha=0.3)
            plt.savefig(os.path.join(output_dir, "visualizations", "precision_recall_curve.png"), dpi=300)
            plt.close()
            
            # 4. Score distribution
            plt.figure(figsize=(12, 6))
            sns.histplot(all_preds[all_true == 0], bins=50, alpha=0.5, label='Negative Class', color='blue')
            sns.histplot(all_preds[all_true == 1], bins=50, alpha=0.5, label='Positive Class', color='red')
            plt.xlabel('Prediction Score')
            plt.ylabel('Count')
            plt.title('Distribution of Prediction Scores by Class')
            plt.axvline(x=0.5, color='black', linestyle='--', alpha=0.7)
            plt.legend()
            plt.grid(alpha=0.3)
            plt.savefig(os.path.join(output_dir, "visualizations", "score_distribution.png"), dpi=300)
            plt.close()
            
            log_message("Saved all visualization plots")
        except Exception as plot_e:
            log_message(f"Error creating visualization plots: {str(plot_e)}")
            log_message(traceback.format_exc())
        
        # Save predictions with metadata
        if metadata and all_meta:
            try:
                # Create results DataFrame with metadata
                results_data = []
                
                for i in range(len(all_preds)):
                    if i < len(all_meta):
                        result = {
                            'prediction': float(all_preds[i]),
                            'true_label': int(all_true[i]),
                            'predicted_label': int(all_preds_binary[i]),
                            'correct': int(all_preds_binary[i] == all_true[i])
                        }
                        
                        # Add metadata
                        meta = all_meta[i]
                        for key, value in meta.items():
                            # Convert numpy types to Python native types
                            if hasattr(value, 'dtype'):
                                if np.issubdtype(value.dtype, np.integer):
                                    value = int(value)
                                elif np.issubdtype(value.dtype, np.floating):
                                    value = float(value)
                            result[key] = value
                        
                        results_data.append(result)
                
                # Create DataFrame
                results_df = pd.DataFrame(results_data)
                
                # Save to CSV
                results_df.to_csv(os.path.join(output_dir, "test_predictions_with_metadata.csv"), index=False)
                log_message(f"Saved predictions with metadata to CSV, shape: {results_df.shape}")
            except Exception as csv_e:
                log_message(f"Error saving predictions CSV: {str(csv_e)}")
                log_message(traceback.format_exc())
                
                # Try a simpler format
                try:
                    simple_df = pd.DataFrame({
                        'prediction': all_preds,
                        'true_label': all_true,
                        'predicted_label': all_preds_binary
                    })
                    simple_df.to_csv(os.path.join(output_dir, "simple_predictions.csv"), index=False)
                    log_message("Saved simplified predictions to CSV")
                except:
                    log_message("Failed to save predictions in any format")
        
        # Return metrics dictionary
        evaluation_metrics = {
            'roc_auc': float(roc_auc),
            'avg_precision': float(avg_precision),
            'accuracy': float(report['accuracy']),
            'precision': float(report['1']['precision']),
            'recall': float(report['1']['recall']),
            'f1_score': float(report['1']['f1-score']),
            'num_samples': len(all_true),
            'positive_rate': float(np.mean(all_true))
        }
        
        # Save metrics to JSON
        import json
        with open(os.path.join(output_dir, "evaluation_metrics.json"), "w") as f:
            json.dump(evaluation_metrics, f, indent=4)
        
        log_message("Evaluation complete")
        return evaluation_metrics
    
    except Exception as metrics_e:
        log_message(f"Error calculating metrics: {str(metrics_e)}")
        log_message(traceback.format_exc())
        
        # Return basic metrics that we can calculate
        return {
            'num_samples': len(all_true),
            'positive_rate': float(np.mean(all_true)),
            'mean_prediction': float(np.mean(all_preds)),
            'error': str(metrics_e)
        }

# def run_improved_pipeline_fixed():
#     """
#     Run the fixed and optimized pipeline with all improvements.
#     """
#     import os
#     import numpy as np
#     import tensorflow as tf
#     import gc
#     import json
#     import pickle
#     import time
#     from datetime import datetime, timedelta
    
#     # Configure TensorFlow with memory optimizations
#     from tensorflow.keras.layers import BatchNormalization, Layer

#     # BatchNorm5D class definition (for loading models)
#     class BatchNorm5D(Layer):
#         """
#         Fixed implementation of BatchNorm5D that properly handles 5D inputs from ConvLSTM2D.
#         """
#         def __init__(self, **kwargs):
#             super(BatchNorm5D, self).__init__(**kwargs)
#             self.bn = BatchNormalization()
            
#         def call(self, inputs, training=None):
#             # Get the dimensions
#             input_shape = tf.shape(inputs)
#             batch_size = input_shape[0]
#             time_steps = input_shape[1]
#             height = input_shape[2]
#             width = input_shape[3]
#             channels = input_shape[4]
            
#             # Reshape to 4D for BatchNorm by combining batch and time dimensions
#             x_reshaped = tf.reshape(inputs, [-1, height, width, channels])
            
#             # Apply BatchNorm
#             x_bn = self.bn(x_reshaped, training=training)
            
#             # Reshape back to 5D
#             x_back = tf.reshape(x_bn, [batch_size, time_steps, height, width, channels])
#             return x_back
        
#         def get_config(self):
#             config = super(BatchNorm5D, self).get_config()
#             return config
    
#     # Register custom objects
#     tf.keras.utils.get_custom_objects().update({'BatchNorm5D': BatchNorm5D})
    
#     # Configure TensorFlow memory
#     def configure_tensorflow_memory():
#         """Configure TensorFlow memory settings to prevent OOM errors"""
#         try:
#             # GPU memory growth
#             gpus = tf.config.list_physical_devices('GPU')
#             for gpu in gpus:
#                 try:
#                     tf.config.experimental.set_memory_growth(gpu, True)
#                     print(f"Memory growth enabled for {gpu}")
#                 except Exception as e:
#                     print(f"Error configuring GPU {gpu}: {e}")
            
#             # Set soft device placement
#             tf.config.set_soft_device_placement(True)
            
#             # Set operation timeout
#             tf.config.threading.set_intra_op_parallelism_threads(2)
#             tf.config.threading.set_inter_op_parallelism_threads(2)
            
#             print("TensorFlow memory configuration complete")
#         except Exception as e:
#             print(f"Error in TensorFlow configuration: {e}")
    
#     # Setup logging
#     def setup_logging(output_dir):
#         """Create a logger for the pipeline"""
#         log_file = os.path.join(output_dir, "pipeline_log.txt")
        
#         def log_message(message, print_to_console=True):
#             timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
#             with open(log_file, "a") as f:
#                 f.write(f"[{timestamp}] {message}\n")
#             if print_to_console:
#                 print(message)
                
#         return log_message
    
#     # Configure TensorFlow
#     configure_tensorflow_memory()
    
#     # Paths and configuration
#     output_dir = '/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling/improved_model'
#     data_dir = '/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling/ml_data'
#     X_file = os.path.join(data_dir, 'X_features.npy')
#     y_file = os.path.join(data_dir, 'y_labels.npy')
#     metadata_file = os.path.join(data_dir, 'metadata.pkl')
    
#     # Create directories
#     os.makedirs(output_dir, exist_ok=True)
#     os.makedirs(os.path.join(output_dir, "checkpoints"), exist_ok=True)
#     os.makedirs(os.path.join(output_dir, "visualizations"), exist_ok=True)
#     os.makedirs(os.path.join(output_dir, "logs"), exist_ok=True)
    
#     # Create logger
#     log = setup_logging(output_dir)
    
#     log("="*80)
#     log(f"Starting improved pipeline run at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
#     log("="*80)
    
#     # Record pipeline start time
#     pipeline_start_time = time.time()
    
#     try:
#         # Step 1: Load split indices
#         log("\nStep 1: Loading data splits...")
#         split_file = "zero_curtain_pipeline/modeling/checkpoints/spatiotemporal_split.pkl"
#         if not os.path.exists(split_file):
#             log(f"Error: Split file not found at {split_file}", True)
#             return {'error': f"Split file not found: {split_file}"}
            
#         with open(split_file, "rb") as f:
#             split_data = pickle.load(f)
            
#         train_indices = split_data["train_indices"]
#         val_indices = split_data["val_indices"]
#         test_indices = split_data["test_indices"]
        
#         log(f"Loaded splits: {len(train_indices)} train, {len(val_indices)} validation, {len(test_...
        
#         # Step 2: Calculate class weights for imbalanced data
#         log("\nStep 2: Calculating class weights...")
#         try:
#             # Load labels
#             y = np.load(y_file, mmap_mode='r')
#             train_y = y[train_indices]
            
#             # Calculate class weights
#             pos_weight = (len(train_y) - np.sum(train_y)) / max(1, np.sum(train_y))
#             class_weight = {0: 1.0, 1: pos_weight}
#             log(f"Class weight for positive class: {pos_weight:.2f}")
#         except Exception as weight_e:
#             log(f"Error calculating class weights: {weight_e}")
#             class_weight = None
        
#         # Step 3: Build model with fixed architecture
#         log("\nStep 3: Building model with fixed architecture...")
#         try:
#             # Get sample shape
#             X = np.load(X_file, mmap_mode='r')
#             sample = X[train_indices[0]]
#             input_shape = sample.shape
#             log(f"Input shape: {input_shape}")
            
#             # Import the fixed model
#             from tensorflow.keras.models import Model
#             from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
#             from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, Glob...
#             from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add, Lambd...
#             from tensorflow.keras.optimizers import Adam
#             from tensorflow.keras.regularizers import l2
            
#             # Build with fixed model function
#             model = build_improved_zero_curtain_model_fixed(input_shape)
            
#             # Log model summary
#             model_summary = []
#             model.summary(print_fn=lambda x: model_summary.append(x))
#             for line in model_summary:
#                 log(line, False)  # Log without printing to console
#             log(f"Model built successfully with {model.count_params():,} parameters")
#         except Exception as model_e:
#             log(f"Error building model: {model_e}")
#             import traceback
#             log(traceback.format_exc())
#             return {'error': f"Model building failed: {str(model_e)}"}
        
#         # Step 4: Train model
#         log("\nStep 4: Training model...")
#         try:
#             # Check for existing checkpoints
#             latest_chunk = 0
#             recovery_file = os.path.join(output_dir, "last_completed_chunk.txt")
#             if os.path.exists(recovery_file):
#                 with open(recovery_file, "r") as f:
#                     try:
#                         latest_chunk = int(f.read().strip())
#                         log(f"Found recovery point at chunk {latest_chunk}")
#                     except:
#                         log("Could not parse recovery file")
            
#             # Ask for confirmation if resuming
#             if latest_chunk > 0:
#                 log(f"Resuming training from chunk {latest_chunk}")
#             else:
#                 log("Starting training from beginning")
            
#             # Set chunk size based on available memory
#             # Smaller chunks for better stability
#             chunk_size = 10000
            
#             # Train model with fixed training function
#             training_start = time.time()
#             model, model_path = improved_resumable_training_fixed(
#                 model=model,
#                 X_file=X_file,
#                 y_file=y_file,
#                 train_indices=train_indices,
#                 val_indices=val_indices,
#                 test_indices=test_indices,
#                 output_dir=output_dir,
#                 batch_size=256,
#                 chunk_size=chunk_size,
#                 epochs_per_chunk=3,
#                 save_frequency=5,
#                 class_weight=class_weight,
#                 start_chunk=latest_chunk
#             )
            
#             training_time = time.time() - training_start
#             log(f"Training completed in {timedelta(seconds=int(training_time))}")
#             log(f"Trained model saved to {model_path}")
#         except Exception as train_e:
#             log(f"Error during training: {train_e}")
#             import traceback
#             log(traceback.format_exc())
            
#             # Try to recover best model if training failed
#             try:
#                 checkpoint_dir = os.path.join(output_dir, "checkpoints")
#                 checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith("best_mo...
                
#                 if checkpoint_files:
#                     best_model_path = os.path.join(checkpoint_dir, sorted(checkpoint_files)[-1])
#                     log(f"Loading best model from {best_model_path}")
#                     model = tf.keras.models.load_model(best_model_path, custom_objects={'BatchNorm...
#                     model_path = best_model_path
#                 else:
#                     log("No best model found. Training failed.")
#                     return {'error': f"Training failed: {str(train_e)}"}
#             except Exception as recovery_e:
#                 log(f"Failed to recover model: {recovery_e}")
#                 return {'error': f"Training and recovery failed: {str(train_e)}"}
        
#         # Clean up between steps
#         gc.collect()
#         tf.keras.backend.clear_session()
        
#         # Step 5: Evaluate model
#         log("\nStep 5: Evaluating model...")
#         try:
#             # Reload the model to ensure clean state
#             if model_path and os.path.exists(model_path):
#                 model = tf.keras.models.load_model(model_path, custom_objects={'BatchNorm5D': Batc...
#                 log(f"Loaded model from {model_path}")
            
#             eval_start = time.time()
#             eval_results = evaluate_model_with_visualizations_fixed(
#                 model=model,
#                 X_file=X_file,
#                 y_file=y_file,
#                 test_indices=test_indices,
#                 output_dir=output_dir,
#                 metadata_file=metadata_file
#             )
            
#             eval_time = time.time() - eval_start
#             log(f"Evaluation completed in {timedelta(seconds=int(eval_time))}")
            
#             # Log key metrics
#             log("\nEvaluation Results:")
#             for metric, value in eval_results.items():
#                 if not isinstance(value, dict):
#                     log(f"  {metric}: {value}")
#         except Exception as eval_e:
#             log(f"Error during evaluation: {eval_e}")
#             import traceback
#             log(traceback.format_exc())
#             eval_results = {'error': str(eval_e)}
        
#         # Clean up between steps
#         gc.collect()
#         tf.keras.backend.clear_session()
        
#         # Step 6: Analyze spatial performance
#         log("\nStep 6: Analyzing spatial performance...")
#         try:
#             spatial_start = time.time()
#             spatial_results = analyze_spatial_performance_fixed(
#                 output_dir=output_dir,
#                 metadata_file=metadata_file
#             )
            
#             spatial_time = time.time() - spatial_start
#             log(f"Spatial analysis completed in {timedelta(seconds=int(spatial_time))}")
            
#             # Log key information
#             log(f"Analyzed spatial performance across {spatial_results.get('latitude_bands', {}).g...
#         except Exception as spatial_e:
#             log(f"Error during spatial analysis: {spatial_e}")
#             import traceback
#             log(traceback.format_exc())
#             spatial_results = {'error': str(spatial_e)}
        
#         # Clean up between steps
#         gc.collect()
#         tf.keras.backend.clear_session()
        
#         # Step 7: Analyze feature importance
#         log("\nStep 7: Analyzing feature importance...")
#         try:
#             # Reload the model to ensure clean state
#             if model_path and os.path.exists(model_path):
#                 model = tf.keras.models.load_model(model_path, custom_objects={'BatchNorm5D': Batc...
#                 log(f"Loaded model from {model_path}")
            
#             feature_start = time.time()
#             feature_results = analyze_feature_importance_fixed(
#                 model=model,
#                 X_file=X_file,
#                 y_file=y_file,
#                 test_indices=test_indices,
#                 output_dir=output_dir
#             )
            
#             feature_time = time.time() - feature_start
#             log(f"Feature importance analysis completed in {timedelta(seconds=int(feature_time))}"...
            
#             # Log feature importance
#             if 'feature_importance' in feature_results:
#                 log("\nFeature Importance Rankings:")
#                 for feature in feature_results['feature_importance']:
#                     log(f"  {feature['Feature']}: {feature['Importance']:.4f} ± {feature['Std']:.4...
#         except Exception as feature_e:
#             log(f"Error during feature importance analysis: {feature_e}")
#             import traceback
#             log(traceback.format_exc())
#             feature_results = {'error': str(feature_e)}
        
#         # Step 8: Save all results
#         log("\nStep 8: Compiling and saving all results...")
#         try:
#             all_results = {
#                 'evaluation': eval_results,
#                 'spatial_analysis': spatial_results,
#                 'feature_importance': feature_results,
#                 'pipeline_runtime': {
#                     'start_time': datetime.fromtimestamp(pipeline_start_time).strftime('%Y-%m-%d %...
#                     'end_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
#                     'total_seconds': time.time() - pipeline_start_time
#                 }
#             }
            
#             # Save as JSON with NumPy type handling
#             with open(os.path.join(output_dir, "complete_analysis.json"), "w") as f:
#                 # Convert numpy values to Python native types
#                 class NumpyEncoder(json.JSONEncoder):
#                     def default(self, obj):
#                         if isinstance(obj, (np.integer, np.floating)):
#                             return float(obj)
#                         elif isinstance(obj, np.ndarray):
#                             return obj.tolist()
#                         elif isinstance(obj, np.bool_):
#                             return bool(obj)
#                         elif np.isnan(obj):
#                             return None
#                         return super(NumpyEncoder, self).default(obj)
                
#                 json.dump(all_results, f, indent=4, cls=NumpyEncoder)
            
#             log(f"All results saved to {os.path.join(output_dir, 'complete_analysis.json')}")
#         except Exception as save_e:
#             log(f"Error saving results: {save_e}")
#             import traceback
#             log(traceback.format_exc())
        
#         # Pipeline complete
#         pipeline_end_time = time.time()
#         elapsed = pipeline_end_time - pipeline_start_time
        
#         log("\n" + "="*80)
#         log(f"Pipeline completed in {timedelta(seconds=int(elapsed))}")
#         log(f"Results saved to {output_dir}")
#         log("="*80)
        
#         return all_results
        
#     except Exception as pipeline_e:
#         log(f"Critical error in pipeline: {pipeline_e}")
#         import traceback
#         log(traceback.format_exc())
#         return {'error': f"Pipeline failed: {str(pipeline_e)}"}


# # Helper function to be called from command line
# if __name__ == "__main__":
#     run_improved_pipeline_fixed()

# def ensure_training_progress(model, X_file, y_file, train_indices, output_dir, start_chunk=0, save...
#     """
#     A more reliable training approach that completely rebuilds the model periodically
#     to avoid memory leaks and state accumulation issues.
#     """
#     import os
#     import gc
#     import numpy as np
#     import tensorflow as tf
#     from datetime import datetime
#     import time
    
#     # Configuration
#     max_chunks_per_session = 5  # Rebuild model every 5 chunks
#     chunk_size = 10000
    
#     # Create log directory
#     os.makedirs(os.path.join(output_dir, "logs"), exist_ok=True)
#     log_file = os.path.join(output_dir, "logs", "robust_training.txt")
    
#     def log_message(message):
#         with open(log_file, "a") as f:
#             timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
#             f.write(f"[{timestamp}] {message}\n")
#         print(message)
    
#     log_message("Starting robust training with complete model rebuilding")
    
#     # Total chunks to process
#     num_chunks = int(np.ceil(len(train_indices) / chunk_size))
#     log_message(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
    
#     # Memory-mapped data access
#     X_mmap = np.load(X_file, mmap_mode='r')
#     y_mmap = np.load(y_file, mmap_mode='r')
    
#     # Get input shape for model building
#     sample_shape = X_mmap[train_indices[0]].shape
#     log_message(f"Input shape: {sample_shape}")
    
#     # Training loop with rebuilding
#     current_chunk = start_chunk
#     while current_chunk < num_chunks:
#         # Calculate range of chunks for this session
#         session_end = min(current_chunk + max_chunks_per_session, num_chunks)
#         log_message(f"Starting training session for chunks {current_chunk+1} to {session_end}")
        
#         # Build fresh model
#         log_message("Building fresh model")
#         tf.keras.backend.clear_session()
#         gc.collect()
        
#         model = build_improved_zero_curtain_model_fixed(sample_shape)
        
#         # Load latest weights if not starting from beginning
#         if current_chunk > 0:
#             # Find latest weights file
#             latest_chunk = current_chunk
#             while latest_chunk > 0:
#                 weights_path = os.path.join(output_dir, "checkpoints", f"model_weights_{latest_chu...
#                 if os.path.exists(weights_path+".index"):
#                     log_message(f"Loading weights from chunk {latest_chunk}")
#                     model.load_weights(weights_path)
#                     break
#                 latest_chunk -= 1
        
#         # Process chunks in this session
#         for chunk_idx in range(current_chunk, session_end):
#             log_message(f"\n{'='*50}")
#             log_message(f"Processing chunk {chunk_idx+1}/{num_chunks}")
            
#             # Get chunk indices
#             start_idx = chunk_idx * chunk_size
#             end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
#             chunk_indices = train_indices[start_idx:end_idx]
            
#             # Load data in batches
#             X_chunks = []
#             y_chunks = []
            
#             for i in range(0, len(chunk_indices), 1000):
#                 end_i = min(i + 1000, len(chunk_indices))
#                 log_message(f"  Loading batch {i+1}-{end_i} of {len(chunk_indices)}")
                
#                 batch_indices = chunk_indices[i:end_i]
#                 X_batch = np.array([X_mmap[idx] for idx in batch_indices])
#                 y_batch = np.array([y_mmap[idx] for idx in batch_indices])
                
#                 X_chunks.append(X_batch)
#                 y_chunks.append(y_batch)
                
#                 # Clean up
#                 del X_batch, y_batch
#                 gc.collect()
            
#             # Combine data
#             X_data = np.concatenate(X_chunks)
#             y_data = np.concatenate(y_chunks)
#             log_message(f"Data loaded: X={X_data.shape}, y={y_data.shape}")
            
#             # Train for a few epochs
#             try:
#                 history = model.fit(
#                     X_data, y_data,
#                     epochs=3,
#                     batch_size=64,
#                     verbose=1
#                 )
                
#                 # Save weights after each chunk
#                 weights_path = os.path.join(output_dir, "checkpoints", f"model_weights_{chunk_idx+...
#                 model.save_weights(weights_path)
#                 log_message(f"Saved weights to {weights_path}")
                
#                 # Save last metrics
#                 metrics = {k: float(v[-1]) for k, v in history.history.items()}
#                 log_message(f"Metrics: {metrics}")
                
#                 # Update progress file
#                 with open(os.path.join(output_dir, "last_completed_chunk.txt"), "w") as f:
#                     f.write(str(chunk_idx + 1))
                
#             except Exception as e:
#                 log_message(f"Error during training: {str(e)}")
#                 # Save weights to recover later
#                 try:
#                     error_path = os.path.join(output_dir, "checkpoints", f"error_model_weights_{ch...
#                     model.save_weights(error_path)
#                     log_message(f"Saved error state weights to {error_path}")
#                 except:
#                     log_message("Could not save error state weights")
            
#             # Clean up
#             del X_chunks, y_chunks, X_data, y_data
#             gc.collect()
        
#         # Update current chunk for next session
#         current_chunk = session_end
        
#         # Explicitly delete model and clear TF session
#         del model
#         tf.keras.backend.clear_session()
#         gc.collect()
        
#         log_message(f"Completed training session. Progress: {current_chunk}/{num_chunks} chunks")
    
#     log_message("Training complete!")
#     return os.path.join(output_dir, "checkpoints", f"model_weights_{num_chunks}")

def ensure_training_progress(model, X_file, y_file, train_indices, output_dir, start_chunk=0, save_frequency=5, build_model_func=None):
    """
    A more reliable training approach that completely rebuilds the model periodically.
    
    Parameters:
    -----------
    model : tf.keras.Model or None
        Initial model. If None, a new one will be built.
    X_file : str
        Path to features numpy file
    y_file : str
        Path to labels numpy file
    train_indices : numpy.ndarray
        Indices of training samples
    output_dir : str
        Output directory for logs and checkpoints
    start_chunk : int
        Chunk to start/resume from
    save_frequency : int
        How often to save model weights (in chunks)
    build_model_func : function
        Function to build model - should accept input_shape parameter
    """
    import os
    import gc
    import numpy as np
    import tensorflow as tf
    from datetime import datetime
    import time
    
    # Make sure we have a model building function
    if build_model_func is None:
        build_model_func = build_improved_zero_curtain_model_fixed
    
    # Configuration
    max_chunks_per_session = 5  # Rebuild model every 5 chunks
    chunk_size = 10000
    
    # Create log directory
    os.makedirs(os.path.join(output_dir, "logs"), exist_ok=True)
    os.makedirs(os.path.join(output_dir, "checkpoints"), exist_ok=True)
    log_file = os.path.join(output_dir, "logs", "robust_training.txt")
    
    def log_message(message):
        with open(log_file, "a") as f:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            f.write(f"[{timestamp}] {message}\n")
        print(message)
    
    log_message("Starting robust training with complete model rebuilding")
    
    # Total chunks to process
    num_chunks = int(np.ceil(len(train_indices) / chunk_size))
    log_message(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
    
    # Memory-mapped data access
    X_mmap = np.load(X_file, mmap_mode='r')
    y_mmap = np.load(y_file, mmap_mode='r')
    
    # Get input shape for model building
    sample_shape = X_mmap[train_indices[0]].shape
    log_message(f"Input shape: {sample_shape}")
    
    # Initialize best model tracking
    best_model_info = {
        'val_auc': 0.0,
        'checkpoint': None,
        'epoch': None,
        'chunk': None
    }
    
    # Load previous best model info if it exists
    best_model_file = os.path.join(output_dir, "best_model_info.json")
    if os.path.exists(best_model_file):
        try:
            import json
            with open(best_model_file, 'r') as f:
                best_model_info = json.load(f)
            log_message(f"Loaded previous best model info: {best_model_info}")
        except Exception as e:
            log_message(f"Error loading best model info: {e}")
    
    # Training loop with rebuilding
    current_chunk = start_chunk
    while current_chunk < num_chunks:
        # Calculate range of chunks for this session
        session_end = min(current_chunk + max_chunks_per_session, num_chunks)
        log_message(f"Starting training session for chunks {current_chunk+1} to {session_end}")
        
        # Build fresh model
        log_message("Building fresh model")
        tf.keras.backend.clear_session()
        gc.collect()
        
        model = build_model_func(sample_shape)
        
        # Load latest weights if not starting from beginning
        if current_chunk > 0:
            # Find latest weights file
            latest_chunk = current_chunk
            while latest_chunk > 0:
                weights_path = os.path.join(output_dir, "checkpoints", f"model_weights_{latest_chunk}")
                if os.path.exists(weights_path+".index"):
                    log_message(f"Loading weights from chunk {latest_chunk}")
                    model.load_weights(weights_path)
                    break
                latest_chunk -= 1
        
        # Process chunks in this session
        for chunk_idx in range(current_chunk, session_end):
            log_message(f"\n{'='*50}")
            log_message(f"Processing chunk {chunk_idx+1}/{num_chunks}")
            
            # Get chunk indices
            start_idx = chunk_idx * chunk_size
            end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
            chunk_indices = train_indices[start_idx:end_idx]
            
            # Load data in batches
            X_chunks = []
            y_chunks = []
            
            for i in range(0, len(chunk_indices), 1000):
                end_i = min(i + 1000, len(chunk_indices))
                log_message(f"  Loading batch {i+1}-{end_i} of {len(chunk_indices)}")
                
                batch_indices = chunk_indices[i:end_i]
                X_batch = np.array([X_mmap[idx] for idx in batch_indices])
                y_batch = np.array([y_mmap[idx] for idx in batch_indices])
                
                X_chunks.append(X_batch)
                y_chunks.append(y_batch)
                
                # Clean up
                del X_batch, y_batch
                gc.collect()
            
            # Combine data
            X_data = np.concatenate(X_chunks)
            y_data = np.concatenate(y_chunks)
            log_message(f"Data loaded: X={X_data.shape}, y={y_data.shape}")
            
            # Train for a few epochs
            try:
                history = model.fit(
                    X_data, y_data,
                    epochs=3,
                    batch_size=64,
                    verbose=1
                )
                
                # Save weights after each chunk
                if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
                    weights_path = os.path.join(output_dir, "checkpoints", f"model_weights_{chunk_idx+1}")
                    model.save_weights(weights_path)
                    log_message(f"Saved weights to {weights_path}")
                
                # Save last metrics
                metrics = {k: float(v[-1]) for k, v in history.history.items()}
                log_message(f"Metrics: {metrics}")
                
                # Check for best model (if validation data was used)
                if 'val_accuracy' in metrics:
                    current_val_acc = metrics.get('val_accuracy', 0)
                    if current_val_acc > best_model_info.get('val_accuracy', 0):
                        previous_best = best_model_info.get('val_accuracy', 0)
                        best_model_info = {
                            'val_accuracy': current_val_acc,
                            'checkpoint': weights_path,
                            'chunk': chunk_idx + 1,
                            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                        
                        # Save best model info
                        import json
                        with open(best_model_file, 'w') as f:
                            json.dump(best_model_info, f, indent=4)
                        
                        # Make a copy of best weights
                        import shutil
                        best_weights_path = os.path.join(output_dir, "checkpoints", "best_model_weights")
                        for file in os.listdir(os.path.dirname(weights_path)):
                            if file.startswith(os.path.basename(weights_path)):
                                src_path = os.path.join(os.path.dirname(weights_path), file)
                                dst_file = file.replace(os.path.basename(weights_path), "best_model_weights")
                                dst_path = os.path.join(os.path.dirname(best_weights_path), dst_file)
                                shutil.copy2(src_path, dst_path)
                        
                        log_message(f"New best model! Accuracy improved from {previous_best:.4f} to {current_val_acc:.4f}")
                
                # Update progress file
                with open(os.path.join(output_dir, "last_completed_chunk.txt"), "w") as f:
                    f.write(str(chunk_idx + 1))
                
            except Exception as e:
                log_message(f"Error during training: {str(e)}")
                # Save weights to recover later
                try:
                    error_path = os.path.join(output_dir, "checkpoints", f"error_model_weights_{chunk_idx+1}")
                    model.save_weights(error_path)
                    log_message(f"Saved error state weights to {error_path}")
                except:
                    log_message("Could not save error state weights")
            
            # Clean up
            del X_chunks, y_chunks, X_data, y_data
            gc.collect()
        
        # Update current chunk for next session
        current_chunk = session_end
        
        # Explicitly delete model and clear TF session
        del model
        tf.keras.backend.clear_session()
        gc.collect()
        
        log_message(f"Completed training session. Progress: {current_chunk}/{num_chunks} chunks")
    
    # Save final model weights
    final_weights_path = os.path.join(output_dir, "final_model_weights")
    
    # Need to rebuild model one last time to save final weights
    model = build_model_func(sample_shape)
    
    # Load last chunk weights
    last_weights = os.path.join(output_dir, "checkpoints", f"model_weights_{num_chunks}")
    if os.path.exists(last_weights + ".index"):
        model.load_weights(last_weights)
    
    # Save final weights
    model.save_weights(final_weights_path)
    log_message(f"Final model weights saved to {final_weights_path}")
    
    log_message("Training complete!")
    return final_weights_path

def run_improved_pipeline_fixed():
    """
    Run the fixed and optimized pipeline with all improvements.
    """
    import os
    import numpy as np
    import tensorflow as tf
    import gc
    import json
    import pickle
    import time
    from datetime import datetime, timedelta
    
    # First ensure BatchNorm5D is properly defined in the global scope 
    # (Import this from a separate module in production code)
    from tensorflow.keras.layers import BatchNormalization, Layer

    class BatchNorm5D(Layer):
        """
        Fixed implementation of BatchNorm5D that properly handles 5D inputs from ConvLSTM2D.
        """
        def __init__(self, **kwargs):
            super(BatchNorm5D, self).__init__(**kwargs)
            self.bn = BatchNormalization()
            
        def call(self, inputs, training=None):
            # Get the dimensions
            input_shape = tf.shape(inputs)
            batch_size = input_shape[0]
            time_steps = input_shape[1]
            height = input_shape[2]
            width = input_shape[3]
            channels = input_shape[4]
            
            # Reshape to 4D for BatchNorm by combining batch and time dimensions
            x_reshaped = tf.reshape(inputs, [-1, height, width, channels])
            
            # Apply BatchNorm
            x_bn = self.bn(x_reshaped, training=training)
            
            # Reshape back to 5D
            x_back = tf.reshape(x_bn, [batch_size, time_steps, height, width, channels])
            return x_back
        
        def get_config(self):
            config = super(BatchNorm5D, self).get_config()
            return config
    
    # Register custom objects
    tf.keras.utils.get_custom_objects().update({'BatchNorm5D': BatchNorm5D})
    
    # Configure TensorFlow memory
    def configure_tensorflow_memory():
        """Configure TensorFlow memory settings to prevent OOM errors"""
        try:
            # GPU memory growth
            gpus = tf.config.list_physical_devices('GPU')
            for gpu in gpus:
                try:
                    tf.config.experimental.set_memory_growth(gpu, True)
                    print(f"Memory growth enabled for {gpu}")
                except Exception as e:
                    print(f"Error configuring GPU {gpu}: {e}")
            
            # Set soft device placement
            tf.config.set_soft_device_placement(True)
            
            # Set operation timeout
            tf.config.threading.set_intra_op_parallelism_threads(2)
            tf.config.threading.set_inter_op_parallelism_threads(2)
            
            print("TensorFlow memory configuration complete")
        except Exception as e:
            print(f"Error in TensorFlow configuration: {e}")
    
    # Setup logging
    def setup_logging(output_dir):
        """Create a logger for the pipeline"""
        log_file = os.path.join(output_dir, "pipeline_log.txt")
        
        def log_message(message, print_to_console=True):
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            with open(log_file, "a") as f:
                f.write(f"[{timestamp}] {message}\n")
            if print_to_console:
                print(message)
                
        return log_message
    
    # Import the fixed build function before use
    def build_improved_zero_curtain_model_fixed(input_shape, include_moisture=True):
        """
        Fixed model architecture that works with the ConvLSTM2D and BatchNormalization
        """
        from tensorflow.keras.models import Model
        from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
        from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D
        from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add, Lambda
        from tensorflow.keras.layers import ConvLSTM2D
        from tensorflow.keras.optimizers import Adam
        from tensorflow.keras.regularizers import l2
        import tensorflow as tf
        
        # Import the BatchNorm5D class from the global scope
        # This assumes it's been registered with get_custom_objects()
        BatchNorm5D = tf.keras.utils.get_custom_objects()['BatchNorm5D']
        
        # Reduced L2 regularization strength
        reg_strength = 0.00005
        # Slightly reduced dropout rate
        dropout_rate = 0.25
        
        # Input layer
        inputs = Input(shape=input_shape)
        
        # Add BatchNormalization to input
        x = BatchNormalization()(inputs)
        
        # Reshape for ConvLSTM (add spatial dimension)
        x = Reshape((input_shape[0], 1, 1, input_shape[1]))(x)
        
        # ConvLSTM layer with regularization
        convlstm = ConvLSTM2D(
            filters=64,
            kernel_size=(3, 1),
            padding='same',
            return_sequences=True,
            activation='tanh',
            recurrent_dropout=dropout_rate,
            kernel_regularizer=l2(reg_strength)
        )(x)
        
        # Use BatchNorm5D for the ConvLSTM output
        convlstm = BatchNorm5D()(convlstm)
        
        # Reshape back to (sequence_length, features)
        convlstm = Reshape((input_shape[0], 64))(convlstm)
        
        # Add positional encoding for transformer
        def positional_encoding(length, depth):
            positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
            depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
            
            angle_rates = 1 / tf.pow(10000.0, depths)
            angle_rads = positions * angle_rates
            
            # Only use sin to ensure output depth matches input depth
            pos_encoding = tf.sin(angle_rads)
            
            # Add batch dimension
            pos_encoding = tf.expand_dims(pos_encoding, 0)
            
            return pos_encoding
        
        # Add positional encoding
        pos_encoding = positional_encoding(input_shape[0], 64)
        transformer_input = convlstm + pos_encoding
        
        # Add BatchNormalization before transformer
        transformer_input = BatchNormalization()(transformer_input)
        
        # Improved transformer encoder block
        def transformer_encoder(x, num_heads=8, key_dim=64, ff_dim=128):
            # Multi-head attention with regularization
            attention_output = MultiHeadAttention(
                num_heads=num_heads, key_dim=key_dim,
                kernel_regularizer=l2(reg_strength)
            )(x, x)
            
            # Skip connection 1 with dropout
            x1 = Add()([attention_output, x])
            x1 = LayerNormalization(epsilon=1e-6)(x1)
            # Add BatchNormalization after LayerNormalization
            x1 = BatchNormalization()(x1)
            x1 = Dropout(dropout_rate)(x1)
            
            # Feed-forward network with regularization
            ff_output = Dense(ff_dim, activation='relu', kernel_regularizer=l2(reg_strength))(x1)
            ff_output = BatchNormalization()(ff_output)  # Add BatchNorm
            ff_output = Dropout(dropout_rate)(ff_output)
            ff_output = Dense(64, kernel_regularizer=l2(reg_strength))(ff_output)
            ff_output = BatchNormalization()(ff_output)  # Add BatchNorm
            
            # Skip connection 2
            x2 = Add()([ff_output, x1])
            return LayerNormalization(epsilon=1e-6)(x2)
        
        # Apply transformer encoder
        transformer_output = transformer_encoder(transformer_input)
        
        # Parallel CNN paths for multi-scale feature extraction (with regularization)
        cnn_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu', 
                      kernel_regularizer=l2(reg_strength))(inputs)
        cnn_1 = BatchNormalization()(cnn_1)  
        cnn_1 = Dropout(dropout_rate/2)(cnn_1)
        
        cnn_2 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu',
                      kernel_regularizer=l2(reg_strength))(inputs)
        cnn_2 = BatchNormalization()(cnn_2)  
        cnn_2 = Dropout(dropout_rate/2)(cnn_2)
        
        cnn_3 = Conv1D(filters=32, kernel_size=7, padding='same', activation='relu',
                      kernel_regularizer=l2(reg_strength))(inputs)
        cnn_3 = BatchNormalization()(cnn_3)  
        cnn_3 = Dropout(dropout_rate/2)(cnn_3)
        
        # VAE components
        def sampling(args):
            z_mean, z_log_var = args
            batch = tf.shape(z_mean)[0]
            dim = tf.shape(z_mean)[1]
            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
            return z_mean + tf.exp(0.5 * z_log_var) * epsilon
        
        # Global temporal features
        global_max = GlobalMaxPooling1D()(transformer_output)
        global_avg = GlobalAveragePooling1D()(transformer_output)
        
        # VAE encoding with reduced regularization
        z_concat = Concatenate()([global_max, global_avg])
        z_concat = BatchNormalization()(z_concat)  # Add BatchNorm
        
        z_mean = Dense(32, kernel_regularizer=l2(reg_strength))(z_concat)
        z_log_var = Dense(32, kernel_regularizer=l2(reg_strength))(z_concat)
        z = Lambda(sampling)([z_mean, z_log_var])
        
        # Combine all features
        merged_features = Concatenate()(
            [
                GlobalMaxPooling1D()(cnn_1),
                GlobalMaxPooling1D()(cnn_2),
                GlobalMaxPooling1D()(cnn_3),
                global_max,
                global_avg,
                z
            ]
        )
        
        # Add BatchNorm to merged features
        merged_features = BatchNormalization()(merged_features)
        
        # Final classification layers with regularization
        x = Dense(128, activation='relu', kernel_regularizer=l2(reg_strength))(merged_features)
        x = BatchNormalization()(x)  
        x = Dropout(dropout_rate)(x)
        x = Dense(64, activation='relu', kernel_regularizer=l2(reg_strength))(x)
        x = BatchNormalization()(x)  
        x = Dropout(dropout_rate)(x)
        
        # Output layer
        outputs = Dense(1, activation='sigmoid', kernel_regularizer=l2(reg_strength))(x)
        
        # Create model
        model = Model(inputs=inputs, outputs=outputs)
        
        # CRITICAL FIX: Reduce VAE loss weight
        kl_loss = -0.5 * tf.reduce_mean(
            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
        )
        model.add_loss(0.0002 * kl_loss)  # Reduced from 0.0005 to 0.0002
        
        # Compile model with gradient clipping but simpler metrics to avoid errors
        model.compile(
            optimizer=Adam(
                learning_rate=0.0005,
                clipvalue=0.5,  # Reduced from 1.0 to 0.5 for better stability
                epsilon=1e-7  # Increased epsilon for better numerical stability
            ),
            loss='binary_crossentropy',
            metrics=[
                'accuracy',
                tf.keras.metrics.AUC(name='auc', num_thresholds=200),
                tf.keras.metrics.Precision(name='precision'),
                tf.keras.metrics.Recall(name='recall')
            ]
        )
        
        return model
    
    # Import the fixed training function
    def improved_resumable_training_fixed(model, X_file, y_file, train_indices, val_indices, test_indices,
                                  output_dir, batch_size=256, chunk_size=10000, epochs_per_chunk=3, 
                                  save_frequency=5, class_weight=None, start_chunk=0):
        """
        Enhanced training function with improved error handling and memory management.
        
        This version fixes the module pickling issue by using saved_model format instead of h5
        """
        import os
        import gc
        import json
        import numpy as np
        import tensorflow as tf
        from datetime import datetime, timedelta
        import time
        import psutil
        
        # Create output directories
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(os.path.join(output_dir, "checkpoints"), exist_ok=True)
        os.makedirs(os.path.join(output_dir, "logs"), exist_ok=True)
        
        # Setup logging
        log_file = os.path.join(output_dir, "logs", "training_log.txt")
        def log_message(message):
            with open(log_file, "a") as f:
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                f.write(f"[{timestamp}] {message}\n")
            print(message)
        
        log_message("Starting robust training with fixed architecture")
        
        # Process in chunks to manage memory
        num_chunks = int(np.ceil(len(train_indices) / chunk_size))
        log_message(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
        log_message(f"Starting from chunk {start_chunk+1}")
        
        # Open data files with memory mapping
        X_mmap = np.load(X_file, mmap_mode='r')
        y_mmap = np.load(y_file, mmap_mode='r')
        
        # Create validation set once (limited size for memory efficiency)
        val_limit = min(1000, len(val_indices))
        val_indices_subset = val_indices[:val_limit]
        
        # Load validation data
        val_X = np.array([X_mmap[idx] for idx in val_indices_subset])
        val_y = np.array([y_mmap[idx] for idx in val_indices_subset])
        log_message(f"Loaded {len(val_X)} validation samples")
        
        # Load existing history if resuming
        history_log = []
        history_path = os.path.join(output_dir, "training_history.json")
        if start_chunk > 0 and os.path.exists(history_path):
            try:
                with open(history_path, "r") as f:
                    history_log = json.load(f)
            except Exception as e:
                log_message(f"Could not load existing history: {e}")
                # Try pickle format
                pickle_path = os.path.join(output_dir, "training_history.pkl")
                if os.path.exists(pickle_path):
                    import pickle
                    with open(pickle_path, "rb") as f:
                        history_log = pickle.load(f)
        
        # If resuming, load latest model
        if start_chunk > 0:
            # Find the most recent checkpoint before start_chunk
            checkpoint_indices = []
            checkpoints_dir = os.path.join(output_dir, "checkpoints")
            
            # Look for SavedModel format directories instead of h5 files
            for dirname in os.listdir(checkpoints_dir):
                if dirname.startswith("model_chunk_"):
                    try:
                        # Extract chunk number from directory name
                        idx = int(dirname.split("_")[-1])
                        if idx < start_chunk:
                            checkpoint_indices.append(idx)
                    except ValueError:
                        continue
            
            # if checkpoint_indices:
            #     latest_idx = max(checkpoint_indices)
            #     model_path = os.path.join(checkpoints_dir, f"model_chunk_{latest_idx}")
            #     if os.path.exists(model_path):
            #         log_message(f"Loading model from checkpoint {model_path}")
            #         # Load with custom objects to handle BatchNorm5D
            #         model = tf.keras.models.load_model(model_path)
            #     else:
            #         log_message(f"Warning: Could not find model checkpoint for chunk {latest_idx}")

            if checkpoint_indices:
                latest_idx = max(checkpoint_indices)
                weights_path = os.path.join(checkpoints_dir, f"model_weights_{latest_idx}")
                if os.path.exists(weights_path):
                    log_message(f"Loading model weights from checkpoint {weights_path}")
                    # Need to initialize the model structure first
                    X = np.load(X_file, mmap_mode='r')
                    sample = X[train_indices[0]]
                    input_shape = sample.shape
                    model = build_improved_zero_curtain_model_fixed(input_shape)
                    # Then load weights
                    model.load_weights(weights_path)
                else:
                    log_message(f"Warning: Could not find model checkpoint for chunk {latest_idx}")
        
        # Setup callbacks
        callbacks = [
            # Early stopping with increased patience
            tf.keras.callbacks.EarlyStopping(
                patience=10, 
                restore_best_weights=True,
                monitor='val_auc',
                mode='max',
                min_delta=0.001
            ),
            # Reduced LR with increased patience
            tf.keras.callbacks.ReduceLROnPlateau(
                factor=0.5,
                patience=5,
                min_lr=1e-6,
                monitor='val_auc',
                mode='max'
            ),
            # Memory cleanup after each epoch
            tf.keras.callbacks.LambdaCallback(
                on_epoch_end=lambda epoch, logs: gc.collect()
            )
        ]
        
        # Track metrics across chunks
        start_time = time.time()
        
        # For safe recovery
        recovery_file = os.path.join(output_dir, "last_completed_chunk.txt")
        
        # Process each chunk
        for chunk_idx in range(start_chunk, num_chunks):
            # Track progress in file for recovery purposes
            with open(os.path.join(output_dir, "current_progress.txt"), "w") as f:
                f.write(f"Processing chunk {chunk_idx+1}/{num_chunks}\n")
                f.write(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            
            # Get chunk indices
            start_idx = chunk_idx * chunk_size
            end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
            chunk_indices = train_indices[start_idx:end_idx]
            
            # Report memory
            memory_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
            log_message(f"\n{'='*50}")
            log_message(f"Processing chunk {chunk_idx+1}/{num_chunks} with {len(chunk_indices)} samples")
            log_message(f"Memory before: {memory_before:.1f} MB")
            
            # Force garbage collection before loading new data
            gc.collect()
            
            try:
                # Load chunk data in smaller batches
                chunk_X = []
                chunk_y = []
                
                # Use a smaller batch size for loading
                load_batch_size = 1000  # Reduced batch size for more stability
                for i in range(0, len(chunk_indices), load_batch_size):
                    end_i = min(i + load_batch_size, len(chunk_indices))
                    log_message(f"  Loading batch {i}-{end_i} of {len(chunk_indices)}...")
                    
                    try:
                        # Load this batch
                        batch_indices = chunk_indices[i:end_i]
                        X_batch = np.array([X_mmap[idx] for idx in batch_indices])
                        y_batch = np.array([y_mmap[idx] for idx in batch_indices])
                        
                        # Append to list
                        chunk_X.append(X_batch)
                        chunk_y.append(y_batch)
                        
                        # Free memory
                        del X_batch, y_batch
                        gc.collect()
                    except Exception as inner_e:
                        log_message(f"Warning: Error loading batch {i}-{end_i}: {inner_e}")
                        # Continue to next batch
                
                # Combine batches
                try:
                    chunk_X = np.concatenate(chunk_X)
                    chunk_y = np.concatenate(chunk_y)
                    log_message(f"Data loaded. Shape: X={chunk_X.shape}, y={chunk_y.shape}")
                    log_message(f"Memory after loading: {psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024):.1f} MB")
                except Exception as concat_e:
                    log_message(f"Error combining data batches: {concat_e}")
                    # Skip this chunk
                    continue
                
                # Train on chunk using a safe batch size
                mini_batch_size = min(batch_size, 64)  # Use smaller mini-batches for training
                log_message(f"Training with mini-batch size: {mini_batch_size}")
                
                # Track each epoch separately for better error handling
                chunk_history = {}
                for epoch in range(epochs_per_chunk):
                    try:
                        log_message(f"Epoch {epoch+1}/{epochs_per_chunk}")
                        
                        # Train for a single epoch
                        history = model.fit(
                            chunk_X, chunk_y,
                            validation_data=(val_X, val_y),
                            epochs=1,
                            batch_size=mini_batch_size,
                            class_weight=class_weight,
                            callbacks=callbacks,
                            verbose=1
                        )
                        
                        # Check if loss is NaN and recover if needed
                        if np.isnan(history.history['loss'][0]):
                            log_message("WARNING: NaN loss detected! Reducing learning rate...")
                            current_lr = float(tf.keras.backend.get_value(model.optimizer.lr))
                            new_lr = current_lr * 0.1
                            tf.keras.backend.set_value(model.optimizer.lr, new_lr)
                            log_message(f"Reduced learning rate from {current_lr} to {new_lr}")
                            continue
                        
                        # Store history
                        for k, v in history.history.items():
                            if k not in chunk_history:
                                chunk_history[k] = []
                            chunk_history[k].extend([float(val) for val in v])
                        
                        # Save after each epoch for safety using SavedModel format instead of h5
                        #epoch_model_path = os.path.join(output_dir, "checkpoints", f"model_chunk_{c...
                        #model.save(epoch_model_path, save_format='tf')
                        #log_message(f"Saved model after epoch {epoch+1} to {epoch_model_path}")
                        epoch_weights_path = os.path.join(output_dir, "checkpoints", f"model_weights_{chunk_idx+1}_epoch_{epoch+1}")
                        model.save_weights(epoch_weights_path)
                        log_message(f"Saved model weights after epoch {epoch+1} to {epoch_weights_path}")
                        
                    except Exception as epoch_e:
                        log_message(f"Error during epoch {epoch+1}: {epoch_e}")
                        import traceback
                        traceback.print_exc()
                        # Try to continue with next epoch
                
                # Store history from all completed epochs
                if chunk_history:
                    history_log.append(chunk_history)
                    
                    # Save history
                    try:
                        with open(history_path, "w") as f:
                            json.dump(history_log, f)
                    except Exception as history_e:
                        log_message(f"Warning: Could not save history to JSON: {history_e}")
                        # Fallback - save as pickle
                        import pickle
                        with open(os.path.join(output_dir, "training_history.pkl"), "wb") as f:
                            pickle.dump(history_log, f)
                
                # Save model periodically using SavedModel format
                if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
                    try:
                        # model_path = os.path.join(output_dir, "checkpoints", f"model_chunk_{chunk_idx+1}")
                        # model.save(model_path, save_format='tf')
                        # log_message(f"Model saved to {model_path}")
                        # Save weights only, avoiding architecture serialization
                        weights_path = os.path.join(output_dir, "checkpoints", f"model_weights_{chunk_idx+1}")
                        model.save_weights(weights_path)
                        log_message(f"Saved model weights to {weights_path}")
                    except Exception as save_e:
                        log_message(f"Error saving model: {save_e}")
                        # Try saving weights only - already using weights now
                        try:
                            weights_path = os.path.join(output_dir, "checkpoints", f"model_weights_backup_{chunk_idx+1}")
                            model.save_weights(weights_path)
                            log_message(f"Saved model weights to backup location: {weights_path}")
                        except:
                            log_message("Could not save model in any format")
                    except Exception as save_e:
                        log_message(f"Error saving model: {save_e}")
                        # Try saving weights only
                        try:
                            weights_path = os.path.join(output_dir, "checkpoints", f"model_weights_{chunk_idx+1}")
                            model.save_weights(weights_path)
                            log_message(f"Saved model weights to {weights_path}")
                        except:
                            log_message("Could not save model in any format")
                
                # Explicitly delete everything from memory
                del chunk_X, chunk_y
                
            except Exception as outer_e:
                log_message(f"Critical error processing chunk {chunk_idx+1}: {outer_e}")
                import traceback
                traceback.print_exc()
                
                # Write error to file for debugging
                with open(os.path.join(output_dir, f"error_chunk_{chunk_idx+1}.txt"), "w") as f:
                    f.write(f"Error processing chunk {chunk_idx+1}:\n")
                    f.write(traceback.format_exc())
                    
                # Try saving model in error state with SavedModel format
                try:
                    error_model_path = os.path.join(output_dir, "checkpoints", f"error_recovery_{chunk_idx+1}")
                    model.save(error_model_path, save_format='tf')
                    log_message(f"Saved model in error state to {error_model_path}")
                except:
                    log_message("Could not save model in error state")
                    
                # Continue to next chunk
                continue
            
            # Force garbage collection
            gc.collect()
            
            # Report memory after cleanup
            memory_after = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
            log_message(f"Memory after: {memory_after:.1f} MB (Change: {memory_after - memory_before:.1f} MB)")
            
            # Write recovery file with last completed chunk
            with open(recovery_file, "w") as f:
                f.write(str(chunk_idx + 1))
            
            # Estimate time
            elapsed = time.time() - start_time
            avg_time_per_chunk = elapsed / (chunk_idx - start_chunk + 1) if chunk_idx > start_chunk else elapsed
            remaining = avg_time_per_chunk * (num_chunks - chunk_idx - 1)
            log_message(f"Estimated remaining time: {timedelta(seconds=int(remaining))}")
            
            # Reset TensorFlow session periodically to prevent memory growth
            if (chunk_idx + 1) % 10 == 0:  # Reset more frequently
                log_message("Resetting TensorFlow session to prevent memory issues")
                try:
                    temp_model_path = os.path.join(output_dir, "checkpoints", f"temp_reset_{chunk_idx+1}")
                    model.save(temp_model_path, save_format='tf')
                    
                    # Clear session
                    tf.keras.backend.clear_session()
                    gc.collect()
                    
                    # Reload model
                    model = tf.keras.models.load_model(temp_model_path)
                    log_message("TensorFlow session reset complete")
                except Exception as reset_e:
                    log_message(f"Error during TensorFlow reset: {reset_e}")
                    # Continue anyway
        
        # Save final model with SavedModel format
        final_model_path = os.path.join(output_dir, "final_model")
        try:
            model.save(final_model_path, save_format='tf')
            log_message(f"Final model saved to {final_model_path}")
        except Exception as final_save_e:
            log_message(f"Error saving final model: {final_save_e}")
            try:
                # Try alternative location
                alt_path = os.path.join(output_dir, "checkpoints", "final_model_backup")
                model.save(alt_path, save_format='tf')
                final_model_path = alt_path
                log_message(f"Saved final model to alternate location: {alt_path}")
            except:
                log_message("Could not save final model in any format")
        
        return model, final_model_path
    
    # Import evaluation function to avoid duplication
    def evaluate_model_with_visualizations_fixed(model, X_file, y_file, test_indices, output_dir, metadata_file=None):
        """Comprehensive evaluation with detailed visualizations and robust error handling."""
        import os
        import gc
        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        import seaborn as sns
        from sklearn.metrics import (
            classification_report, confusion_matrix, roc_curve, 
            auc, precision_recall_curve, average_precision_score
        )
        import traceback
        
        os.makedirs(os.path.join(output_dir, "visualizations"), exist_ok=True)
        
        # Setup logging
        log_file = os.path.join(output_dir, "evaluation_log.txt")
        def log_message(message):
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            with open(log_file, "a") as f:
                f.write(f"[{timestamp}] {message}\n")
            print(message)
        
        log_message(f"Starting evaluation on {len(test_indices)} test samples")
        
        # Load metadata if available
        metadata = None
        if metadata_file and os.path.exists(metadata_file):
            try:
                import pickle
                with open(metadata_file, "rb") as f:
                    metadata = pickle.load(f)
                log_message(f"Loaded metadata from {metadata_file}")
            except Exception as e:
                log_message(f"Error loading metadata: {str(e)}")
        
        # Load memory-mapped data
        X = np.load(X_file, mmap_mode='r')
        y = np.load(y_file, mmap_mode='r')
        
        # Process test data in batches - use smaller batches for better stability
        batch_size = 100  # Reduced batch size for more reliable processing
        num_batches = int(np.ceil(len(test_indices) / batch_size))
        
        all_preds = []
        all_true = []
        all_meta = []
        
        log_message(f"Processing test data in {num_batches} batches of size {batch_size}")
        
        # Save test indices for reference
        np.save(os.path.join(output_dir, "test_indices.npy"), test_indices)
        
        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(test_indices))
            batch_indices = test_indices[start_idx:end_idx]
            
            log_message(f"Processing batch {batch_idx+1}/{num_batches} with {len(batch_indices)} samples")
            
            try:
                # Load batch data
                batch_X = np.array([X[idx] for idx in batch_indices])
                batch_y = np.array([y[idx] for idx in batch_indices])
                
                # Store metadata if available
                if metadata:
                    try:
                        batch_meta = []
                        for idx in batch_indices:
                            if idx in metadata:
                                batch_meta.append(metadata[idx])
                            else:
                                # Create empty metadata if not found
                                batch_meta.append({})
                        all_meta.extend(batch_meta)
                    except Exception as meta_e:
                        log_message(f"Error extracting metadata for batch {batch_idx}: {str(meta_e)}")
                        # Continue without metadata for this batch
                
                # Predict with error handling
                try:
                    batch_preds = model.predict(batch_X, verbose=0)
                except Exception as pred_e:
                    log_message(f"Error during prediction on batch {batch_idx}: {str(pred_e)}")
                    # Try with smaller sub-batches
                    batch_preds = []
                    sub_batch_size = 10
                    for i in range(0, len(batch_X), sub_batch_size):
                        end_i = min(i + sub_batch_size, len(batch_X))
                        sub_batch = batch_X[i:end_i]
                        sub_preds = model.predict(sub_batch, verbose=0)
                        batch_preds.append(sub_preds)
                    batch_preds = np.concatenate(batch_preds)
                
                # Store results
                all_preds.extend(batch_preds.flatten())
                all_true.extend(batch_y)
                
                # Clean up
                del batch_X, batch_y, batch_preds
                gc.collect()
                
                log_message(f"Completed batch {batch_idx+1}/{num_batches}")
                
            except Exception as e:
                log_message(f"Error processing batch {batch_idx+1}: {str(e)}")
                log_message(traceback.format_exc())
                
                # Try to process the batch in smaller chunks
                try:
                    log_message("Attempting to process batch in smaller chunks...")
                    sub_batch_size = 10  # Much smaller batch
                    sub_batches = int(np.ceil(len(batch_indices) / sub_batch_size))
                    
                    for sub_idx in range(sub_batches):
                        sub_start = sub_idx * sub_batch_size
                        sub_end = min((sub_idx + 1) * sub_batch_size, len(batch_indices))
                        sub_indices = batch_indices[sub_start:sub_end]
                        
                        # Load and process sub-batch
                        sub_X = np.array([X[idx] for idx in sub_indices])
                        sub_y = np.array([y[idx] for idx in sub_indices])
                        
                        # Add metadata if available
                        if metadata:
                            sub_meta = []
                            for idx in sub_indices:
                                if idx in metadata:
                                    sub_meta.append(metadata[idx])
                                else:
                                    sub_meta.append({})
                            all_meta.extend(sub_meta)
                        
                        # Predict
                        sub_preds = model.predict(sub_X, verbose=0)
                        
                        # Store results
                        all_preds.extend(sub_preds.flatten())
                        all_true.extend(sub_y)
                        
                        # Clean up
                        del sub_X, sub_y, sub_preds
                        gc.collect()
                    
                    log_message(f"Successfully processed batch {batch_idx+1} in {sub_batches} sub-batches")
                except Exception as sub_e:
                    log_message(f"Error processing sub-batches: {str(sub_e)}")
                    log_message(traceback.format_exc())
                    log_message(f"Skipping batch {batch_idx+1}")
        
        log_message(f"Prediction complete. Total samples: {len(all_preds)}")
        
        # Save raw predictions for reference
        np.save(os.path.join(output_dir, "test_predictions.npy"), np.array(all_preds))
        np.save(os.path.join(output_dir, "test_true_labels.npy"), np.array(all_true))
        
        # Convert to numpy arrays
        all_preds = np.array(all_preds)
        all_true = np.array(all_true)
        
        try:
            # Create binary predictions
            all_preds_binary = (all_preds > 0.5).astype(int)
            
            # Calculate metrics
            report = classification_report(all_true, all_preds_binary, output_dict=True)
            report_str = classification_report(all_true, all_preds_binary)
            conf_matrix = confusion_matrix(all_true, all_preds_binary)
            
            log_message("\nClassification Report:")
            log_message(report_str)
            
            log_message("\nConfusion Matrix:")
            log_message(str(conf_matrix))
            
            # Calculate ROC curve and AUC
            fpr, tpr, _ = roc_curve(all_true, all_preds)
            roc_auc = auc(fpr, tpr)
            
            # Calculate Precision-Recall curve
            precision, recall, _ = precision_recall_curve(all_true, all_preds)
            avg_precision = average_precision_score(all_true, all_preds)
            
            log_message(f"\nROC AUC: {roc_auc:.4f}")
            log_message(f"Average Precision: {avg_precision:.4f}")
            
            # Save results to file
            with open(os.path.join(output_dir, "evaluation_results.txt"), "w") as f:
                f.write("Classification Report:\n")
                f.write(report_str)
                f.write("\n\nConfusion Matrix:\n")
                f.write(str(conf_matrix))
                f.write(f"\n\nROC AUC: {roc_auc:.4f}")
                f.write(f"\nAverage Precision: {avg_precision:.4f}")
            
            # Create visualizations
            try:
                # 1. Confusion Matrix
                plt.figure(figsize=(10, 8))
                sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                          xticklabels=['Negative', 'Positive'],
                          yticklabels=['Negative', 'Positive'])
                plt.xlabel('Predicted Label')
                plt.ylabel('True Label')
                plt.title('Confusion Matrix')
                plt.tight_layout()
                plt.savefig(os.path.join(output_dir, "visualizations", "confusion_matrix.png"), dpi=300)
                plt.close()
                
                # 2. ROC Curve
                plt.figure(figsize=(10, 8))
                plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')
                plt.plot([0, 1], [0, 1], 'k--')
                plt.xlabel('False Positive Rate')
                plt.ylabel('True Positive Rate')
                plt.title('Receiver Operating Characteristic (ROC) Curve')
                plt.legend(loc="lower right")
                plt.grid(alpha=0.3)
                plt.savefig(os.path.join(output_dir, "visualizations", "roc_curve.png"), dpi=300)
                plt.close()
                
                # 3. Precision-Recall Curve
                plt.figure(figsize=(10, 8))
                plt.plot(recall, precision, label=f'Precision-Recall curve (AP = {avg_precision:.3f})')
                plt.xlabel('Recall')
                plt.ylabel('Precision')
                plt.title('Precision-Recall Curve')
                plt.legend(loc="upper right")
                plt.grid(alpha=0.3)
                plt.savefig(os.path.join(output_dir, "visualizations", "precision_recall_curve.png"), dpi=300)
                plt.close()
                
                # 4. Score distribution
                plt.figure(figsize=(12, 6))
                sns.histplot(all_preds[all_true == 0], bins=50, alpha=0.5, label='Negative Class', color='blue')
                sns.histplot(all_preds[all_true == 1], bins=50, alpha=0.5, label='Positive Class', color='red')
                plt.xlabel('Prediction Score')
                plt.ylabel('Count')
                plt.title('Distribution of Prediction Scores by Class')
                plt.axvline(x=0.5, color='black', linestyle='--', alpha=0.7)
                plt.legend()
                plt.grid(alpha=0.3)
                plt.savefig(os.path.join(output_dir, "visualizations", "score_distribution.png"), dpi=300)
                plt.close()
                
                log_message("Saved all visualization plots")
            except Exception as plot_e:
                log_message(f"Error creating visualization plots: {str(plot_e)}")
                log_message(traceback.format_exc())
            
            # Save predictions with metadata
            if metadata and all_meta:
                try:
                    # Create results DataFrame with metadata
                    results_data = []
                    
                    for i in range(len(all_preds)):
                        if i < len(all_meta):
                            result = {
                                'prediction': float(all_preds[i]),
                                'true_label': int(all_true[i]),
                                'predicted_label': int(all_preds_binary[i]),
                                'correct': int(all_preds_binary[i] == all_true[i])
                            }
                            
                            # Add metadata
                            meta = all_meta[i]
                            for key, value in meta.items():
                                # Convert numpy types to Python native types
                                if hasattr(value, 'dtype'):
                                    if np.issubdtype(value.dtype, np.integer):
                                        value = int(value)
                                    elif np.issubdtype(value.dtype, np.floating):
                                        value = float(value)
                                result[key] = value
                            
                            results_data.append(result)
                    
                    # Create DataFrame
                    results_df = pd.DataFrame(results_data)
                    
                    # Save to CSV
                    results_df.to_csv(os.path.join(output_dir, "test_predictions_with_metadata.csv"), index=False)
                    log_message(f"Saved predictions with metadata to CSV, shape: {results_df.shape}")
                except Exception as csv_e:
                    log_message(f"Error saving predictions CSV: {str(csv_e)}")
                    log_message(traceback.format_exc())
                    
                    # Try a simpler format
                    try:
                        simple_df = pd.DataFrame({
                            'prediction': all_preds,
                            'true_label': all_true,
                            'predicted_label': all_preds_binary
                        })
                        simple_df.to_csv(os.path.join(output_dir, "simple_predictions.csv"), index=False)
                        log_message("Saved simplified predictions to CSV")
                    except:
                        log_message("Failed to save predictions in any format")
            
            # Return metrics dictionary
            evaluation_metrics = {
                'roc_auc': float(roc_auc),
                'avg_precision': float(avg_precision),
                'accuracy': float(report['accuracy']),
                'precision': float(report['1']['precision']),
                'recall': float(report['1']['recall']),
                'f1_score': float(report['1']['f1-score']),
                'num_samples': len(all_true),
                'positive_rate': float(np.mean(all_true))
            }
            
            # Save metrics to JSON
            import json
            with open(os.path.join(output_dir, "evaluation_metrics.json"), "w") as f:
                json.dump(evaluation_metrics, f, indent=4)
            
            log_message("Evaluation complete")
            return evaluation_metrics
        
        except Exception as metrics_e:
            log_message(f"Error calculating metrics: {str(metrics_e)}")
            log_message(traceback.format_exc())
            
            # Return basic metrics that we can calculate
            return {
                'num_samples': len(all_true),
                'positive_rate': float(np.mean(all_true)),
                'mean_prediction': float(np.mean(all_preds)),
                'error': str(metrics_e)
            }
    
    # Continue with main pipeline execution
    # Configure TensorFlow
    configure_tensorflow_memory()
    
    # Paths and configuration
    output_dir = '/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling/improved_model'
    data_dir = '/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling'
    X_file = os.path.join(data_dir, 'ml_data/X_features.npy')
    y_file = os.path.join(data_dir, 'ml_data/y_labels.npy')
    metadata_file = os.path.join(data_dir, 'ml_data/metadata.pkl')
    
    # Create directories
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(os.path.join(output_dir, "checkpoints"), exist_ok=True)
    os.makedirs(os.path.join(output_dir, "visualizations"), exist_ok=True)
    os.makedirs(os.path.join(output_dir, "logs"), exist_ok=True)
    
    # Create logger
    log = setup_logging(output_dir)
    
    log("="*80)
    log(f"Starting improved pipeline run at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    log("="*80)
    
    # Record pipeline start time
    pipeline_start_time = time.time()
    
    try:
        # Step 1: Load split indices
        log("\nStep 1: Loading data splits...")
        split_file = os.path.join(data_dir, "checkpoints/spatiotemporal_split.pkl")
        if not os.path.exists(split_file):
            log(f"Error: Split file not found at {split_file}", True)
            return {'error': f"Split file not found: {split_file}"}
            
        with open(split_file, "rb") as f:
            split_data = pickle.load(f)
            
        train_indices = split_data["train_indices"]
        val_indices = split_data["val_indices"]
        test_indices = split_data["test_indices"]
        
        log(f"Loaded splits: {len(train_indices)} train, {len(val_indices)} validation, {len(test_indices)} test samples")
        
        # Step 2: Calculate class weights for imbalanced data
        log("\nStep 2: Calculating class weights...")
        try:
            # Load labels
            y = np.load(y_file, mmap_mode='r')
            train_y = y[train_indices]
            
            # Calculate class weights
            pos_weight = (len(train_y) - np.sum(train_y)) / max(1, np.sum(train_y))
            class_weight = {0: 1.0, 1: pos_weight}
            log(f"Class weight for positive class: {pos_weight:.2f}")
        except Exception as weight_e:
            log(f"Error calculating class weights: {weight_e}")
            class_weight = None
        
        # Step 3: Build model with fixed architecture
        log("\nStep 3: Building model with fixed architecture...")
        try:
            # Get sample shape
            X = np.load(X_file, mmap_mode='r')
            sample = X[train_indices[0]]
            input_shape = sample.shape
            log(f"Input shape: {input_shape}")
            
            # Build with fixed model function
            model = build_improved_zero_curtain_model_fixed(input_shape)
            
            # Log model summary
            model_summary = []
            model.summary(print_fn=lambda x: model_summary.append(x))
            for line in model_summary:
                log(line, False)  # Log without printing to console
            log(f"Model built successfully with {model.count_params():,} parameters")
        except Exception as model_e:
            log(f"Error building model: {model_e}")
            import traceback
            log(traceback.format_exc())
            return {'error': f"Model building failed: {str(model_e)}"}
        
        # Step 4: Train model
        log("\nStep 4: Training model...")
        try:
            # Check for existing checkpoints
            latest_chunk = 0
            recovery_file = os.path.join(output_dir, "last_completed_chunk.txt")
            if os.path.exists(recovery_file):
                with open(recovery_file, "r") as f:
                    try:
                        latest_chunk = int(f.read().strip())
                        log(f"Found recovery point at chunk {latest_chunk}")
                    except:
                        log("Could not parse recovery file")
            
            # Ask for confirmation if resuming
            if latest_chunk > 0:
                log(f"Resuming training from chunk {latest_chunk}")
            else:
                log("Starting training from beginning")
            
            # Set chunk size based on available memory
            # Smaller chunks for better stability
            chunk_size = 10000
            
            # Train model with fixed training function
            training_start = time.time()
            # model, model_path = improved_resumable_training_fixed(
            #     model=model,
            #     X_file=X_file,
            #     y_file=y_file,
            #     train_indices=train_indices,
            #     val_indices=val_indices,
            #     test_indices=test_indices,
            #     output_dir=output_dir,
            #     batch_size=256,
            #     chunk_size=chunk_size,
            #     epochs_per_chunk=3,
            #     save_frequency=5,
            #     class_weight=class_weight,
            #     start_chunk=latest_chunk
            # )
            final_weights_path = ensure_training_progress(
                model=model,
                X_file=X_file,
                y_file=y_file,
                train_indices=train_indices,
                output_dir=output_dir,
                start_chunk=latest_chunk,
                save_frequency=5,
                build_model_func=build_improved_zero_curtain_model_fixed  # Pass the function directly
            )
            
            training_time = time.time() - training_start
            log(f"Training completed in {timedelta(seconds=int(training_time))}")
            log(f"Final model weights saved to {final_weights_path}")
            
        except Exception as train_e:
            log(f"Error during training: {train_e}")
            import traceback
            log(traceback.format_exc())
            
            # Try to recover best model if training failed
            try:
            #     checkpoint_dir = os.path.join(output_dir, "checkpoints")
            #     checkpoint_dirs = [d for d in os.listdir(checkpoint_dir) 
            #                    if os.path.isdir(os.path.join(checkpoint_dir, d)) and d.startswith(...
                
            #     if checkpoint_dirs:
            #         best_model_path = os.path.join(checkpoint_dir, sorted(checkpoint_dirs)[-1])
            #         log(f"Loading best model from {best_model_path}")
            #         model = tf.keras.models.load_model(best_model_path)
            #         model_path = best_model_path
            #     else:
            #         log("No best model found. Training failed.")
            #         return {'error': f"Training failed: {str(train_e)}"}
            # except Exception as recovery_e:
            #     log(f"Failed to recover model: {recovery_e}")
            #     return {'error': f"Training and recovery failed: {str(train_e)}"}

                best_weights_path = os.path.join(output_dir, "checkpoints", "best_model_weights")
                if os.path.exists(best_weights_path + ".index"):
                    log(f"Found best model weights at {best_weights_path}")
                    # Get input shape from sample
                    X = np.load(X_file, mmap_mode='r')
                    sample = X[train_indices[0]]
                    input_shape = sample.shape
                    
                    # Create model and load weights
                    model = build_improved_zero_curtain_model_fixed(input_shape)
                    model.load_weights(best_weights_path)
                    final_weights_path = best_weights_path
                    log("Successfully loaded best model weights")
                else:
                    # Look for any weights file
                    checkpoint_dir = os.path.join(output_dir, "checkpoints")
                    weight_files = [f for f in os.listdir(checkpoint_dir) 
                                 if f.startswith("model_weights_") and f.endswith(".index")]
                    
                    if weight_files:
                        # Get the latest weights file
                        latest_weights = sorted(weight_files)[-1].replace(".index", "")
                        weights_path = os.path.join(checkpoint_dir, latest_weights)
                        
                        # Get input shape from sample
                        X = np.load(X_file, mmap_mode='r')
                        sample = X[train_indices[0]]
                        input_shape = sample.shape
                        
                        # Create model and load weights
                        model = build_improved_zero_curtain_model_fixed(input_shape)
                        model.load_weights(weights_path)
                        final_weights_path = weights_path
                        log(f"Loaded most recent weights from {weights_path}")
                    else:
                        log("No model weights found. Training failed.")
                        return {'error': f"Training failed: {str(train_e)}"}
            except Exception as recovery_e:
                log(f"Failed to recover model: {recovery_e}")
                return {'error': f"Training and recovery failed: {str(train_e)}"}
        
        # Clean up between steps
        gc.collect()
        tf.keras.backend.clear_session()
        
        # Step 5: Evaluate model
        log("\nStep 5: Evaluating model...")
        try:
            # Reload the model to ensure clean state
            #if model_path and os.path.exists(model_path):
            #    model = tf.keras.models.load_model(model_path)
            #    log(f"Loaded model from {model_path}")
            #
            #eval_start = time.time()

            # Get the best model weights path
            best_weights_path = os.path.join(output_dir, "checkpoints", "best_model_weights")
            
            # Get input shape from a sample
            X = np.load(X_file, mmap_mode='r')
            sample = X[test_indices[0]]
            input_shape = sample.shape
            
            # Load the model using weights
            #model = load_model_from_weights(best_weights_path, input_shape)
            #log(f"Loaded best model weights from {best_weights_path}")
            best_weights_path = os.path.join(output_dir, "checkpoints", "best_model_weights")
            if os.path.exists(best_weights_path + ".index"):
                model = load_model_from_weights(best_weights_path, input_shape)
                log(f"Loaded best model weights from {best_weights_path}")
                
                # Load best model info if available
                best_model_file = os.path.join(output_dir, "best_model_info.json")
                if os.path.exists(best_model_file):
                    with open(best_model_file, 'r') as f:
                        best_info = json.load(f)
                    if 'val_accuracy' in best_info:
                        log(f"Best model validation accuracy: {best_info['val_accuracy']:.4f}")
            else:
                # Fall back to final weights
                #model = load_model_from_weights(final_weights_path, input_shape)
                model = load_model_from_weights(
                    final_weights_path, 
                    input_shape,
                    build_model_func=build_improved_zero_curtain_model_fixed  # Pass the function directly
                )
                log(f"Loaded model from final weights: {final_weights_path}")
            
            # Also load the best model info if available
            #best_model_file = os.path.join(output_dir, "best_model_info.json")
            #if os.path.exists(best_model_file):
            #    with open(best_model_file, 'r') as f:
            #        best_info = json.load(f)
            #    log(f"Best model was from chunk {best_info['chunk']}, epoch {best_info['epoch']} wi...
            
            eval_start = time.time()
            
            eval_results = evaluate_model_with_visualizations_fixed(
                model=model,
                X_file=X_file,
                y_file=y_file,
                test_indices=test_indices,
                output_dir=output_dir,
                metadata_file=metadata_file
            )
            
            eval_time = time.time() - eval_start
            log(f"Evaluation completed in {timedelta(seconds=int(eval_time))}")
            
            # Log key metrics
            log("\nEvaluation Results:")
            for metric, value in eval_results.items():
                if not isinstance(value, dict):
                    log(f"  {metric}: {value}")
        except Exception as eval_e:
            log(f"Error during evaluation: {eval_e}")
            import traceback
            log(traceback.format_exc())
            eval_results = {'error': str(eval_e)}
        
        # Clean up between steps
        gc.collect()
        tf.keras.backend.clear_session()
        
        # Step 6: Analyze spatial performance (import from original functions)
        log("\nStep 6: Analyzing spatial performance...")
        try:
            # Import analyze_spatial_performance_fixed function from the original module
            from zero_curtain_pipeline.modeling.fixed_code import analyze_spatial_performance_fixed
            
            spatial_start = time.time()
            spatial_results = analyze_spatial_performance_fixed(
                output_dir=output_dir,
                metadata_file=metadata_file
            )
            
            spatial_time = time.time() - spatial_start
            log(f"Spatial analysis completed in {timedelta(seconds=int(spatial_time))}")
            
            # Log key information
            log(f"Analyzed spatial performance across {spatial_results.get('latitude_bands', {}).get('count', 0)} latitude bands")
        except Exception as spatial_e:
            log(f"Error during spatial analysis: {spatial_e}")
            import traceback
            log(traceback.format_exc())
            spatial_results = {'error': str(spatial_e)}
        
        # Clean up between steps
        gc.collect()
        tf.keras.backend.clear_session()
        
        # Step 7: Analyze feature importance (import from original functions)
        log("\nStep 7: Analyzing feature importance...")
        try:
            # Import analyze_feature_importance_fixed function from the original module
            from zero_curtain_pipeline.modeling.fixed_code import analyze_feature_importance_fixed
            
            # Reload the model to ensure clean state
            # if model_path and os.path.exists(model_path):
            #     model = tf.keras.models.load_model(model_path)
            #     log(f"Loaded model from {model_path}")
            
            # feature_start = time.time()

            # Load the model using weights
            best_weights_path = os.path.join(output_dir, "checkpoints", "best_model_weights")
            
            X = np.load(X_file, mmap_mode='r')
            sample = X[test_indices[0]]
            input_shape = sample.shape
            
            #model = load_model_from_weights(best_weights_path, input_shape)
            #log(f"Loaded best model weights for feature importance analysis")
            best_weights_path = os.path.join(output_dir, "checkpoints", "best_model_weights")
            if os.path.exists(best_weights_path + ".index"):
                model = load_model_from_weights(best_weights_path, input_shape)
                log(f"Loaded best model weights from {best_weights_path}")
                
                # Load best model info if available
                best_model_file = os.path.join(output_dir, "best_model_info.json")
                if os.path.exists(best_model_file):
                    with open(best_model_file, 'r') as f:
                        best_info = json.load(f)
                    if 'val_accuracy' in best_info:
                        log(f"Best model validation accuracy: {best_info['val_accuracy']:.4f}")
            else:
                # Fall back to final weights
                model = load_model_from_weights(final_weights_path, input_shape)
                log(f"Loaded model from final weights: {final_weights_path}")
            
            feature_start = time.time()
            
            feature_results = analyze_feature_importance_fixed(
                model=model,
                X_file=X_file,
                y_file=y_file,
                test_indices=test_indices,
                output_dir=output_dir
            )
            
            feature_time = time.time() - feature_start
            log(f"Feature importance analysis completed in {timedelta(seconds=int(feature_time))}")
            
            # Log feature importance
            if 'feature_importance' in feature_results:
                log("\nFeature Importance Rankings:")
                for feature in feature_results['feature_importance']:
                    log(f"  {feature['Feature']}: {feature['Importance']:.4f} ± {feature['Std']:.4f}")
        except Exception as feature_e:
            log(f"Error during feature importance analysis: {feature_e}")
            import traceback
            log(traceback.format_exc())
            feature_results = {'error': str(feature_e)}
        
        # Step 8: Save all results
        log("\nStep 8: Compiling and saving all results...")
        try:
            all_results = {
                'evaluation': eval_results,
                'spatial_analysis': spatial_results,
                'feature_importance': feature_results,
                'pipeline_runtime': {
                    'start_time': datetime.fromtimestamp(pipeline_start_time).strftime('%Y-%m-%d %H:%M:%S'),
                    'end_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'total_seconds': time.time() - pipeline_start_time
                }
            }
            
            # Save as JSON with NumPy type handling
            with open(os.path.join(output_dir, "complete_analysis.json"), "w") as f:
                # Convert numpy values to Python native types
                class NumpyEncoder(json.JSONEncoder):
                    def default(self, obj):
                        if isinstance(obj, (np.integer, np.floating)):
                            return float(obj)
                        elif isinstance(obj, np.ndarray):
                            return obj.tolist()
                        elif isinstance(obj, np.bool_):
                            return bool(obj)
                        elif np.isnan(obj):
                            return None
                        return super(NumpyEncoder, self).default(obj)
                
                json.dump(all_results, f, indent=4, cls=NumpyEncoder)
            
            log(f"All results saved to {os.path.join(output_dir, 'complete_analysis.json')}")
        except Exception as save_e:
            log(f"Error saving results: {save_e}")
            import traceback
            log(traceback.format_exc())
        
        # Pipeline complete
        pipeline_end_time = time.time()
        elapsed = pipeline_end_time - pipeline_start_time
        
        log("\n" + "="*80)
        log(f"Pipeline completed in {timedelta(seconds=int(elapsed))}")
        log(f"Results saved to {output_dir}")
        log("="*80)
        
        return all_results
        
    except Exception as pipeline_e:
        log(f"Critical error in pipeline: {pipeline_e}")
        import traceback
        log(traceback.format_exc())
        return {'error': f"Pipeline failed: {str(pipeline_e)}"}

# Helper function to be called from command line
if __name__ == "__main__":
    run_improved_pipeline_fixed()

import os
import pandas as pd
import numpy as np
import pyarrow as pa
import pyarrow.feather as pf
import pyarrow.compute as pc
import pyarrow.dataset as ds
import gc
from tqdm.auto import tqdm

def get_time_range(feather_path):
    """Get min and max datetime from feather file efficiently"""
    print("Determining dataset time range")
    try:
        # Use PyArrow for efficient metadata access
        table = pf.read_table(feather_path, columns=['datetime'])
        datetime_col = table['datetime']
        min_date = pd.Timestamp(pc.min(datetime_col).as_py())
        max_date = pd.Timestamp(pc.max(datetime_col).as_py())
        del table, datetime_col
        gc.collect()
    except Exception as e:
        print(f"Error with PyArrow min/max: {str(e)}")
        print("Falling back to reading sample rows...")
        
        # Read just first and last rows after sorting
        # This is more efficient than reading all data
        try:
            dataset = ds.dataset(feather_path, format='feather')
            
            # Get min date from sorted data (first row)
            table_min = dataset.to_table(
                columns=['datetime'],
                filter=None,
                use_threads=True,
                limit=1
            )
            min_date = pd.Timestamp(table_min['datetime'][0].as_py())
            
            # Get max date from reverse sorted data (first row)
            # Note: PyArrow Dataset API doesn't support reverse sort directly
            # So we read all datetime values and find max
            table_all = dataset.to_table(columns=['datetime'])
            max_date = pd.Timestamp(pc.max(table_all['datetime']).as_py())
            
            del table_min, table_all, dataset
            gc.collect()
        except Exception as inner_e:
            print(f"Error with optimized approach: {str(inner_e)}")
            print("Using full scan method (slow)...")
            
            # Last resort: read all timestamps in chunks
            min_date = pd.Timestamp.max
            max_date = pd.Timestamp.min
            
            # Open file directly to avoid loading everything
            table = pf.read_table(feather_path, columns=['datetime'])
            datetime_values = table['datetime'].to_pandas()
            
            min_date = datetime_values.min()
            max_date = datetime_values.max()
            
            del datetime_values, table
            gc.collect()
    
    print(f"Data timespan: {min_date} to {max_date}")
    return min_date, max_date

def get_unique_site_depths(feather_path):
    """Get unique site-depth combinations efficiently"""
    print("Finding unique site-depth combinations")
    print(f"Memory before processing: {memory_usage():.1f} MB")
    
    try:
        # Use PyArrow for efficient operation
        dataset = ds.dataset(feather_path, format='feather')
        
        # Read only the columns we need
        table = dataset.to_table(columns=['source', 'soil_temp_depth'])
        
        # Convert to pandas
        df = table.to_pandas()
        
        # Get valid combinations (where temp_depth is not NaN)
        valid_df = df.dropna(subset=['soil_temp_depth'])
        
        # Get unique combinations
        site_depths = valid_df.drop_duplicates(['source', 'soil_temp_depth'])
        
        # Clean up
        del table, df, valid_df, dataset
        gc.collect()
        
    except Exception as e:
        print(f"Error with PyArrow approach: {str(e)}")
        print("Falling back to chunked pandas approach...")
        
        # Fallback: Process in chunks
        chunk_size = 1000000
        unique_combos = set()
        
        # Read feather in chunks (this is slower but more robust)
        with pd.read_feather(feather_path, columns=['source', 'soil_temp_depth']) as reader:
            for chunk in reader:
                # Get valid rows and unique combinations
                valid_rows = chunk.dropna(subset=['soil_temp_depth'])
                chunk_combinations = set(zip(valid_rows['source'], valid_rows['soil_temp_depth']))
                unique_combos.update(chunk_combinations)
                
                # Clean up
                del chunk, valid_rows, chunk_combinations
                gc.collect()
        
        # Convert to DataFrame
        site_depths = pd.DataFrame(list(unique_combos), columns=['source', 'soil_temp_depth'])
    
    print(f"Found {len(site_depths)} unique site-depth combinations")
    print(f"Memory after processing: {memory_usage():.1f} MB")
    
    return site_depths[['source', 'soil_temp_depth']]

def load_site_depth_data(feather_path, site, temp_depth):
    """Load ONLY data for a specific site and depth using PyArrow filtering"""
    print(f"Loading data for site: {site}, depth: {temp_depth}")
    print(f"Memory before loading: {memory_usage():.1f} MB")
    
    try:
        # Use PyArrow Dataset API for efficient filtering
        dataset = ds.dataset(feather_path, format='feather')
        
        # Create filter expressions
        site_filter = ds.field('source') == site
        depth_filter = ds.field('soil_temp_depth') == float(temp_depth)
        combined_filter = site_filter & depth_filter
        
        # Apply filter and read only filtered data
        table = dataset.to_table(filter=combined_filter)
        filtered_df = table.to_pandas()
        
        # Clean up
        del table, dataset
        gc.collect()
        
    except Exception as e:
        print(f"Error with PyArrow filtering: {str(e)}")
        print("Falling back to pandas filtering...")
        
        # Define chunk size based on available memory
        available_mem = psutil.virtual_memory().available / (1024**2)  # MB
        chunk_size = min(500000, max(100000, int(available_mem / 10)))
        
        filtered_chunks = []
        
        # Read and filter in chunks
        for chunk in pd.read_feather(feather_path, columns=None, chunksize=chunk_size):
            # Filter by site and depth
            chunk_filtered = chunk[(chunk['source'] == site) & 
                                   (chunk['soil_temp_depth'] == temp_depth)]
            
            if len(chunk_filtered) > 0:
                filtered_chunks.append(chunk_filtered)
            
            # Clean up
            del chunk, chunk_filtered
            gc.collect()
        
        # Combine filtered chunks
        if filtered_chunks:
            filtered_df = pd.concat(filtered_chunks, ignore_index=True)
            del filtered_chunks
        else:
            filtered_df = pd.DataFrame()
        
        gc.collect()
    
    # Ensure datetime is in datetime format
    if 'datetime' in filtered_df.columns and not pd.api.types.is_datetime64_dtype(filtered_df['datetime']):
        filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])
    
    print(f"Loaded {len(filtered_df)} rows for site-depth")
    print(f"Memory after loading: {memory_usage():.1f} MB")
    
    return filtered_df

def prepare_data_for_deep_learning_efficiently(feather_path, events_df, sequence_length=6, 
                                               output_dir=None, batch_size=500, start_batch=0):
    """
    Memory-efficient version of prepare_data_for_deep_learning that processes
    site-depths in batches and saves intermediate results without accumulating all data in memory.
    """
    import numpy as np
    from tqdm.auto import tqdm
    import os
    import gc
    import pandas as pd
    
    print("Preparing data for deep learning model...")
    print(f"Memory before preparation: {memory_usage():.1f} MB")
    
    # Ensure datetime columns are proper datetime objects
    if not pd.api.types.is_datetime64_dtype(events_df['datetime_min']):
        events_df['datetime_min'] = pd.to_datetime(events_df['datetime_min'], format='mixed')
    if not pd.api.types.is_datetime64_dtype(events_df['datetime_max']):
        events_df['datetime_max'] = pd.to_datetime(events_df['datetime_max'], format='mixed')
    
    # Create a mapping of detected events for labeling
    print("Creating event mapping...")
    event_map = {}
    for _, event in events_df.iterrows():
        site = event['source']
        depth = event['soil_temp_depth']
        start = event['datetime_min']
        end = event['datetime_max']
        
        if (site, depth) not in event_map:
            event_map[(site, depth)] = []
        
        event_map[(site, depth)].append((start, end))
    
    # Get site-depth combinations for progress tracking
    print("Finding unique site-depth combinations")
    print(f"Memory before processing: {memory_usage():.1f} MB")
    
    # Use a function that efficiently gets unique site-depths without loading all data
    site_depths = get_unique_site_depths(feather_path)
    total_combinations = len(site_depths)
    
    print(f"Found {total_combinations} unique site-depth combinations")
    print(f"Memory after processing: {memory_usage():.1f} MB")
    print(f"Preparing sequences from {total_combinations} site-depth combinations...")
    
    # Check for existing batch files to support resuming
    if output_dir is not None:
        os.makedirs(output_dir, exist_ok=True)
        import glob
        existing_batches = glob.glob(os.path.join(output_dir, 'X_batch_*_*.npy'))
        if existing_batches and start_batch == 0:
            # Extract batch numbers from filenames
            batch_ends = [int(os.path.basename(f).split('_')[-1].split('.')[0]) for f in existing_batches]
            if batch_ends:
                last_processed_batch = max(batch_ends)
                # Start from the next batch
                start_batch = (last_processed_batch // batch_size) * batch_size + batch_size
                print(f"Found existing batch files, resuming from batch {start_batch}")
    
    # Track total counts for reporting
    total_sequences = 0
    total_positive = 0
    
    # Process in batches to manage memory
    for batch_start in range(start_batch, total_combinations, batch_size):
        batch_end = min(batch_start + batch_size, total_combinations)
        print(f"\nProcessing batch {batch_start+1}-{batch_end}/{total_combinations}")
        
        # Initialize lists for this batch only
        batch_features = []
        batch_labels = []
        batch_metadata = []
        
        # Process each site-depth in the batch
        for i in tqdm(range(batch_start, batch_end), desc="Creating sequences"):
            site = site_depths.iloc[i]['source']
            temp_depth = site_depths.iloc[i]['soil_temp_depth']
            
            # Skip if no events exist for this site-depth
            if (site, temp_depth) not in event_map and len(event_map) > 0:
                continue
            
            try:
                # Load only the data for this site-depth
                print(f"Loading data for site: {site}, depth: {temp_depth}")
                print(f"Memory before loading: {memory_usage():.1f} MB")
                
                group = load_site_depth_data(feather_path, site, temp_depth)
                
                print(f"Loaded {len(group)} rows for site-depth")
                print(f"Memory after loading: {memory_usage():.1f} MB")
                
                if len(group) < sequence_length + 1:
                    continue
                
                # Ensure datetime is in datetime format
                if not pd.api.types.is_datetime64_dtype(group['datetime']):
                    group['datetime'] = pd.to_datetime(group['datetime'], format='mixed')
                
                # Sort by time
                group = group.sort_values('datetime')
                
                # Create feature set
                feature_cols = ['soil_temp_standardized']
                
                # Calculate gradient features
                group['temp_gradient'] = group['soil_temp_standardized'].diff()
                feature_cols.append('temp_gradient')
                
                # Add soil depth as feature
                group['depth_normalized'] = temp_depth / 10.0
                feature_cols.append('depth_normalized')
                
                # Add soil moisture if available
                has_moisture = False
                if 'soil_moist_standardized' in group.columns and not group['soil_moist_standardized'].isna().all():
                    has_moisture = True
                    feature_cols.append('soil_moist_standardized')
                    group['moist_gradient'] = group['soil_moist_standardized'].diff()
                    feature_cols.append('moist_gradient')
                
                # Fill missing values
                group[feature_cols] = group[feature_cols].fillna(0)
                
                # Create sequences with sliding window
                for j in range(len(group) - sequence_length):
                    # Get time window
                    start_time = group.iloc[j]['datetime']
                    end_time = group.iloc[j + sequence_length - 1]['datetime']
                    
                    # Extract sequence data
                    sequence = group.iloc[j:j+sequence_length][feature_cols].values
                    
                    # Check if this sequence overlaps with known zero curtain event
                    is_zero_curtain = 0
                    if (site, temp_depth) in event_map:
                        for event_start, event_end in event_map[(site, temp_depth)]:
                            # Ensure proper datetime comparison
                            # Check for significant overlap (at least 50% of sequence)
                            if (min(end_time, event_end) - max(start_time, event_start)).total_seconds() > \
                               0.5 * (end_time - start_time).total_seconds():
                                is_zero_curtain = 1
                                break
                    
                    # Store features and labels
                    batch_features.append(sequence)
                    batch_labels.append(is_zero_curtain)
                    total_positive += is_zero_curtain
                    total_sequences += 1
                    
                    # Store metadata
                    meta = {
                        'source': site,
                        'soil_temp_depth': temp_depth,
                        'start_time': start_time,
                        'end_time': end_time,
                        'latitude': group.iloc[j]['latitude'] if 'latitude' in group.columns else None,
                        'longitude': group.iloc[j]['longitude'] if 'longitude' in group.columns else None,
                        'has_moisture_data': has_moisture
                    }
                    batch_metadata.append(meta)
                
                # Clean up to free memory
                del group
                gc.collect()
                
            except Exception as e:
                print(f"Error processing {site}, depth {temp_depth}: {str(e)}")
                import traceback
                traceback.print_exc()
                continue
        
        # Save batch results if output directory provided and we have data
        if output_dir is not None and batch_features:
            os.makedirs(output_dir, exist_ok=True)
            
            # Save batch as numpy files
            batch_X = np.array(batch_features)
            batch_y = np.array(batch_labels)
            
            np.save(os.path.join(output_dir, f'X_batch_{batch_start}_{batch_end}.npy'), batch_X)
            np.save(os.path.join(output_dir, f'y_batch_{batch_start}_{batch_end}.npy'), batch_y)
            
            # Save metadata as pickle
            import pickle
            with open(os.path.join(output_dir, f'metadata_batch_{batch_start}_{batch_end}.pkl'), 'wb') as f:
                pickle.dump(batch_metadata, f)
            
            # Save progress marker
            with open(os.path.join(output_dir, 'progress.txt'), 'w') as f:
                f.write(f"Last processed batch: {batch_start}-{batch_end}\n")
                f.write(f"Total sequences: {total_sequences}\n")
                f.write(f"Positive examples: {total_positive} ({total_positive/total_sequences*100:.1f}%)\n")
        
        # Clean up batch variables - this is key for memory efficiency
        del batch_features, batch_labels, batch_metadata
        if 'batch_X' in locals(): del batch_X
        if 'batch_y' in locals(): del batch_y
        gc.collect()
        
        print(f"Memory after batch: {memory_usage():.1f} MB")
        #print(f"Progress: {total_sequences} sequences processed, {total_positive} positive examples...
        positive_percentage = (total_positive/total_sequences*100) if total_sequences > 0 else 0
        print(f"Progress: {total_sequences} sequences processed, {total_positive} positive examples ({positive_percentage:.1f}%)")

    print("Data preparation complete!")
    print(f"Total sequences: {total_sequences}")
    #print(f"Positive examples: {total_positive} ({total_positive/total_sequences*100:.1f}% if total...
    positive_percentage = (total_positive/total_sequences*100) if total_sequences > 0 else 0
    print(f"Positive examples: {total_positive} ({positive_percentage:.1f}%)")
    
    if output_dir is not None:
        print(f"Results saved to {output_dir}")
        print("To merge batch files into final dataset, use merge_batch_files(output_dir)")
    
    print(f"Memory after preparation: {memory_usage():.1f} MB")
    
    # Return info instead of data - prevents memory issues
    return {
        'total_sequences': total_sequences,
        'total_positive': total_positive,
        'positive_percentage': total_positive/total_sequences*100 if total_sequences > 0 else 0,
        'output_dir': output_dir
    }

def merge_batch_files(output_dir):
    """
    Merge all batch files into single X_features.npy and y_labels.npy files.
    """
    import numpy as np
    import os
    import glob
    import pickle
    import gc
    
    print(f"Memory before merging: {memory_usage():.1f} MB")
    
    # Find all batch files
    x_batch_files = sorted(glob.glob(os.path.join(output_dir, 'X_batch_*_*.npy')))
    y_batch_files = sorted(glob.glob(os.path.join(output_dir, 'y_batch_*_*.npy')))
    metadata_batch_files = sorted(glob.glob(os.path.join(output_dir, 'metadata_batch_*_*.pkl')))
    
    print(f"Found {len(x_batch_files)} X batches, {len(y_batch_files)} y batches, and {len(metadata_batch_files)} metadata batches")
    
    # Get dimensions of first batch to initialize arrays
    if x_batch_files:
        first_batch = np.load(x_batch_files[0])
        shape = first_batch.shape
        del first_batch
        gc.collect()
        
        # Count total sequences
        total_sequences = 0
        for batch_file in x_batch_files:
            batch = np.load(batch_file)
            total_sequences += batch.shape[0]
            del batch
            gc.collect()
        
        # Pre-allocate arrays
        X = np.zeros((total_sequences, shape[1], shape[2]), dtype=np.float32)
        y = np.zeros(total_sequences, dtype=np.int32)
        
        # Load and copy batches
        idx = 0
        for i, (x_file, y_file) in enumerate(zip(x_batch_files, y_batch_files)):
            print(f"Processing batch {i+1}/{len(x_batch_files)}")
            print(f"Memory: {memory_usage():.1f} MB")
            
            batch_x = np.load(x_file)
            batch_y = np.load(y_file)
            
            batch_size = batch_x.shape[0]
            X[idx:idx+batch_size] = batch_x
            y[idx:idx+batch_size] = batch_y
            
            idx += batch_size
            
            # Clean up
            del batch_x, batch_y
            gc.collect()
        
        # Save merged X and y
        print(f"Saving merged arrays: X.shape={X.shape}, y.shape={y.shape}")
        np.save(os.path.join(output_dir, 'X_features.npy'), X)
        np.save(os.path.join(output_dir, 'y_labels.npy'), y)
        
        # Clean up
        del X, y
        gc.collect()
        
        # Load and concatenate metadata batches
        all_metadata = []
        for i, batch_file in enumerate(metadata_batch_files):
            print(f"Processing metadata batch {i+1}/{len(metadata_batch_files)}")
            print(f"Memory: {memory_usage():.1f} MB")
            
            with open(batch_file, 'rb') as f:
                batch_metadata = pickle.load(f)
            all_metadata.extend(batch_metadata)
            
            # Clean up
            del batch_metadata
            gc.collect()
        
        # Save merged metadata
        print(f"Saving merged metadata: {len(all_metadata)} entries")
        with open(os.path.join(output_dir, 'metadata.pkl'), 'wb') as f:
            pickle.dump(all_metadata, f)
        
        # Final cleanup
        del all_metadata
        gc.collect()
        
        print(f"Memory after merging: {memory_usage():.1f} MB")
        return {
            'total_sequences': total_sequences,
            'output_dir': output_dir
        }
    else:
        print("No batch files found to merge")
        return None

def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
    """
    Advanced model architecture combining ConvLSTM, Transformers, and 
    Variational Autoencoder components to better capture complex zero curtain dynamics.
    
    Parameters:
    -----------
    input_shape : tuple
        Shape of input data (sequence_length, num_features)
    include_moisture : bool
        Whether soil moisture features are included
        
    Returns:
    --------
    tensorflow.keras.Model
        Compiled model ready for training
    """
    import os
    os.environ["DEVICE_COUNT_GPU"] = "0"
    import tensorflow as tf
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
    from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D
    from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
    from tensorflow.keras.optimizers import Adam
    
    # Input layer
    inputs = Input(shape=input_shape)
    
    # Reshape for ConvLSTM (add spatial dimension)
    # From (sequence_length, features) to (sequence_length, 1, features)
    
    #x = Reshape((input_shape[0], 1, input_shape[1]))(inputs)
    x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
    # ConvLSTM layer to capture spatiotemporal patterns
    from tensorflow.keras.layers import ConvLSTM2D
    convlstm = ConvLSTM2D(
        filters=64,
        kernel_size=(3, 1),
        padding='same',
        return_sequences=True,
        activation='tanh',
        recurrent_dropout=0.2
    )(x)
    
    # Reshape back to (sequence_length, features)
    convlstm = Reshape((input_shape[0], 64))(convlstm)
    
    # Add positional encoding for transformer
    def positional_encoding(length, depth):
        positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
        angle_rates = 1 / tf.pow(10000.0, depths)
        angle_rads = positions * angle_rates
        
        pos_encoding = tf.concat(
            [tf.sin(angle_rads), tf.cos(angle_rads)], axis=-1)
        
        return pos_encoding
    
    # Add positional encoding
    pos_encoding = positional_encoding(input_shape[0], 64)
    transformer_input = convlstm + pos_encoding
    
    # Transformer encoder block
    def transformer_encoder(x, num_heads=8, key_dim=64, ff_dim=128):
        # Multi-head attention
        attention_output = MultiHeadAttention(
            num_heads=num_heads, key_dim=key_dim
        )(x, x)
        
        # Skip connection 1
        x1 = Add()([attention_output, x])
        x1 = LayerNormalization(epsilon=1e-6)(x1)
        
        # Feed-forward network
        ff_output = Dense(ff_dim, activation='relu')(x1)
        ff_output = Dropout(0.1)(ff_output)
        ff_output = Dense(64)(ff_output)
        
        # Skip connection 2
        x2 = Add()([ff_output, x1])
        return LayerNormalization(epsilon=1e-6)(x2)
    
    # Apply transformer encoder
    transformer_output = transformer_encoder(transformer_input)
    
    # Parallel CNN paths for multi-scale feature extraction
    cnn_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(inputs)
    cnn_1 = BatchNormalization()(cnn_1)
    
    cnn_2 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(inputs)
    cnn_2 = BatchNormalization()(cnn_2)
    
    cnn_3 = Conv1D(filters=32, kernel_size=7, padding='same', activation='relu')(inputs)
    cnn_3 = BatchNormalization()(cnn_3)
    
    # Variational Autoencoder components
    def sampling(args):
        z_mean, z_log_var = args
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
    # Global temporal features
    global_max = GlobalMaxPooling1D()(transformer_output)
    global_avg = GlobalAveragePooling1D()(transformer_output)
    
    # VAE encoding
    z_mean = Dense(32)(Concatenate()([global_max, global_avg]))
    z_log_var = Dense(32)(Concatenate()([global_max, global_avg]))
    z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
    # Combine all features
    merged_features = Concatenate()(
        [
            GlobalMaxPooling1D()(cnn_1),
            GlobalMaxPooling1D()(cnn_2),
            GlobalMaxPooling1D()(cnn_3),
            global_max,
            global_avg,
            z
        ]
    )
    
    # Final classification layers
    x = Dense(128, activation='relu')(merged_features)
    x = Dropout(0.3)(x)
    x = BatchNormalization()(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.2)(x)
    
    # Output layer
    outputs = Dense(1, activation='sigmoid')(x)
    
    # Create model
    model = Model(inputs=inputs, outputs=outputs)
    
    # Add VAE loss
    kl_loss = -0.5 * tf.reduce_mean(
        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
    )
    model.add_loss(0.001 * kl_loss)  # Small weight for VAE loss
    
    # Compile model with appropriate metrics
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=[
            'accuracy',
            tf.keras.metrics.AUC(name='auc'),
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall')
        ]
    )
    
    return model

# Time-based split rather than random split
def temporal_train_test_split(X, y, metadata, val_ratio=0.2, test_ratio=0.1):
    """
    Split data temporally for time series modeling.
    
    Parameters:
    -----------
    X : numpy.ndarray
        Input features
    y : numpy.ndarray
        Output labels
    metadata : list
        Metadata containing timestamps for each sequence
    val_ratio : float
        Proportion of data for validation
    test_ratio : float
        Proportion of data for testing
        
    Returns:
    --------
    tuple
        (X_train, X_val, X_test, y_train, y_val, y_test)
    """
    # Extract timestamps from metadata
    timestamps = [meta['start_time'] for meta in metadata]
    
    # Sort indices by timestamp
    sorted_indices = sorted(range(len(timestamps)), key=lambda i: timestamps[i])
    
    # Calculate split points
    n_samples = len(sorted_indices)
    test_start = int(n_samples * (1 - test_ratio))
    val_start = int(n_samples * (1 - test_ratio - val_ratio))
    
    # Split indices into train, validation, and test sets
    train_indices = sorted_indices[:val_start]
    val_indices = sorted_indices[val_start:test_start]
    test_indices = sorted_indices[test_start:]
    
    # Create the splits
    X_train = X[train_indices]
    y_train = y[train_indices]
    
    X_val = X[val_indices]
    y_val = y[val_indices]
    
    X_test = X[test_indices]
    y_test = y[test_indices]
    
    return X_train, X_val, X_test, y_train, y_val, y_test

def train_zero_curtain_model_efficiently(X, y, metadata=None, output_dir=None):
    """
    Memory-efficient version of train_zero_curtain_model that implements
    batch training and model checkpointing with temporal data splitting.
    
    Parameters:
    -----------
    X : numpy.ndarray
        Input features
    y : numpy.ndarray
        Output labels
    metadata : list, optional
        Metadata about each sequence (must contain timestamps)
    output_dir : str, optional
        Directory to save model and results
        
    Returns:
    --------
    tuple
        (trained_model, training_history, evaluation_results)
    """
    import tensorflow as tf
    from tensorflow.keras.callbacks import (
        EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, 
        CSVLogger, TensorBoard
    )
    import matplotlib.pyplot as plt
    import os
    import gc
    import numpy as np
    
    # Enable memory growth to avoid pre-allocating all GPU memory
    physical_devices = tf.config.list_physical_devices('GPU')
    if physical_devices:
        for device in physical_devices:
            tf.config.experimental.set_memory_growth(device, True)
            print(f"Enabled memory growth for {device}")
    
    print("Training zero curtain model...")
    print(f"Memory before training: {memory_usage():.1f} MB")
    
    # Create output directory
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    # Temporal split for time series data
    print("Performing temporal split for train/validation/test sets...")
    if metadata is None:
        raise ValueError("Metadata with timestamps is required for temporal splitting")
    
    # Extract timestamps from metadata
    timestamps = np.array([meta['start_time'] for meta in metadata])
    
    # Sort indices by timestamp
    sorted_indices = np.argsort(timestamps)
    
    # Calculate split points (70% train, 15% validation, 15% test)
    n_samples = len(sorted_indices)
    test_ratio = 0.15
    val_ratio = 0.15
    
    test_start = int(n_samples * (1 - test_ratio))
    val_start = int(n_samples * (1 - test_ratio - val_ratio))
    
    # Split indices into train, validation, and test sets
    train_indices = sorted_indices[:val_start]
    val_indices = sorted_indices[val_start:test_start]
    test_indices = sorted_indices[test_start:]
    
    print(f"Training on data from {timestamps[train_indices[0]]} to {timestamps[train_indices[-1]]}")
    print(f"Validating on data from {timestamps[val_indices[0]]} to {timestamps[val_indices[-1]]}")
    print(f"Testing on data from {timestamps[test_indices[0]]} to {timestamps[test_indices[-1]]}")
    
    # Create the splits
    X_train = X[train_indices]
    y_train = y[train_indices]
    
    X_val = X[val_indices]
    y_val = y[val_indices]
    
    X_test = X[test_indices]
    y_test = y[test_indices]
    
    print(f"Split sizes: Train={len(X_train)}, Validation={len(X_val)}, Test={len(X_test)}")
    
    # Check class balance in each split
    train_pos = np.sum(y_train)
    val_pos = np.sum(y_val)
    test_pos = np.sum(y_test)
    
    print(f"Positive examples: Train={train_pos} ({train_pos/len(y_train)*100:.1f}%), " +
          f"Val={val_pos} ({val_pos/len(y_val)*100:.1f}%), " +
          f"Test={test_pos} ({test_pos/len(y_test)*100:.1f}%)")
    
    # Clean up to free memory
    del sorted_indices, timestamps
    gc.collect()
    
    # Build model with appropriate input shape
    print("Building model...")
    input_shape = (X_train.shape[1], X_train.shape[2])
    
    model = build_advanced_zero_curtain_model(input_shape)
    
    # If output directory exists, check for existing model checkpoint
    model_checkpoint_path = None
    if output_dir:
        model_checkpoint_path = os.path.join(output_dir, 'checkpoint.h5')
        if os.path.exists(model_checkpoint_path):
            print(f"Loading existing model checkpoint from {model_checkpoint_path}")
            try:
                model = tf.keras.models.load_model(model_checkpoint_path)
                print("Checkpoint loaded successfully")
            except Exception as e:
                print(f"Error loading checkpoint: {str(e)}")
    
    # Set up callbacks with additional memory management
    callbacks = [
        # Stop early if validation performance plateaus
        EarlyStopping(
            patience=15,  # Increased patience for temporal data
            restore_best_weights=True, 
            monitor='val_auc', 
            mode='max'
        ),
        # Reduce learning rate when improvement slows
        ReduceLROnPlateau(
            factor=0.5, 
            patience=7,  # Increased patience for temporal data
            min_lr=1e-6, 
            monitor='val_auc', 
            mode='max'
        ),
        # Manual garbage collection after each epoch
        tf.keras.callbacks.LambdaCallback(
            on_epoch_end=lambda epoch, logs: gc.collect()
        )
    ]
    
    # Add additional callbacks if output directory provided
    if output_dir:
        callbacks.extend([
            # Save best model
            ModelCheckpoint(
                os.path.join(output_dir, 'checkpoint.h5'),
                save_best_only=True,
                monitor='val_auc',
                mode='max'
            ),
            # Log training progress to CSV
            CSVLogger(
                os.path.join(output_dir, 'training_log.csv'),
                append=True
            ),
            # TensorBoard visualization
            TensorBoard(
                log_dir=os.path.join(output_dir, 'tensorboard_logs'),
                histogram_freq=1,
                profile_batch=0  # Disable profiling to save memory
            )
        ])
    
    # Calculate class weights to handle imbalance
    pos_weight = len(y_train) / max(sum(y_train), 1)
    class_weight = {0: 1, 1: pos_weight}
    print(f"Using class weight {pos_weight:.2f} for positive class")
    
    # Train model with memory-efficient settings
    print("Training model...")
    batch_size = 32  # Adjust based on available memory
    epochs = 100
    
    # Use fit with appropriate memory settings
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=callbacks,
        class_weight=class_weight,
        verbose=1,
        # Memory efficiency settings
        shuffle=True,  # Still shuffle within the temporal train split
        use_multiprocessing=False,  # Avoid extra memory overhead
        workers=1  # Reduce parallel processing to save memory
    )
    
    # Clean up to free memory
    del X_train, y_train, X_val, y_val
    gc.collect()
    
    # Evaluate on test set
    print("Evaluating model on test set...")
    evaluation = model.evaluate(X_test, y_test, batch_size=batch_size)
    print("Test performance:")
    for metric, value in zip(model.metrics_names, evaluation):
        print(f"  {metric}: {value:.4f}")
    
    # Generate predictions for visualization and further analysis
    y_pred_prob = model.predict(X_test, batch_size=batch_size)
    y_pred = (y_pred_prob > 0.5).astype(int).flatten()
    
    # Calculate and save additional evaluation metrics
    from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
    report = classification_report(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    
    print("Classification Report:")
    print(report)
    
    print("Confusion Matrix:")
    print(conf_matrix)
    
    # Plot and save training history
    if output_dir:
        # Save evaluation metrics
        with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w') as f:
            f.write("Classification Report:\n")
            f.write(report)
            f.write("\n\nConfusion Matrix:\n")
            f.write(str(conf_matrix))
            f.write("\n\nTest Metrics:\n")
            for metric, value in zip(model.metrics_names, evaluation):
                f.write(f"{metric}: {value:.4f}\n")
        
        # Plot training history
        plt.figure(figsize=(16, 6))
        
        plt.subplot(1, 3, 1)
        plt.plot(history.history['auc'])
        plt.plot(history.history['val_auc'])
        plt.title('Model AUC')
        plt.ylabel('AUC')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'], loc='lower right')
        
        plt.subplot(1, 3, 2)
        plt.plot(history.history['loss'])
        plt.plot(history.history['val_loss'])
        plt.title('Model Loss')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'], loc='upper right')
        
        # Plot ROC curve
        plt.subplot(1, 3, 3)
        fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve (Test Set)')
        plt.legend(loc='lower right')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'training_performance.png'), dpi=300)
        
        # Save detailed model summary
        from contextlib import redirect_stdout
        with open(os.path.join(output_dir, 'model_summary.txt'), 'w') as f:
            with redirect_stdout(f):
                model.summary()
    
    # Clean up to free memory
    del X_test, y_test, y_pred, y_pred_prob
    gc.collect()
    
    print(f"Memory after training: {memory_usage():.1f} MB")
    return model, history, evaluation

def run_full_analysis_pipeline(feather_path, output_base_dir='results', batch_size=50):
    """
    Run the complete zero curtain analysis pipeline with progress tracking
    and memory efficiency.
    
    Parameters:
    -----------
    feather_path : str
        Path to the feather file with merged data
    output_base_dir : str
        Base directory for saving outputs
    batch_size : int
        Number of site-depths to process per batch
        
    Returns:
    --------
    dict
        Dictionary containing analysis results
    """
    from tqdm.auto import tqdm
    import time
    import os
    import gc
    import pickle
    
    # Create output directories
    os.makedirs(output_base_dir, exist_ok=True)
    checkpoint_dir = os.path.join(output_base_dir, 'checkpoints')
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # Function to save checkpoint
    def save_checkpoint(data, name):
        with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'wb') as f:
            pickle.dump(data, f)
    
    # Function to load checkpoint
    def load_checkpoint(name):
        try:
            with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'rb') as f:
                return pickle.load(f)
        except:
            return None
    
    # Initialize results
    results = load_checkpoint('pipeline_results') or {}
    
    # Check for completed stages
    completed_stages = set(results.get('completed_stages', []))
    print(f"Found {len(completed_stages)} completed stages: {completed_stages}")
    
    # Add a progress indicator for the overall workflow
    stages = ['Enhanced Detection', 'Data Preparation', 'Model Training', 
              'Model Application', 'Visualization', 'Comparison']
    
    with tqdm(total=len(stages), desc="Overall Progress", initial=len(completed_stages)) as pbar:
        # Stage 1: Enhanced physical detection
        if 'Enhanced Detection' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 1/{len(stages)}: {stages[0]}")
            
            print(f"Current memory usage: {memory_usage():.1f} MB")
            
            # Run memory-efficient detection using your implementation
            # This part is already implemented in your code
            #from zero_curtain_pipeline import run_memory_efficient_pipeline
            
            enhanced_events = run_memory_efficient_pipeline(
                feather_path=feather_path,
                output_dir=os.path.join(output_base_dir, 'enhanced'),
                site_batch_size=batch_size,
                checkpoint_interval=5,
                max_gap_hours=6,
                interpolation_method='cubic'
            )
            
            results['enhanced_events'] = enhanced_events
            results['enhanced_time'] = time.time() - start_time
            
            # Save progress
            completed_stages.add('Enhanced Detection')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            # Clean up memory
            gc.collect()
            print(f"Memory after Stage 1: {memory_usage():.1f} MB")
            pbar.update(1)
        else:
            # Load enhanced events if needed
            if 'enhanced_events' not in results:
                enhanced_events = load_checkpoint('enhanced_events')
                if enhanced_events is None:
                    # Try loading from CSV
                    csv_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events.csv')
                    if os.path.exists(csv_path):
                        enhanced_events = pd.read_csv(csv_path, parse_dates=['datetime_min', 'datetime_max'])
                    else:
                        print("Warning: No enhanced events found, cannot proceed with deep learning")
                        enhanced_events = pd.DataFrame()
                results['enhanced_events'] = enhanced_events
        
        # Stage 2: Data Preparation for Deep Learning
        if 'Data Preparation' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 2/{len(stages)}: {stages[1]}")
            
            # Get enhanced events
            enhanced_events = results.get('enhanced_events')
            if enhanced_events is not None and len(enhanced_events) > 0:
                try:
                    # Prepare data for deep learning with memory efficiency
                    X, y, metadata = prepare_data_for_deep_learning_efficiently(
                        feather_path=feather_path,
                        events_df=enhanced_events,
                        sequence_length=24,  # Use 24 time steps as in your original code
                        output_dir=os.path.join(output_base_dir, 'ml_data'),
                        batch_size=batch_size
                    )
                    
                    results['X'] = X.shape  # Store only shape to save memory
                    results['y'] = y.shape
                    results['data_preparation_time'] = time.time() - start_time
                    
                    # Clean up to free memory
                    del X, y
                    gc.collect()
                except Exception as e:
                    print(f"Error in data preparation: {str(e)}")
                    results['data_preparation_error'] = str(e)
            else:
                print("Skipping data preparation: No enhanced events available")
            
            # Save progress
            completed_stages.add('Data Preparation')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            # Clean up memory
            gc.collect()
            print(f"Memory after Stage 2: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 3: Model Training
        if 'Model Training' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 3/{len(stages)}: {stages[2]}")
            
            try:
                # Load prepared data
                data_dir = os.path.join(output_base_dir, 'ml_data')
                X = np.load(os.path.join(data_dir, 'X_features.npy'))
                y = np.load(os.path.join(data_dir, 'y_labels.npy'))
                
                # Load metadata if needed
                with open(os.path.join(data_dir, 'metadata.pkl'), 'rb') as f:
                    metadata = pickle.load(f)
                
                # Train model
                model, history, evaluation = train_zero_curtain_model_efficiently(
                    X=X, 
                    y=y,
                    metadata=metadata,
                    output_dir=os.path.join(output_base_dir, 'model')
                )
                
                # Store minimal results to save memory
                results['model_evaluation'] = evaluation
                results['model_training_time'] = time.time() - start_time
                
                # Clean up to free memory
                del X, y, metadata, model, history
                gc.collect()
            except Exception as e:
                print(f"Error in model training: {str(e)}")
                results['model_training_error'] = str(e)
            
            # Save progress
            completed_stages.add('Model Training')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            # Clean up memory
            gc.collect()
            print(f"Memory after Stage 3: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 4: Model Application
        if 'Model Application' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 4/{len(stages)}: {stages[3]}")
            
            try:
                # Load model
                model_path = os.path.join(output_base_dir, 'model', 'checkpoint.h5')
                if os.path.exists(model_path):
                    import tensorflow as tf
                    model = tf.keras.models.load_model(model_path)
                    
                    # Create directory for predictions
                    pred_dir = os.path.join(output_base_dir, 'predictions')
                    os.makedirs(pred_dir, exist_ok=True)
                    
                    # Apply model with memory efficiency (batched processing)
                    #from apply_model_efficiently import apply_model_to_new_data_efficiently
                    
                    predictions = apply_model_to_new_data_efficiently(
                        model=model,
                        feather_path=feather_path,
                        sequence_length=24,
                        output_dir=pred_dir,
                        batch_size=batch_size
                    )
                    
                    results['model_predictions_count'] = len(predictions)
                    results['model_application_time'] = time.time() - start_time
                    
                    # Clean up
                    del model, predictions
                    gc.collect()
                else:
                    print("Skipping model application: No model checkpoint found")
            except Exception as e:
                print(f"Error in model application: {str(e)}")
                results['model_application_error'] = str(e)
            
            # Save progress
            completed_stages.add('Model Application')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            # Clean up memory
            gc.collect()
            print(f"Memory after Stage 4: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stages 5 and 6: Visualization and Comparison
        # (Follow the same pattern - load data, process, clean up memory)
        
    # Generate final summary report
    total_time = sum([
        results.get('enhanced_time', 0),
        results.get('data_preparation_time', 0),
        results.get('model_training_time', 0),
        results.get('model_application_time', 0),
        results.get('visualization_time', 0),
        results.get('comparison_time', 0)
    ])
    
    print("\n" + "=" * 80)
    print("ZERO CURTAIN ANALYSIS COMPLETE")
    print("=" * 80)
    print(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
    
    return results

def apply_model_to_new_data_efficiently(model, feather_path, sequence_length=6, 
                                        output_dir=None, batch_size=50):
    """
    Apply a trained model to detect zero curtain events in new data with memory efficiency.
    
    Parameters:
    -----------
    model : tensorflow.keras.Model
        Trained zero curtain detection model
    feather_path : str
        Path to the feather file
    sequence_length : int
        Length of sequences used for model input
    output_dir : str, optional
        Output directory for results
    batch_size : int
        Number of site-depths to process per batch
        
    Returns:
    --------
    pandas.DataFrame
        DataFrame with predictions and probabilities
    """
    #from data_loader import get_unique_site_depths, load_site_depth_data
    import numpy as np
    from tqdm.auto import tqdm
    import os
    import gc
    import pandas as pd
    
    print("Applying model to new data...")
    print(f"Memory before application: {memory_usage():.1f} MB")
    
    # Create output directory
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    # Get site-depth combinations
    site_depths = get_unique_site_depths(feather_path)
    total_combinations = len(site_depths)
    print(f"Applying model to {total_combinations} site-depth combinations...")
    
    # Initialize list for all predictions
    all_predictions = []
    
    # Process in batches
    for batch_start in range(0, total_combinations, batch_size):
        batch_end = min(batch_start + batch_size, total_combinations)
        print(f"\nProcessing batch {batch_start+1}-{batch_end}/{total_combinations}")
        
        batch_predictions = []
        
        # Process each site-depth in batch
        for i in tqdm(range(batch_start, batch_end), desc="Making predictions"):
            site = site_depths.iloc[i]['source']
            temp_depth = site_depths.iloc[i]['soil_temp_depth']
            
            try:
                # Load data for this site-depth
                group = load_site_depth_data(feather_path, site, temp_depth)
                
                if len(group) < sequence_length + 1:
                    continue
                
                # Sort by time
                group = group.sort_values('datetime')
                
                # Prepare features (same as in training)
                feature_cols = ['soil_temp_standardized']
                group['temp_gradient'] = group['soil_temp_standardized'].diff()
                feature_cols.append('temp_gradient')
                group['depth_normalized'] = temp_depth / 10.0
                feature_cols.append('depth_normalized')
                
                # Add soil moisture if available
                if 'soil_moist_standardized' in group.columns and not group['soil_moist_standardized'].isna().all():
                    feature_cols.append('soil_moist_standardized')
                    group['moist_gradient'] = group['soil_moist_standardized'].diff()
                    feature_cols.append('moist_gradient')
                
                # Fill missing values
                group[feature_cols] = group[feature_cols].fillna(0)
                
                # Create sequences and predict in mini-batches to save memory
                sequences = []
                sequence_meta = []
                
                for j in range(len(group) - sequence_length):
                    # Get time window
                    start_time = group.iloc[j]['datetime']
                    end_time = group.iloc[j + sequence_length - 1]['datetime']
                    
                    # Extract sequence
                    sequence = group.iloc[j:j+sequence_length][feature_cols].values
                    sequences.append(sequence)
                    
                    # Store metadata
                    meta = {
                        'source': site,
                        'soil_temp_depth': temp_depth,
                        'datetime_min': start_time,
                        'datetime_max': end_time,
                        'latitude': group.iloc[j]['latitude'] if 'latitude' in group.columns else None,
                        'longitude': group.iloc[j]['longitude'] if 'longitude' in group.columns else None
                    }
                    sequence_meta.append(meta)
                    
                    # Process in mini-batches of 1000 sequences
                    if len(sequences) >= 1000:
                        # Make predictions
                        X_batch = np.array(sequences)
                        pred_probs = model.predict(X_batch, verbose=0)
                        
                        # Store results
                        for k, prob in enumerate(pred_probs):
                            meta = sequence_meta[k]
                            prediction = {
                                'source': meta['source'],
                                'soil_temp_depth': meta['soil_temp_depth'],
                                'datetime_min': meta['datetime_min'],
                                'datetime_max': meta['datetime_max'],
                                'zero_curtain_probability': float(prob[0]),
                                'is_zero_curtain': 1 if prob[0] > 0.5 else 0,
                                'latitude': meta['latitude'],
                                'longitude': meta['longitude']
                            }
                            batch_predictions.append(prediction)
                        
                        # Clear mini-batch to free memory
                        sequences = []
                        sequence_meta = []
                
                # Process any remaining sequences
                if sequences:
                    X_batch = np.array(sequences)
                    pred_probs = model.predict(X_batch, verbose=0)
                    
                    for k, prob in enumerate(pred_probs):
                        meta = sequence_meta[k]
                        prediction = {
                            'source': meta['source'],
                            'soil_temp_depth': meta['soil_temp_depth'],
                            'datetime_min': meta['datetime_min'],
                            'datetime_max': meta['datetime_max'],
                            'zero_curtain_probability': float(prob[0]),
                            'is_zero_curtain': 1 if prob[0] > 0.5 else 0,
                            'latitude': meta['latitude'],
                            'longitude': meta['longitude']
                        }
                        batch_predictions.append(prediction)
                
                # Clean up to free memory
                del group, sequences, sequence_meta, X_batch, pred_probs
                gc.collect()
                
            except Exception as e:
                print(f"Error processing {site}, depth {temp_depth}: {str(e)}")
                continue
        
        # Add batch predictions to all predictions
        all_predictions.extend(batch_predictions)
        
        # Save batch predictions
        if output_dir and batch_predictions:
            batch_df = pd.DataFrame(batch_predictions)
            batch_df.to_csv(os.path.join(output_dir, f'predictions_batch_{batch_start}_{batch_end}.csv'), index=False)
        
        # Clear batch to free memory
        del batch_predictions
        gc.collect()
        
        print(f"Memory after batch: {memory_usage():.1f} MB")
    
    # Consolidate all predictions
    print(f"Generated {len(all_predictions)} raw predictions")
    
    # Convert to DataFrame
    predictions_df = pd.DataFrame(all_predictions)
    
    # Save all predictions
    if output_dir and len(predictions_df) > 0:
        predictions_df.to_csv(os.path.join(output_dir, 'all_predictions.csv'), index=False)
    
    # Consolidate overlapping events to get final events
    if len(predictions_df) > 0:
        print("Consolidating overlapping events...")
        consolidated_events = consolidate_overlapping_events(predictions_df)
        print(f"Consolidated into {len(consolidated_events)} events")
        
        # Save consolidated events
        if output_dir:
            consolidated_events.to_csv(os.path.join(output_dir, 'consolidated_events.csv'), index=False)
        
        print(f"Memory after application: {memory_usage():.1f} MB")
        return consolidated_events
    else:
        print("No predictions generated")
        print(f"Memory after application: {memory_usage():.1f} MB")
        return pd.DataFrame()

def consolidate_overlapping_events(predictions_df, probability_threshold=0.5, gap_threshold=6):
    """
    Consolidate overlapping zero curtain events from model predictions.
    
    Parameters:
    -----------
    predictions_df : pandas.DataFrame
        DataFrame with model predictions
    probability_threshold : float
        Minimum probability to consider as zero curtain
    gap_threshold : float
        Maximum gap in hours to consider events as continuous
        
    Returns:
    --------
    pandas.DataFrame
        DataFrame with consolidated zero curtain events
    """
    import pandas as pd
    import numpy as np
    from tqdm.auto import tqdm
    
    print(f"Consolidating {len(predictions_df)} predictions...")
    
    # Filter to likely zero curtain events
    zero_curtain_events = predictions_df[predictions_df['zero_curtain_probability'] >= probability_threshold].copy()
    
    # Ensure datetime columns are datetime type
    if not pd.api.types.is_datetime64_dtype(zero_curtain_events['datetime_min']):
        zero_curtain_events['datetime_min'] = pd.to_datetime(zero_curtain_events['datetime_min'])
    if not pd.api.types.is_datetime64_dtype(zero_curtain_events['datetime_max']):
        zero_curtain_events['datetime_max'] = pd.to_datetime(zero_curtain_events['datetime_max'])
    
    # Process each site and depth separately
    consolidated_events = []
    
    # Get unique site-depth combinations
    site_depths = zero_curtain_events[['source', 'soil_temp_depth']].drop_duplicates()
    
    # Process each site-depth
    for _, row in tqdm(site_depths.iterrows(), total=len(site_depths), desc="Consolidating events"):
        site = row['source']
        depth = row['soil_temp_depth']
        
        # Get events for this site-depth
        group = zero_curtain_events[
            (zero_curtain_events['source'] == site) & 
            (zero_curtain_events['soil_temp_depth'] == depth)
        ].sort_values('datetime_min')
        
        current_event = None
        
        for _, event in group.iterrows():
            if current_event is None:
                # Start a new event
                current_event = {
                    'source': site,
                    'soil_temp_depth': depth,
                    'datetime_min': event['datetime_min'],
                    'datetime_max': event['datetime_max'],
                    'zero_curtain_probability': [event['zero_curtain_probability']],
                    'latitude': event['latitude'],
                    'longitude': event['longitude']
                }
            else:
                # Check if this event overlaps or is close to the current event
                time_gap = (event['datetime_min'] - current_event['datetime_max']).total_seconds() / 3600
                
                if time_gap <= gap_threshold:
                    # Extend the current event
                    current_event['datetime_max'] = max(current_event['datetime_max'], event['datetime_max'])
                    current_event['zero_curtain_probability'].append(event['zero_curtain_probability'])
                else:
                    # Finalize the current event
                    duration_hours = (current_event['datetime_max'] - current_event['datetime_min']).total_seconds() / 3600
                    
                    if duration_hours >= 12:  # Minimum duration threshold
                        final_event = {
                            'source': current_event['source'],
                            'soil_temp_depth': current_event['soil_temp_depth'],
                            'datetime_min': current_event['datetime_min'],
                            'datetime_max': current_event['datetime_max'],
                            'duration_hours': duration_hours,
                            'zero_curtain_probability': np.mean(current_event['zero_curtain_probability']),
                            'latitude': current_event['latitude'],
                            'longitude': current_event['longitude']
                        }
                        consolidated_events.append(final_event)
                    
                    # Start a new event
                    current_event = {
                        'source': site,
                        'soil_temp_depth': depth,
                        'datetime_min': event['datetime_min'],
                        'datetime_max': event['datetime_max'],
                        'zero_curtain_probability': [event['zero_curtain_probability']],
                        'latitude': event['latitude'],
                        'longitude': event['longitude']
                    }
        
        # Handle the last event
        if current_event is not None:
            duration_hours = (current_event['datetime_max'] - current_event['datetime_min']).total_seconds() / 3600
            
            if duration_hours >= 12:  # Minimum duration threshold
                final_event = {
                    'source': current_event['source'],
                    'soil_temp_depth': current_event['soil_temp_depth'],
                    'datetime_min': current_event['datetime_min'],
                    'datetime_max': current_event['datetime_max'],
                    'duration_hours': duration_hours,
                    'zero_curtain_probability': np.mean(current_event['zero_curtain_probability']),
                    'latitude': current_event['latitude'],
                    'longitude': current_event['longitude']
                }
                consolidated_events.append(final_event)
    
    # Convert to DataFrame
    consolidated_df = pd.DataFrame(consolidated_events)
    
    # Add region and latitude band classifications if latitude is available
    if len(consolidated_df) > 0 and 'latitude' in consolidated_df.columns:
        # Add region classification
        def assign_region(lat):
            if lat is None or pd.isna(lat):
                return None
            elif lat >= 66.5:
                return 'Arctic'
            elif lat >= 60:
                return 'Subarctic'
            elif lat >= 50:
                return 'Northern Boreal'
            else:
                return 'Other'
        
        consolidated_df['region'] = consolidated_df['latitude'].apply(assign_region)
        
        # Add latitude band
        def assign_lat_band(lat):
            if lat is None or pd.isna(lat):
                return None
            elif lat < 55:
                return '<55°N'
            elif lat < 60:
                return '55-60°N'
            elif lat < 66.5:
                return '60-66.5°N'
            elif lat < 70:
                return '66.5-70°N'
            elif lat < 75:
                return '70-75°N'
            elif lat < 80:
                return '75-80°N'
            else:
                return '>80°N'
        
        consolidated_df['lat_band'] = consolidated_df['latitude'].apply(assign_lat_band)
    
    return consolidated_df

def visualize_events_efficiently(events_df, output_file=None):
    """
    Create visualizations for zero curtain events with memory efficiency.
    
    Parameters:
    -----------
    events_df : pandas.DataFrame
        DataFrame containing zero curtain events
    output_file : str, optional
        Path to save the visualization
        
    Returns:
    --------
    dict
        Statistics about the visualized events
    """
    import matplotlib.pyplot as plt
    import cartopy.crs as ccrs
    import cartopy.feature as cfeature
    import numpy as np
    from matplotlib.colors import PowerNorm
    import gc
    
    print(f"Creating visualization for {len(events_df)} events...")
    print(f"Memory before visualization: {memory_usage():.1f} MB")
    
    # Calculate percentile boundaries for better scaling
    p10 = np.percentile(events_df['duration_hours'], 10)
    p25 = np.percentile(events_df['duration_hours'], 25)
    p50 = np.percentile(events_df['duration_hours'], 50)  # median
    p75 = np.percentile(events_df['duration_hours'], 75)
    p90 = np.percentile(events_df['duration_hours'], 90)
    
    # Aggregate by site to reduce memory usage and plotting overhead
    site_data = events_df.groupby(['source', 'latitude', 'longitude']).agg({
        'duration_hours': ['count', 'mean', 'median', 'min', 'max']
    }).reset_index()
    
    # Flatten column names
    site_data.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col 
                        for col in site_data.columns]
    
    # Create figure
    fig, axes = plt.subplots(1, 2, figsize=(14, 7), 
                           subplot_kw={'projection': ccrs.NorthPolarStereo()})
    
    # Set map features
    for ax in axes:
        ax.set_extent([-180, 180, 45, 90], ccrs.PlateCarree())
        ax.add_feature(cfeature.LAND, facecolor='whitesmoke')
        ax.add_feature(cfeature.OCEAN, facecolor='aliceblue')
        ax.add_feature(cfeature.COASTLINE, linewidth=0.5)
        
        # Add Arctic Circle with label
        ax.plot(
            np.linspace(-180, 180, 60),
            np.ones(60) * 66.5,
            transform=ccrs.PlateCarree(),
            linestyle='-',
            color='gray',
            linewidth=1.0,
            alpha=0.7
        )
        
        ax.text(
            0, 66.5 + 2,
            "Arctic Circle",
            transform=ccrs.PlateCarree(),
            horizontalalignment='center',
            verticalalignment='bottom',
            fontsize=9,
            bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.2')
        )
    
    # Plot 1: Event count
    count_max = site_data['duration_hours_count'].quantile(0.95)
    scatter1 = axes[0].scatter(
        site_data['longitude'],
        site_data['latitude'],
        transform=ccrs.PlateCarree(),
        c=site_data['duration_hours_count'],
        s=30,
        cmap='viridis',
        vmin=1,
        vmax=count_max,
        alpha=0.8,
        edgecolor='none'
    )
    plt.colorbar(scatter1, ax=axes[0], shrink=0.7, pad=0.05, label='Event Count')
    axes[0].set_title('Zero Curtain Event Count', fontsize=12)
    
    # Plot 2: Mean duration using percentile bounds
    lower_bound = p10
    upper_bound = p90
    
    # Non-linear scaling for better color differentiation
    scatter2 = axes[1].scatter(
        site_data['longitude'],
        site_data['latitude'],
        transform=ccrs.PlateCarree(),
        c=site_data['duration_hours_mean'],
        s=30,
        cmap='RdYlBu_r',
        norm=PowerNorm(gamma=0.7, vmin=lower_bound, vmax=upper_bound),
        alpha=0.8,
        edgecolor='none'
    )
    
    # Create better colorbar with percentile markers
    cbar = plt.colorbar(scatter2, ax=axes[1], shrink=0.7, pad=0.05, 
                       label='Mean Duration (hours)')
    
    # Show percentile ticks
    percentile_ticks = [p10, p25, p50, p75, p90]
    cbar.set_ticks(percentile_ticks)
    cbar.set_ticklabels([f"{h:.0f}h\n({h/24:.1f}d)" for h in percentile_ticks])
    
    axes[1].set_title('Mean Zero Curtain Duration', fontsize=12)
    
    # Add comprehensive title with statistics
    plt.suptitle(
        f'Zero Curtain Analysis: {len(site_data)} Sites, {len(events_df)} Events\n' +
        f'Duration: median={p50:.1f}h ({p50/24:.1f}d), 10-90%={p10:.1f}-{p90:.1f}h',
        fontsize=14
    )
    
    plt.tight_layout(rect=[0, 0, 1, 0.93])
    
    # Save if requested
    if output_file:
        plt.savefig(output_file, dpi=200, bbox_inches='tight')
        print(f"Visualization saved to {output_file}")
    
    # Clean up to free memory
    plt.close(fig)
    del site_data, fig, axes
    gc.collect()
    
    print(f"Memory after visualization: {memory_usage():.1f} MB")
    
    # Return statistics
    return {
        'p10': p10,
        'p25': p25,
        'p50': p50,
        'p75': p75,
        'p90': p90,
        'mean': events_df['duration_hours'].mean(),
        'std': events_df['duration_hours'].std(),
        'min': events_df['duration_hours'].min(),
        'max': events_df['duration_hours'].max()
    }

def compare_detection_methods_efficiently(physical_events_file, model_events_file, output_dir=None):
    """
    Compare zero curtain events detected by different methods with memory efficiency.
    
    Parameters:
    -----------
    physical_events_file : str
        Path to CSV file with events detected by the physics-based method
    model_events_file : str
        Path to CSV file with events detected by the deep learning model
    output_dir : str, optional
        Directory to save comparison results
        
    Returns:
    --------
    dict
        Comparison statistics and metrics
    """
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    from datetime import timedelta
    import os
    import gc
    
    print("Comparing detection methods...")
    print(f"Memory before comparison: {memory_usage():.1f} MB")
    
    # Load events
    physical_events = pd.read_csv(physical_events_file, parse_dates=['datetime_min', 'datetime_max'])
    model_events = pd.read_csv(model_events_file, parse_dates=['datetime_min', 'datetime_max'])
    
    # Calculate basic statistics for each method
    physical_stats = {
        'total_events': len(physical_events),
        'unique_sites': physical_events['source'].nunique(),
        'median_duration': physical_events['duration_hours'].median(),
        'mean_duration': physical_events['duration_hours'].mean()
    }
    
    model_stats = {
        'total_events': len(model_events),
        'unique_sites': model_events['source'].nunique(),
        'median_duration': model_events['duration_hours'].median(),
        'mean_duration': model_events['duration_hours'].mean()
    }
    
    # Create a site-day matching table for overlap analysis
    # Process in batches to save memory
    physical_days = set()
    model_days = set()
    
    # Process physical events in batches
    batch_size = 1000
    for i in range(0, len(physical_events), batch_size):
        batch = physical_events.iloc[i:i+batch_size]
        
        for _, event in batch.iterrows():
            site = event['source']
            depth = event['soil_temp_depth']
            start_day = event['datetime_min'].date()
            end_day = event['datetime_max'].date()
            
            # Add each day of the event
            current_day = start_day
            while current_day <= end_day:
                physical_days.add((site, depth, current_day))
                current_day += timedelta(days=1)
        
        # Clear batch to free memory
        del batch
        gc.collect()
    
    # Process model events in batches
    for i in range(0, len(model_events), batch_size):
        batch = model_events.iloc[i:i+batch_size]
        
        for _, event in batch.iterrows():
            site = event['source']
            depth = event['soil_temp_depth']
            start_day = event['datetime_min'].date()
            end_day = event['datetime_max'].date()
            
            # Add each day of the event
            current_day = start_day
            while current_day <= end_day:
                model_days.add((site, depth, current_day))
                current_day += timedelta(days=1)
        
        # Clear batch to free memory
        del batch
        gc.collect()
    
    # Calculate overlap metrics
    overlap_days = physical_days.intersection(model_days)
    
    overlap_metrics = {
        'physical_only_days': len(physical_days - model_days),
        'model_only_days': len(model_days - physical_days),
        'overlap_days': len(overlap_days),
        'jaccard_index': len(overlap_days) / len(physical_days.union(model_days)) if len(physical_days.union(model_days)) > 0 else 0
    }
    
    # Print comparison results
    print("\n=== DETECTION METHOD COMPARISON ===\n")
    
    print("Physics-based Detection:")
    print(f"  Total Events: {physical_stats['total_events']}")
    print(f"  Unique Sites: {physical_stats['unique_sites']}")
    print(f"  Median Duration: {physical_stats['median_duration']:.1f} hours ({physical_stats['median_duration']/24:.1f} days)")
    print(f"  Mean Duration: {physical_stats['mean_duration']:.1f} hours ({physical_stats['mean_duration']/24:.1f} days)")
    
    print("\nDeep Learning Model Detection:")
    print(f"  Total Events: {model_stats['total_events']}")
    print(f"  Unique Sites: {model_stats['unique_sites']}")
    print(f"  Median Duration: {model_stats['median_duration']:.1f} hours ({model_stats['median_duration']/24:.1f} days)")
    print(f"  Mean Duration: {model_stats['mean_duration']:.1f} hours ({model_stats['mean_duration']/24:.1f} days)")
    
    print("\nOverlap Analysis:")
    print(f"  Days with Events (Physics-based): {len(physical_days)}")
    print(f"  Days with Events (Deep Learning): {len(model_days)}")
    print(f"  Days detected by both methods: {overlap_metrics['overlap_days']}")
    print(f"  Days detected only by Physics-based: {overlap_metrics['physical_only_days']}")
    print(f"  Days detected only by Deep Learning: {overlap_metrics['model_only_days']}")
    print(f"  Jaccard Index (overlap): {overlap_metrics['jaccard_index']:.4f}")
    
    # Generate comparison visualizations
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        
        # Create a Venn diagram of detection overlap
        try:
            from matplotlib_venn import venn2
            
            plt.figure(figsize=(8, 6))
            venn2(subsets=(len(physical_days - model_days), 
                          len(model_days - physical_days), 
                          len(overlap_days)),
                 set_labels=('Physics-based', 'Deep Learning'))
            plt.title('Overlap between Detection Methods', fontsize=14)
            plt.savefig(os.path.join(output_dir, 'detection_overlap.png'), dpi=200, bbox_inches='tight')
            plt.close()
        except ImportError:
            print("matplotlib_venn not installed. Skipping Venn diagram.")
        
        # Compare duration distributions
        plt.figure(figsize=(10, 6))
        
        sns.histplot(physical_events['duration_hours'], kde=True, alpha=0.5, 
                    label='Physics-based', color='blue', bins=50)
        sns.histplot(model_events['duration_hours'], kde=True, alpha=0.5, 
                    label='Deep Learning', color='red', bins=50)
        
        plt.xlabel('Duration (hours)')
        plt.ylabel('Frequency')
        plt.title('Comparison of Zero Curtain Duration Distributions')
        plt.legend()
        plt.grid(alpha=0.3)
        
        plt.savefig(os.path.join(output_dir, 'duration_comparison.png'), dpi=200, bbox_inches='tight')
        plt.close()
    
    # Clean up to free memory
    del physical_events, model_events, physical_days, model_days, overlap_days
    gc.collect()
    
    print(f"Memory after comparison: {memory_usage():.1f} MB")
    
    comparison_results = {
        'physical_stats': physical_stats,
        'model_stats': model_stats,
        'overlap_metrics': overlap_metrics
    }
    
    return comparison_results

def run_complete_pipeline(feather_path, output_base_dir='results'):
    """
    Run the complete zero curtain analysis pipeline with memory efficiency.
    
    Parameters:
    -----------
    feather_path : str
        Path to the feather file
    output_base_dir : str
        Base directory for outputs
        
    Returns:
    --------
    dict
        Summary of results
    """
    import os
    import time
    import gc
    import pickle
    from tqdm.auto import tqdm
    
    # Create output directories
    os.makedirs(output_base_dir, exist_ok=True)
    checkpoint_dir = os.path.join(output_base_dir, 'checkpoints')
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # Function to save/load checkpoint
    def save_checkpoint(data, name):
        with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'wb') as f:
            pickle.dump(data, f)
    
    def load_checkpoint(name):
        try:
            with open(os.path.join(checkpoint_dir, f'{name}.pkl'), 'rb') as f:
                return pickle.load(f)
        except:
            return None
    
    # Initialize results
    results = load_checkpoint('pipeline_results') or {}
    
    # Check for completed stages
    completed_stages = set(results.get('completed_stages', []))
    print(f"Found {len(completed_stages)} completed stages: {completed_stages}")
    
    # Define stages
    stages = ['Zero Curtain Detection', 'Data Preparation', 'Model Training', 
              'Model Application', 'Visualization', 'Comparison']
    
    # Overall progress bar
    with tqdm(total=len(stages), desc="Overall Progress", initial=len(completed_stages)) as pbar:
        # Stage 1: Zero Curtain Detection
        if 'Zero Curtain Detection' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 1/{len(stages)}: {stages[0]}")
            
            #from zero_curtain_pipeline import run_memory_efficient_pipeline
            
            enhanced_events = run_memory_efficient_pipeline(
                feather_path=feather_path,
                output_dir=os.path.join(output_base_dir, 'enhanced'),
                site_batch_size=20,
                checkpoint_interval=5,
                max_gap_hours=6,
                interpolation_method='cubic'
            )
            
            results['enhanced_events_count'] = len(enhanced_events)
            results['enhanced_time'] = time.time() - start_time
            
            # Save checkpoint
            completed_stages.add('Zero Curtain Detection')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            # Free memory
            del enhanced_events
            gc.collect()
            
            print(f"Memory after stage 1: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 2: Data Preparation
        if 'Data Preparation' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 2/{len(stages)}: {stages[1]}")
            
            # Load events
            enhanced_events_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events.csv')
            if os.path.exists(enhanced_events_path):
                import pandas as pd
                enhanced_events = pd.read_csv(enhanced_events_path, parse_dates=['datetime_min', 'datetime_max'])
                
                # Prepare data for model
                X, y, metadata = prepare_data_for_deep_learning_efficiently(
                    feather_path=feather_path,
                    events_df=enhanced_events,
                    sequence_length=24,
                    output_dir=os.path.join(output_base_dir, 'ml_data'),
                    batch_size=20
                )
                
                results['data_prep_time'] = time.time() - start_time
                results['data_shape'] = X.shape
                results['positive_examples'] = int(sum(y))
                results['positive_percentage'] = float(sum(y)/len(y)*100)
                
                # Clean up
                del X, y, metadata, enhanced_events
                gc.collect()
            else:
                print("No enhanced events file found, cannot proceed with data preparation")
                results['data_prep_error'] = "No enhanced events file found"
            
            # Save checkpoint
            completed_stages.add('Data Preparation')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            print(f"Memory after stage 2: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 3: Model Training
        if 'Model Training' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 3/{len(stages)}: {stages[2]}")
            
            ml_data_dir = os.path.join(output_base_dir, 'ml_data')
            x_path = os.path.join(ml_data_dir, 'X_features.npy')
            y_path = os.path.join(ml_data_dir, 'y_labels.npy')
            
            if os.path.exists(x_path) and os.path.exists(y_path):
                import numpy as np
                X = np.load(x_path)
                y = np.load(y_path)
                
                # Train model
                model, history, evaluation = train_zero_curtain_model_efficiently(
                    X=X,
                    y=y,
                    output_dir=os.path.join(output_base_dir, 'model')
                )
                
                results['model_training_time'] = time.time() - start_time
                results['model_evaluation'] = evaluation
                
                # Clean up
                del X, y, model, history, evaluation
                gc.collect()
            else:
                print("No prepared data found, cannot proceed with model training")
                results['model_training_error'] = "No prepared data found"
            
            # Save checkpoint
            completed_stages.add('Model Training')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            print(f"Memory after stage 3: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 4: Model Application
        if 'Model Application' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 4/{len(stages)}: {stages[3]}")
            
            model_path = os.path.join(output_base_dir, 'model', 'checkpoint.h5')
            
            if os.path.exists(model_path):
                import tensorflow as tf
                model = tf.keras.models.load_model(model_path)
                
                # Apply model
                predictions = apply_model_to_new_data_efficiently(
                    model=model,
                    feather_path=feather_path,
                    sequence_length=24,
                    output_dir=os.path.join(output_base_dir, 'predictions'),
                    batch_size=20
                )
                
                results['model_application_time'] = time.time() - start_time
                results['predictions_count'] = len(predictions)
                
                # Clean up
                del model, predictions
                gc.collect()
            else:
                print("No model checkpoint found, cannot proceed with model application")
                results['model_application_error'] = "No model checkpoint found"
            
            # Save checkpoint
            completed_stages.add('Model Application')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            print(f"Memory after stage 4: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 5: Visualization
        if 'Visualization' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 5/{len(stages)}: {stages[4]}")
            
            import pandas as pd
            
            # Visualize enhanced events
            enhanced_events_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events.csv')
            if os.path.exists(enhanced_events_path):
                enhanced_events = pd.read_csv(enhanced_events_path, parse_dates=['datetime_min', 'datetime_max'])
                
                enhanced_stats = visualize_events_efficiently(
                    events_df=enhanced_events,
                    output_file=os.path.join(output_base_dir, 'enhanced_visualization.png')
                )
                
                results['enhanced_visualization_stats'] = enhanced_stats
                
                # Clean up
                del enhanced_events, enhanced_stats
                gc.collect()
            
            # Visualize model predictions
            predictions_path = os.path.join(output_base_dir, 'predictions', 'consolidated_events.csv')
            if os.path.exists(predictions_path):
                model_events = pd.read_csv(predictions_path, parse_dates=['datetime_min', 'datetime_max'])
                
                model_stats = visualize_events_efficiently(
                    events_df=model_events,
                    output_file=os.path.join(output_base_dir, 'model_visualization.png')
                )
                
                results['model_visualization_stats'] = model_stats
                
                # Clean up
                del model_events, model_stats
                gc.collect()
            
            results['visualization_time'] = time.time() - start_time
            
            # Save checkpoint
            completed_stages.add('Visualization')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            print(f"Memory after stage 5: {memory_usage():.1f} MB")
            pbar.update(1)
        
        # Stage 6: Comparison
        if 'Comparison' not in completed_stages:
            start_time = time.time()
            pbar.set_description(f"Stage 6/{len(stages)}: {stages[5]}")
            
            enhanced_events_path = os.path.join(output_base_dir, 'enhanced', 'zero_curtain_events.csv')
            predictions_path = os.path.join(output_base_dir, 'predictions', 'consolidated_events.csv')
            
            if os.path.exists(enhanced_events_path) and os.path.exists(predictions_path):
                comparison_results = compare_detection_methods_efficiently(
                    physical_events_file=enhanced_events_path,
                    model_events_file=predictions_path,
                    output_dir=os.path.join(output_base_dir, 'comparison')
                )
                
                results['comparison'] = comparison_results
            else:
                print("Missing events files, cannot perform comparison")
                results['comparison_error'] = "Missing events files"
            
            results['comparison_time'] = time.time() - start_time
            
            # Save checkpoint
            completed_stages.add('Comparison')
            results['completed_stages'] = list(completed_stages)
            save_checkpoint(results, 'pipeline_results')
            
            print(f"Memory after stage 6: {memory_usage():.1f} MB")
            pbar.update(1)
    
    # Generate summary report
    total_time = (
        results.get('enhanced_time', 0) +
        results.get('data_prep_time', 0) +
        results.get('model_training_time', 0) +
        results.get('model_application_time', 0) +
        results.get('visualization_time', 0) +
        results.get('comparison_time', 0)
    )
    
    # Save summary to file
    with open(os.path.join(output_base_dir, 'summary.txt'), 'w') as f:
        f.write("ZERO CURTAIN ANALYSIS SUMMARY\n")
        f.write("=" * 30 + "\n\n")
        
        f.write(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\n\n")
        
        f.write("STAGE TIMINGS:\n")
        f.write(f"  Zero Curtain Detection: {results.get('enhanced_time', 0):.2f} seconds\n")
        f.write(f"  Data Preparation: {results.get('data_prep_time', 0):.2f} seconds\n")
        f.write(f"  Model Training: {results.get('model_training_time', 0):.2f} seconds\n")
        f.write(f"  Model Application: {results.get('model_application_time', 0):.2f} seconds\n")
        f.write(f"  Visualization: {results.get('visualization_time', 0):.2f} seconds\n")
        f.write(f"  Comparison: {results.get('comparison_time', 0):.2f} seconds\n\n")
        
        f.write("RESULTS SUMMARY:\n")
        f.write(f"  Enhanced Detection: {results.get('enhanced_events_count', 0)} events\n")
        f.write(f"  Model Predictions: {results.get('predictions_count', 0)} events\n")
        
        if 'comparison' in results and 'overlap_metrics' in results['comparison']:
            overlap = results['comparison']['overlap_metrics']['jaccard_index']
            f.write(f"  Method Agreement: {overlap*100:.1f}% overlap\n")
    
    print("\n" + "=" * 80)
    print("ZERO CURTAIN ANALYSIS COMPLETE")
    print("=" * 80)
    print(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
    print(f"Results saved to {output_base_dir}")
    
    return results

# Add at the beginning of your code
def configure_tensorflow_memory():
    """Configure TensorFlow to use memory growth and limit GPU memory allocation"""
    physical_devices = tf.config.list_physical_devices('GPU')
    if physical_devices:
        for device in physical_devices:
            try:
                # Allow memory growth - prevents TF from allocating all GPU memory at once
                tf.config.experimental.set_memory_growth(device, True)
                
                # Optional: Set memory limit (e.g., 4GB)
                # tf.config.experimental.set_virtual_device_configuration(
                #     device,
                #     [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]
                # )
                print(f"Memory growth enabled for {device}")
            except Exception as e:
                print(f"Error configuring GPU: {e}")
    
    # Limit CPU threads
    tf.config.threading.set_intra_op_parallelism_threads(4)
    tf.config.threading.set_inter_op_parallelism_threads(2)
    
    # Set soft device placement
    tf.config.set_soft_device_placement(True)

# Load split indices
with open("zero_curtain_pipeline/modeling/checkpoints/spatiotemporal_split.pkl", "rb") as f:
    split_data = pickle.load(f)
train_indices = split_data["train_indices"]
val_indices = split_data["val_indices"]
test_indices = split_data["test_indices"]

# Load spatial weights
with open("zero_curtain_pipeline/modeling/checkpoints/spatial_density.pkl", "rb") as f:
    weights_data = pickle.load(f)

data_dir = os.path.join('/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling', 'ml_data')

# Initialize generators
X_file = os.path.join(data_dir, 'X_features.npy')
y_file = os.path.join(data_dir, 'y_labels.npy')

# Get sample weights for training set
sample_weights = weights_data["weights"][train_indices]
sample_weights = sample_weights / np.mean(sample_weights) * len(sample_weights)

# Load data and metadata; use memory mapping to reduce memory usage
print("Loading data...")
X = np.load(X_file, mmap_mode='r')
y = np.load(y_file, mmap_mode='r')
print("Loading metadata...")
with open(os.path.join(data_dir, 'metadata.pkl'), 'rb') as f:
    metadata = pickle.load(f)
print("Done.")

train_y = y[train_indices]
val_y = y[val_indices]
test_y = y[test_indices]

print(f"Train/val/test sizes: {len(train_indices)}/{len(val_indices)}/{len(test_indices)}")
print(f"Positive examples: Train={np.sum(train_y)} ({np.sum(train_y)/len(train_y)*100:.1f}%), " +
      f"Val={np.sum(val_y)} ({np.sum(val_y)/len(val_y)*100:.1f}%), " +
      f"Test={np.sum(test_y)} ({np.sum(test_y)/len(test_y)*100:.1f}%)")

# Combine sample weights with class weights for imbalanced data
pos_weight = (len(train_y) - np.sum(train_y)) / max(1, np.sum(train_y))
class_weight = {0: 1.0, 1: pos_weight}
print(f"Using class weight {pos_weight:.2f} for positive examples")
# Free memory
del train_y, val_y, test_y
gc.collect()



def build_advanced_zero_curtain_model(input_shape, include_moisture=True):
    """
    Build the advanced zero curtain detection model.
    """
    # This is your existing model building function
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, Dropout
    from tensorflow.keras.layers import Reshape, Concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D
    from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
    from tensorflow.keras.optimizers import Adam
    
    # Input layer
    inputs = Input(shape=input_shape)
    
    # Reshape for ConvLSTM (add spatial dimension)
    # From (sequence_length, features) to (sequence_length, 1, features)
    x = tf.keras.layers.Reshape((input_shape[0], 1, 1, input_shape[1]))(inputs)
    
    # ConvLSTM layer to capture spatiotemporal patterns
    from tensorflow.keras.layers import ConvLSTM2D
    convlstm = ConvLSTM2D(
        filters=64,
        kernel_size=(3, 1),
        padding='same',
        return_sequences=True,
        activation='tanh',
        recurrent_dropout=0.2
    )(x)
    
    # Reshape back to (sequence_length, features)
    convlstm = Reshape((input_shape[0], 64))(convlstm)
    
    # Add positional encoding for transformer
    def positional_encoding(length, depth):
        """Create positional encoding with correct dimensions"""
        positions = tf.range(length, dtype=tf.float32)[:, tf.newaxis]
        depths = tf.range(depth, dtype=tf.float32)[tf.newaxis, :] / depth
        
        angle_rates = 1 / tf.pow(10000.0, depths)
        angle_rads = positions * angle_rates
        
        # This creates a tensor of shape (length, depth)
        # Only use sin to ensure output depth matches input depth
        pos_encoding = tf.sin(angle_rads)
        
        # Add batch dimension to match convlstm output format
        # Result shape will be (1, length, depth)
        pos_encoding = tf.expand_dims(pos_encoding, 0)
        
        return pos_encoding
    
    # Add positional encoding
    pos_encoding = positional_encoding(input_shape[0], 64)
    transformer_input = convlstm + pos_encoding
    
    # Transformer encoder block
    def transformer_encoder(x, num_heads=8, key_dim=64, ff_dim=128):
        # Multi-head attention
        attention_output = MultiHeadAttention(
            num_heads=num_heads, key_dim=key_dim
        )(x, x)
        
        # Skip connection 1
        x1 = Add()([attention_output, x])
        x1 = LayerNormalization(epsilon=1e-6)(x1)
        
        # Feed-forward network
        ff_output = Dense(ff_dim, activation='relu')(x1)
        ff_output = Dropout(0.1)(ff_output)
        ff_output = Dense(64)(ff_output)
        
        # Skip connection 2
        x2 = Add()([ff_output, x1])
        return LayerNormalization(epsilon=1e-6)(x2)
    
    # Apply transformer encoder
    transformer_output = transformer_encoder(transformer_input)
    
    # Parallel CNN paths for multi-scale feature extraction
    cnn_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(inputs)
    cnn_1 = BatchNormalization()(cnn_1)
    
    cnn_2 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(inputs)
    cnn_2 = BatchNormalization()(cnn_2)
    
    cnn_3 = Conv1D(filters=32, kernel_size=7, padding='same', activation='relu')(inputs)
    cnn_3 = BatchNormalization()(cnn_3)
    
    # Variational Autoencoder components
    def sampling(args):
        z_mean, z_log_var = args
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
    # Global temporal features
    global_max = GlobalMaxPooling1D()(transformer_output)
    global_avg = GlobalAveragePooling1D()(transformer_output)
    
    # VAE encoding
    z_mean = Dense(32)(Concatenate()([global_max, global_avg]))
    z_log_var = Dense(32)(Concatenate()([global_max, global_avg]))
    z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])
    
    # Combine all features
    merged_features = Concatenate()(
        [
            GlobalMaxPooling1D()(cnn_1),
            GlobalMaxPooling1D()(cnn_2),
            GlobalMaxPooling1D()(cnn_3),
            global_max,
            global_avg,
            z
        ]
    )
    
    # Final classification layers
    x = Dense(128, activation='relu')(merged_features)
    x = Dropout(0.3)(x)
    x = BatchNormalization()(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.2)(x)
    
    # Output layer
    outputs = Dense(1, activation='sigmoid')(x)
    
    # Create model
    model = Model(inputs=inputs, outputs=outputs)
    
    # Add VAE loss
    kl_loss = -0.5 * tf.reduce_mean(
        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
    )
    model.add_loss(0.001 * kl_loss)  # Small weight for VAE loss
    
    # Compile model with appropriate metrics
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=[
            'accuracy',
            tf.keras.metrics.AUC(name='auc'),
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall')
        ]
    )
    
    return model

#Now try a more sophisticated architecture from before
from tensorflow.keras.optimizers import Adam
input_shape = X[train_indices[0]].shape
model = build_advanced_zero_curtain_model(input_shape)

model.summary()

from tensorflow.keras.utils import plot_model

plot_model(model, to_file='zero_curtain_pipeline/modeling/spatial_model/insitu_model_plot.png', show_shapes=True, show_dtype=True, \
           show_layer_names=True, expand_nested=True, dpi=300, layer_range=None, show_layer_activations=True);#, rankdir='TB')

output_dir = os.path.join('/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling', 'spatial_model')

# Set up callbacks
callbacks = [
    # Stop training when validation performance plateaus
    tf.keras.callbacks.EarlyStopping(
        patience=15,
        restore_best_weights=True,
        monitor='val_auc',
        mode='max'
    ),
    # Reduce learning rate when improvement slows
    tf.keras.callbacks.ReduceLROnPlateau(
        factor=0.5,
        patience=7,
        min_lr=1e-6,
        monitor='val_auc',
        mode='max'
    ),
    # Manual garbage collection after each epoch
    tf.keras.callbacks.LambdaCallback(
        on_epoch_end=lambda epoch, logs: gc.collect()
    )
]

# Add additional callbacks if output directory provided
if output_dir:
    callbacks.extend([
        # Save best model
        tf.keras.callbacks.ModelCheckpoint(
            os.path.join(output_dir, 'model_checkpoint.h5'),
            save_best_only=True,
            monitor='val_auc',
            mode='max'
        ),
        # Log training progress to CSV
        tf.keras.callbacks.CSVLogger(
            os.path.join(output_dir, 'training_log.csv'),
            append=True
        )
    ])

def create_optimized_tf_dataset(X_file, y_file, indices, batch_size=256, shuffle=True, 
                               weights=None, cache=False, prefetch_factor=tf.data.AUTOTUNE):
    """
    Create an optimized TensorFlow dataset with detailed progress reporting.
    """
    import time
    
    # Log start time for loading data
    load_start = time.time()
    print(f"  Loading memory-mapped arrays...")
    
    # Load as memory-mapped arrays
    X = np.load(X_file, mmap_mode='r')
    y = np.load(y_file, mmap_mode='r')
    
    print(f"  Arrays loaded in {time.time() - load_start:.2f} seconds")
    
    # Get input shape from first sample
    sample_start = time.time()
    input_shape = X[indices[0]].shape
    print(f"  Input shape: {input_shape}, obtained in {time.time() - sample_start:.2f} seconds")
    
    # Define generator function with progress reporting
    def generator():
        total = len(indices)
        start_time = time.time()
        last_report = start_time
        
        for i, idx in enumerate(indices):
            # Report progress every 10000 samples or 10 seconds
            current_time = time.time()
            if i % 10000 == 0 or current_time - last_report > 10:
                elapsed = current_time - start_time
                if i > 0:
                    rate = i / elapsed
                    eta = (total - i) / rate if rate > 0 else 0
                    print(f"  Generator progress: {i}/{total} ({i/total*100:.1f}%), "
                          f"Rate: {rate:.1f} samples/sec, ETA: {int(eta)} seconds")
                last_report = current_time
            
            if weights is not None:
                # Find position of idx in original indices array
                pos = np.where(indices == idx)[0][0]
                yield X[idx], y[idx], weights[pos]
            else:
                yield X[idx], y[idx]
    
    # Create dataset from generator
    create_start = time.time()
    print(f"  Creating dataset from generator...")
    
    if weights is not None:
        output_signature = (
            tf.TensorSpec(shape=input_shape, dtype=tf.float32),
            tf.TensorSpec(shape=(), dtype=tf.int32),
            tf.TensorSpec(shape=(), dtype=tf.float32)
        )
    else:
        output_signature = (
            tf.TensorSpec(shape=input_shape, dtype=tf.float32),
            tf.TensorSpec(shape=(), dtype=tf.int32)
        )
    
    dataset = tf.data.Dataset.from_generator(
        generator,
        output_signature=output_signature
    )
    
    print(f"  Dataset created in {time.time() - create_start:.2f} seconds")
    
    # Apply dataset optimizations
    print(f"  Applying dataset optimizations...")
    opt_start = time.time()
    
    if shuffle:
        buffer_size = min(len(indices), 10000)
        print(f"  Shuffling with buffer size {buffer_size}...")
        dataset = dataset.shuffle(buffer_size)
    
    print(f"  Batching with size {batch_size}...")
    dataset = dataset.batch(batch_size)
    
    if cache:
        print(f"  Caching dataset...")
        dataset = dataset.cache()
    
    print(f"  Setting prefetch to {prefetch_factor}...")
    dataset = dataset.prefetch(prefetch_factor)
    
    print(f"  Optimizations applied in {time.time() - opt_start:.2f} seconds")
    
    return dataset

def train_in_chunks_with_tf_datasets(model, X_file, y_file, train_indices, val_indices,
                                     batch_size=256, chunks=20, epochs_per_chunk=5,
                                     callbacks=None, class_weight=None, sample_weights=None):
    """
    Train model sequentially in chunks using TensorFlow datasets with detailed progress logging.
    """
    import time
    from datetime import timedelta
    
    # Log start time
    start_time = time.time()
    print(f"Starting chunked training at {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Total samples: {len(train_indices)}, Chunks: {chunks}, Batch size: {batch_size}")
    
    # Create validation dataset once
    print(f"Creating validation dataset with {len(val_indices)} samples...")
    val_dataset_start = time.time()
    val_dataset = create_optimized_tf_dataset(
        X_file, y_file, val_indices, 
        batch_size=batch_size, shuffle=False
    )
    print(f"Validation dataset created in {time.time() - val_dataset_start:.2f} seconds")
    
    # Split training indices into chunks
    train_chunks = np.array_split(train_indices, chunks)
    
    history_aggregate = None
    
    # Train on each chunk sequentially
    for chunk_idx, chunk_indices in enumerate(train_chunks):
        chunk_start_time = time.time()
        print(f"\n{'='*80}")
        print(f"Training on chunk {chunk_idx+1}/{len(train_chunks)} with {len(chunk_indices)} samples")
        print(f"Memory usage before creating dataset: {memory_usage():.1f} MB")
        
        # Create TF dataset for this chunk only
        print(f"Creating training dataset for chunk {chunk_idx+1}...")
        dataset_start = time.time()
        
        if sample_weights is not None:
            # Get weights for current chunk
            print(f"Extracting sample weights for {len(chunk_indices)} samples...")
            weights_start = time.time()
            chunk_weights = np.array([sample_weights[np.where(train_indices == idx)[0][0]] 
                                     for idx in chunk_indices])
            print(f"Weights extracted in {time.time() - weights_start:.2f} seconds")
        else:
            chunk_weights = None
            
        train_dataset = create_optimized_tf_dataset(
            X_file, y_file, chunk_indices,
            batch_size=batch_size, shuffle=True,
            weights=chunk_weights
        )
        print(f"Training dataset created in {time.time() - dataset_start:.2f} seconds")
        print(f"Memory usage after creating dataset: {memory_usage():.1f} MB")
        
        # Count batches for progress reporting
        steps_per_epoch = len(chunk_indices) // batch_size + (1 if len(chunk_indices) % batch_size > 0 else 0)
        print(f"Steps per epoch: {steps_per_epoch}")
        
        # Create a custom callback to log batch progress
        class BatchProgressCallback(tf.keras.callbacks.Callback):
            def on_train_batch_end(self, batch, logs=None):
                if batch % 10 == 0:  # Log every 10 batches
                    print(f"  Batch {batch}/{steps_per_epoch}, Loss: {logs['loss']:.4f}")
        
        # Add our progress callback
        chunk_callbacks = callbacks.copy() if callbacks else []
        chunk_callbacks.append(BatchProgressCallback())
        
        # Train on this chunk
        print(f"Starting training for {epochs_per_chunk} epochs on chunk {chunk_idx+1}...")
        train_start = time.time()
        
        history = model.fit(
            train_dataset,
            validation_data=val_dataset,
            epochs=epochs_per_chunk,
            callbacks=chunk_callbacks,
            class_weight=class_weight,
            verbose=1
        )
        
        train_time = time.time() - train_start
        print(f"Chunk {chunk_idx+1} training completed in {timedelta(seconds=int(train_time))}")
        print(f"Average time per epoch: {train_time/epochs_per_chunk:.2f} seconds")
        
        # Aggregate history
        if history_aggregate is None:
            history_aggregate = {k: v for k, v in history.history.items()}
        else:
            for k, v in history.history.items():
                history_aggregate[k].extend(v)
        
        # Force garbage collection
        del train_dataset
        if chunk_weights is not None:
            del chunk_weights
        gc.collect()
        
        # Log memory usage after training
        print(f"Memory usage after training: {memory_usage():.1f} MB")
        print(f"Chunk {chunk_idx+1} total time: {timedelta(seconds=int(time.time() - chunk_start_time))}")
        
        # Estimate remaining time
        elapsed_time = time.time() - start_time
        avg_chunk_time = elapsed_time / (chunk_idx + 1)
        remaining_chunks = chunks - (chunk_idx + 1)
        estimated_remaining = avg_chunk_time * remaining_chunks
        print(f"Estimated remaining time: {timedelta(seconds=int(estimated_remaining))}")
        print(f"{'='*80}")
    
    total_time = time.time() - start_time
    print(f"\nTotal training time: {timedelta(seconds=int(total_time))}")
    
    # Create a History object with the aggregated metrics
    agg_history = type('History', (), {'history': history_aggregate})
    
    return agg_history

def efficient_balanced_training(model, X_file, y_file, train_indices, val_indices, test_indices,
                               output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
                               save_frequency=5, class_weight=None):
    """
    More efficient training balancing speed and memory use.
    
    Parameters:
    -----------
    model : tf.keras.Model
        Pre-compiled model
    X_file, y_file : str
        Paths to feature and label files
    train_indices, val_indices, test_indices : array
        Training, validation, and test indices
    output_dir : str
        Directory to save results
    batch_size : int
        Batch size for training
    chunk_size : int
        Number of samples to process at once
    epochs_per_chunk : int
        Epochs to train each chunk
    save_frequency : int
        Save model every N chunks
    class_weight : dict, optional
        Class weights for handling imbalanced data
    """
    import os
    import gc
    import json
    import numpy as np
    import tensorflow as tf
    import matplotlib.pyplot as plt
    from datetime import datetime, timedelta
    import time
    import psutil
    
    # Create output directories
    os.makedirs(output_dir, exist_ok=True)
    predictions_dir = os.path.join(output_dir, "predictions")
    os.makedirs(predictions_dir, exist_ok=True)
    checkpoints_dir = os.path.join(output_dir, "checkpoints")
    os.makedirs(checkpoints_dir, exist_ok=True)
    
    # Process in chunks
    num_chunks = int(np.ceil(len(train_indices) / chunk_size))
    print(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
    
    # Create validation set once (small size)
    val_limit = min(2000, len(val_indices))
    val_indices_subset = val_indices[:val_limit]
    
    # Open data files
    X_mmap = np.load(X_file, mmap_mode='r')
    y_mmap = np.load(y_file, mmap_mode='r')
    
    # Load validation data once
    val_X = np.array([X_mmap[idx] for idx in val_indices_subset])
    val_y = np.array([y_mmap[idx] for idx in val_indices_subset])
    print(f"Loaded {len(val_X)} validation samples")
    
    # Setup callbacks - EarlyStopping and ReduceLROnPlateau
    callbacks = [
        tf.keras.callbacks.EarlyStopping(
            patience=3, 
            restore_best_weights=True,
            monitor='val_loss',
            min_delta=0.01
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            factor=0.5,
            patience=2,
            min_lr=1e-6,
            monitor='val_loss'
        ),
        # Memory cleanup after each epoch
        tf.keras.callbacks.LambdaCallback(
            on_epoch_end=lambda epoch, logs: gc.collect()
        )
    ]
    
    # Track metrics across chunks
    history_log = []
    start_time = time.time()
    
    # Process each chunk
    for chunk_idx in range(num_chunks):
        # Get chunk indices
        start_idx = chunk_idx * chunk_size
        end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
        chunk_indices = train_indices[start_idx:end_idx]
        
        # Report memory
        memory_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
        print(f"\n{'='*50}")
        print(f"Processing chunk {chunk_idx+1}/{num_chunks} with {len(chunk_indices)} samples")
        print(f"Memory before: {memory_before:.1f} MB")
        
        # Force garbage collection before loading new data
        gc.collect()
        
        # Load chunk data
        chunk_X = np.array([X_mmap[idx] for idx in chunk_indices])
        chunk_y = np.array([y_mmap[idx] for idx in chunk_indices])
        
        print(f"Data loaded. Memory: {psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024):.1f} MB")
        
        # Train on chunk
        print(f"Training for {epochs_per_chunk} epochs...")
        history = model.fit(
            chunk_X, chunk_y,
            validation_data=(val_X, val_y),
            epochs=epochs_per_chunk,
            batch_size=batch_size,
            class_weight=class_weight,
            callbacks=callbacks,
            verbose=1
        )
        
        # Store serializable metrics
        chunk_metrics = {}
        for k, v in history.history.items():
            chunk_metrics[k] = [float(val) for val in v]
        history_log.append(chunk_metrics)
        
        # Save model periodically instead of after every chunk
        if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
            model_path = os.path.join(checkpoints_dir, f"model_checkpoint_{chunk_idx+1}.h5")
            model.save(model_path)
            print(f"Model saved to {model_path}")
            
            # Also save history
            try:
                with open(os.path.join(output_dir, "training_history.json"), "w") as f:
                    json.dump(history_log, f)
            except Exception as e:
                print(f"Warning: Could not save history to JSON: {e}")
                # Fallback - save as pickle
                import pickle
                with open(os.path.join(output_dir, "training_history.pkl"), "wb") as f:
                    pickle.dump(history_log, f)
        
        # Generate predictions only for selected chunks to save time
        if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
            chunk_preds = model.predict(chunk_X, batch_size=batch_size)
            np.save(os.path.join(predictions_dir, f"chunk_{chunk_idx+1}_predictions.npy"), chunk_preds)
            np.save(os.path.join(predictions_dir, f"chunk_{chunk_idx+1}_indices.npy"), chunk_indices)
            del chunk_preds
            
        # Explicitly delete everything from memory
        del chunk_X, chunk_y
        
        # Force garbage collection
        gc.collect()
        
        # Report memory after cleanup
        memory_after = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
        print(f"Memory after: {memory_after:.1f} MB (Change: {memory_after - memory_before:.1f} MB)")
        
        # Estimate time
        elapsed = time.time() - start_time
        avg_time_per_chunk = elapsed / (chunk_idx + 1)
        remaining = avg_time_per_chunk * (num_chunks - chunk_idx - 1)
        print(f"Estimated remaining time: {timedelta(seconds=int(remaining))}")
    
    # Save final model
    final_model_path = os.path.join(output_dir, "final_model.h5")
    model.save(final_model_path)
    print(f"Final model saved to {final_model_path}")
    
    # Save all metrics
    try:
        with open(os.path.join(output_dir, "final_training_metrics.json"), "w") as f:
            json.dump(history_log, f)
    except Exception as e:
        print(f"Error saving metrics: {e}")
        # Save as pickle instead
        import pickle
        with open(os.path.join(output_dir, "final_training_metrics.pkl"), "wb") as f:
            pickle.dump(history_log, f)
    
    # Clean up validation data
    del val_X, val_y
    gc.collect()
    
    # Final evaluation on test set
    print("\nPerforming final evaluation on test set...")
    
    # Process test data in batches
    test_batch_size = 5000
    num_test_batches = int(np.ceil(len(test_indices) / test_batch_size))
    
    all_test_predictions = []
    all_test_true = []
    test_metrics = {'loss': 0, 'accuracy': 0, 'samples': 0}
    
    for test_batch_idx in range(num_test_batches):
        start_idx = test_batch_idx * test_batch_size
        end_idx = min((test_batch_idx + 1) * test_batch_size, len(test_indices))
        batch_indices = test_indices[start_idx:end_idx]
        
        # Load batch data
        test_X = np.array([X_mmap[idx] for idx in batch_indices])
        test_y = np.array([y_mmap[idx] for idx in batch_indices])
        
        # Evaluate
        metrics = model.evaluate(test_X, test_y, verbose=1)
        metrics_dict = {name: value for name, value in zip(model.metrics_names, metrics)}
        
        # Weight metrics by batch size
        for key in ['loss', 'accuracy']:
            if key in metrics_dict:
                test_metrics[key] += metrics_dict[key] * len(batch_indices)
        test_metrics['samples'] += len(batch_indices)
        
        # Get predictions
        test_preds = model.predict(test_X, batch_size=batch_size)
        
        # Store
        all_test_predictions.append(test_preds.flatten())
        all_test_true.append(test_y)
        
        # Clean up
        del test_X, test_y, test_preds
        gc.collect()
    
    # Combine results
    all_test_predictions = np.concatenate(all_test_predictions)
    all_test_true = np.concatenate(all_test_true)
    
    # Calculate final test metrics
    test_loss = test_metrics['loss'] / test_metrics['samples']
    test_accuracy = test_metrics['accuracy'] / test_metrics['samples']
    
    # Calculate additional metrics
    from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
    test_preds_binary = (all_test_predictions > 0.5).astype(int)
    report = classification_report(all_test_true, test_preds_binary)
    conf_matrix = confusion_matrix(all_test_true, test_preds_binary)
    fpr, tpr, _ = roc_curve(all_test_true, all_test_predictions)
    roc_auc = auc(fpr, tpr)
    
    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    np.save(os.path.join(output_dir, f'test_predictions_{timestamp}.npy'), all_test_predictions)
    np.save(os.path.join(output_dir, 'test_predictions_latest.npy'), all_test_predictions)
    
    with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w') as f:
        f.write("Classification Report:\n")
        f.write(report)
        f.write("\n\nConfusion Matrix:\n")
        f.write(str(conf_matrix))
        f.write("\n\nTest Metrics:\n")
        f.write(f"loss: {test_loss:.4f}\n")
        f.write(f"accuracy: {test_accuracy:.4f}\n")
        f.write(f"AUC: {roc_auc:.4f}\n")
        
    print(f"Test evaluation complete. Final test AUC: {roc_auc:.4f}")
    
    total_time = time.time() - start_time
    print(f"\nTotal training time: {timedelta(seconds=int(total_time))}")
    
    return model, final_model_path

import time

# Diagnostic test with proper tensor conversion
print("Testing batch processing...")
X = np.load(X_file, mmap_mode='r')
y = np.load(y_file, mmap_mode='r')

# Get a small sample batch and convert to tensors
sample_indices = train_indices[:32]
X_sample = tf.convert_to_tensor(np.array([X[i] for i in sample_indices]), dtype=tf.float32)
y_sample = tf.convert_to_tensor(np.array([y[i] for i in sample_indices]), dtype=tf.float32)

print(f"Sample batch shapes: X={X_sample.shape}, y={y_sample.shape}")
print(f"Testing forward pass...")
start = time.time()
y_pred = model(X_sample)
print(f"Forward pass completed in {time.time() - start:.2f} seconds")

# Reshape y_sample if needed
if len(y_sample.shape) == 1 and len(y_pred.shape) == 2:
    y_sample = tf.reshape(y_sample, (-1, 1))
    print(f"Reshaped y_sample to {y_sample.shape} to match y_pred {y_pred.shape}")

print(f"Testing backward pass...")
start = time.time()
with tf.GradientTape() as tape:
    y_pred = model(X_sample, training=True)
    # Use tf.keras.losses directly instead of model.compiled_loss
    loss_fn = tf.keras.losses.BinaryCrossentropy()
    loss = loss_fn(y_sample, y_pred)
gradients = tape.gradient(loss, model.trainable_variables)
model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))
print(f"Backward pass completed in {time.time() - start:.2f} seconds")

# Configuration
# Configure TensorFlow
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    for device in physical_devices:
        try:
            tf.config.experimental.set_memory_growth(device, True)
            print(f"Memory growth enabled for {device}")
        except Exception as e:
            print(f"Error configuring GPU: {e}")

# Data paths
data_dir = os.path.join('/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling', 'ml_data')
X_file = os.path.join(data_dir, 'X_features.npy')
y_file = os.path.join(data_dir, 'y_labels.npy')

# Load memory-mapped data for shape information
X = np.load(X_file, mmap_mode='r')
y = np.load(y_file, mmap_mode='r')

# Verify indices
print("Verifying indices and class weights...")
train_y = y[train_indices]
val_y = y[val_indices]
test_y = y[test_indices]

print(f"Train/val/test sizes: {len(train_indices)}/{len(val_indices)}/{len(test_indices)}")
print(f"Positive examples: Train={np.sum(train_y)} ({np.sum(train_y)/len(train_y)*100:.1f}%), " +
      f"Val={np.sum(val_y)} ({np.sum(val_y)/len(val_y)*100:.1f}%), " +
      f"Test={np.sum(test_y)} ({np.sum(test_y)/len(test_y)*100:.1f}%)")

# Combine sample weights with class weights for imbalanced data
pos_weight = (len(train_y) - np.sum(train_y)) / max(1, np.sum(train_y))
class_weight = {0: 1.0, 1: pos_weight}
print(f"Using class weight {pos_weight:.2f} for positive examples")

# Free memory
del train_y, val_y, test_y
gc.collect()

# Get input shape for the model
input_shape = X[train_indices[0]].shape
print(f"Input shape: {input_shape}")

# Build model
model = build_advanced_zero_curtain_model(input_shape)

model.summary()

# def resumable_efficient_training(model, X_file, y_file, train_indices, val_indices, test_indices,
#                                output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
#                                save_frequency=5, class_weight=None, start_chunk=360):
# def resumable_efficient_training(model, X_file, y_file, train_indices, val_indices, test_indices,
#                                output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
#                                save_frequency=5, class_weight=None, start_chunk=405):
# def resumable_efficient_training(model, X_file, y_file, train_indices, val_indices, test_indices,
#                                output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
#                                save_frequency=5, class_weight=None, start_chunk=450):
# def resumable_efficient_training(model, X_file, y_file, train_indices, val_indices, test_indices,
#                                output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
#                                save_frequency=5, class_weight=None, start_chunk=495):
def resumable_efficient_training(model, X_file, y_file, train_indices, val_indices, test_indices,
                               output_dir, batch_size=256, chunk_size=25000, epochs_per_chunk=2, 
                               save_frequency=5, class_weight=None, start_chunk=540):
    """
    Training function with built-in resume capability.
    
    Parameters:
    -----------
    model : tf.keras.Model
        Pre-compiled model
    X_file, y_file : str
        Paths to feature and label files
    train_indices, val_indices, test_indices : array
        Training, validation, and test indices
    output_dir : str
        Directory to save results
    batch_size : int
        Batch size for training
    chunk_size : int
        Number of samples to process at once
    epochs_per_chunk : int
        Epochs to train each chunk
    save_frequency : int
        Save model every N chunks
    class_weight : dict, optional
        Class weights for handling imbalanced data
    start_chunk : int, optional
        Chunk index to start/resume from
    """
    import os
    import gc
    import json
    import numpy as np
    import tensorflow as tf
    from datetime import datetime, timedelta
    import time
    import psutil
    
    # Create output directories
    os.makedirs(output_dir, exist_ok=True)
    predictions_dir = os.path.join(output_dir, "predictions")
    os.makedirs(predictions_dir, exist_ok=True)
    checkpoints_dir = os.path.join(output_dir, "checkpoints")
    os.makedirs(checkpoints_dir, exist_ok=True)
    
    # Process in chunks
    num_chunks = int(np.ceil(len(train_indices) / chunk_size))
    print(f"Processing {len(train_indices)} samples in {num_chunks} chunks of {chunk_size}")
    print(f"Starting from chunk {start_chunk+1}")
    
    # Create validation set once
    val_limit = min(2000, len(val_indices))
    val_indices_subset = val_indices[:val_limit]
    
    # Open data files
    X_mmap = np.load(X_file, mmap_mode='r')
    y_mmap = np.load(y_file, mmap_mode='r')
    
    # Load validation data once
    val_X = np.array([X_mmap[idx] for idx in val_indices_subset])
    val_y = np.array([y_mmap[idx] for idx in val_indices_subset])
    print(f"Loaded {len(val_X)} validation samples")
    
    # Load existing history if resuming
    history_log = []
    history_path = os.path.join(output_dir, "training_history.json")
    if start_chunk > 0 and os.path.exists(history_path):
        try:
            with open(history_path, "r") as f:
                history_log = json.load(f)
        except Exception as e:
            print(f"Could not load existing history: {e}")
            # Try pickle format
            pickle_path = os.path.join(output_dir, "training_history.pkl")
            if os.path.exists(pickle_path):
                import pickle
                with open(pickle_path, "rb") as f:
                    history_log = pickle.load(f)
    
    # If resuming, load latest model
    if start_chunk > 0:
        # Find the most recent checkpoint before start_chunk
        checkpoint_indices = []
        for filename in os.listdir(checkpoints_dir):
            if filename.startswith("model_checkpoint_") and filename.endswith(".h5"):
                try:
                    idx = int(filename.split("_")[-1].split(".")[0])
                    if idx < start_chunk:
                        checkpoint_indices.append(idx)
                except ValueError:
                    continue
        
        if checkpoint_indices:
            latest_idx = max(checkpoint_indices)
            model_path = os.path.join(checkpoints_dir, f"model_checkpoint_{latest_idx}.h5")
            if os.path.exists(model_path):
                print(f"Loading model from checkpoint {model_path}")
                model = tf.keras.models.load_model(model_path)
            else:
                print(f"Warning: Could not find model checkpoint for chunk {latest_idx}")
    
    # Setup callbacks
    callbacks = [
        tf.keras.callbacks.EarlyStopping(
            patience=3, 
            restore_best_weights=True,
            monitor='val_loss',
            min_delta=0.01
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            factor=0.5,
            patience=2,
            min_lr=1e-6,
            monitor='val_loss'
        ),
        # Memory cleanup after each epoch
        tf.keras.callbacks.LambdaCallback(
            on_epoch_end=lambda epoch, logs: gc.collect()
        )
    ]
    
    # Track metrics across chunks
    start_time = time.time()
    
    # For safe recovery
    recovery_file = os.path.join(output_dir, "last_completed_chunk.txt")
    
    # Process each chunk
    for chunk_idx in range(start_chunk, num_chunks):
        # Get chunk indices
        start_idx = chunk_idx * chunk_size
        end_idx = min((chunk_idx + 1) * chunk_size, len(train_indices))
        chunk_indices = train_indices[start_idx:end_idx]
        
        # Report memory
        memory_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
        print(f"\n{'='*50}")
        print(f"Processing chunk {chunk_idx+1}/{num_chunks} with {len(chunk_indices)} samples")
        print(f"Memory before: {memory_before:.1f} MB")
        
        # Force garbage collection before loading new data
        gc.collect()
        
        # Load chunk data
        chunk_X = np.array([X_mmap[idx] for idx in chunk_indices])
        chunk_y = np.array([y_mmap[idx] for idx in chunk_indices])
        
        print(f"Data loaded. Memory: {psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024):.1f} MB")
        
        # Train on chunk
        print(f"Training for {epochs_per_chunk} epochs...")
        history = model.fit(
            chunk_X, chunk_y,
            validation_data=(val_X, val_y),
            epochs=epochs_per_chunk,
            batch_size=batch_size,
            class_weight=class_weight,
            callbacks=callbacks,
            verbose=1
        )
        
        # Store serializable metrics
        chunk_metrics = {}
        for k, v in history.history.items():
            chunk_metrics[k] = [float(val) for val in v]
        history_log.append(chunk_metrics)
        
        # Save model periodically instead of after every chunk
        if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
            model_path = os.path.join(checkpoints_dir, f"model_checkpoint_{chunk_idx+1}.h5")
            model.save(model_path)
            print(f"Model saved to {model_path}")
            
            # Also save history
            try:
                with open(history_path, "w") as f:
                    json.dump(history_log, f)
            except Exception as e:
                print(f"Warning: Could not save history to JSON: {e}")
                # Fallback - save as pickle
                import pickle
                with open(os.path.join(output_dir, "training_history.pkl"), "wb") as f:
                    pickle.dump(history_log, f)
        
        # Generate predictions only for selected chunks to save time
        if (chunk_idx + 1) % save_frequency == 0 or chunk_idx == num_chunks - 1:
            chunk_preds = model.predict(chunk_X, batch_size=batch_size)
            np.save(os.path.join(predictions_dir, f"chunk_{chunk_idx+1}_predictions.npy"), chunk_preds)
            np.save(os.path.join(predictions_dir, f"chunk_{chunk_idx+1}_indices.npy"), chunk_indices)
            del chunk_preds
            
        # Explicitly delete everything from memory
        del chunk_X, chunk_y
        
        # Force garbage collection
        gc.collect()
        
        # Report memory after cleanup
        memory_after = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
        print(f"Memory after: {memory_after:.1f} MB (Change: {memory_after - memory_before:.1f} MB)")
        
        # Write recovery file with last completed chunk
        with open(recovery_file, "w") as f:
            f.write(str(chunk_idx + 1))
        
        # Estimate time
        elapsed = time.time() - start_time
        avg_time_per_chunk = elapsed / (chunk_idx - start_chunk + 1)
        remaining = avg_time_per_chunk * (num_chunks - chunk_idx - 1)
        print(f"Estimated remaining time: {timedelta(seconds=int(remaining))}")
        
        # If we're approaching the problematic chunk, reset tensorflow session
        if (chunk_idx + 1) % 40 == 0:
            print("Approaching potential freeze point - resetting TensorFlow session")
            # Save model for this chunk
            temp_model_path = os.path.join(checkpoints_dir, f"temp_reset_point_{chunk_idx+1}.h5")
            model.save(temp_model_path)
            
            # Clear session
            tf.keras.backend.clear_session()
            gc.collect()
            
            # Reload model
            model = tf.keras.models.load_model(temp_model_path)
            print("TensorFlow session reset complete")
    
    # Training complete - save final model
    final_model_path = os.path.join(output_dir, "final_model.h5")
    model.save(final_model_path)
    print(f"Final model saved to {final_model_path}")
    
    # Save all metrics
    try:
        with open(os.path.join(output_dir, "final_training_metrics.json"), "w") as f:
            json.dump(history_log, f)
    except Exception as e:
        print(f"Error saving metrics: {e}")
        # Save as pickle instead
        import pickle
        with open(os.path.join(output_dir, "final_training_metrics.pkl"), "wb") as f:
            pickle.dump(history_log, f)
    
    # Clean up validation data
    del val_X, val_y
    gc.collect()
    
    # Final evaluation on test set
    print("\nPerforming final evaluation on test set...")
    # [Rest of evaluation code remains the same]
    
    return model, final_model_path

# # For initial run:
# model, final_model_path = resumable_efficient_training(
#     model, X_file, y_file,
#     train_indices, val_indices, test_indices,
#     output_dir=output_dir,
#     batch_size=256,
#     chunk_size=25000,
#     epochs_per_chunk=2,
#     save_frequency=5,
#     class_weight=class_weight,
#     start_chunk=0  # Start from beginning
# )

# To resume after a freeze (e.g., after chunk #):
model, final_model_path = resumable_efficient_training(
    model, X_file, y_file,
    train_indices, val_indices, test_indices,
    output_dir=output_dir,
    batch_size=256,
    chunk_size=25000,
    epochs_per_chunk=2,
    save_frequency=5,
    class_weight=class_weight,
    #start_chunk=360
    #start_chunk=405
    #start_chunk=450
    #start_chunk=495
    start_chunk=540
)


####


def plot_training_history(output_dir):
    """Visualize learning curves across all training chunks"""
    import matplotlib.pyplot as plt
    import numpy as np
    import json
    import os
    
    # Load training history
    history_path = os.path.join(output_dir, "training_history.json")
    if not os.path.exists(history_path):
        history_path = os.path.join(output_dir, "final_training_metrics.json")
    
    try:
        with open(history_path, "r") as f:
            history_log = json.load(f)
    except:
        # Try pickle format
        import pickle
        with open(os.path.join(output_dir, "training_history.pkl"), "rb") as f:
            history_log = pickle.load(f)
    
    # Extract metrics across all chunks
    metrics = ['loss', 'accuracy', 'auc', 'precision', 'recall']
    val_metrics = [f'val_{m}' for m in metrics]
    
    # Prepare aggregated metrics
    all_metrics = {m: [] for m in metrics + val_metrics}
    
    # Collect metrics across chunks
    for chunk_history in history_log:
        for metric in metrics:
            # Training metrics
            if metric in chunk_history:
                all_metrics[metric].extend(chunk_history[metric])
            
            # Validation metrics
            val_metric = f'val_{metric}'
            if val_metric in chunk_history:
                all_metrics[val_metric].extend(chunk_history[val_metric])
    
    # Create learning curve visualizations
    plt.figure(figsize=(18, 12))
    
    # Plot loss
    plt.subplot(2, 2, 1)
    plt.plot(all_metrics['loss'], label='Training Loss')
    plt.plot(all_metrics['val_loss'], label='Validation Loss')
    plt.title('Loss Over Time')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Plot accuracy
    plt.subplot(2, 2, 2)
    plt.plot(all_metrics['accuracy'], label='Training Accuracy')
    plt.plot(all_metrics['val_accuracy'], label='Validation Accuracy')
    plt.title('Accuracy Over Time')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Plot AUC
    plt.subplot(2, 2, 3)
    plt.plot(all_metrics['auc'], label='Training AUC')
    plt.plot(all_metrics['val_auc'], label='Validation AUC')
    plt.title('AUC Over Time')
    plt.xlabel('Epoch')
    plt.ylabel('AUC')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Plot precision-recall
    plt.subplot(2, 2, 4)
    plt.plot(all_metrics['precision'], label='Training Precision')
    plt.plot(all_metrics['recall'], label='Training Recall')
    plt.plot(all_metrics['val_precision'], label='Validation Precision')
    plt.plot(all_metrics['val_recall'], label='Validation Recall')
    plt.title('Precision-Recall Over Time')
    plt.xlabel('Epoch')
    plt.ylabel('Score')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'learning_curves.png'), dpi=300)
    plt.show()
    
    return all_metrics

# Define output directory
output_dir = '/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling/efficient_model'

# Data paths
data_dir = os.path.join('/Users/bgay/Desktop/Research/Code/zero_curtain_pipeline/modeling', 'ml_data')
X_file = os.path.join(data_dir, 'X_features.npy')
y_file = os.path.join(data_dir, 'y_labels.npy')

# Load metadata for spatial analysis
metadata_file = os.path.join(data_dir, 'metadata.pkl')
with open(metadata_file, 'rb') as f:
    metadata = pickle.load(f)

# Run all analyses
print("1. Analyzing learning curves...")
learning_metrics = plot_training_history(output_dir)

# import pandas as pd
# import numpy as np
# from sklearn.preprocessing import StandardScaler
# from datetime import datetime, timedelta
# import tensorflow as tf

# class ZeroCurtainDataProcessor:
#     def __init__(self, temp_df, zc_metrics_df):
#         self.temp_df = temp_df
#         self.zc_metrics_df = zc_metrics_df
#         self.depth_zones = {'shallow': 0, 'intermediate': 1, 'deep': 2, 'very_deep': 3}
#         self.scaler = StandardScaler()
        
#     def preprocess_temperature_data(self):
#         """Preprocess soil temperature time series data"""
#         # Convert datetime to pandas datetime
#         self.temp_df['datetime'] = pd.to_datetime(self.temp_df['datetime'])
        
#         # Create spatial grid for each site
#         site_groups = self.temp_df.groupby('site_id')
        
#         processed_data = []
#         for site_id, group in site_groups:
#             # Sort by datetime and depth
#             group = group.sort_values(['datetime', 'depth'])
            
#             # Create regular time-depth grid
#             pivot_table = group.pivot(
#                 index='datetime',
#                 columns='depth',
#                 values='temperature'
#             ).sort_index()
            
#             # Get site metadata
#             site_lat = group['latitude'].iloc[0]
#             site_lon = group['longitude'].iloc[0]
            
#             processed_data.append({
#                 'site_id': site_id,
#                 'temperature_profile': pivot_table,
#                 'latitude': site_lat,
#                 'longitude': site_lon
#             })
            
#         return processed_data
    
#     def extract_zero_curtain_features(self):
#         """Extract features related to zero curtain events"""
#         zc_features = self.zc_metrics_df.copy()
        
#         # Convert dates to datetime
#         zc_features['start_date'] = pd.to_datetime(zc_features['start_date'])
#         zc_features['end_date'] = pd.to_datetime(zc_features['end_date'])
        
#         # Calculate additional features
#         zc_features['season_numeric'] = zc_features['season'].map({
#             'Spring': 0, 'Summer': 1, 'Fall': 2, 'Winter': 3
#         })
        
#         # Encode depth zones
#         zc_features['depth_zone_numeric'] = zc_features['depth_zone'].map(self.depth_zones)
        
#         return zc_features
    
#     def create_spatiotemporal_windows(self, processed_temp_data, window_size=30):
#         """Create sliding windows for temporal analysis"""
#         windowed_data = []
        
#         for site_data in processed_temp_data:
#             temp_profile = site_data['temperature_profile']
            
#             # Create sliding windows
#             for i in range(len(temp_profile) - window_size + 1):
#                 window = temp_profile.iloc[i:i+window_size]
                
#                 # Check for zero curtain event in this window
#                 window_start = window.index[0]
#                 window_end = window.index[-1]
                
#                 zc_events = self.zc_metrics_df[
#                     (self.zc_metrics_df['site_id'] == site_data['site_id']) &
#                     (self.zc_metrics_df['start_date'] >= window_start) &
#                     (self.zc_metrics_df['end_date'] <= window_end)
#                 ]
                
#                 if not zc_events.empty:
#                     windowed_data.append({
#                         'site_id': site_data['site_id'],
#                         'window_data': window.values,
#                         'latitude': site_data['latitude'],
#                         'longitude': site_data['longitude'],
#                         'has_zero_curtain': 1,
#                         'zc_duration': zc_events['duration_hours'].values[0]
#                     })
#                 else:
#                     windowed_data.append({
#                         'site_id': site_data['site_id'],
#                         'window_data': window.values,
#                         'latitude': site_data['latitude'],
#                         'longitude': site_data['longitude'],
#                         'has_zero_curtain': 0,
#                         'zc_duration': 0
#                     })
        
#         return windowed_data
    
#     def prepare_for_training(self, windowed_data, val_split=0.2):
#         """Prepare data for model training"""
#         # Convert to numpy arrays
#         X = np.array([d['window_data'] for d in windowed_data])
#         y = np.array([d['has_zero_curtain'] for d in windowed_data])
        
#         # Add spatial coordinates
#         lats = np.array([d['latitude'] for d in windowed_data])
#         lons = np.array([d['longitude'] for d in windowed_data])
        
#         # Normalize spatial coordinates
#         lats_normalized = self.scaler.fit_transform(lats.reshape(-1, 1))
#         lons_normalized = self.scaler.fit_transform(lons.reshape(-1, 1))
        
#         # Combine features
#         X_combined = np.concatenate([
#             X,
#             lats_normalized,
#             lons_normalized
#         ], axis=1)
        
#         # Split into training and validation sets
#         n_samples = len(X_combined)
#         n_val = int(n_samples * val_split)
        
#         train_indices = np.random.choice(
#             n_samples, n_samples - n_val, replace=False
#         )
#         val_indices = np.setdiff1d(np.arange(n_samples), train_indices)
        
#         train_data = tf.data.Dataset.from_tensor_slices((
#             X_combined[train_indices],
#             y[train_indices]
#         ))
#         val_data = tf.data.Dataset.from_tensor_slices((
#             X_combined[val_indices],
#             y[val_indices]
#         ))
        
#         return train_data, val_data

# def prepare_remote_sensing_data(rs_data):
#     """Prepare remote sensing data for future integration"""
#     # This function will be expanded when RS data becomes available
#     pass

# import pandas as pd
# import numpy as np
# from sklearn.preprocessing import StandardScaler
# import tensorflow as tf
# from datetime import datetime, timedelta

# def preprocess_in_chunks(temp_df, zc_df, chunk_size=50, window_size=30, min_depths=4, depth_range=...
#     """Process the data in chunks to manage memory efficiently"""
    
#     # Convert to datetime
#     temp_df['datetime'] = pd.to_datetime(temp_df['datetime'], format='mixed')
#     zc_df['start_date'] = pd.to_datetime(zc_df['start_date'], format='mixed')
#     zc_df['end_date'] = pd.to_datetime(zc_df['end_date'], format='mixed')
    
#     # Get unique sites
#     unique_sites = temp_df['site_id'].unique()
#     print(f"Total unique sites: {len(unique_sites)}")
    
#     # Process sites in chunks
#     processed_chunks = []
    
#     for i in range(0, len(unique_sites), chunk_size):
#         chunk_sites = unique_sites[i:i + chunk_size]
#         print(f"Processing sites {i} to {i + len(chunk_sites)}")
        
#         # Filter data for current chunk and relevant depths
#         chunk_temp_df = temp_df[
#             (temp_df['site_id'].isin(chunk_sites)) &
#             (temp_df['depth'].between(depth_range[0], depth_range[1]))
#         ].copy()
        
#         print(f"Processing chunk with {len(chunk_temp_df):,} measurements")
        
#         # Handle duplicate measurements by averaging
#         chunk_temp_df = chunk_temp_df.groupby(
#             ['site_id', 'datetime', 'depth']
#         ).agg({
#             'temperature': 'mean',
#             'latitude': 'first',
#             'longitude': 'first'
#         }).reset_index()
        
#         # Process each site in the chunk
#         for site_id in chunk_sites:
#             site_data = chunk_temp_df[chunk_temp_df['site_id'] == site_id]
            
#             if len(site_data) < window_size:
#                 continue
                
#             # Create pivot table
#             try:
#                 pivot = site_data.pivot_table(
#                     index='datetime',
#                     columns='depth',
#                     values='temperature',
#                     aggfunc='mean'
#                 ).sort_index()
                
#                 # Ensure regular time intervals
#                 pivot = pivot.resample('D').mean()
                
#                 # Skip if insufficient depths
#                 if len(pivot.columns) < min_depths:
#                     continue
                    
#                 # Create sliding windows
#                 dates = pivot.index
#                 for j in range(len(dates) - window_size + 1):
#                     window = pivot.iloc[j:j+window_size]
                    
#                     # Skip windows with too many missing values
#                     if window.isnull().sum().sum() / (window.shape[0] * window.shape[1]) > 0.3:
#                         continue
                    
#                     # Check for zero curtain events
#                     window_start = dates[j]
#                     window_end = dates[j + window_size - 1]
                    
#                     zc_events = zc_df[
#                         (zc_df['site_id'] == site_id) &
#                         (zc_df['start_date'] >= window_start) &
#                         (zc_df['end_date'] <= window_end)
#                     ]
                    
#                     # Fill missing values
#                     window_filled = window.interpolate(method='linear', axis=0)
#                     window_filled = window_filled.fillna(method='ffill').fillna(method='bfill')
                    
#                     if not window_filled.isnull().any().any():
#                         processed_chunks.append({
#                             'site_id': site_id,
#                             'window_data': window_filled.values,
#                             'window_start': window_start,
#                             'window_end': window_end,
#                             'latitude': site_data['latitude'].iloc[0],
#                             'longitude': site_data['longitude'].iloc[0],
#                             'depths': window_filled.columns.values,
#                             'has_zero_curtain': 1 if not zc_events.empty else 0,
#                             'zc_duration': zc_events['duration_hours'].iloc[0] if not zc_events.em...
#                         })
                
#             except Exception as e:
#                 print(f"Error processing site {site_id}: {str(e)}")
#                 continue
        
#         # Clear memory
#         del chunk_temp_df
        
#         print(f"Processed chunks so far: {len(processed_chunks)}")
    
#     return processed_chunks

# def create_tf_dataset(processed_data, batch_size=32, val_split=0.2):
#     """Create TensorFlow dataset from processed chunks"""
    
#     # Extract features
#     X_list = []
#     y_list = []
    
#     for data in processed_data:
#         # Flatten temperature data
#         temp_data = data['window_data'].flatten()
        
#         # Add spatial features
#         features = np.concatenate([
#             temp_data,
#             [data['latitude'], data['longitude']]
#         ])
        
#         X_list.append(features)
#         y_list.append(data['has_zero_curtain'])
    
#     X = np.array(X_list)
#     y = np.array(y_list)
    
#     # Normalize features
#     scaler = StandardScaler()
#     X = scaler.fit_transform(X)
    
#     # Split data
#     n_val = int(len(X) * val_split)
#     indices = np.random.permutation(len(X))
#     train_idx, val_idx = indices[n_val:], indices[:n_val]
    
#     # Create datasets
#     train_dataset = tf.data.Dataset.from_tensor_slices(
#         (X[train_idx], y[train_idx])
#     ).shuffle(buffer_size=len(train_idx)).batch(batch_size).prefetch(tf.data.AUTOTUNE)
    
#     val_dataset = tf.data.Dataset.from_tensor_slices(
#         (X[val_idx], y[val_idx])
#     ).batch(batch_size).prefetch(tf.data.AUTOTUNE)
    
#     return train_dataset, val_dataset

# #This is the code from our use case previously; if we can retain as much as possible, great; but w...

# model_params = {
#     'temporal_window': 30,
#     'n_depths': n_depths,
#     'learning_rate': 1e-4,
#     'batch_size': 32,
#     'epochs': 100,
#     'early_stopping_patience': 10
# }

# print("\nInitializing model with dimensions:")
# print(f"- Temporal window: {model_params['temporal_window']}")
# print(f"- Number of depths: {model_params['n_depths']}")

# Initializing model with dimensions:
# - Temporal window: 30
# - Number of depths: 4

# training_config = {
#     'learning_rate': 1e-4,
#     'batch_size': 32,
#     'epochs': 100,
#     'early_stopping_patience': 10
# }

# class PhysicsLayer(tf.keras.layers.Layer):
#     def __init__(self):
#         super().__init__()
#         # Physical constants
#         self.L_fusion = 334000.0  # J/kg, latent heat of fusion for water
#         self.k_ice = 2.22  # W/m/K, thermal conductivity of ice
#         self.k_water = 0.58  # W/m/K, thermal conductivity of water
#         self.c_ice = 2090.0  # J/kg/K, specific heat capacity of ice
#         self.c_water = 4186.0  # J/kg/K, specific heat capacity of water
        
#     def build(self, input_shape):
#         self.porosity = self.add_weight(
#             name="porosity",
#             shape=[1],
#             initializer=tf.keras.initializers.Constant(0.4),
#             constraint=tf.keras.constraints.MinMaxNorm(min_value=0.1, max_value=0.9)
#         )
#         self.output_shape = input_shape[0]
#         super().build(input_shape)
    
#     def compute_output_shape(self, input_shape):
#         return self.output_shape
        
#     def call(self, inputs):
#         temperatures, time_steps = inputs
#         temp_padded = tf.pad(temperatures, [[0, 0], [1, 1], [0, 0], [0, 0]])
#         temp_gradients = (temp_padded[:, 2:] - temp_padded[:, :-2]) / 2.0
#         phase_fraction = tf.sigmoid((temperatures + 0.1) * 100)
#         k_eff = (phase_fraction * self.k_water + (1 - phase_fraction) * self.k_ice) * self.porosit...
#         return temperatures + k_eff * temp_gradients

# class ZeroCurtainModel(tf.keras.Model):
#     def __init__(self, temporal_window=30, n_depths=4):
#         super().__init__()
#         self.temporal_window = temporal_window
#         self.n_depths = n_depths
        
#         # Encoder
#         self.encoder = tf.keras.Sequential([
#             tf.keras.layers.Reshape((temporal_window, n_depths, 1)),
#             tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same'),
#             tf.keras.layers.BatchNormalization(),
#             tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same')
#         ])
        
#         # Physics layer
#         self.physics_layer = PhysicsLayer()
        
#         # Define reshape dimensions based on input size
#         channels = 64  # from previous Conv2D layer
#         h = 2
#         w = 2
#         self.reshape = tf.keras.layers.Reshape(
#             (temporal_window, h, w, channels * n_depths // (h * w))
#         )
        
#         # ConvLSTM2D layer
#         self.temporal = tf.keras.layers.ConvLSTM2D(
#             filters=32,
#             kernel_size=(2, 2),
#             padding='same',
#             return_sequences=False,
#             activation='relu'
#         )
        
#         # Output layers
#         self.flatten = tf.keras.layers.Flatten()
#         self.dense = tf.keras.layers.Dense(32, activation='relu')
#         self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid')
    
#     def build(self, input_shape):
#         # Build the model with a sample input
#         super().build(input_shape)
    
#     def call(self, inputs):
#         x = self.encoder(inputs)
#         time_steps = tf.range(self.temporal_window, dtype=tf.float32)
#         x = self.physics_layer([x, time_steps])
#         x = self.reshape(x)
#         x = self.temporal(x)
#         x = self.flatten(x)
#         x = self.dense(x)
#         return self.output_layer(x)

# for x, y in train_dataset.take(1):
#     print("Input shape:", x.shape)
#     print("Label shape:", y.shape)
#     input_shape = x.shape[1:]
#     break

# Input shape: (32, 30, 4)
# Label shape: (32,)

# with tf.device('/CPU:0'):
#     inputs = tf.keras.Input(shape=input_shape)
#     # Encoder
#     x = tf.keras.layers.Reshape((30, 4, 1))(inputs)
#     x = tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same')(x)
#     x = tf.keras.layers.BatchNormalization()(x)
#     x = tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)
#     # Physics layer
#     time_steps = tf.range(30, dtype=tf.float32)
#     physics_layer = PhysicsLayer()
#     x = physics_layer([x, time_steps])
#     # Reshape for ConvLSTM2D
#     x = tf.keras.layers.Reshape((30, 2, 2, -1))(x)
#     # ConvLSTM2D
#     x = tf.keras.layers.ConvLSTM2D(
#         filters=32,
#         kernel_size=(2, 2),
#         padding='same',
#         return_sequences=False,
#         activation='relu'
#     )(x)
#     # Output layers
#     x = tf.keras.layers.Flatten()(x)
#     x = tf.keras.layers.Dense(32, activation='relu')(x)
#     outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)
#     # Create model
#     model = tf.keras.Model(inputs=inputs, outputs=outputs)
#     # Compile model
#     model.compile(
#         optimizer=tf.keras.optimizers.Adam(training_config['learning_rate']),
#         loss='binary_crossentropy',
#         metrics=['accuracy', 
#                 tf.keras.metrics.Precision(name='precision'),
#                 tf.keras.metrics.Recall(name='recall'),
#                 tf.keras.metrics.AUC(name='auc')]
#     )

# # Print model summary
# model.summary()

# Model: "functional"
# 
#  Layer (type)                     Output Shape                  Param # 
# 
#  input_layer (InputLayer)         (None, 30, 4)                       0 
# 
#  reshape (Reshape)                (None, 30, 4, 1)                    0 
# 
#  conv2d (Conv2D)                  (None, 30, 4, 32)                 320 
# 
#  batch_normalization              (None, 30, 4, 32)                 128 
#  (BatchNormalization)                                                   
# 
#  conv2d_1 (Conv2D)                (None, 30, 4, 64)              18,496 
# 
#  physics_layer (PhysicsLayer)     (None, 30, 4, 64)                   1 
# 
#  reshape_1 (Reshape)              (None, 30, 2, 2, 64)                0 
# 
#  conv_lstm2d (ConvLSTM2D)         (None, 2, 2, 32)               49,280 
# 
#  flatten (Flatten)                (None, 128)                         0 
# 
#  dense (Dense)                    (None, 32)                      4,128 
# 
#  dense_1 (Dense)                  (None, 1)                          33 
# 
#  Total params: 72,386 (282.76 KB)
#  Trainable params: 72,322 (282.51 KB)
#  Non-trainable params: 64 (256.00 B)

# callbacks = [
#     # Early stopping
#     tf.keras.callbacks.EarlyStopping(
#         monitor='val_loss',
#         patience=training_config['early_stopping_patience'],
#         restore_best_weights=True,
#         verbose=1
#     ),
    
#     # Model checkpoint
#     tf.keras.callbacks.ModelCheckpoint(
#         'best_zero_curtain_model.keras',
#         monitor='val_loss',
#         save_best_only=True,
#         verbose=1
#     ),
    
#     # Learning rate reduction
#     tf.keras.callbacks.ReduceLROnPlateau(
#         monitor='val_loss',
#         factor=0.5,
#         patience=5,
#         verbose=1,
#         min_lr=1e-6
#     ),
    
#     # TensorBoard logging
#     tf.keras.callbacks.TensorBoard(
#         log_dir=f'logs/zero_curtain_{datetime.now().strftime("%Y%m%d-%H%M%S")}',
#         histogram_freq=1,
#         update_freq='epoch'
#     )
# ]

# print("Converting dataset to numpy...")
# x_train = []
# y_train = []

# for x, y in train_dataset.take(2):
#     x_train.append(x.numpy())
#     y_train.append(y.numpy())

# x_train = np.concatenate(x_train, axis=0)
# y_train = np.concatenate(y_train, axis=0)

# print(f"Training data shape: {x_train.shape}")
# print(f"Training labels shape: {y_train.shape}")

# simple_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)

# Converting dataset to numpy...
# Training data shape: (64, 30, 4)
# Training labels shape: (64,)
# 2025-02-11 12:33:21.176273: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous ...

# import tensorflow as tf
# print(f"TensorFlow version: {tf.__version__}")
# print("\nDevice configuration:")
# print(tf.config.list_physical_devices())

# TensorFlow version: 2.16.2

# Device configuration:
# [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_...

# tf.keras.backend.clear_session()

# #Instead of taking slices of dataset, let's now try the whole dataset (train_dataset) for training

# print("Converting dataset to numpy...")
# x_train = []
# y_train = []

# for x, y in train_dataset:#.take(2):
#     x_train.append(x.numpy())
#     y_train.append(y.numpy())

# x_train = np.concatenate(x_train, axis=0)
# y_train = np.concatenate(y_train, axis=0)

# print(f"Training data shape: {x_train.shape}")
# print(f"Training labels shape: {y_train.shape}")

# Converting dataset to numpy...
# Training data shape: (127920, 30, 4)
# Training labels shape: (127920,)
# 2025-02-11 13:03:57.888272: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous ...

# with tf.device('/CPU:0'):
#     # Training configuration
#     batch_size = 32
#     n_samples = len(x_train)
#     n_epochs = 10
    
#     def train_step(x, y):
#         with tf.GradientTape() as tape:
#             predictions = model(x, training=True)
#             loss = loss_fn(y, predictions)
#         grads = tape.gradient(loss, model.trainable_variables)
#         optimizer.apply_gradients(zip(grads, model.trainable_variables))
#         return loss, predictions
    
#     print(f"Starting training on {n_samples} samples...")
#     best_auc = 0
#     best_weights = None
    
#     for epoch in range(n_epochs):
#         print(f"\nEpoch {epoch + 1}/{n_epochs}")
#         epoch_loss = 0
#         n_batches = int(np.ceil(n_samples / batch_size))
        
#         # Reset metrics
#         train_acc_metric.reset_state()
#         train_precision.reset_state()
#         train_recall.reset_state()
#         train_auc.reset_state()
        
#         # Shuffle data
#         #perm = np.random.permutation(n_samples)
#         #x_train_shuffled = x_train[perm]
#         #y_train_shuffled = y_train[perm]
        
#         # Process batches
#         for batch_idx in range(n_batches):
#             start_idx = batch_idx * batch_size
#             end_idx = min(start_idx + batch_size, n_samples)
            
#             x_batch = x_train[start_idx:end_idx]
#             y_batch = y_train[start_idx:end_idx]
            
#             loss, predictions = train_step(x_batch, y_batch)
#             epoch_loss += loss.numpy()
            
#             # Update metrics
#             train_acc_metric.update_state(y_batch, predictions)
#             train_precision.update_state(y_batch, predictions)
#             train_recall.update_state(y_batch, predictions)
#             train_auc.update_state(y_batch, predictions)
            
#             # Print batch progress
#             print(f"\rBatch {batch_idx + 1}/{n_batches}", end='')
        
#         # Calculate epoch metrics
#         epoch_loss = epoch_loss / n_batches
#         epoch_acc = train_acc_metric.result().numpy()
#         epoch_prec = train_precision.result().numpy()
#         epoch_recall = train_recall.result().numpy()
#         epoch_auc = train_auc.result().numpy()
        
#         # Print epoch results
#         print(f"\nLoss: {epoch_loss:.4f}")
#         print(f"Accuracy: {epoch_acc:.4f}")
#         print(f"Precision: {epoch_prec:.4f}")
#         print(f"Recall: {epoch_recall:.4f}")
#         print(f"AUC: {epoch_auc:.4f}")
        
#         # Save best model
#         if epoch_auc > best_auc:
#             best_auc = epoch_auc
#             best_weights = model.get_weights()
#             print("New best model saved!")

#     # Restore best weights
#     if best_weights is not None:
#         model.set_weights(best_weights)
#         print(f"\nRestored best model with AUC: {best_auc:.4f}")

# print("\nTraining complete!")

# print("Converting dataset to numpy...")
# x_train = []
# y_train = []

# for x, y in train_dataset.take(2):
#     x_train.append(x.numpy())
#     y_train.append(y.numpy())

# x_train = np.concatenate(x_train, axis=0)
# y_train = np.concatenate(y_train, axis=0)

# print(f"Training data shape: {x_train.shape}")
# print(f"Training labels shape: {y_train.shape}")

# simple_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)

# #Instead of taking slices of dataset, let's now try the whole dataset (train_dataset) for training

# print("Converting dataset to numpy...")
# x_train = []
# y_train = []

# for x, y in train_dataset:#.take(2):
#     x_train.append(x.numpy())
#     y_train.append(y.numpy())

# x_train = np.concatenate(x_train, axis=0)
# y_train = np.concatenate(y_train, axis=0)

# print(f"Training data shape: {x_train.shape}")
# print(f"Training labels shape: {y_train.shape}")

print("Converting dataset to numpy...")
x_train = []
y_train = []
x_val = []
y_val = []
x_test = []
y_test = []

# Convert training data
for x, y in train_dataset:
    x_train.append(x.numpy())
    y_train.append(y.numpy())

# Convert validation data
for x, y in val_dataset:
    x_val.append(x.numpy())
    y_val.append(y.numpy())

# Convert test data
for x, y in test_dataset:
    x_test.append(x.numpy())
    y_test.append(y.numpy())

x_train = np.concatenate(x_train, axis=0)
y_train = np.concatenate(y_train, axis=0)
x_val = np.concatenate(x_val, axis=0)
y_val = np.concatenate(y_val, axis=0)
x_test = np.concatenate(x_test, axis=0)
y_test = np.concatenate(y_test, axis=0)

print(f"Training data shape: {x_train.shape}")
print(f"Training labels shape: {y_train.shape}")
print(f"Validation data shape: {x_val.shape}")
print(f"Validation labels shape: {y_val.shape}")
print(f"Test data shape: {x_test.shape}")
print(f"Test labels shape: {y_test.shape}")

# print("Converting dataset to numpy...")
# x_train = []
# y_train = []
# x_val = []
# y_val = []
# x_test = []
# y_test = []

# # Convert training data
# for x, y in train_dataset:
#     x_train.append(x.numpy())
#     y_train.append(y.numpy())

# # Convert validation data
# for x, y in val_dataset:
#     x_val.append(x.numpy())
#     y_val.append(y.numpy())

# # Convert test data
# for x, y in test_dataset:
#     x_test.append(x.numpy())
#     y_test.append(y.numpy())

# x_train = np.concatenate(x_train, axis=0)
# y_train = np.concatenate(y_train, axis=0)
# x_val = np.concatenate(x_val, axis=0)
# y_val = np.concatenate(y_val, axis=0)
# x_test = np.concatenate(x_test, axis=0)
# y_test = np.concatenate(y_test, axis=0)

# print(f"Training data shape: {x_train.shape}")
# print(f"Training labels shape: {y_train.shape}")
# print(f"Validation data shape: {x_val.shape}")
# print(f"Validation labels shape: {y_val.shape}")
# print(f"Test data shape: {x_test.shape}")
# print(f"Test labels shape: {y_test.shape}")

# Converting dataset to numpy...
# Training data shape: (9194, 30, 10)
# Training labels shape: (9194,)
# Validation data shape: (2115, 30, 10)
# Validation labels shape: (2115,)
# Test data shape: (1776, 30, 10)
# Test labels shape: (1776,)
# 2025-02-11 22:19:39.314517: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous ...
# 2025-02-11 22:19:39.321613: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous ...
# 2025-02-11 22:19:39.328758: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous ...

# with tf.device('/CPU:0'):
#     for x, y in train_dataset.take(1):
#         print("Input shape:", x.shape)
#         print("Label shape:", y.shape)
#         input_shape = x.shape[1:]
#         break
        
#     inputs = tf.keras.Input(shape=(30, 10))  # Updated for new depth dimension
    
#     # Encoder
#     x = tf.keras.layers.Reshape((30, 10, 1))(inputs)
#     x = tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same')(x)
#     x = tf.keras.layers.BatchNormalization()(x)
#     x = tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)
    
#     # Physics layer
#     time_steps = tf.range(30, dtype=tf.float32)
#     physics_layer = PhysicsLayer()
#     x = physics_layer([x, time_steps])
    
#     # Reshape for ConvLSTM2D
#     x = tf.keras.layers.Reshape((30, 2, 5, -1))(x)  # Adjusted for 10 depths
    
#     # ConvLSTM2D
#     x = tf.keras.layers.ConvLSTM2D(
#         filters=32,
#         kernel_size=(2, 2),
#         padding='same',
#         return_sequences=False,
#         activation='relu'
#     )(x)
    
#     # Output layers
#     x = tf.keras.layers.Flatten()(x)
#     x = tf.keras.layers.Dense(32, activation='relu')(x)
#     outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    
#     model = tf.keras.Model(inputs=inputs, outputs=outputs)
    
#     # Create optimizer and compile model
#     optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
#     model.compile(
#         optimizer=optimizer,
#         loss='binary_crossentropy',
#         metrics=['accuracy', 
#                 tf.keras.metrics.Precision(name='precision'),
#                 tf.keras.metrics.Recall(name='recall'),
#                 tf.keras.metrics.AUC(name='auc')]
#     )
    
#     # Print model summary
#     model.summary()
    
#     # Initialize metrics for custom training
#     train_acc_metric = tf.keras.metrics.BinaryAccuracy()
#     train_precision = tf.keras.metrics.Precision()
#     train_recall = tf.keras.metrics.Recall()
#     train_auc = tf.keras.metrics.AUC()
    
#     # Training configuration
#     batch_size = 32
#     n_samples = len(x_train)
#     n_epochs = 10
    
#     def train_step(x, y):
#         # Ensure consistent data types
#         x = tf.cast(x, tf.float32)
#         y = tf.cast(y, tf.float32)
#         y = tf.reshape(y, (-1, 1))
        
#         with tf.GradientTape() as tape:
#             predictions = model(x, training=True)
#             # Use tf.keras.losses.BinaryCrossentropy instead of manual calculation
#             loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)
#             loss = loss_fn(y, predictions)
            
#         grads = tape.gradient(loss, model.trainable_variables)
#         optimizer.apply_gradients(zip(grads, model.trainable_variables))
#         return loss, predictions

#     def validation_step(x, y):
#         x = tf.cast(x, tf.float32)
#         y = tf.cast(y, tf.float32)
#         y = tf.reshape(y, (-1, 1))
        
#         predictions = model(x, training=False)
#         loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)
#         loss = loss_fn(y, predictions)
#         return loss, predictions
    
#     print(f"Starting training on {n_samples} samples...")
#     best_auc = 0
#     best_weights = None

#     for epoch in range(n_epochs):
#         print(f"\nEpoch {epoch + 1}/{n_epochs}")
#         epoch_loss = 0
#         val_loss = 0
#         n_batches = int(np.ceil(n_samples / batch_size))
#         n_val_batches = int(np.ceil(len(x_val) / batch_size))
        
#         # Reset metrics
#         train_acc_metric.reset_state()
#         train_precision.reset_state()
#         train_recall.reset_state()
#         train_auc.reset_state()
        
#         # Ensure training data is properly cast before shuffling
#         x_train = tf.cast(x_train, tf.float32)
#         y_train = tf.cast(y_train, tf.float32)
        
#         # Shuffle training data
#         #indices = tf.random.shuffle(tf.range(n_samples))
#         #x_train_shuffled = tf.gather(x_train, indices)
#         #y_train_shuffled = tf.gather(y_train, indices)
        
#         # Training loop
#         for batch_idx in range(n_batches):
#             start_idx = batch_idx * batch_size
#             end_idx = min(start_idx + batch_size, n_samples)
            
#             x_batch = x_train[start_idx:end_idx]
#             y_batch = y_train[start_idx:end_idx]
            
#             loss, predictions = train_step(x_batch, y_batch)
#             epoch_loss += loss.numpy()
            
#             # Update metrics
#             train_acc_metric.update_state(y_batch, predictions)
#             train_precision.update_state(y_batch, predictions)
#             train_recall.update_state(y_batch, predictions)
#             train_auc.update_state(y_batch, predictions)
            
#             # Print batch progress
#             print(f"\rBatch {batch_idx + 1}/{n_batches}", end='')
        
#         # Validation loop
#         for batch_idx in range(n_val_batches):
#             start_idx = batch_idx * batch_size
#             end_idx = min(start_idx + batch_size, len(x_val))
            
#             x_batch = x_val[start_idx:end_idx]
#             y_batch = y_val[start_idx:end_idx]
            
#             loss, _ = validation_step(x_batch, y_batch)
#             val_loss += loss.numpy()
        
#         # Calculate metrics
#         epoch_loss = epoch_loss / n_batches
#         val_loss = val_loss / n_val_batches
#         epoch_acc = train_acc_metric.result().numpy()
#         epoch_prec = train_precision.result().numpy()
#         epoch_recall = train_recall.result().numpy()
#         epoch_auc = train_auc.result().numpy()
        
#         # Print results
#         print(f"\nTrain Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}")
#         print(f"Accuracy: {epoch_acc:.4f}")
#         print(f"Precision: {epoch_prec:.4f}")
#         print(f"Recall: {epoch_recall:.4f}")
#         print(f"AUC: {epoch_auc:.4f}")
        
#         # Save best model
#         if epoch_auc > best_auc:
#             best_auc = epoch_auc
#             best_weights = model.get_weights()
#             print("New best model saved!")

#     # Restore best weights
#     if best_weights is not None:
#         model.set_weights(best_weights)
#         print(f"\nRestored best model with AUC: {best_auc:.4f}")

# print("\nTraining complete!")

# Input shape: (32, 30, 10)
# Label shape: (32,)
# Model: "functional_5"
# 
#  Layer (type)                     Output Shape                  Param # 
# 
#  input_layer_5 (InputLayer)       (None, 30, 10)                      0 
# 
#  reshape_10 (Reshape)             (None, 30, 10, 1)                   0 
# 
#  conv2d_10 (Conv2D)               (None, 30, 10, 32)                320 
# 
#  batch_normalization_5            (None, 30, 10, 32)                128 
#  (BatchNormalization)                                                   
# 
#  conv2d_11 (Conv2D)               (None, 30, 10, 64)             18,496 
# 
#  physics_layer_5 (PhysicsLayer)   (None, 30, 10, 64)                  1 
# 
#  reshape_11 (Reshape)             (None, 30, 2, 5, 64)                0 
# 
#  conv_lstm2d_5 (ConvLSTM2D)       (None, 2, 5, 32)               49,280 
# 
#  flatten_5 (Flatten)              (None, 320)                         0 
# 
#  dense_10 (Dense)                 (None, 32)                     10,272 
# 
#  dense_11 (Dense)                 (None, 1)                          33 
# 
#  Total params: 78,530 (306.76 KB)
#  Trainable params: 78,466 (306.51 KB)
#  Non-trainable params: 64 (256.00 B)
# Starting training on 9194 samples...

# Epoch 1/10
# Batch 288/288
# Train Loss: 0.1703, Val Loss: 0.2885
# Accuracy: 0.9538
# Precision: 0.9939
# Recall: 0.9594
# AUC: 0.2815
# New best model saved!

# Epoch 2/10
# Batch 288/288
# Train Loss: 0.0411, Val Loss: 0.2315
# Accuracy: 0.9934
# Precision: 0.9941
# Recall: 0.9992
# AUC: 0.7496
# New best model saved!

# Epoch 3/10
# Batch 288/288
# Train Loss: 0.0397, Val Loss: 0.2044
# Accuracy: 0.9941
# Precision: 0.9941
# Recall: 1.0000
# AUC: 0.7072

# Epoch 4/10
# Batch 288/288
# Train Loss: 0.0339, Val Loss: 0.2048
# Accuracy: 0.9941
# Precision: 0.9941
# Recall: 1.0000
# AUC: 0.7705
# New best model saved!

# Epoch 5/10
# Batch 288/288
# Train Loss: 0.0297, Val Loss: 0.2891
# Accuracy: 0.9941
# Precision: 0.9941
# Recall: 1.0000
# AUC: 0.7423

# Epoch 6/10
# Batch 288/288
# Train Loss: 0.0278, Val Loss: 0.2040
# Accuracy: 0.9941
# Precision: 0.9941
# Recall: 1.0000
# AUC: 0.7819
# New best model saved!

# Epoch 7/10
# Batch 288/288
# Train Loss: 0.0310, Val Loss: 0.1816
# Accuracy: 0.9941
# Precision: 0.9941
# Recall: 1.0000
# AUC: 0.7044

# Epoch 8/10
# Batch 288/288
# Train Loss: 0.0263, Val Loss: 0.2146
# Accuracy: 0.9941
# Precision: 0.9941
# Recall: 1.0000
# AUC: 0.7659

# Epoch 9/10
# Batch 288/288
# Train Loss: 0.0264, Val Loss: 0.2619
# Accuracy: 0.9941
# Precision: 0.9941
# Recall: 1.0000
# AUC: 0.7787

# Epoch 10/10
# Batch 288/288
# Train Loss: 0.0284, Val Loss: 0.2422
# Accuracy: 0.9949
# Precision: 0.9949
# Recall: 1.0000
# AUC: 0.7847
# New best model saved!

# Restored best model with AUC: 0.7847

# Training complete!

print("Starting manual evaluation...")
with tf.device('/CPU:0'):
    batch_size = 32
    n_batches = int(np.ceil(len(x_test) / batch_size))
    
    # Initialize metrics
    total_loss = 0
    predictions_list = []
    true_labels = []
    
    print(f"Processing {len(x_test)} samples in {n_batches} batches...")
    
    for batch_idx in range(n_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(x_test))
        
        # Get batch data
        x_batch = x_test[start_idx:end_idx]
        y_batch = y_test[start_idx:end_idx]

        y_batch = y_batch.reshape(-1, 1)
        
        # Get predictions for this batch
        batch_pred = best_model(x_batch, training=False)
        predictions_list.append(batch_pred.numpy())
        true_labels.append(y_batch)
        
        # Calculate loss
        loss = tf.keras.losses.binary_crossentropy(y_batch, batch_pred)
        total_loss += tf.reduce_mean(loss)
        
        # Print progress
        print(f"\rProcessing batch {batch_idx + 1}/{n_batches} ({((batch_idx + 1)/n_batches)*100:.1f}%)", end='')
    
    # Combine predictions and calculate metrics
    predictions = np.concatenate(predictions_list)
    y_true = np.concatenate(true_labels)

    predictions = predictions.ravel()
    y_true = y_true.ravel()
    
    metrics = {
        'loss': float(total_loss / n_batches),
        'accuracy': accuracy_score(y_true, predictions > 0.5),
        'precision': precision_score(y_true, predictions > 0.5),
        'recall': recall_score(y_true, predictions > 0.5),
        'auc': roc_auc_score(y_true, predictions)
    }
    
    print("\n\nTest Metrics:")
    for metric, value in metrics.items():
        print(f"{metric}: {value:.4f}")

print("Getting predictions for all datasets...")
with tf.device('/CPU:0'):
    # Create prediction function
    @tf.function(jit_compile=True)
    def predict_batch(batch):
        return best_model(batch, training=False)
    
    print("\nProcessing training set...")
    train_pred = []
    for i in range(0, len(x_train), 128):
        batch = x_train[i:i+128]
        pred = predict_batch(batch)
        train_pred.append(pred.numpy())
        print(f"\rProcessing batch {i//128 + 1}/{len(x_train)//128 + 1}", end='')
    train_pred = np.concatenate(train_pred)
    
    print("\nProcessing validation set...")
    val_pred = []
    for i in range(0, len(x_val), 128):
        batch = x_val[i:i+128]
        pred = predict_batch(batch)
        val_pred.append(pred.numpy())
        print(f"\rProcessing batch {i//128 + 1}/{len(x_val)//128 + 1}", end='')
    val_pred = np.concatenate(val_pred)
    
    print("\nProcessing test set...")
    test_pred = []
    for i in range(0, len(x_test), 128):
        batch = x_test[i:i+128]
        pred = predict_batch(batch)
        test_pred.append(pred.numpy())
        print(f"\rProcessing batch {i//128 + 1}/{len(x_test)//128 + 1}", end='')
    test_pred = np.concatenate(test_pred)

