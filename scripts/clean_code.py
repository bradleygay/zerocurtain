#!/usr/bin/env python3
"""
Clean text-based files before committing to GitHub.
Removes emojis, personal info, AI references, and adds professional headers.
Applies to all files (text and structured data) across the repository.
"""

import re
from pathlib import Path
import sys
import json

# Conditional imports for specialized file formats
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    print("Warning: pandas not available. Parquet/CSV cleaning will be limited.")

try:
    import pyarrow.parquet as pq
    PYARROW_AVAILABLE = True
except ImportError:
    PYARROW_AVAILABLE = False
    print("Warning: pyarrow not available. Parquet file cleaning will be limited.")

PROJECT_ROOT = Path("/Users/[USER]/arctic_zero_curtain_pipeline")

# Personal info or sensitive data to redact - [RESEARCHER]
PERSONAL_INFO = {
    # Name variations
    '[RESEARCHER]': '[RESEARCHER]',
    '[RESEARCHER]': '[RESEARCHER]',
    '[RESEARCHER]': '[RESEARCHER]',
    '[RESEARCHER]': '[RESEARCHER]',
    '[RESEARCHER]': '[RESEARCHER]',
    '[RESEARCHER]': '[RESEARCHER]',
    '[RESEARCHER]': '[RESEARCHER]',
    '[RESEARCHER]': '[RESEARCHER]',
    '[RESEARCHER]': '[RESEARCHER]',
    '[RESEARCHER]': '[RESEARCHER]',
    '[RESEARCHER]': '[RESEARCHER]',
    '[RESEARCHER]': '[RESEARCHER]',
    '[RESEARCHER]': '[RESEARCHER]',
    '[RESEARCHER]': '[RESEARCHER]',
    '[RESEARCHER]': '[RESEARCHER]',
    '[RESEARCHER]': '[RESEARCHER]',
    
    # Email variations
    '[RESEARCHER_EMAIL]': '[RESEARCHER_EMAIL]',
    '[RESEARCHER_EMAIL]': '[RESEARCHER_EMAIL]',
    '[RESEARCHER_EMAIL]': '[RESEARCHER_EMAIL]',
    '[RESEARCHER_EMAIL]': '[RESEARCHER_EMAIL]',
    '[RESEARCHER_EMAIL]': '[RESEARCHER_EMAIL]',
    
    # Username/path variations
    '[USER]': '[USER]',
    '/Users/[USER]': '/Users/[USER]',
    '/Users/[USER]/': '/Users/[USER]/',
    'Users/[USER]': 'Users/[USER]',
    
    # Affiliation
    '[RESEARCH_INSTITUTION]': '[RESEARCH_INSTITUTION]',
    '[RESEARCH_INSTITUTION_DOMAIN]': '[RESEARCH_INSTITUTION_DOMAIN]',
}

# AI-related terms to remove (case-insensitive)
AI_REFERENCES = [
    r'\bclaude\b',
    r'\bai[- ]?generated\b',
    r'\bai[- ]?assisted\b',
    r'\bgenerated by ai\b',
    r'\bchatgpt\b',
    r'\bgpt[-\s]?[0-9o]\b',
    r'\blarge language model\b',
    r'\bllm\b',
    r'\banthropic\b',
    r'\bopenai\b',
    r'\bai model\b',
    r'\bai[- ]?powered\b',
    r'\bmachine[- ]?generated\b',
    r'\bauto[- ]?generated by\b',
    r'\bassisted by ai\b',
    r'\bcreated with ai\b',
    r'\bai[- ]?tool\b',
    r'\bai assistance\b',
]

# Compile AI pattern for efficient matching
AI_PATTERN = re.compile('|'.join(AI_REFERENCES), flags=re.IGNORECASE)

# Emoji pattern to remove
EMOJI_PATTERN = re.compile(
    "["
    "\U0001F600-\U0001F64F"  # emoticons
    "\U0001F300-\U0001F5FF"  # symbols & pictographs
    "\U0001F680-\U0001F6FF"  # transport & map symbols
    "\U0001F1E0-\U0001F1FF"  # flags
    "\U00002702-\U000027B0"
    "\U000024C2-\U0001F251"
    "]+", flags=re.UNICODE
)

# File extensions to process
TEXT_EXTS = {'.py', '.md', '.txt', '.json', '.yml', '.yaml', '.ini', '.cfg', '.log', '.sh', '.rst', '.tex'}
DATA_EXTS = {'.csv', '.tsv'}
PARQUET_EXTS = {'.parquet', '.pq'}

def apply_text_replacements(text: str) -> str:
    """Apply all text-based sanitization rules."""
    if not isinstance(text, str):
        return text
    
    # Remove AI references
    text = AI_PATTERN.sub('[AUTOMATED_TOOL]', text)
    
    # Remove emojis
    text = EMOJI_PATTERN.sub('', text)
    
    # Replace personal info (order matters - more specific first)
    for old, new in sorted(PERSONAL_INFO.items(), key=lambda x: len(x[0]), reverse=True):
        text = text.replace(old, new)
    
    return text

def remove_ai_comment_lines(text: str) -> str:
    """Remove entire lines that are AI attribution comments."""
    lines = text.split('\n')
    cleaned_lines = []
    
    for line in lines:
        stripped = line.strip()
        if stripped.startswith('#') and AI_PATTERN.search(stripped):
            # Skip lines that are purely AI attribution comments
            if any(indicator in stripped.lower() for indicator in 
                   ['generated', 'assisted', 'created with', 'powered by']):
                continue
        cleaned_lines.append(line)
    
    return '\n'.join(cleaned_lines)

def clean_text_file(file_path: Path) -> tuple[bool, str]:
    """Clean standard text files."""
    try:
        content = file_path.read_text(encoding='utf-8', errors='ignore')
        original_content = content
        
        # Remove AI comment lines
        content = remove_ai_comment_lines(content)
        
        # Apply all replacements
        content = apply_text_replacements(content)
        
        # Remove excessively long comment lines
        lines = content.split('\n')
        cleaned_lines = []
        for line in lines:
            stripped = line.strip()
            if stripped.startswith('#') and len(stripped) > 120:
                cleaned_lines.append(line[:120] + '...')
            else:
                cleaned_lines.append(line)
        content = '\n'.join(cleaned_lines)
        
        if content != original_content:
            file_path.write_text(content, encoding='utf-8')
            return True, "Cleaned (text)"
        return False, "No changes needed"
        
    except Exception as e:
        return False, f"Error (text): {e}"

def clean_json_file(file_path: Path) -> tuple[bool, str]:
    """Clean JSON files by parsing and sanitizing values."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        original_data = json.dumps(data, sort_keys=True)
        
        def sanitize_json_recursive(obj):
            """Recursively sanitize JSON structures."""
            if isinstance(obj, dict):
                return {k: sanitize_json_recursive(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [sanitize_json_recursive(item) for item in obj]
            elif isinstance(obj, str):
                return apply_text_replacements(obj)
            else:
                return obj
        
        cleaned_data = sanitize_json_recursive(data)
        new_data = json.dumps(cleaned_data, sort_keys=True)
        
        if new_data != original_data:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(cleaned_data, f, indent=2, ensure_ascii=False)
            return True, "Cleaned (JSON)"
        return False, "No changes needed"
        
    except Exception as e:
        return False, f"Error (JSON): {e}"

def clean_csv_file(file_path: Path) -> tuple[bool, str]:
    """Clean CSV files using pandas."""
    if not PANDAS_AVAILABLE:
        return False, "Skipped (pandas unavailable)"
    
    try:
        df = pd.read_csv(file_path, encoding='utf-8', low_memory=False)
        original_shape = df.shape
        
        # Apply sanitization to all string columns
        for col in df.columns:
            if df[col].dtype == 'object':
                df[col] = df[col].apply(lambda x: apply_text_replacements(str(x)) if pd.notna(x) else x)
        
        # Also sanitize column names
        df.columns = [apply_text_replacements(str(col)) for col in df.columns]
        
        # Check if anything changed
        df_check = pd.read_csv(file_path, encoding='utf-8', low_memory=False)
        if not df.equals(df_check) or list(df.columns) != list(df_check.columns):
            df.to_csv(file_path, index=False, encoding='utf-8')
            return True, "Cleaned (CSV)"
        return False, "No changes needed"
        
    except Exception as e:
        return False, f"Error (CSV): {e}"

def clean_parquet_file(file_path: Path) -> tuple[bool, str]:
    """Clean Parquet files using pandas and pyarrow."""
    if not PANDAS_AVAILABLE or not PYARROW_AVAILABLE:
        return False, "Skipped (pandas/pyarrow unavailable)"
    
    try:
        df = pd.read_parquet(file_path)
        changed = False
        
        # Apply sanitization to all string columns
        for col in df.columns:
            if df[col].dtype == 'object' or pd.api.types.is_string_dtype(df[col]):
                original_col = df[col].copy()
                df[col] = df[col].apply(lambda x: apply_text_replacements(str(x)) if pd.notna(x) else x)
                if not original_col.equals(df[col]):
                    changed = True
        
        # Sanitize column names
        original_columns = df.columns.tolist()
        df.columns = [apply_text_replacements(str(col)) for col in df.columns]
        if original_columns != df.columns.tolist():
            changed = True
        
        if changed:
            df.to_parquet(file_path, index=False, engine='pyarrow')
            return True, "Cleaned (Parquet)"
        return False, "No changes needed"
        
    except Exception as e:
        return False, f"Error (Parquet): {e}"

def clean_markdown_file(file_path: Path) -> tuple[bool, str]:
    """Clean Markdown files with special handling for formatting."""
    try:
        content = file_path.read_text(encoding='utf-8', errors='ignore')
        original_content = content
        
        # Apply all replacements
        content = apply_text_replacements(content)
        
        if content != original_content:
            file_path.write_text(content, encoding='utf-8')
            return True, "Cleaned (Markdown)"
        return False, "No changes needed"
        
    except Exception as e:
        return False, f"Error (Markdown): {e}"

def clean_file(file_path: Path) -> tuple[bool, str]:
    """Route file to appropriate cleaning function based on type."""
    suffix = file_path.suffix.lower()
    
    try:
        if suffix == '.json':
            return clean_json_file(file_path)
        elif suffix == '.md':
            return clean_markdown_file(file_path)
        elif suffix in DATA_EXTS:
            return clean_csv_file(file_path)
        elif suffix in PARQUET_EXTS:
            return clean_parquet_file(file_path)
        elif suffix in TEXT_EXTS:
            return clean_text_file(file_path)
        else:
            # Try as text file for any other extension
            return clean_text_file(file_path)
            
    except Exception as e:
        return False, f"Error: {e}"

def main():
    """Clean all files in the repository."""
    print("=" * 90)
    print(" ARCTIC ZERO CURTAIN PIPELINE â€” COMPREHENSIVE REPOSITORY SANITIZATION ")
    print("=" * 90)
    print(f"\nTarget Directory: {PROJECT_ROOT}")
    print(f"Pandas Available: {PANDAS_AVAILABLE}")
    print(f"PyArrow Available: {PYARROW_AVAILABLE}")
    
    # Collect all files, excluding virtual environments and git directories
    exclude_dirs = {'.git', '__pycache__', 'venv', 'env', 'isce3', '.venv', 
                    'node_modules', '.pytest_cache', '.ipynb_checkpoints'}
    
    all_files = [
        f for f in PROJECT_ROOT.rglob('*')
        if f.is_file() and not any(excluded in f.parts for excluded in exclude_dirs)
    ]

    print(f"\nFound {len(all_files)} files to inspect.\n")
    print("-" * 90)

    cleaned_count = 0
    error_count = 0
    skipped_count = 0
    
    for file in all_files:
        rel_path = file.relative_to(PROJECT_ROOT)
        changed, message = clean_file(file)
        
        status_symbol = "" if changed else "" if "No changes" in message else ""
        print(f"{status_symbol} {rel_path}: {message}")
        
        if changed:
            cleaned_count += 1
        elif "Error" in message:
            error_count += 1
        elif "Skipped" in message:
            skipped_count += 1

    print("\n" + "=" * 90)
    print(f"SUMMARY:")
    print(f"  Files Cleaned:  {cleaned_count}")
    print(f"  Files Skipped:  {skipped_count}")
    print(f"  Files w/Errors: {error_count}")
    print(f"  Total Processed: {len(all_files)}")
    print("=" * 90)

if __name__ == "__main__":
    main()